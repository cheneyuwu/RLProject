""" Active learning project

    This python script loads a trained rl policy and uses it to navigate the agent in its environment. You only need to
    provide the policy file generated by train*.py and the script will figure out which env should be used.

"""
import click
import os
import numpy as np
import pickle

from yw.util.mpi_util import set_global_seeds
from yw.ddpg_main import config


# DDPG Package import
from yw.tool import logger


def play(policy_file, demo_policy_file, seed, num_itr, render):

    set_global_seeds(seed)

    # Load policy.
    with open(policy_file, "rb") as f:
        policy = pickle.load(f)

    # Add demonstration policy.
    if demo_policy_file:
        with open(demo_policy_file, "rb") as f:
            demo_policy = pickle.load(f)
        policy.demo_policy = demo_policy

    # extract environment construction information
    env_name = policy.info["env_name"]
    r_scale = policy.info["r_scale"]
    r_shift = policy.info["r_shift"]
    eps_length = policy.info["eps_length"]

    # Prepare params.
    params = {}
    params["env_name"] = env_name
    params["r_scale"] = r_scale
    params["r_shift"] = r_shift
    params["eps_length"] = eps_length
    params["rank_seed"] = seed
    params["render"] = render
    params["rollout_batch_size"] = 1
    params = config.add_env_params(params=params)
    demo = config.config_demo(params=params, policy=policy)

    # Run evaluation.
    demo.clear_history()
    for _ in range(num_itr):
        demo.generate_rollouts()

    # record logs
    for key, val in demo.logs("test"):
        logger.record_tabular(key, np.mean(val))
    logger.dump_tabular()


@click.command()
@click.option("--policy_file", type=str, default=None)
@click.option("--demo_policy_file", default=None, type=str)
@click.option("--seed", type=int, default=2)
@click.option("--num_itr", type=int, default=10)
@click.option("--render", type=int, default=1)
def main(**kwargs):
    play(**kwargs)


if __name__ == "__main__":
    main()

