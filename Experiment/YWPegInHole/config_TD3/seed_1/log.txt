Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3/seed_1
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.41174904  |
| stats_o/std                    | 0.072720684 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.1        |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.406      |
| test/Q                         | -1.3059382  |
| test/Q_plus_P                  | -1.3059382  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.285      |
| train/info_shaping_reward_min  | -0.499      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.4115474   |
| stats_o/std                    | 0.060853004 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.282      |
| test/info_shaping_reward_min   | -0.387      |
| test/Q                         | -1.5403781  |
| test/Q_plus_P                  | -1.5403781  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.397      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.40773305  |
| stats_o/std                    | 0.056920435 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.327      |
| test/info_shaping_reward_min   | -0.438      |
| test/Q                         | -1.8913351  |
| test/Q_plus_P                  | -1.8913351  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.195      |
| train/info_shaping_reward_mean | -0.294      |
| train/info_shaping_reward_min  | -0.403      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.40412375  |
| stats_o/std                    | 0.054089557 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.302      |
| test/info_shaping_reward_min   | -0.435      |
| test/Q                         | -2.252585   |
| test/Q_plus_P                  | -2.252585   |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.192      |
| train/info_shaping_reward_mean | -0.306      |
| train/info_shaping_reward_min  | -0.42       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.40210268 |
| stats_o/std                    | 0.05191378 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.315     |
| test/info_shaping_reward_min   | -0.379     |
| test/Q                         | -2.6458685 |
| test/Q_plus_P                  | -2.6458685 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.198     |
| train/info_shaping_reward_mean | -0.316     |
| train/info_shaping_reward_min  | -0.415     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.401745   |
| stats_o/std                    | 0.05012333 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.308     |
| test/info_shaping_reward_min   | -0.388     |
| test/Q                         | -3.040981  |
| test/Q_plus_P                  | -3.040981  |
| test/reward_per_eps            | -40        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.194     |
| train/info_shaping_reward_mean | -0.307     |
| train/info_shaping_reward_min  | -0.416     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.40041104  |
| stats_o/std                    | 0.048486236 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.292      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -3.4511242  |
| test/Q_plus_P                  | -3.4511242  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.2        |
| train/info_shaping_reward_mean | -0.301      |
| train/info_shaping_reward_min  | -0.386      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.4003986   |
| stats_o/std                    | 0.047321413 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.218      |
| test/info_shaping_reward_mean  | -0.3        |
| test/info_shaping_reward_min   | -0.37       |
| test/Q                         | -3.8758228  |
| test/Q_plus_P                  | -3.8758228  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.201      |
| train/info_shaping_reward_mean | -0.296      |
| train/info_shaping_reward_min  | -0.409      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.4000353  |
| stats_o/std                    | 0.04635806 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.318     |
| test/info_shaping_reward_min   | -0.441     |
| test/Q                         | -4.282467  |
| test/Q_plus_P                  | -4.282467  |
| test/reward_per_eps            | -40        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.189     |
| train/info_shaping_reward_mean | -0.298     |
| train/info_shaping_reward_min  | -0.398     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.39979395  |
| stats_o/std                    | 0.045532737 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.201      |
| test/info_shaping_reward_mean  | -0.287      |
| test/info_shaping_reward_min   | -0.369      |
| test/Q                         | -4.7002745  |
| test/Q_plus_P                  | -4.7002745  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.181      |
| train/info_shaping_reward_mean | -0.298      |
| train/info_shaping_reward_min  | -0.402      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.39942908 |
| stats_o/std                    | 0.04452945 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.286     |
| test/info_shaping_reward_min   | -0.342     |
| test/Q                         | -5.0818515 |
| test/Q_plus_P                  | -5.0818515 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.181     |
| train/info_shaping_reward_mean | -0.28      |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.39896098 |
| stats_o/std                    | 0.0439657  |
| test/episodes                  | 120        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.335     |
| test/Q                         | -5.5115676 |
| test/Q_plus_P                  | -5.5115676 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.205     |
| train/info_shaping_reward_mean | -0.297     |
| train/info_shaping_reward_min  | -0.394     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.39906144  |
| stats_o/std                    | 0.043459635 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.221      |
| test/info_shaping_reward_mean  | -0.297      |
| test/info_shaping_reward_min   | -0.353      |
| test/Q                         | -5.9025273  |
| test/Q_plus_P                  | -5.9025273  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.278      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.39899328  |
| stats_o/std                    | 0.042957496 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.285      |
| test/info_shaping_reward_min   | -0.352      |
| test/Q                         | -6.2967014  |
| test/Q_plus_P                  | -6.2967014  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.193      |
| train/info_shaping_reward_mean | -0.285      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.39904764 |
| stats_o/std                    | 0.04262999 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.286     |
| test/info_shaping_reward_min   | -0.339     |
| test/Q                         | -6.685736  |
| test/Q_plus_P                  | -6.685736  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.191     |
| train/info_shaping_reward_mean | -0.287     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.3991933   |
| stats_o/std                    | 0.042212065 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.27       |
| test/info_shaping_reward_min   | -0.35       |
| test/Q                         | -7.0897713  |
| test/Q_plus_P                  | -7.0897713  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.278      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.39918372 |
| stats_o/std                    | 0.04195723 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.283     |
| test/info_shaping_reward_min   | -0.354     |
| test/Q                         | -7.4754553 |
| test/Q_plus_P                  | -7.4754553 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.198     |
| train/info_shaping_reward_mean | -0.292     |
| train/info_shaping_reward_min  | -0.383     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.399436   |
| stats_o/std                    | 0.0416259  |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.272     |
| test/info_shaping_reward_min   | -0.358     |
| test/Q                         | -7.8520026 |
| test/Q_plus_P                  | -7.8520026 |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.198     |
| train/info_shaping_reward_mean | -0.278     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.39935982 |
| stats_o/std                    | 0.04132449 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.285     |
| test/info_shaping_reward_min   | -0.363     |
| test/Q                         | -8.225791  |
| test/Q_plus_P                  | -8.225791  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.18      |
| train/info_shaping_reward_mean | -0.271     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.3994259   |
| stats_o/std                    | 0.040955987 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.284      |
| test/info_shaping_reward_min   | -0.353      |
| test/Q                         | -8.626062   |
| test/Q_plus_P                  | -8.626062   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.274      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.39938593  |
| stats_o/std                    | 0.040695507 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.277      |
| test/info_shaping_reward_min   | -0.363      |
| test/Q                         | -8.9910145  |
| test/Q_plus_P                  | -8.9910145  |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.18       |
| train/info_shaping_reward_mean | -0.283      |
| train/info_shaping_reward_min  | -0.379      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.39929697 |
| stats_o/std                    | 0.04038989 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.268     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -9.369149  |
| test/Q_plus_P                  | -9.369149  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.18      |
| train/info_shaping_reward_mean | -0.277     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.39933658  |
| stats_o/std                    | 0.040170494 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.285      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -9.6972685  |
| test/Q_plus_P                  | -9.6972685  |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.272      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.39920405  |
| stats_o/std                    | 0.039958403 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.309      |
| test/info_shaping_reward_min   | -0.359      |
| test/Q                         | -10.038429  |
| test/Q_plus_P                  | -10.038429  |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.192      |
| train/info_shaping_reward_mean | -0.293      |
| train/info_shaping_reward_min  | -0.383      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.39929745  |
| stats_o/std                    | 0.039928082 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.295      |
| test/info_shaping_reward_min   | -0.367      |
| test/Q                         | -10.426635  |
| test/Q_plus_P                  | -10.426635  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.187      |
| train/info_shaping_reward_mean | -0.287      |
| train/info_shaping_reward_min  | -0.39       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.39919004 |
| stats_o/std                    | 0.03969517 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.3       |
| test/info_shaping_reward_min   | -0.353     |
| test/Q                         | -10.779712 |
| test/Q_plus_P                  | -10.779712 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.174     |
| train/info_shaping_reward_mean | -0.278     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.39896062  |
| stats_o/std                    | 0.039688382 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.3        |
| test/info_shaping_reward_min   | -0.37       |
| test/Q                         | -11.118318  |
| test/Q_plus_P                  | -11.118318  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.214      |
| train/info_shaping_reward_mean | -0.31       |
| train/info_shaping_reward_min  | -0.407      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.3988612   |
| stats_o/std                    | 0.039633494 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.294      |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -11.473105  |
| test/Q_plus_P                  | -11.473105  |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.195      |
| train/info_shaping_reward_mean | -0.292      |
| train/info_shaping_reward_min  | -0.395      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.39892995 |
| stats_o/std                    | 0.03952195 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.21      |
| test/info_shaping_reward_mean  | -0.301     |
| test/info_shaping_reward_min   | -0.35      |
| test/Q                         | -11.794465 |
| test/Q_plus_P                  | -11.794465 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.179     |
| train/info_shaping_reward_mean | -0.282     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.39912692 |
| stats_o/std                    | 0.0394173  |
| test/episodes                  | 300        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.301     |
| test/info_shaping_reward_min   | -0.367     |
| test/Q                         | -12.153132 |
| test/Q_plus_P                  | -12.153132 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.194     |
| train/info_shaping_reward_mean | -0.286     |
| train/info_shaping_reward_min  | -0.376     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.39912668 |
| stats_o/std                    | 0.03929184 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.271     |
| test/info_shaping_reward_min   | -0.356     |
| test/Q                         | -12.524527 |
| test/Q_plus_P                  | -12.524527 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.174     |
| train/info_shaping_reward_mean | -0.279     |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.3991203  |
| stats_o/std                    | 0.03919194 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.281     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -12.808581 |
| test/Q_plus_P                  | -12.808581 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.175     |
| train/info_shaping_reward_mean | -0.275     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.39916357 |
| stats_o/std                    | 0.03902788 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.194     |
| test/info_shaping_reward_mean  | -0.295     |
| test/info_shaping_reward_min   | -0.347     |
| test/Q                         | -13.183603 |
| test/Q_plus_P                  | -13.183603 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.174     |
| train/info_shaping_reward_mean | -0.273     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.39920744 |
| stats_o/std                    | 0.03890236 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.268     |
| test/info_shaping_reward_min   | -0.34      |
| test/Q                         | -13.485569 |
| test/Q_plus_P                  | -13.485569 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.181     |
| train/info_shaping_reward_mean | -0.276     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.399214    |
| stats_o/std                    | 0.038846474 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.293      |
| test/info_shaping_reward_min   | -0.35       |
| test/Q                         | -13.8033085 |
| test/Q_plus_P                  | -13.8033085 |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.193      |
| train/info_shaping_reward_mean | -0.287      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.39921078 |
| stats_o/std                    | 0.03876878 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.284     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -14.136706 |
| test/Q_plus_P                  | -14.136706 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.184     |
| train/info_shaping_reward_mean | -0.278     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.3992834  |
| stats_o/std                    | 0.03869279 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.281     |
| test/info_shaping_reward_min   | -0.346     |
| test/Q                         | -14.456958 |
| test/Q_plus_P                  | -14.456958 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.191     |
| train/info_shaping_reward_mean | -0.277     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.3992044   |
| stats_o/std                    | 0.038628403 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.297      |
| test/info_shaping_reward_min   | -0.35       |
| test/Q                         | -14.734215  |
| test/Q_plus_P                  | -14.734215  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.181      |
| train/info_shaping_reward_mean | -0.274      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.39922678  |
| stats_o/std                    | 0.038503062 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.277      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -15.057673  |
| test/Q_plus_P                  | -15.057673  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.273      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.39926496 |
| stats_o/std                    | 0.03837061 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.277     |
| test/info_shaping_reward_min   | -0.34      |
| test/Q                         | -15.344097 |
| test/Q_plus_P                  | -15.344097 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.176     |
| train/info_shaping_reward_mean | -0.266     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.399256    |
| stats_o/std                    | 0.038299445 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.275      |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -15.662925  |
| test/Q_plus_P                  | -15.662925  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.202      |
| train/info_shaping_reward_mean | -0.287      |
| train/info_shaping_reward_min  | -0.375      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.39929435  |
| stats_o/std                    | 0.038264494 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.275      |
| test/info_shaping_reward_min   | -0.348      |
| test/Q                         | -15.921858  |
| test/Q_plus_P                  | -15.921858  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.39942738  |
| stats_o/std                    | 0.038263034 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -16.227472  |
| test/Q_plus_P                  | -16.227472  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.272      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.39945254  |
| stats_o/std                    | 0.038238443 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -16.536623  |
| test/Q_plus_P                  | -16.536623  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.183      |
| train/info_shaping_reward_mean | -0.276      |
| train/info_shaping_reward_min  | -0.387      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.39951602  |
| stats_o/std                    | 0.038196206 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -16.824177  |
| test/Q_plus_P                  | -16.824177  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.182      |
| train/info_shaping_reward_mean | -0.271      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.39955544  |
| stats_o/std                    | 0.038207877 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.272      |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -17.10635   |
| test/Q_plus_P                  | -17.10635   |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.194      |
| train/info_shaping_reward_mean | -0.284      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.39967665  |
| stats_o/std                    | 0.038140003 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.277      |
| test/info_shaping_reward_min   | -0.353      |
| test/Q                         | -17.374449  |
| test/Q_plus_P                  | -17.374449  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.268      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.3997501   |
| stats_o/std                    | 0.038116314 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.206      |
| test/info_shaping_reward_mean  | -0.28       |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -17.646795  |
| test/Q_plus_P                  | -17.646795  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.269      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.39983216  |
| stats_o/std                    | 0.038022384 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.283      |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -17.897446  |
| test/Q_plus_P                  | -17.897446  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.39982867  |
| stats_o/std                    | 0.037994448 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.284      |
| test/info_shaping_reward_min   | -0.348      |
| test/Q                         | -18.180439  |
| test/Q_plus_P                  | -18.180439  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.39997745 |
| stats_o/std                    | 0.03798313 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.211     |
| test/info_shaping_reward_mean  | -0.297     |
| test/info_shaping_reward_min   | -0.356     |
| test/Q                         | -18.443623 |
| test/Q_plus_P                  | -18.443623 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.267     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.40010324 |
| stats_o/std                    | 0.03798208 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.272     |
| test/info_shaping_reward_min   | -0.361     |
| test/Q                         | -18.713558 |
| test/Q_plus_P                  | -18.713558 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.181     |
| train/info_shaping_reward_mean | -0.272     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.4002595   |
| stats_o/std                    | 0.038010377 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.269      |
| test/info_shaping_reward_min   | -0.339      |
| test/Q                         | -18.999172  |
| test/Q_plus_P                  | -18.999172  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.40035692  |
| stats_o/std                    | 0.038014967 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -19.262896  |
| test/Q_plus_P                  | -19.262896  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.181      |
| train/info_shaping_reward_mean | -0.275      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.4005359  |
| stats_o/std                    | 0.03799025 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -19.478682 |
| test/Q_plus_P                  | -19.478682 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.267     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.4005977   |
| stats_o/std                    | 0.038001314 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.287      |
| test/info_shaping_reward_min   | -0.377      |
| test/Q                         | -19.709967  |
| test/Q_plus_P                  | -19.709967  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.275      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.40075457 |
| stats_o/std                    | 0.03796018 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.322     |
| test/Q                         | -19.966297 |
| test/Q_plus_P                  | -19.966297 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.40083885  |
| stats_o/std                    | 0.037959002 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.274      |
| test/info_shaping_reward_min   | -0.381      |
| test/Q                         | -20.20862   |
| test/Q_plus_P                  | -20.20862   |
| test/reward_per_eps            | -40         |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.276      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.40092638  |
| stats_o/std                    | 0.037941694 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -20.481056  |
| test/Q_plus_P                  | -20.481056  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.268      |
| train/info_shaping_reward_min  | -0.39       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.4010432   |
| stats_o/std                    | 0.037984516 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.275      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -20.724014  |
| test/Q_plus_P                  | -20.724014  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.40121305  |
| stats_o/std                    | 0.037979495 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.271      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -20.939566  |
| test/Q_plus_P                  | -20.939566  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.40123746 |
| stats_o/std                    | 0.03796912 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -21.176834 |
| test/Q_plus_P                  | -21.176834 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.40141937  |
| stats_o/std                    | 0.038013596 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.264      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -21.397984  |
| test/Q_plus_P                  | -21.397984  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.4016008  |
| stats_o/std                    | 0.03801391 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.329     |
| test/Q                         | -21.62019  |
| test/Q_plus_P                  | -21.62019  |
| test/reward_per_eps            | -40        |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.177     |
| train/info_shaping_reward_mean | -0.269     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.40173948  |
| stats_o/std                    | 0.038006056 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -21.861328  |
| test/Q_plus_P                  | -21.861328  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.262      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.40179214 |
| stats_o/std                    | 0.03803236 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -22.067026 |
| test/Q_plus_P                  | -22.067026 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.263     |
| train/info_shaping_reward_min  | -0.381     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.40195715  |
| stats_o/std                    | 0.038073994 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -22.28429   |
| test/Q_plus_P                  | -22.28429   |
| test/reward_per_eps            | -40         |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.40204915  |
| stats_o/std                    | 0.038071252 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -22.5312    |
| test/Q_plus_P                  | -22.5312    |
| test/reward_per_eps            | -40         |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.181      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.4021807   |
| stats_o/std                    | 0.038089927 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.271      |
| test/info_shaping_reward_min   | -0.349      |
| test/Q                         | -22.71151   |
| test/Q_plus_P                  | -22.71151   |
| test/reward_per_eps            | -40         |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.269      |
| train/info_shaping_reward_min  | -0.379      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.40220687 |
| stats_o/std                    | 0.03810859 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.188     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -22.914927 |
| test/Q_plus_P                  | -22.914927 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.261     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.40231457 |
| stats_o/std                    | 0.03815822 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -23.143076 |
| test/Q_plus_P                  | -23.143076 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.179     |
| train/info_shaping_reward_mean | -0.265     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.4024357   |
| stats_o/std                    | 0.038177542 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -23.349815  |
| test/Q_plus_P                  | -23.349815  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.191      |
| train/info_shaping_reward_mean | -0.273      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.40252247 |
| stats_o/std                    | 0.03817338 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -23.5616   |
| test/Q_plus_P                  | -23.5616   |
| test/reward_per_eps            | -40        |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.4026209  |
| stats_o/std                    | 0.03820203 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -23.718609 |
| test/Q_plus_P                  | -23.718609 |
| test/reward_per_eps            | -40        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.40271798 |
| stats_o/std                    | 0.03820009 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.351     |
| test/Q                         | -23.946386 |
| test/Q_plus_P                  | -23.946386 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.177     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.40278134  |
| stats_o/std                    | 0.038252987 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -24.172215  |
| test/Q_plus_P                  | -24.172215  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.268      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.4028627   |
| stats_o/std                    | 0.038276713 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -24.389973  |
| test/Q_plus_P                  | -24.389973  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.40294257  |
| stats_o/std                    | 0.038266394 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -24.530937  |
| test/Q_plus_P                  | -24.530937  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.40301087  |
| stats_o/std                    | 0.038253438 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -24.721401  |
| test/Q_plus_P                  | -24.721401  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.40305662  |
| stats_o/std                    | 0.038245868 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -24.892023  |
| test/Q_plus_P                  | -24.892023  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.40318558  |
| stats_o/std                    | 0.038266946 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -25.082941  |
| test/Q_plus_P                  | -25.082941  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.4032298   |
| stats_o/std                    | 0.038278863 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -25.2767    |
| test/Q_plus_P                  | -25.2767    |
| test/reward_per_eps            | -40         |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.40325508  |
| stats_o/std                    | 0.038254656 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -25.441397  |
| test/Q_plus_P                  | -25.441397  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.40332556 |
| stats_o/std                    | 0.03830762 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.266     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -25.629776 |
| test/Q_plus_P                  | -25.629776 |
| test/reward_per_eps            | -40        |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.40337682  |
| stats_o/std                    | 0.038284324 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.21       |
| test/info_shaping_reward_mean  | -0.273      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -25.833183  |
| test/Q_plus_P                  | -25.833183  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.40344608 |
| stats_o/std                    | 0.03832205 |
| test/episodes                  | 860        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -26.011091 |
| test/Q_plus_P                  | -26.011091 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.369     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.40349686  |
| stats_o/std                    | 0.038341966 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -26.155437  |
| test/Q_plus_P                  | -26.155437  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.40358958  |
| stats_o/std                    | 0.038388748 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -26.330605  |
| test/Q_plus_P                  | -26.330605  |
| test/reward_per_eps            | -40         |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.40363923 |
| stats_o/std                    | 0.03839667 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -26.505991 |
| test/Q_plus_P                  | -26.505991 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.40374923 |
| stats_o/std                    | 0.03843072 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.188     |
| test/info_shaping_reward_mean  | -0.272     |
| test/info_shaping_reward_min   | -0.353     |
| test/Q                         | -26.611465 |
| test/Q_plus_P                  | -26.611465 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.40378293  |
| stats_o/std                    | 0.038449787 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.209      |
| test/info_shaping_reward_mean  | -0.272      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -26.817955  |
| test/Q_plus_P                  | -26.817955  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.4038627   |
| stats_o/std                    | 0.038497902 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -27.005787  |
| test/Q_plus_P                  | -27.005787  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.40387538  |
| stats_o/std                    | 0.038492117 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.357      |
| test/Q                         | -27.143417  |
| test/Q_plus_P                  | -27.143417  |
| test/reward_per_eps            | -40         |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.4039084   |
| stats_o/std                    | 0.038511638 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -27.307411  |
| test/Q_plus_P                  | -27.307411  |
| test/reward_per_eps            | -40         |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.4039687   |
| stats_o/std                    | 0.038504377 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.344      |
| test/Q                         | -27.457779  |
| test/Q_plus_P                  | -27.457779  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.40399358  |
| stats_o/std                    | 0.038518388 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -27.610458  |
| test/Q_plus_P                  | -27.610458  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.40400997 |
| stats_o/std                    | 0.0384899  |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -27.771475 |
| test/Q_plus_P                  | -27.771475 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.4040909   |
| stats_o/std                    | 0.038533755 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -27.919062  |
| test/Q_plus_P                  | -27.919062  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.4041498  |
| stats_o/std                    | 0.03856617 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -28.062944 |
| test/Q_plus_P                  | -28.062944 |
| test/reward_per_eps            | -40        |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.4042195  |
| stats_o/std                    | 0.03864724 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.204     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -28.215273 |
| test/Q_plus_P                  | -28.215273 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.40432504 |
| stats_o/std                    | 0.03866472 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -28.37854  |
| test/Q_plus_P                  | -28.37854  |
| test/reward_per_eps            | -40        |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.40434158  |
| stats_o/std                    | 0.038649257 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -28.512388  |
| test/Q_plus_P                  | -28.512388  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.40440118 |
| stats_o/std                    | 0.03864232 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -28.65027  |
| test/Q_plus_P                  | -28.65027  |
| test/reward_per_eps            | -40        |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.4044598  |
| stats_o/std                    | 0.03866278 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.265     |
| test/info_shaping_reward_min   | -0.334     |
| test/Q                         | -28.78615  |
| test/Q_plus_P                  | -28.78615  |
| test/reward_per_eps            | -40        |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.266     |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.40453577  |
| stats_o/std                    | 0.038695596 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.358      |
| test/Q                         | -28.913284  |
| test/Q_plus_P                  | -28.913284  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.40455225 |
| stats_o/std                    | 0.03872051 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.321     |
| test/Q                         | -29.096909 |
| test/Q_plus_P                  | -29.096909 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.369     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.40457758 |
| stats_o/std                    | 0.03872466 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -29.173637 |
| test/Q_plus_P                  | -29.173637 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.40461016  |
| stats_o/std                    | 0.038739778 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -29.326723  |
| test/Q_plus_P                  | -29.326723  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.40468094  |
| stats_o/std                    | 0.038752295 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -29.515049  |
| test/Q_plus_P                  | -29.515049  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.40467718 |
| stats_o/std                    | 0.03877363 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.22      |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -29.570053 |
| test/Q_plus_P                  | -29.570053 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.181     |
| train/info_shaping_reward_mean | -0.263     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.40468994 |
| stats_o/std                    | 0.038786   |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -29.685596 |
| test/Q_plus_P                  | -29.685596 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.40476796  |
| stats_o/std                    | 0.038780574 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -29.75511   |
| test/Q_plus_P                  | -29.75511   |
| test/reward_per_eps            | -40         |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.40490773  |
| stats_o/std                    | 0.038842414 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -29.973953  |
| test/Q_plus_P                  | -29.973953  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 113        |
| stats_o/mean                   | 0.40496162 |
| stats_o/std                    | 0.0388813  |
| test/episodes                  | 1140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -30.080425 |
| test/Q_plus_P                  | -30.080425 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45600      |
| train/episodes                 | 4560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 182400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.40499535 |
| stats_o/std                    | 0.03890625 |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -30.19989  |
| test/Q_plus_P                  | -30.19989  |
| test/reward_per_eps            | -40        |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.40500292  |
| stats_o/std                    | 0.038926307 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -30.300581  |
| test/Q_plus_P                  | -30.300581  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.40498683  |
| stats_o/std                    | 0.038930923 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -30.407095  |
| test/Q_plus_P                  | -30.407095  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.40501317 |
| stats_o/std                    | 0.03895072 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.344     |
| test/Q                         | -30.562124 |
| test/Q_plus_P                  | -30.562124 |
| test/reward_per_eps            | -40        |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.4050859   |
| stats_o/std                    | 0.038967643 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -30.698858  |
| test/Q_plus_P                  | -30.698858  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.40512523  |
| stats_o/std                    | 0.038973115 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -30.785006  |
| test/Q_plus_P                  | -30.785006  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.40518582  |
| stats_o/std                    | 0.039003164 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -30.910862  |
| test/Q_plus_P                  | -30.910862  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.4053025  |
| stats_o/std                    | 0.03902861 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -31.003864 |
| test/Q_plus_P                  | -31.003864 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.40530625 |
| stats_o/std                    | 0.03904625 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -31.13475  |
| test/Q_plus_P                  | -31.13475  |
| test/reward_per_eps            | -40        |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.4053528   |
| stats_o/std                    | 0.039035037 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -31.224848  |
| test/Q_plus_P                  | -31.224848  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.40538004  |
| stats_o/std                    | 0.039048053 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -31.31814   |
| test/Q_plus_P                  | -31.31814   |
| test/reward_per_eps            | -40         |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.40542948  |
| stats_o/std                    | 0.039082907 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -31.46351   |
| test/Q_plus_P                  | -31.46351   |
| test/reward_per_eps            | -40         |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.382      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.40549675  |
| stats_o/std                    | 0.039120026 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -31.57358   |
| test/Q_plus_P                  | -31.57358   |
| test/reward_per_eps            | -40         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.40556565 |
| stats_o/std                    | 0.03913444 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -31.673111 |
| test/Q_plus_P                  | -31.673111 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.4056354  |
| stats_o/std                    | 0.03915908 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.123     |
| test/info_shaping_reward_mean  | -0.2       |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -31.722963 |
| test/Q_plus_P                  | -31.722963 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.4056784   |
| stats_o/std                    | 0.039170884 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -31.85899   |
| test/Q_plus_P                  | -31.85899   |
| test/reward_per_eps            | -40         |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.4057022   |
| stats_o/std                    | 0.039164025 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -31.967417  |
| test/Q_plus_P                  | -31.967417  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.40574792 |
| stats_o/std                    | 0.03916964 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -32.053535 |
| test/Q_plus_P                  | -32.053535 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 132        |
| stats_o/mean                   | 0.4058145  |
| stats_o/std                    | 0.03918815 |
| test/episodes                  | 1330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -32.203747 |
| test/Q_plus_P                  | -32.203747 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53200      |
| train/episodes                 | 5320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 212800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.40585554 |
| stats_o/std                    | 0.03920367 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -32.274864 |
| test/Q_plus_P                  | -32.274864 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.405871    |
| stats_o/std                    | 0.039210547 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -32.272495  |
| test/Q_plus_P                  | -32.272495  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.4058734   |
| stats_o/std                    | 0.039221223 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.191      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -32.423683  |
| test/Q_plus_P                  | -32.423683  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.40593246 |
| stats_o/std                    | 0.03924353 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -32.531036 |
| test/Q_plus_P                  | -32.531036 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.40594897  |
| stats_o/std                    | 0.039231893 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -32.62046   |
| test/Q_plus_P                  | -32.62046   |
| test/reward_per_eps            | -40         |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.4060173   |
| stats_o/std                    | 0.039245736 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.212      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -32.7219    |
| test/Q_plus_P                  | -32.7219    |
| test/reward_per_eps            | -40         |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.40605906  |
| stats_o/std                    | 0.039267857 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -32.81469   |
| test/Q_plus_P                  | -32.81469   |
| test/reward_per_eps            | -40         |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.40610507  |
| stats_o/std                    | 0.039255638 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -32.91309   |
| test/Q_plus_P                  | -32.91309   |
| test/reward_per_eps            | -40         |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.406226   |
| stats_o/std                    | 0.03928184 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -33.045605 |
| test/Q_plus_P                  | -33.045605 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.4062402  |
| stats_o/std                    | 0.03927921 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -33.058346 |
| test/Q_plus_P                  | -33.058346 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.4062506   |
| stats_o/std                    | 0.039317764 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -33.149918  |
| test/Q_plus_P                  | -33.149918  |
| test/reward_per_eps            | -40         |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.40626767  |
| stats_o/std                    | 0.039324027 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -33.297367  |
| test/Q_plus_P                  | -33.297367  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.40625617  |
| stats_o/std                    | 0.039330937 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -33.359737  |
| test/Q_plus_P                  | -33.359737  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.40629196  |
| stats_o/std                    | 0.039340135 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -33.365894  |
| test/Q_plus_P                  | -33.365894  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.40631887  |
| stats_o/std                    | 0.039379586 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -33.5296    |
| test/Q_plus_P                  | -33.5296    |
| test/reward_per_eps            | -40         |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.40633878 |
| stats_o/std                    | 0.03940049 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -33.57408  |
| test/Q_plus_P                  | -33.57408  |
| test/reward_per_eps            | -40        |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.4063753   |
| stats_o/std                    | 0.039415974 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -33.61293   |
| test/Q_plus_P                  | -33.61293   |
| test/reward_per_eps            | -40         |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.40636268  |
| stats_o/std                    | 0.039402287 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -33.728275  |
| test/Q_plus_P                  | -33.728275  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.40642083 |
| stats_o/std                    | 0.03940743 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -33.780216 |
| test/Q_plus_P                  | -33.780216 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.40641454  |
| stats_o/std                    | 0.039404787 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -33.86754   |
| test/Q_plus_P                  | -33.86754   |
| test/reward_per_eps            | -40         |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.40639603  |
| stats_o/std                    | 0.039404947 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -33.957485  |
| test/Q_plus_P                  | -33.957485  |
| test/reward_per_eps            | -40         |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.40639398  |
| stats_o/std                    | 0.039417695 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -34.077396  |
| test/Q_plus_P                  | -34.077396  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.40640283 |
| stats_o/std                    | 0.03941441 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -34.10889  |
| test/Q_plus_P                  | -34.10889  |
| test/reward_per_eps            | -40        |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.40640667  |
| stats_o/std                    | 0.039434846 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -34.173042  |
| test/Q_plus_P                  | -34.173042  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.40641677  |
| stats_o/std                    | 0.039430965 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -34.2401    |
| test/Q_plus_P                  | -34.2401    |
| test/reward_per_eps            | -40         |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.40640292  |
| stats_o/std                    | 0.039432842 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -34.367462  |
| test/Q_plus_P                  | -34.367462  |
| test/reward_per_eps            | -40         |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.40639362 |
| stats_o/std                    | 0.03942537 |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -34.42723  |
| test/Q_plus_P                  | -34.42723  |
| test/reward_per_eps            | -40        |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.40640494  |
| stats_o/std                    | 0.039404396 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -34.46462   |
| test/Q_plus_P                  | -34.46462   |
| test/reward_per_eps            | -40         |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.4063991   |
| stats_o/std                    | 0.039398555 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -34.56704   |
| test/Q_plus_P                  | -34.56704   |
| test/reward_per_eps            | -40         |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.40639105  |
| stats_o/std                    | 0.039402205 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -34.63536   |
| test/Q_plus_P                  | -34.63536   |
| test/reward_per_eps            | -40         |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.40640727  |
| stats_o/std                    | 0.039416716 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -34.698116  |
| test/Q_plus_P                  | -34.698116  |
| test/reward_per_eps            | -40         |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.40641502 |
| stats_o/std                    | 0.03942071 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.343     |
| test/Q                         | -34.73684  |
| test/Q_plus_P                  | -34.73684  |
| test/reward_per_eps            | -40        |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.40646693  |
| stats_o/std                    | 0.039420813 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -34.84824   |
| test/Q_plus_P                  | -34.84824   |
| test/reward_per_eps            | -40         |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.40649095  |
| stats_o/std                    | 0.039425056 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -34.891655  |
| test/Q_plus_P                  | -34.891655  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.40651223 |
| stats_o/std                    | 0.03941756 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -34.941044 |
| test/Q_plus_P                  | -34.941044 |
| test/reward_per_eps            | -40        |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.4065007   |
| stats_o/std                    | 0.039428253 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -35.015797  |
| test/Q_plus_P                  | -35.015797  |
| test/reward_per_eps            | -40         |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.40653512 |
| stats_o/std                    | 0.0394338  |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -35.103756 |
| test/Q_plus_P                  | -35.103756 |
| test/reward_per_eps            | -40        |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.174     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.40655413  |
| stats_o/std                    | 0.039436575 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.264      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -35.159737  |
| test/Q_plus_P                  | -35.159737  |
| test/reward_per_eps            | -40         |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.40655208 |
| stats_o/std                    | 0.03945011 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.197     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -35.216053 |
| test/Q_plus_P                  | -35.216053 |
| test/reward_per_eps            | -40        |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.267     |
| train/info_shaping_reward_min  | -0.4       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.40658414 |
| stats_o/std                    | 0.03945514 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -35.259583 |
| test/Q_plus_P                  | -35.259583 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.4066092   |
| stats_o/std                    | 0.039452773 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -35.330338  |
| test/Q_plus_P                  | -35.330338  |
| test/reward_per_eps            | -40         |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.40661848  |
| stats_o/std                    | 0.039466433 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -35.412094  |
| test/Q_plus_P                  | -35.412094  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.40662193  |
| stats_o/std                    | 0.039473828 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -35.500378  |
| test/Q_plus_P                  | -35.500378  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 176        |
| stats_o/mean                   | 0.40662467 |
| stats_o/std                    | 0.03945512 |
| test/episodes                  | 1770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -35.494934 |
| test/Q_plus_P                  | -35.494934 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70800      |
| train/episodes                 | 7080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 283200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.4066544   |
| stats_o/std                    | 0.039437603 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -35.56411   |
| test/Q_plus_P                  | -35.56411   |
| test/reward_per_eps            | -40         |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.40668288  |
| stats_o/std                    | 0.039433073 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -35.60236   |
| test/Q_plus_P                  | -35.60236   |
| test/reward_per_eps            | -40         |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.4066979  |
| stats_o/std                    | 0.03943007 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.194     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -35.6379   |
| test/Q_plus_P                  | -35.6379   |
| test/reward_per_eps            | -40        |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.4067209   |
| stats_o/std                    | 0.039439067 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -35.751575  |
| test/Q_plus_P                  | -35.751575  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.4067756  |
| stats_o/std                    | 0.03945471 |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -35.778145 |
| test/Q_plus_P                  | -35.778145 |
| test/reward_per_eps            | -40        |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.40675363  |
| stats_o/std                    | 0.039435264 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.338      |
| test/Q                         | -35.791145  |
| test/Q_plus_P                  | -35.791145  |
| test/reward_per_eps            | -40         |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.40677336  |
| stats_o/std                    | 0.039440427 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -35.819202  |
| test/Q_plus_P                  | -35.819202  |
| test/reward_per_eps            | -40         |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.4068153   |
| stats_o/std                    | 0.039442163 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -35.94012   |
| test/Q_plus_P                  | -35.94012   |
| test/reward_per_eps            | -40         |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.40681255  |
| stats_o/std                    | 0.039434478 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -35.974396  |
| test/Q_plus_P                  | -35.974396  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.40684518  |
| stats_o/std                    | 0.039441474 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -35.99675   |
| test/Q_plus_P                  | -35.99675   |
| test/reward_per_eps            | -40         |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.40686348  |
| stats_o/std                    | 0.039437834 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -36.082752  |
| test/Q_plus_P                  | -36.082752  |
| test/reward_per_eps            | -40         |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.4068879   |
| stats_o/std                    | 0.039433915 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -36.109894  |
| test/Q_plus_P                  | -36.109894  |
| test/reward_per_eps            | -40         |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.40685663  |
| stats_o/std                    | 0.039425593 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -36.17417   |
| test/Q_plus_P                  | -36.17417   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.40684322  |
| stats_o/std                    | 0.039427143 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -36.22411   |
| test/Q_plus_P                  | -36.22411   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.40687123  |
| stats_o/std                    | 0.039424006 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -36.26629   |
| test/Q_plus_P                  | -36.26629   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.4068812   |
| stats_o/std                    | 0.039436918 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -36.336613  |
| test/Q_plus_P                  | -36.336613  |
| test/reward_per_eps            | -40         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.40689602  |
| stats_o/std                    | 0.039431017 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -36.3558    |
| test/Q_plus_P                  | -36.3558    |
| test/reward_per_eps            | -40         |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.40690756  |
| stats_o/std                    | 0.039434616 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -36.392757  |
| test/Q_plus_P                  | -36.392757  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.40693498  |
| stats_o/std                    | 0.039444383 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -36.448795  |
| test/Q_plus_P                  | -36.448795  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.4068962   |
| stats_o/std                    | 0.039439727 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.264      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -36.493088  |
| test/Q_plus_P                  | -36.493088  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.40686345 |
| stats_o/std                    | 0.03945238 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -36.572193 |
| test/Q_plus_P                  | -36.572193 |
| test/reward_per_eps            | -40        |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.40688682  |
| stats_o/std                    | 0.039447334 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -36.56044   |
| test/Q_plus_P                  | -36.56044   |
| test/reward_per_eps            | -40         |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.40689704  |
| stats_o/std                    | 0.039460756 |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -36.601154  |
| test/Q_plus_P                  | -36.601154  |
| test/reward_per_eps            | -40         |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.40689775  |
| stats_o/std                    | 0.039454553 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -36.633434  |
| test/Q_plus_P                  | -36.633434  |
| test/reward_per_eps            | -40         |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.4068927   |
| stats_o/std                    | 0.039457034 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -36.662357  |
| test/Q_plus_P                  | -36.662357  |
| test/reward_per_eps            | -40         |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.40691593  |
| stats_o/std                    | 0.039453734 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -36.7104    |
| test/Q_plus_P                  | -36.7104    |
| test/reward_per_eps            | -40         |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.40691105 |
| stats_o/std                    | 0.03945765 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -36.782475 |
| test/Q_plus_P                  | -36.782475 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.40692595 |
| stats_o/std                    | 0.03946378 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.192     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -36.714077 |
| test/Q_plus_P                  | -36.714077 |
| test/reward_per_eps            | -40        |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.40693054 |
| stats_o/std                    | 0.0394768  |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -36.85024  |
| test/Q_plus_P                  | -36.85024  |
| test/reward_per_eps            | -40        |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.40694824  |
| stats_o/std                    | 0.039493818 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -36.894592  |
| test/Q_plus_P                  | -36.894592  |
| test/reward_per_eps            | -40         |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 207         |
| stats_o/mean                   | 0.40693256  |
| stats_o/std                    | 0.039503105 |
| test/episodes                  | 2080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -36.89372   |
| test/Q_plus_P                  | -36.89372   |
| test/reward_per_eps            | -40         |
| test/steps                     | 83200       |
| train/episodes                 | 8320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 332800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.4069306   |
| stats_o/std                    | 0.039520215 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -36.96342   |
| test/Q_plus_P                  | -36.96342   |
| test/reward_per_eps            | -40         |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.40693644  |
| stats_o/std                    | 0.039511222 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -36.9879    |
| test/Q_plus_P                  | -36.9879    |
| test/reward_per_eps            | -40         |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.4069346   |
| stats_o/std                    | 0.039525423 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -37.044212  |
| test/Q_plus_P                  | -37.044212  |
| test/reward_per_eps            | -40         |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.40692928 |
| stats_o/std                    | 0.03952701 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -37.061203 |
| test/Q_plus_P                  | -37.061203 |
| test/reward_per_eps            | -40        |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.40694582 |
| stats_o/std                    | 0.03953932 |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -37.152966 |
| test/Q_plus_P                  | -37.152966 |
| test/reward_per_eps            | -40        |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.40697613  |
| stats_o/std                    | 0.039557874 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -37.144726  |
| test/Q_plus_P                  | -37.144726  |
| test/reward_per_eps            | -40         |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.40698698 |
| stats_o/std                    | 0.03955536 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -37.205147 |
| test/Q_plus_P                  | -37.205147 |
| test/reward_per_eps            | -40        |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.40697947  |
| stats_o/std                    | 0.039551545 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -37.227203  |
| test/Q_plus_P                  | -37.227203  |
| test/reward_per_eps            | -40         |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.40699935  |
| stats_o/std                    | 0.039552353 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -37.22723   |
| test/Q_plus_P                  | -37.22723   |
| test/reward_per_eps            | -40         |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.4070272   |
| stats_o/std                    | 0.039549552 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -37.288944  |
| test/Q_plus_P                  | -37.288944  |
| test/reward_per_eps            | -40         |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.40705845 |
| stats_o/std                    | 0.03954804 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.329     |
| test/Q                         | -37.298683 |
| test/Q_plus_P                  | -37.298683 |
| test/reward_per_eps            | -40        |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.40705916 |
| stats_o/std                    | 0.03954751 |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -37.25873  |
| test/Q_plus_P                  | -37.25873  |
| test/reward_per_eps            | -40        |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.407048    |
| stats_o/std                    | 0.039554443 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -37.39654   |
| test/Q_plus_P                  | -37.39654   |
| test/reward_per_eps            | -40         |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.40706995  |
| stats_o/std                    | 0.039563034 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -37.39168   |
| test/Q_plus_P                  | -37.39168   |
| test/reward_per_eps            | -40         |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.40712222 |
| stats_o/std                    | 0.03955407 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -37.36258  |
| test/Q_plus_P                  | -37.36258  |
| test/reward_per_eps            | -40        |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.40712738  |
| stats_o/std                    | 0.039543238 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -37.42885   |
| test/Q_plus_P                  | -37.42885   |
| test/reward_per_eps            | -40         |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.40714994 |
| stats_o/std                    | 0.03956497 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.129     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -37.51659  |
| test/Q_plus_P                  | -37.51659  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.40715304 |
| stats_o/std                    | 0.03957087 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -37.57356  |
| test/Q_plus_P                  | -37.57356  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.4071599   |
| stats_o/std                    | 0.039578076 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -37.57585   |
| test/Q_plus_P                  | -37.57585   |
| test/reward_per_eps            | -40         |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.4071841  |
| stats_o/std                    | 0.03957071 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -37.591583 |
| test/Q_plus_P                  | -37.591583 |
| test/reward_per_eps            | -40        |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.40718535  |
| stats_o/std                    | 0.039566934 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -37.608585  |
| test/Q_plus_P                  | -37.608585  |
| test/reward_per_eps            | -40         |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.40720573  |
| stats_o/std                    | 0.039565973 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -37.66113   |
| test/Q_plus_P                  | -37.66113   |
| test/reward_per_eps            | -40         |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.40723196  |
| stats_o/std                    | 0.039569326 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.118      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -37.69071   |
| test/Q_plus_P                  | -37.69071   |
| test/reward_per_eps            | -40         |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.40726092  |
| stats_o/std                    | 0.039584886 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.13       |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -37.714787  |
| test/Q_plus_P                  | -37.714787  |
| test/reward_per_eps            | -40         |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.4072844   |
| stats_o/std                    | 0.039586693 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -37.74427   |
| test/Q_plus_P                  | -37.74427   |
| test/reward_per_eps            | -40         |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.40733555  |
| stats_o/std                    | 0.039588936 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -37.754284  |
| test/Q_plus_P                  | -37.754284  |
| test/reward_per_eps            | -40         |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 234         |
| stats_o/mean                   | 0.40735433  |
| stats_o/std                    | 0.039598476 |
| test/episodes                  | 2350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -37.81251   |
| test/Q_plus_P                  | -37.81251   |
| test/reward_per_eps            | -40         |
| test/steps                     | 94000       |
| train/episodes                 | 9400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 376000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.40738964  |
| stats_o/std                    | 0.039611798 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -37.748657  |
| test/Q_plus_P                  | -37.748657  |
| test/reward_per_eps            | -40         |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.40741038  |
| stats_o/std                    | 0.039607823 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -37.888424  |
| test/Q_plus_P                  | -37.888424  |
| test/reward_per_eps            | -40         |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.407449   |
| stats_o/std                    | 0.03962022 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -37.903572 |
| test/Q_plus_P                  | -37.903572 |
| test/reward_per_eps            | -40        |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.4074774   |
| stats_o/std                    | 0.039632272 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -37.911148  |
| test/Q_plus_P                  | -37.911148  |
| test/reward_per_eps            | -40         |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.40747246  |
| stats_o/std                    | 0.039632104 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -37.914333  |
| test/Q_plus_P                  | -37.914333  |
| test/reward_per_eps            | -40         |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.4074676   |
| stats_o/std                    | 0.039636243 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -37.94692   |
| test/Q_plus_P                  | -37.94692   |
| test/reward_per_eps            | -40         |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.4074931   |
| stats_o/std                    | 0.039645795 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -37.987316  |
| test/Q_plus_P                  | -37.987316  |
| test/reward_per_eps            | -40         |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.40751338 |
| stats_o/std                    | 0.03965261 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -37.98905  |
| test/Q_plus_P                  | -37.98905  |
| test/reward_per_eps            | -40        |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.4075183  |
| stats_o/std                    | 0.03965341 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -38.050976 |
| test/Q_plus_P                  | -38.050976 |
| test/reward_per_eps            | -40        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.40753523 |
| stats_o/std                    | 0.03966743 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -38.03906  |
| test/Q_plus_P                  | -38.03906  |
| test/reward_per_eps            | -40        |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.40754354 |
| stats_o/std                    | 0.03966705 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -38.07754  |
| test/Q_plus_P                  | -38.07754  |
| test/reward_per_eps            | -40        |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.4075605  |
| stats_o/std                    | 0.03966636 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.271     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -38.124657 |
| test/Q_plus_P                  | -38.124657 |
| test/reward_per_eps            | -40        |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.40757573  |
| stats_o/std                    | 0.039669193 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -38.102715  |
| test/Q_plus_P                  | -38.102715  |
| test/reward_per_eps            | -40         |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.4075812  |
| stats_o/std                    | 0.03967995 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.188     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -38.15794  |
| test/Q_plus_P                  | -38.15794  |
| test/reward_per_eps            | -40        |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.4075962   |
| stats_o/std                    | 0.039684277 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -38.223175  |
| test/Q_plus_P                  | -38.223175  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.40759206  |
| stats_o/std                    | 0.039686847 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -38.213985  |
| test/Q_plus_P                  | -38.213985  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.40758988  |
| stats_o/std                    | 0.039695576 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -38.224968  |
| test/Q_plus_P                  | -38.224968  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.4076048   |
| stats_o/std                    | 0.039700847 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -38.29918   |
| test/Q_plus_P                  | -38.29918   |
| test/reward_per_eps            | -40         |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.40763077  |
| stats_o/std                    | 0.039704423 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -38.31083   |
| test/Q_plus_P                  | -38.31083   |
| test/reward_per_eps            | -40         |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.40762734  |
| stats_o/std                    | 0.039708126 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -38.266705  |
| test/Q_plus_P                  | -38.266705  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.40765408  |
| stats_o/std                    | 0.039704427 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -38.338608  |
| test/Q_plus_P                  | -38.338608  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.40763712 |
| stats_o/std                    | 0.03971471 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -38.30202  |
| test/Q_plus_P                  | -38.30202  |
| test/reward_per_eps            | -40        |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.4076487   |
| stats_o/std                    | 0.039713327 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -38.35557   |
| test/Q_plus_P                  | -38.35557   |
| test/reward_per_eps            | -40         |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.40765074  |
| stats_o/std                    | 0.039716125 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -38.37737   |
| test/Q_plus_P                  | -38.37737   |
| test/reward_per_eps            | -40         |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.4076606   |
| stats_o/std                    | 0.039728213 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -38.381172  |
| test/Q_plus_P                  | -38.381172  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.40767083  |
| stats_o/std                    | 0.039739423 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -38.414494  |
| test/Q_plus_P                  | -38.414494  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.40768588  |
| stats_o/std                    | 0.039742883 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -38.402706  |
| test/Q_plus_P                  | -38.402706  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.40769532  |
| stats_o/std                    | 0.039761804 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -38.430878  |
| test/Q_plus_P                  | -38.430878  |
| test/reward_per_eps            | -40         |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.40770623  |
| stats_o/std                    | 0.039774653 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -38.477993  |
| test/Q_plus_P                  | -38.477993  |
| test/reward_per_eps            | -40         |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.40770683  |
| stats_o/std                    | 0.039788958 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -38.490047  |
| test/Q_plus_P                  | -38.490047  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.40771118  |
| stats_o/std                    | 0.039812595 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -38.496483  |
| test/Q_plus_P                  | -38.496483  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.40771043  |
| stats_o/std                    | 0.039810147 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -38.486538  |
| test/Q_plus_P                  | -38.486538  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.4077194   |
| stats_o/std                    | 0.039822694 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -38.50208   |
| test/Q_plus_P                  | -38.50208   |
| test/reward_per_eps            | -40         |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.4077325   |
| stats_o/std                    | 0.039814617 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -38.55881   |
| test/Q_plus_P                  | -38.55881   |
| test/reward_per_eps            | -40         |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.40772787  |
| stats_o/std                    | 0.039825674 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.279      |
| test/info_shaping_reward_min   | -0.367      |
| test/Q                         | -38.568546  |
| test/Q_plus_P                  | -38.568546  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.4077277   |
| stats_o/std                    | 0.039834384 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -38.62483   |
| test/Q_plus_P                  | -38.62483   |
| test/reward_per_eps            | -40         |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.4077284  |
| stats_o/std                    | 0.03984207 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -38.609024 |
| test/Q_plus_P                  | -38.609024 |
| test/reward_per_eps            | -40        |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.40772974 |
| stats_o/std                    | 0.03983584 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -38.60788  |
| test/Q_plus_P                  | -38.60788  |
| test/reward_per_eps            | -40        |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.40772876  |
| stats_o/std                    | 0.039837953 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -38.669395  |
| test/Q_plus_P                  | -38.669395  |
| test/reward_per_eps            | -40         |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.40773216 |
| stats_o/std                    | 0.03983659 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.201     |
| test/info_shaping_reward_mean  | -0.273     |
| test/info_shaping_reward_min   | -0.359     |
| test/Q                         | -38.676914 |
| test/Q_plus_P                  | -38.676914 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.4077284   |
| stats_o/std                    | 0.039847203 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.36       |
| test/Q                         | -38.682373  |
| test/Q_plus_P                  | -38.682373  |
| test/reward_per_eps            | -40         |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.40774003 |
| stats_o/std                    | 0.03985585 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -38.684658 |
| test/Q_plus_P                  | -38.684658 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.40773526 |
| stats_o/std                    | 0.03985951 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -38.705112 |
| test/Q_plus_P                  | -38.705112 |
| test/reward_per_eps            | -40        |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.40776542  |
| stats_o/std                    | 0.039874088 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -38.691208  |
| test/Q_plus_P                  | -38.691208  |
| test/reward_per_eps            | -40         |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.40776846  |
| stats_o/std                    | 0.039877016 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -38.73528   |
| test/Q_plus_P                  | -38.73528   |
| test/reward_per_eps            | -40         |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.40778062  |
| stats_o/std                    | 0.039869588 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -38.73233   |
| test/Q_plus_P                  | -38.73233   |
| test/reward_per_eps            | -40         |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.40775988  |
| stats_o/std                    | 0.039873887 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.339      |
| test/Q                         | -38.7196    |
| test/Q_plus_P                  | -38.7196    |
| test/reward_per_eps            | -40         |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.388      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.4077749  |
| stats_o/std                    | 0.03987984 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -38.741695 |
| test/Q_plus_P                  | -38.741695 |
| test/reward_per_eps            | -40        |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.40779087 |
| stats_o/std                    | 0.03988299 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -38.85231  |
| test/Q_plus_P                  | -38.85231  |
| test/reward_per_eps            | -40        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.40778515 |
| stats_o/std                    | 0.03988332 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -38.814747 |
| test/Q_plus_P                  | -38.814747 |
| test/reward_per_eps            | -40        |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.40778545 |
| stats_o/std                    | 0.0398886  |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -38.826294 |
| test/Q_plus_P                  | -38.826294 |
| test/reward_per_eps            | -40        |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.40780124  |
| stats_o/std                    | 0.039886784 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -38.819546  |
| test/Q_plus_P                  | -38.819546  |
| test/reward_per_eps            | -40         |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.40782294 |
| stats_o/std                    | 0.03988931 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -38.85103  |
| test/Q_plus_P                  | -38.85103  |
| test/reward_per_eps            | -40        |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.40784147 |
| stats_o/std                    | 0.03989729 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -38.85535  |
| test/Q_plus_P                  | -38.85535  |
| test/reward_per_eps            | -40        |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.40783754  |
| stats_o/std                    | 0.039891798 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -38.847916  |
| test/Q_plus_P                  | -38.847916  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.40785217  |
| stats_o/std                    | 0.039904114 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -38.901295  |
| test/Q_plus_P                  | -38.901295  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.40786982  |
| stats_o/std                    | 0.039900262 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -38.875126  |
| test/Q_plus_P                  | -38.875126  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.4078735   |
| stats_o/std                    | 0.039902173 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -38.941895  |
| test/Q_plus_P                  | -38.941895  |
| test/reward_per_eps            | -40         |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.40789154  |
| stats_o/std                    | 0.039917678 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.065365  |
| test/Q_plus_P                  | -39.065365  |
| test/reward_per_eps            | -40         |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.40788922  |
| stats_o/std                    | 0.039920457 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.045723  |
| test/Q_plus_P                  | -39.045723  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.40790954  |
| stats_o/std                    | 0.039920736 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.014793  |
| test/Q_plus_P                  | -39.014793  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.4079239  |
| stats_o/std                    | 0.03991405 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -38.936996 |
| test/Q_plus_P                  | -38.936996 |
| test/reward_per_eps            | -40        |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.40792367 |
| stats_o/std                    | 0.03992023 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.009964 |
| test/Q_plus_P                  | -39.009964 |
| test/reward_per_eps            | -40        |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.4079559   |
| stats_o/std                    | 0.039931398 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.265      |
| test/info_shaping_reward_min   | -0.35       |
| test/Q                         | -39.028526  |
| test/Q_plus_P                  | -39.028526  |
| test/reward_per_eps            | -40         |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.40795782  |
| stats_o/std                    | 0.039938714 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.020138  |
| test/Q_plus_P                  | -39.020138  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.40795413  |
| stats_o/std                    | 0.039945506 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.359      |
| test/Q                         | -39.025738  |
| test/Q_plus_P                  | -39.025738  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.40796474 |
| stats_o/std                    | 0.03994784 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.211     |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.334     |
| test/Q                         | -39.050114 |
| test/Q_plus_P                  | -39.050114 |
| test/reward_per_eps            | -40        |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.40798303 |
| stats_o/std                    | 0.03995634 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.262     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.072964 |
| test/Q_plus_P                  | -39.072964 |
| test/reward_per_eps            | -40        |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.4079869   |
| stats_o/std                    | 0.039963264 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.090385  |
| test/Q_plus_P                  | -39.090385  |
| test/reward_per_eps            | -40         |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.40796947 |
| stats_o/std                    | 0.03998134 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.339     |
| test/Q                         | -39.081253 |
| test/Q_plus_P                  | -39.081253 |
| test/reward_per_eps            | -40        |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.40797555 |
| stats_o/std                    | 0.03999373 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.188     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -39.119736 |
| test/Q_plus_P                  | -39.119736 |
| test/reward_per_eps            | -40        |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.40798643  |
| stats_o/std                    | 0.039998457 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.338      |
| test/Q                         | -39.123432  |
| test/Q_plus_P                  | -39.123432  |
| test/reward_per_eps            | -40         |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.40797627  |
| stats_o/std                    | 0.040008765 |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -39.0577    |
| test/Q_plus_P                  | -39.0577    |
| test/reward_per_eps            | -40         |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.267      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.407977   |
| stats_o/std                    | 0.04001493 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.122997 |
| test/Q_plus_P                  | -39.122997 |
| test/reward_per_eps            | -40        |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.40797865  |
| stats_o/std                    | 0.040017795 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.15703   |
| test/Q_plus_P                  | -39.15703   |
| test/reward_per_eps            | -40         |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.40796837  |
| stats_o/std                    | 0.040027764 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.337      |
| test/Q                         | -39.173874  |
| test/Q_plus_P                  | -39.173874  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.4079534   |
| stats_o/std                    | 0.040024456 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.353      |
| test/Q                         | -39.166832  |
| test/Q_plus_P                  | -39.166832  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.40793538  |
| stats_o/std                    | 0.040028136 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.279      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -39.348965  |
| test/Q_plus_P                  | -39.348965  |
| test/reward_per_eps            | -40         |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.182      |
| train/info_shaping_reward_mean | -0.267      |
| train/info_shaping_reward_min  | -0.386      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.4079326   |
| stats_o/std                    | 0.040034566 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.204      |
| test/info_shaping_reward_mean  | -0.269      |
| test/info_shaping_reward_min   | -0.339      |
| test/Q                         | -39.17268   |
| test/Q_plus_P                  | -39.17268   |
| test/reward_per_eps            | -40         |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.40791067  |
| stats_o/std                    | 0.040034182 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -39.210087  |
| test/Q_plus_P                  | -39.210087  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.40792403 |
| stats_o/std                    | 0.04003125 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.261     |
| test/info_shaping_reward_min   | -0.322     |
| test/Q                         | -39.224186 |
| test/Q_plus_P                  | -39.224186 |
| test/reward_per_eps            | -40        |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 316       |
| stats_o/mean                   | 0.4079186 |
| stats_o/std                    | 0.04003   |
| test/episodes                  | 3170      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.179    |
| test/info_shaping_reward_mean  | -0.245    |
| test/info_shaping_reward_min   | -0.306    |
| test/Q                         | -39.24221 |
| test/Q_plus_P                  | -39.24221 |
| test/reward_per_eps            | -40       |
| test/steps                     | 126800    |
| train/episodes                 | 12680     |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.194    |
| train/info_shaping_reward_mean | -0.269    |
| train/info_shaping_reward_min  | -0.374    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 507200    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.40791818 |
| stats_o/std                    | 0.04003271 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -39.16446  |
| test/Q_plus_P                  | -39.16446  |
| test/reward_per_eps            | -40        |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.40791264 |
| stats_o/std                    | 0.04003061 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -39.291958 |
| test/Q_plus_P                  | -39.291958 |
| test/reward_per_eps            | -40        |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.40789345 |
| stats_o/std                    | 0.04002466 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.278282 |
| test/Q_plus_P                  | -39.278282 |
| test/reward_per_eps            | -40        |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.40789866  |
| stats_o/std                    | 0.040038753 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.260746  |
| test/Q_plus_P                  | -39.260746  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.40789056  |
| stats_o/std                    | 0.040040907 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -39.247272  |
| test/Q_plus_P                  | -39.247272  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.40788925  |
| stats_o/std                    | 0.040051993 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.364155  |
| test/Q_plus_P                  | -39.364155  |
| test/reward_per_eps            | -40         |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.40788534  |
| stats_o/std                    | 0.040048417 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.357      |
| test/Q                         | -39.22928   |
| test/Q_plus_P                  | -39.22928   |
| test/reward_per_eps            | -40         |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.40787414  |
| stats_o/std                    | 0.040058877 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.30779   |
| test/Q_plus_P                  | -39.30779   |
| test/reward_per_eps            | -40         |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.393      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.40789238 |
| stats_o/std                    | 0.04006313 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.34884  |
| test/Q_plus_P                  | -39.34884  |
| test/reward_per_eps            | -40        |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 326        |
| stats_o/mean                   | 0.40787956 |
| stats_o/std                    | 0.04006592 |
| test/episodes                  | 3270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.359116 |
| test/Q_plus_P                  | -39.359116 |
| test/reward_per_eps            | -40        |
| test/steps                     | 130800     |
| train/episodes                 | 13080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 523200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.40786386  |
| stats_o/std                    | 0.040063944 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.352      |
| test/Q                         | -39.335735  |
| test/Q_plus_P                  | -39.335735  |
| test/reward_per_eps            | -40         |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.388      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.40785357 |
| stats_o/std                    | 0.04007816 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.305267 |
| test/Q_plus_P                  | -39.305267 |
| test/reward_per_eps            | -40        |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.184     |
| train/info_shaping_reward_mean | -0.281     |
| train/info_shaping_reward_min  | -0.403     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.4078296   |
| stats_o/std                    | 0.040085196 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.203      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.345      |
| test/Q                         | -39.349754  |
| test/Q_plus_P                  | -39.349754  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.375      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.4078333   |
| stats_o/std                    | 0.040089063 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.402298  |
| test/Q_plus_P                  | -39.402298  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.4078196   |
| stats_o/std                    | 0.040104136 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.351      |
| test/Q                         | -39.329983  |
| test/Q_plus_P                  | -39.329983  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.385      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.40780327  |
| stats_o/std                    | 0.040095493 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -39.346954  |
| test/Q_plus_P                  | -39.346954  |
| test/reward_per_eps            | -40         |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.40779182  |
| stats_o/std                    | 0.040095653 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.203      |
| test/info_shaping_reward_mean  | -0.284      |
| test/info_shaping_reward_min   | -0.371      |
| test/Q                         | -39.387814  |
| test/Q_plus_P                  | -39.387814  |
| test/reward_per_eps            | -40         |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.40778422 |
| stats_o/std                    | 0.04010091 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -39.386574 |
| test/Q_plus_P                  | -39.386574 |
| test/reward_per_eps            | -40        |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.38      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.40777943  |
| stats_o/std                    | 0.040105373 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.201      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.337      |
| test/Q                         | -39.377243  |
| test/Q_plus_P                  | -39.377243  |
| test/reward_per_eps            | -40         |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.40778902 |
| stats_o/std                    | 0.04010421 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.413868 |
| test/Q_plus_P                  | -39.413868 |
| test/reward_per_eps            | -40        |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.4077889   |
| stats_o/std                    | 0.040105417 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -39.346436  |
| test/Q_plus_P                  | -39.346436  |
| test/reward_per_eps            | -40         |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.40776995 |
| stats_o/std                    | 0.040115   |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.271     |
| test/info_shaping_reward_min   | -0.353     |
| test/Q                         | -39.330196 |
| test/Q_plus_P                  | -39.330196 |
| test/reward_per_eps            | -40        |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.267     |
| train/info_shaping_reward_min  | -0.381     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.40775713  |
| stats_o/std                    | 0.040126506 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.39305   |
| test/Q_plus_P                  | -39.39305   |
| test/reward_per_eps            | -40         |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.393      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.4077321   |
| stats_o/std                    | 0.040122975 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.4069    |
| test/Q_plus_P                  | -39.4069    |
| test/reward_per_eps            | -40         |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.40770826  |
| stats_o/std                    | 0.040121187 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.28       |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -39.407227  |
| test/Q_plus_P                  | -39.407227  |
| test/reward_per_eps            | -40         |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.4077139   |
| stats_o/std                    | 0.040128917 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.421757  |
| test/Q_plus_P                  | -39.421757  |
| test/reward_per_eps            | -40         |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.272      |
| train/info_shaping_reward_min  | -0.389      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.4077101   |
| stats_o/std                    | 0.040125135 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -39.444218  |
| test/Q_plus_P                  | -39.444218  |
| test/reward_per_eps            | -40         |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.40770328  |
| stats_o/std                    | 0.040127028 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.419395  |
| test/Q_plus_P                  | -39.419395  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.40770864  |
| stats_o/std                    | 0.040126342 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.436676  |
| test/Q_plus_P                  | -39.436676  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.40769994  |
| stats_o/std                    | 0.040115796 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.446777  |
| test/Q_plus_P                  | -39.446777  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.40770137  |
| stats_o/std                    | 0.040116396 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.48214   |
| test/Q_plus_P                  | -39.48214   |
| test/reward_per_eps            | -40         |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.40770665  |
| stats_o/std                    | 0.040117074 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.454987  |
| test/Q_plus_P                  | -39.454987  |
| test/reward_per_eps            | -40         |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.40770856  |
| stats_o/std                    | 0.040120598 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.431408  |
| test/Q_plus_P                  | -39.431408  |
| test/reward_per_eps            | -40         |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.40772364 |
| stats_o/std                    | 0.04011758 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.461903 |
| test/Q_plus_P                  | -39.461903 |
| test/reward_per_eps            | -40        |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.40773988  |
| stats_o/std                    | 0.040120743 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.463818  |
| test/Q_plus_P                  | -39.463818  |
| test/reward_per_eps            | -40         |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.4077237  |
| stats_o/std                    | 0.04011446 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -39.474785 |
| test/Q_plus_P                  | -39.474785 |
| test/reward_per_eps            | -40        |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.4077154   |
| stats_o/std                    | 0.040118735 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.483807  |
| test/Q_plus_P                  | -39.483807  |
| test/reward_per_eps            | -40         |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.40770975 |
| stats_o/std                    | 0.04012431 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.347     |
| test/Q                         | -39.487183 |
| test/Q_plus_P                  | -39.487183 |
| test/reward_per_eps            | -40        |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.178     |
| train/info_shaping_reward_mean | -0.269     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.4077169   |
| stats_o/std                    | 0.040121276 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -39.473335  |
| test/Q_plus_P                  | -39.473335  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.40770045  |
| stats_o/std                    | 0.040115893 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -39.479126  |
| test/Q_plus_P                  | -39.479126  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.4077058  |
| stats_o/std                    | 0.04010924 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.493492 |
| test/Q_plus_P                  | -39.493492 |
| test/reward_per_eps            | -40        |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.40771523  |
| stats_o/std                    | 0.040107753 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -39.47785   |
| test/Q_plus_P                  | -39.47785   |
| test/reward_per_eps            | -40         |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.40770912  |
| stats_o/std                    | 0.040108886 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.504456  |
| test/Q_plus_P                  | -39.504456  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.40771165  |
| stats_o/std                    | 0.040110413 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.519333  |
| test/Q_plus_P                  | -39.519333  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.40770352  |
| stats_o/std                    | 0.040110596 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.46894   |
| test/Q_plus_P                  | -39.46894   |
| test/reward_per_eps            | -40         |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.407691   |
| stats_o/std                    | 0.04010094 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.527466 |
| test/Q_plus_P                  | -39.527466 |
| test/reward_per_eps            | -40        |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.4076748   |
| stats_o/std                    | 0.040102754 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.27       |
| test/info_shaping_reward_min   | -0.36       |
| test/Q                         | -39.494156  |
| test/Q_plus_P                  | -39.494156  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.4076544  |
| stats_o/std                    | 0.04011928 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.21      |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.553555 |
| test/Q_plus_P                  | -39.553555 |
| test/reward_per_eps            | -40        |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.392     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.40765402  |
| stats_o/std                    | 0.040129103 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -39.538208  |
| test/Q_plus_P                  | -39.538208  |
| test/reward_per_eps            | -40         |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.40764964 |
| stats_o/std                    | 0.0401376  |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.579613 |
| test/Q_plus_P                  | -39.579613 |
| test/reward_per_eps            | -40        |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.40764198  |
| stats_o/std                    | 0.040135752 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -39.570747  |
| test/Q_plus_P                  | -39.570747  |
| test/reward_per_eps            | -40         |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.4076403  |
| stats_o/std                    | 0.040147   |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.348     |
| test/Q                         | -39.540443 |
| test/Q_plus_P                  | -39.540443 |
| test/reward_per_eps            | -40        |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.261     |
| train/info_shaping_reward_min  | -0.378     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.40762028  |
| stats_o/std                    | 0.040154092 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.532204  |
| test/Q_plus_P                  | -39.532204  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 370        |
| stats_o/mean                   | 0.40760565 |
| stats_o/std                    | 0.0401519  |
| test/episodes                  | 3710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.353     |
| test/Q                         | -39.569824 |
| test/Q_plus_P                  | -39.569824 |
| test/reward_per_eps            | -40        |
| test/steps                     | 148400     |
| train/episodes                 | 14840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 593600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.40761006 |
| stats_o/std                    | 0.04015817 |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -39.59126  |
| test/Q_plus_P                  | -39.59126  |
| test/reward_per_eps            | -40        |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.40761903 |
| stats_o/std                    | 0.04015683 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -39.593338 |
| test/Q_plus_P                  | -39.593338 |
| test/reward_per_eps            | -40        |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.407619    |
| stats_o/std                    | 0.040163904 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -39.58594   |
| test/Q_plus_P                  | -39.58594   |
| test/reward_per_eps            | -40         |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.4076165  |
| stats_o/std                    | 0.04016177 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.61809  |
| test/Q_plus_P                  | -39.61809  |
| test/reward_per_eps            | -40        |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.40762293  |
| stats_o/std                    | 0.040169623 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.60438   |
| test/Q_plus_P                  | -39.60438   |
| test/reward_per_eps            | -40         |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.387      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.40763167 |
| stats_o/std                    | 0.04017787 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.6102   |
| test/Q_plus_P                  | -39.6102   |
| test/reward_per_eps            | -40        |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.4076304   |
| stats_o/std                    | 0.040181734 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.61348   |
| test/Q_plus_P                  | -39.61348   |
| test/reward_per_eps            | -40         |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.40763596  |
| stats_o/std                    | 0.040180854 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.595795  |
| test/Q_plus_P                  | -39.595795  |
| test/reward_per_eps            | -40         |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.4076366   |
| stats_o/std                    | 0.040180776 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.605724  |
| test/Q_plus_P                  | -39.605724  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.40763125  |
| stats_o/std                    | 0.040171586 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.635204  |
| test/Q_plus_P                  | -39.635204  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.407613    |
| stats_o/std                    | 0.040174443 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.639023  |
| test/Q_plus_P                  | -39.639023  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.40762094 |
| stats_o/std                    | 0.04017394 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.127     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.621098 |
| test/Q_plus_P                  | -39.621098 |
| test/reward_per_eps            | -40        |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.40761986  |
| stats_o/std                    | 0.040183596 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.60318   |
| test/Q_plus_P                  | -39.60318   |
| test/reward_per_eps            | -40         |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.40763214 |
| stats_o/std                    | 0.04018208 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -39.60829  |
| test/Q_plus_P                  | -39.60829  |
| test/reward_per_eps            | -40        |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.4076279   |
| stats_o/std                    | 0.040182542 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.63234   |
| test/Q_plus_P                  | -39.63234   |
| test/reward_per_eps            | -40         |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.40762722  |
| stats_o/std                    | 0.040186476 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.633633  |
| test/Q_plus_P                  | -39.633633  |
| test/reward_per_eps            | -40         |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.4076215   |
| stats_o/std                    | 0.040185943 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.604427  |
| test/Q_plus_P                  | -39.604427  |
| test/reward_per_eps            | -40         |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.4076276  |
| stats_o/std                    | 0.04019056 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.339     |
| test/Q                         | -39.57582  |
| test/Q_plus_P                  | -39.57582  |
| test/reward_per_eps            | -40        |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.40762746  |
| stats_o/std                    | 0.040191058 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.63289   |
| test/Q_plus_P                  | -39.63289   |
| test/reward_per_eps            | -40         |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.40764213 |
| stats_o/std                    | 0.04019217 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.326     |
| test/Q                         | -39.704727 |
| test/Q_plus_P                  | -39.704727 |
| test/reward_per_eps            | -40        |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 391       |
| stats_o/mean                   | 0.4076563 |
| stats_o/std                    | 0.0401865 |
| test/episodes                  | 3920      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.15     |
| test/info_shaping_reward_mean  | -0.231    |
| test/info_shaping_reward_min   | -0.307    |
| test/Q                         | -39.66809 |
| test/Q_plus_P                  | -39.66809 |
| test/reward_per_eps            | -40       |
| test/steps                     | 156800    |
| train/episodes                 | 15680     |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.153    |
| train/info_shaping_reward_mean | -0.239    |
| train/info_shaping_reward_min  | -0.326    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 627200    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.40763333 |
| stats_o/std                    | 0.04018435 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.602802 |
| test/Q_plus_P                  | -39.602802 |
| test/reward_per_eps            | -40        |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.40763196  |
| stats_o/std                    | 0.040192984 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -39.662197  |
| test/Q_plus_P                  | -39.662197  |
| test/reward_per_eps            | -40         |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.40763965  |
| stats_o/std                    | 0.040191267 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.273      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -39.67191   |
| test/Q_plus_P                  | -39.67191   |
| test/reward_per_eps            | -40         |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.182      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.40762308  |
| stats_o/std                    | 0.040196244 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.714333  |
| test/Q_plus_P                  | -39.714333  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.271      |
| train/info_shaping_reward_min  | -0.393      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.40762857  |
| stats_o/std                    | 0.040202703 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.678043  |
| test/Q_plus_P                  | -39.678043  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.40764797  |
| stats_o/std                    | 0.040208258 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.356      |
| test/Q                         | -39.667843  |
| test/Q_plus_P                  | -39.667843  |
| test/reward_per_eps            | -40         |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.40764436  |
| stats_o/std                    | 0.040210687 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.27       |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -39.668827  |
| test/Q_plus_P                  | -39.668827  |
| test/reward_per_eps            | -40         |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.393      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.40763113  |
| stats_o/std                    | 0.040207174 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.364      |
| test/Q                         | -39.632725  |
| test/Q_plus_P                  | -39.632725  |
| test/reward_per_eps            | -40         |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.4076133   |
| stats_o/std                    | 0.040210996 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -39.68997   |
| test/Q_plus_P                  | -39.68997   |
| test/reward_per_eps            | -40         |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.40761244 |
| stats_o/std                    | 0.04021911 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -39.73169  |
| test/Q_plus_P                  | -39.73169  |
| test/reward_per_eps            | -40        |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.177     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.40762842 |
| stats_o/std                    | 0.04021477 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.263     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -39.678646 |
| test/Q_plus_P                  | -39.678646 |
| test/reward_per_eps            | -40        |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.40763584  |
| stats_o/std                    | 0.040215615 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.690334  |
| test/Q_plus_P                  | -39.690334  |
| test/reward_per_eps            | -40         |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.4076483   |
| stats_o/std                    | 0.040208142 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.687714  |
| test/Q_plus_P                  | -39.687714  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 405        |
| stats_o/mean                   | 0.40763953 |
| stats_o/std                    | 0.04021026 |
| test/episodes                  | 4060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -39.69509  |
| test/Q_plus_P                  | -39.69509  |
| test/reward_per_eps            | -40        |
| test/steps                     | 162400     |
| train/episodes                 | 16240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 649600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.40763435 |
| stats_o/std                    | 0.0402099  |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -39.617935 |
| test/Q_plus_P                  | -39.617935 |
| test/reward_per_eps            | -40        |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.40764424  |
| stats_o/std                    | 0.040208302 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.716347  |
| test/Q_plus_P                  | -39.716347  |
| test/reward_per_eps            | -40         |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.40764824 |
| stats_o/std                    | 0.04020859 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.667625 |
| test/Q_plus_P                  | -39.667625 |
| test/reward_per_eps            | -40        |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.4076612   |
| stats_o/std                    | 0.040208884 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.69226   |
| test/Q_plus_P                  | -39.69226   |
| test/reward_per_eps            | -40         |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.4076859  |
| stats_o/std                    | 0.04020766 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -39.63791  |
| test/Q_plus_P                  | -39.63791  |
| test/reward_per_eps            | -40        |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.4076933   |
| stats_o/std                    | 0.040198367 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.709736  |
| test/Q_plus_P                  | -39.709736  |
| test/reward_per_eps            | -40         |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.4076943  |
| stats_o/std                    | 0.04020276 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.707886 |
| test/Q_plus_P                  | -39.707886 |
| test/reward_per_eps            | -40        |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.40767953  |
| stats_o/std                    | 0.040207364 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.722897  |
| test/Q_plus_P                  | -39.722897  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.407673   |
| stats_o/std                    | 0.04020653 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.683186 |
| test/Q_plus_P                  | -39.683186 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.40767798  |
| stats_o/std                    | 0.040206302 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.66392   |
| test/Q_plus_P                  | -39.66392   |
| test/reward_per_eps            | -40         |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.40768242  |
| stats_o/std                    | 0.040207807 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.669086  |
| test/Q_plus_P                  | -39.669086  |
| test/reward_per_eps            | -40         |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.40768167 |
| stats_o/std                    | 0.0402016  |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.763508 |
| test/Q_plus_P                  | -39.763508 |
| test/reward_per_eps            | -40        |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 418        |
| stats_o/mean                   | 0.40769312 |
| stats_o/std                    | 0.04019573 |
| test/episodes                  | 4190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.769276 |
| test/Q_plus_P                  | -39.769276 |
| test/reward_per_eps            | -40        |
| test/steps                     | 167600     |
| train/episodes                 | 16760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 670400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.40769067  |
| stats_o/std                    | 0.040196974 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.720776  |
| test/Q_plus_P                  | -39.720776  |
| test/reward_per_eps            | -40         |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.40768102 |
| stats_o/std                    | 0.04019312 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.656307 |
| test/Q_plus_P                  | -39.656307 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.4076868   |
| stats_o/std                    | 0.040194813 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.7043    |
| test/Q_plus_P                  | -39.7043    |
| test/reward_per_eps            | -40         |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.40769657  |
| stats_o/std                    | 0.040191345 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.73079   |
| test/Q_plus_P                  | -39.73079   |
| test/reward_per_eps            | -40         |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.40770352  |
| stats_o/std                    | 0.040183064 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.337      |
| test/Q                         | -39.698658  |
| test/Q_plus_P                  | -39.698658  |
| test/reward_per_eps            | -40         |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.4076917  |
| stats_o/std                    | 0.04018073 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.731735 |
| test/Q_plus_P                  | -39.731735 |
| test/reward_per_eps            | -40        |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.4076846   |
| stats_o/std                    | 0.040183023 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.214      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.344      |
| test/Q                         | -39.762096  |
| test/Q_plus_P                  | -39.762096  |
| test/reward_per_eps            | -40         |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.267      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.4076756  |
| stats_o/std                    | 0.04018465 |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.765022 |
| test/Q_plus_P                  | -39.765022 |
| test/reward_per_eps            | -40        |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.369     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.40767753 |
| stats_o/std                    | 0.04018248 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.34      |
| test/Q                         | -39.712234 |
| test/Q_plus_P                  | -39.712234 |
| test/reward_per_eps            | -40        |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.40768048  |
| stats_o/std                    | 0.040192958 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.753235  |
| test/Q_plus_P                  | -39.753235  |
| test/reward_per_eps            | -40         |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.4076862   |
| stats_o/std                    | 0.040188674 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.21       |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.78027   |
| test/Q_plus_P                  | -39.78027   |
| test/reward_per_eps            | -40         |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.4076762  |
| stats_o/std                    | 0.04018445 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.327     |
| test/Q                         | -39.78247  |
| test/Q_plus_P                  | -39.78247  |
| test/reward_per_eps            | -40        |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.40767252  |
| stats_o/std                    | 0.040180188 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.77318   |
| test/Q_plus_P                  | -39.77318   |
| test/reward_per_eps            | -40         |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.40766206  |
| stats_o/std                    | 0.040179305 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -39.794605  |
| test/Q_plus_P                  | -39.794605  |
| test/reward_per_eps            | -40         |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.40766308  |
| stats_o/std                    | 0.040179443 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.80281   |
| test/Q_plus_P                  | -39.80281   |
| test/reward_per_eps            | -40         |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.40767145  |
| stats_o/std                    | 0.040183324 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.787704  |
| test/Q_plus_P                  | -39.787704  |
| test/reward_per_eps            | -40         |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.4076728   |
| stats_o/std                    | 0.040184367 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.73839   |
| test/Q_plus_P                  | -39.73839   |
| test/reward_per_eps            | -40         |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.40767112  |
| stats_o/std                    | 0.040191434 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.359      |
| test/Q                         | -39.814384  |
| test/Q_plus_P                  | -39.814384  |
| test/reward_per_eps            | -40         |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.179      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.40767524  |
| stats_o/std                    | 0.040190276 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.361      |
| test/Q                         | -39.75753   |
| test/Q_plus_P                  | -39.75753   |
| test/reward_per_eps            | -40         |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.385      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.40768024 |
| stats_o/std                    | 0.04019594 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.363     |
| test/Q                         | -39.757336 |
| test/Q_plus_P                  | -39.757336 |
| test/reward_per_eps            | -40        |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.181     |
| train/info_shaping_reward_mean | -0.273     |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.40768567  |
| stats_o/std                    | 0.040200297 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.781197  |
| test/Q_plus_P                  | -39.781197  |
| test/reward_per_eps            | -40         |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.40768555 |
| stats_o/std                    | 0.04020351 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.35      |
| test/Q                         | -39.834194 |
| test/Q_plus_P                  | -39.834194 |
| test/reward_per_eps            | -40        |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.4076855   |
| stats_o/std                    | 0.040200762 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.80221   |
| test/Q_plus_P                  | -39.80221   |
| test/reward_per_eps            | -40         |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.40767792  |
| stats_o/std                    | 0.040202726 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.337      |
| test/Q                         | -39.74878   |
| test/Q_plus_P                  | -39.74878   |
| test/reward_per_eps            | -40         |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.179      |
| train/info_shaping_reward_mean | -0.262      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.40767565  |
| stats_o/std                    | 0.040201742 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.80467   |
| test/Q_plus_P                  | -39.80467   |
| test/reward_per_eps            | -40         |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.4076806   |
| stats_o/std                    | 0.040198456 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.801254  |
| test/Q_plus_P                  | -39.801254  |
| test/reward_per_eps            | -40         |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.4076973   |
| stats_o/std                    | 0.040202174 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.803864  |
| test/Q_plus_P                  | -39.803864  |
| test/reward_per_eps            | -40         |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.40769395 |
| stats_o/std                    | 0.04020125 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.790085 |
| test/Q_plus_P                  | -39.790085 |
| test/reward_per_eps            | -40        |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.4076911   |
| stats_o/std                    | 0.040198956 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.85674   |
| test/Q_plus_P                  | -39.85674   |
| test/reward_per_eps            | -40         |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.40769577 |
| stats_o/std                    | 0.04019295 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.816185 |
| test/Q_plus_P                  | -39.816185 |
| test/reward_per_eps            | -40        |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.40770862  |
| stats_o/std                    | 0.040190335 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.345      |
| test/Q                         | -39.819733  |
| test/Q_plus_P                  | -39.819733  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.4077175   |
| stats_o/std                    | 0.040192503 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.814457  |
| test/Q_plus_P                  | -39.814457  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.40771124 |
| stats_o/std                    | 0.04019272 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.275     |
| test/info_shaping_reward_min   | -0.379     |
| test/Q                         | -39.7998   |
| test/Q_plus_P                  | -39.7998   |
| test/reward_per_eps            | -40        |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.40770712 |
| stats_o/std                    | 0.04019202 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.265     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -39.82518  |
| test/Q_plus_P                  | -39.82518  |
| test/reward_per_eps            | -40        |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.40771353 |
| stats_o/std                    | 0.0401906  |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -39.858223 |
| test/Q_plus_P                  | -39.858223 |
| test/reward_per_eps            | -40        |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.40772882  |
| stats_o/std                    | 0.040190533 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.82351   |
| test/Q_plus_P                  | -39.82351   |
| test/reward_per_eps            | -40         |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.40772387  |
| stats_o/std                    | 0.040190745 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -39.81655   |
| test/Q_plus_P                  | -39.81655   |
| test/reward_per_eps            | -40         |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.4077313  |
| stats_o/std                    | 0.04019661 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.843246 |
| test/Q_plus_P                  | -39.843246 |
| test/reward_per_eps            | -40        |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.40775982  |
| stats_o/std                    | 0.040203556 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -39.84989   |
| test/Q_plus_P                  | -39.84989   |
| test/reward_per_eps            | -40         |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.40775764  |
| stats_o/std                    | 0.040205713 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.339      |
| test/Q                         | -39.80181   |
| test/Q_plus_P                  | -39.80181   |
| test/reward_per_eps            | -40         |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.40776947 |
| stats_o/std                    | 0.04020851 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.128     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -39.828617 |
| test/Q_plus_P                  | -39.828617 |
| test/reward_per_eps            | -40        |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.4077671   |
| stats_o/std                    | 0.040207203 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.844437  |
| test/Q_plus_P                  | -39.844437  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.40778133 |
| stats_o/std                    | 0.04020359 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.832897 |
| test/Q_plus_P                  | -39.832897 |
| test/reward_per_eps            | -40        |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.40778247  |
| stats_o/std                    | 0.040203545 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.832516  |
| test/Q_plus_P                  | -39.832516  |
| test/reward_per_eps            | -40         |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.40779796  |
| stats_o/std                    | 0.040199753 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.853756  |
| test/Q_plus_P                  | -39.853756  |
| test/reward_per_eps            | -40         |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.40780374  |
| stats_o/std                    | 0.040198233 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.859703  |
| test/Q_plus_P                  | -39.859703  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.40781197  |
| stats_o/std                    | 0.040203977 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.829086  |
| test/Q_plus_P                  | -39.829086  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.4078277   |
| stats_o/std                    | 0.040205095 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.792168  |
| test/Q_plus_P                  | -39.792168  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.40782925  |
| stats_o/std                    | 0.040207025 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.820446  |
| test/Q_plus_P                  | -39.820446  |
| test/reward_per_eps            | -40         |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.40784314  |
| stats_o/std                    | 0.040209047 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.859684  |
| test/Q_plus_P                  | -39.859684  |
| test/reward_per_eps            | -40         |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.40784192  |
| stats_o/std                    | 0.040206786 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.861015  |
| test/Q_plus_P                  | -39.861015  |
| test/reward_per_eps            | -40         |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.40785468  |
| stats_o/std                    | 0.040203203 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.839993  |
| test/Q_plus_P                  | -39.839993  |
| test/reward_per_eps            | -40         |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.40785936 |
| stats_o/std                    | 0.04019409 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -39.846436 |
| test/Q_plus_P                  | -39.846436 |
| test/reward_per_eps            | -40        |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.40786362 |
| stats_o/std                    | 0.04019885 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -39.868946 |
| test/Q_plus_P                  | -39.868946 |
| test/reward_per_eps            | -40        |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.40787208  |
| stats_o/std                    | 0.040199134 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -39.76519   |
| test/Q_plus_P                  | -39.76519   |
| test/reward_per_eps            | -40         |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.40786418  |
| stats_o/std                    | 0.040201195 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.85662   |
| test/Q_plus_P                  | -39.85662   |
| test/reward_per_eps            | -40         |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.40787137  |
| stats_o/std                    | 0.040199585 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.844017  |
| test/Q_plus_P                  | -39.844017  |
| test/reward_per_eps            | -40         |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.40787038  |
| stats_o/std                    | 0.040197182 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.86063   |
| test/Q_plus_P                  | -39.86063   |
| test/reward_per_eps            | -40         |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.40787295 |
| stats_o/std                    | 0.04019695 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -39.874386 |
| test/Q_plus_P                  | -39.874386 |
| test/reward_per_eps            | -40        |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.40787235  |
| stats_o/std                    | 0.040201258 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.768745  |
| test/Q_plus_P                  | -39.768745  |
| test/reward_per_eps            | -40         |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.40787026  |
| stats_o/std                    | 0.040205486 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.86144   |
| test/Q_plus_P                  | -39.86144   |
| test/reward_per_eps            | -40         |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.4078838   |
| stats_o/std                    | 0.040205274 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.131      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.905254  |
| test/Q_plus_P                  | -39.905254  |
| test/reward_per_eps            | -40         |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.40790442  |
| stats_o/std                    | 0.040202096 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.833824  |
| test/Q_plus_P                  | -39.833824  |
| test/reward_per_eps            | -40         |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.4079118   |
| stats_o/std                    | 0.040203594 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.865273  |
| test/Q_plus_P                  | -39.865273  |
| test/reward_per_eps            | -40         |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.4079269  |
| stats_o/std                    | 0.04019684 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.87349  |
| test/Q_plus_P                  | -39.87349  |
| test/reward_per_eps            | -40        |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.40791547  |
| stats_o/std                    | 0.040199187 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.866653  |
| test/Q_plus_P                  | -39.866653  |
| test/reward_per_eps            | -40         |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.40792322 |
| stats_o/std                    | 0.04020245 |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.83365  |
| test/Q_plus_P                  | -39.83365  |
| test/reward_per_eps            | -40        |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.40792155 |
| stats_o/std                    | 0.04020347 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.86413  |
| test/Q_plus_P                  | -39.86413  |
| test/reward_per_eps            | -40        |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.4079272   |
| stats_o/std                    | 0.040199928 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.851284  |
| test/Q_plus_P                  | -39.851284  |
| test/reward_per_eps            | -40         |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.4079293   |
| stats_o/std                    | 0.040205173 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.846447  |
| test/Q_plus_P                  | -39.846447  |
| test/reward_per_eps            | -40         |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.407917    |
| stats_o/std                    | 0.040205833 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.856224  |
| test/Q_plus_P                  | -39.856224  |
| test/reward_per_eps            | -40         |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.40791845 |
| stats_o/std                    | 0.04020727 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.90047  |
| test/Q_plus_P                  | -39.90047  |
| test/reward_per_eps            | -40        |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.40790275  |
| stats_o/std                    | 0.040207922 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.87791   |
| test/Q_plus_P                  | -39.87791   |
| test/reward_per_eps            | -40         |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.40790868 |
| stats_o/std                    | 0.04020047 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -39.835064 |
| test/Q_plus_P                  | -39.835064 |
| test/reward_per_eps            | -40        |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.40791297 |
| stats_o/std                    | 0.04019356 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.202     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.812634 |
| test/Q_plus_P                  | -39.812634 |
| test/reward_per_eps            | -40        |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.40791008 |
| stats_o/std                    | 0.04019056 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.198     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -39.879833 |
| test/Q_plus_P                  | -39.879833 |
| test/reward_per_eps            | -40        |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.40792832  |
| stats_o/std                    | 0.040194567 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.919044  |
| test/Q_plus_P                  | -39.919044  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.40793145 |
| stats_o/std                    | 0.04019739 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -39.873573 |
| test/Q_plus_P                  | -39.873573 |
| test/reward_per_eps            | -40        |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.40793887  |
| stats_o/std                    | 0.040202837 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -39.817196  |
| test/Q_plus_P                  | -39.817196  |
| test/reward_per_eps            | -40         |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.40793642  |
| stats_o/std                    | 0.040199637 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.873825  |
| test/Q_plus_P                  | -39.873825  |
| test/reward_per_eps            | -40         |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.40794495  |
| stats_o/std                    | 0.040199813 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.903976  |
| test/Q_plus_P                  | -39.903976  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.40794837  |
| stats_o/std                    | 0.040203925 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.882217  |
| test/Q_plus_P                  | -39.882217  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.40795627  |
| stats_o/std                    | 0.040206634 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -39.87642   |
| test/Q_plus_P                  | -39.87642   |
| test/reward_per_eps            | -40         |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.40795743  |
| stats_o/std                    | 0.040207673 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.890842  |
| test/Q_plus_P                  | -39.890842  |
| test/reward_per_eps            | -40         |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.40796387 |
| stats_o/std                    | 0.04021005 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.8798   |
| test/Q_plus_P                  | -39.8798   |
| test/reward_per_eps            | -40        |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.407974    |
| stats_o/std                    | 0.040213224 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.876015  |
| test/Q_plus_P                  | -39.876015  |
| test/reward_per_eps            | -40         |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.40797904  |
| stats_o/std                    | 0.040213864 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.857998  |
| test/Q_plus_P                  | -39.857998  |
| test/reward_per_eps            | -40         |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.40799138  |
| stats_o/std                    | 0.040212322 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.89897   |
| test/Q_plus_P                  | -39.89897   |
| test/reward_per_eps            | -40         |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.40799832 |
| stats_o/std                    | 0.04021579 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.87088  |
| test/Q_plus_P                  | -39.87088  |
| test/reward_per_eps            | -40        |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 508        |
| stats_o/mean                   | 0.40799978 |
| stats_o/std                    | 0.04021351 |
| test/episodes                  | 5090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.876312 |
| test/Q_plus_P                  | -39.876312 |
| test/reward_per_eps            | -40        |
| test/steps                     | 203600     |
| train/episodes                 | 20360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 814400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.40800846  |
| stats_o/std                    | 0.040209804 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.213      |
| test/info_shaping_reward_mean  | -0.265      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.8826    |
| test/Q_plus_P                  | -39.8826    |
| test/reward_per_eps            | -40         |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.4080138   |
| stats_o/std                    | 0.040207338 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.910217  |
| test/Q_plus_P                  | -39.910217  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.40799463  |
| stats_o/std                    | 0.040208478 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.90397   |
| test/Q_plus_P                  | -39.90397   |
| test/reward_per_eps            | -40         |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.4080111   |
| stats_o/std                    | 0.040205993 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.937557  |
| test/Q_plus_P                  | -39.937557  |
| test/reward_per_eps            | -40         |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.40801442 |
| stats_o/std                    | 0.0402027  |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.8957   |
| test/Q_plus_P                  | -39.8957   |
| test/reward_per_eps            | -40        |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.4080141  |
| stats_o/std                    | 0.04021032 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -39.877316 |
| test/Q_plus_P                  | -39.877316 |
| test/reward_per_eps            | -40        |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.40803465  |
| stats_o/std                    | 0.040215116 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.915558  |
| test/Q_plus_P                  | -39.915558  |
| test/reward_per_eps            | -40         |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.40804172  |
| stats_o/std                    | 0.040217657 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.958702  |
| test/Q_plus_P                  | -39.958702  |
| test/reward_per_eps            | -40         |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.40804636  |
| stats_o/std                    | 0.040215153 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.940758  |
| test/Q_plus_P                  | -39.940758  |
| test/reward_per_eps            | -40         |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.4080656  |
| stats_o/std                    | 0.04022309 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.127     |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -39.907406 |
| test/Q_plus_P                  | -39.907406 |
| test/reward_per_eps            | -40        |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.40806842  |
| stats_o/std                    | 0.040222235 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.915943  |
| test/Q_plus_P                  | -39.915943  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.408071    |
| stats_o/std                    | 0.040217854 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.91127   |
| test/Q_plus_P                  | -39.91127   |
| test/reward_per_eps            | -40         |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.4080609   |
| stats_o/std                    | 0.040217306 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -39.925602  |
| test/Q_plus_P                  | -39.925602  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.377      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.40805784  |
| stats_o/std                    | 0.040213108 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.92484   |
| test/Q_plus_P                  | -39.92484   |
| test/reward_per_eps            | -40         |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.40805224  |
| stats_o/std                    | 0.040212817 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.928677  |
| test/Q_plus_P                  | -39.928677  |
| test/reward_per_eps            | -40         |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.40803948  |
| stats_o/std                    | 0.040210646 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.88917   |
| test/Q_plus_P                  | -39.88917   |
| test/reward_per_eps            | -40         |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.40805364 |
| stats_o/std                    | 0.04021383 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.92487  |
| test/Q_plus_P                  | -39.92487  |
| test/reward_per_eps            | -40        |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.40806386  |
| stats_o/std                    | 0.040209815 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.9278    |
| test/Q_plus_P                  | -39.9278    |
| test/reward_per_eps            | -40         |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.40806636 |
| stats_o/std                    | 0.04020714 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.907436 |
| test/Q_plus_P                  | -39.907436 |
| test/reward_per_eps            | -40        |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.40807915 |
| stats_o/std                    | 0.04021048 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.932602 |
| test/Q_plus_P                  | -39.932602 |
| test/reward_per_eps            | -40        |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 529        |
| stats_o/mean                   | 0.40808472 |
| stats_o/std                    | 0.04020583 |
| test/episodes                  | 5300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.882797 |
| test/Q_plus_P                  | -39.882797 |
| test/reward_per_eps            | -40        |
| test/steps                     | 212000     |
| train/episodes                 | 21200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 848000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.4080951   |
| stats_o/std                    | 0.040205244 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -39.933956  |
| test/Q_plus_P                  | -39.933956  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.40809643 |
| stats_o/std                    | 0.04020474 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -39.94548  |
| test/Q_plus_P                  | -39.94548  |
| test/reward_per_eps            | -40        |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.4080961  |
| stats_o/std                    | 0.04020111 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.923523 |
| test/Q_plus_P                  | -39.923523 |
| test/reward_per_eps            | -40        |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.40810406  |
| stats_o/std                    | 0.040196564 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.91213   |
| test/Q_plus_P                  | -39.91213   |
| test/reward_per_eps            | -40         |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.4080969   |
| stats_o/std                    | 0.040194374 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.889206  |
| test/Q_plus_P                  | -39.889206  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.40810838  |
| stats_o/std                    | 0.040195193 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -39.936047  |
| test/Q_plus_P                  | -39.936047  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.40810463  |
| stats_o/std                    | 0.040191703 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -39.895958  |
| test/Q_plus_P                  | -39.895958  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.40810403  |
| stats_o/std                    | 0.040191628 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.93672   |
| test/Q_plus_P                  | -39.93672   |
| test/reward_per_eps            | -40         |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.408111   |
| stats_o/std                    | 0.04018787 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.90826  |
| test/Q_plus_P                  | -39.90826  |
| test/reward_per_eps            | -40        |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.4081228   |
| stats_o/std                    | 0.040186536 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.902412  |
| test/Q_plus_P                  | -39.902412  |
| test/reward_per_eps            | -40         |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.4081353   |
| stats_o/std                    | 0.040188033 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.91303   |
| test/Q_plus_P                  | -39.91303   |
| test/reward_per_eps            | -40         |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.40815052  |
| stats_o/std                    | 0.040188003 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.959435  |
| test/Q_plus_P                  | -39.959435  |
| test/reward_per_eps            | -40         |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.40815708  |
| stats_o/std                    | 0.040197443 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.90896   |
| test/Q_plus_P                  | -39.90896   |
| test/reward_per_eps            | -40         |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.4081637   |
| stats_o/std                    | 0.040195946 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.929497  |
| test/Q_plus_P                  | -39.929497  |
| test/reward_per_eps            | -40         |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.4081702   |
| stats_o/std                    | 0.040200196 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.8774    |
| test/Q_plus_P                  | -39.8774    |
| test/reward_per_eps            | -40         |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.40818906  |
| stats_o/std                    | 0.040199924 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.83998   |
| test/Q_plus_P                  | -39.83998   |
| test/reward_per_eps            | -40         |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.40819272  |
| stats_o/std                    | 0.040205564 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.936188  |
| test/Q_plus_P                  | -39.936188  |
| test/reward_per_eps            | -40         |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.40819713  |
| stats_o/std                    | 0.040203515 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.867176  |
| test/Q_plus_P                  | -39.867176  |
| test/reward_per_eps            | -40         |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.4082085   |
| stats_o/std                    | 0.040206127 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.965508  |
| test/Q_plus_P                  | -39.965508  |
| test/reward_per_eps            | -40         |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.40820897  |
| stats_o/std                    | 0.040202837 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.91635   |
| test/Q_plus_P                  | -39.91635   |
| test/reward_per_eps            | -40         |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.4082183   |
| stats_o/std                    | 0.040201448 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -39.946484  |
| test/Q_plus_P                  | -39.946484  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.40822157  |
| stats_o/std                    | 0.040204547 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.87041   |
| test/Q_plus_P                  | -39.87041   |
| test/reward_per_eps            | -40         |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.40822634  |
| stats_o/std                    | 0.040202223 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.93952   |
| test/Q_plus_P                  | -39.93952   |
| test/reward_per_eps            | -40         |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.40823326  |
| stats_o/std                    | 0.040198464 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.926983  |
| test/Q_plus_P                  | -39.926983  |
| test/reward_per_eps            | -40         |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.4082475  |
| stats_o/std                    | 0.04020271 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.93849  |
| test/Q_plus_P                  | -39.93849  |
| test/reward_per_eps            | -40        |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.4082532   |
| stats_o/std                    | 0.040198755 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.94082   |
| test/Q_plus_P                  | -39.94082   |
| test/reward_per_eps            | -40         |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.40827227 |
| stats_o/std                    | 0.04020428 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.911243 |
| test/Q_plus_P                  | -39.911243 |
| test/reward_per_eps            | -40        |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.40828064  |
| stats_o/std                    | 0.040205035 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -39.94568   |
| test/Q_plus_P                  | -39.94568   |
| test/reward_per_eps            | -40         |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.4082782   |
| stats_o/std                    | 0.040202823 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.944725  |
| test/Q_plus_P                  | -39.944725  |
| test/reward_per_eps            | -40         |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.40828323  |
| stats_o/std                    | 0.040207386 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.94075   |
| test/Q_plus_P                  | -39.94075   |
| test/reward_per_eps            | -40         |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.40827766  |
| stats_o/std                    | 0.040204097 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -39.957882  |
| test/Q_plus_P                  | -39.957882  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.40829027 |
| stats_o/std                    | 0.04021    |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.94785  |
| test/Q_plus_P                  | -39.94785  |
| test/reward_per_eps            | -40        |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.40829012  |
| stats_o/std                    | 0.040209047 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.917133  |
| test/Q_plus_P                  | -39.917133  |
| test/reward_per_eps            | -40         |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.40829322 |
| stats_o/std                    | 0.04021584 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.949562 |
| test/Q_plus_P                  | -39.949562 |
| test/reward_per_eps            | -40        |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.4082946  |
| stats_o/std                    | 0.04022034 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -39.947464 |
| test/Q_plus_P                  | -39.947464 |
| test/reward_per_eps            | -40        |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.40830764  |
| stats_o/std                    | 0.040218845 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.90122   |
| test/Q_plus_P                  | -39.90122   |
| test/reward_per_eps            | -40         |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.40830886  |
| stats_o/std                    | 0.040217385 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.899036  |
| test/Q_plus_P                  | -39.899036  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.40831295  |
| stats_o/std                    | 0.040224865 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.933876  |
| test/Q_plus_P                  | -39.933876  |
| test/reward_per_eps            | -40         |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.40830716  |
| stats_o/std                    | 0.040223975 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.976704  |
| test/Q_plus_P                  | -39.976704  |
| test/reward_per_eps            | -40         |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.4083155   |
| stats_o/std                    | 0.040224444 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -39.940292  |
| test/Q_plus_P                  | -39.940292  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.40831408 |
| stats_o/std                    | 0.04023261 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.89231  |
| test/Q_plus_P                  | -39.89231  |
| test/reward_per_eps            | -40        |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.40832436 |
| stats_o/std                    | 0.04023074 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.90871  |
| test/Q_plus_P                  | -39.90871  |
| test/reward_per_eps            | -40        |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.4083289   |
| stats_o/std                    | 0.040233742 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -40.000023  |
| test/Q_plus_P                  | -40.000023  |
| test/reward_per_eps            | -40         |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.40833542  |
| stats_o/std                    | 0.040229026 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.944824  |
| test/Q_plus_P                  | -39.944824  |
| test/reward_per_eps            | -40         |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.40833688  |
| stats_o/std                    | 0.040229965 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.90829   |
| test/Q_plus_P                  | -39.90829   |
| test/reward_per_eps            | -40         |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.40833953  |
| stats_o/std                    | 0.040231355 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.944477  |
| test/Q_plus_P                  | -39.944477  |
| test/reward_per_eps            | -40         |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.40834382  |
| stats_o/std                    | 0.040233586 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.9737    |
| test/Q_plus_P                  | -39.9737    |
| test/reward_per_eps            | -40         |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.4083463   |
| stats_o/std                    | 0.040237438 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.205      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.96144   |
| test/Q_plus_P                  | -39.96144   |
| test/reward_per_eps            | -40         |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.40835658 |
| stats_o/std                    | 0.04024514 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -40.00404  |
| test/Q_plus_P                  | -40.00404  |
| test/reward_per_eps            | -40        |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.4083599   |
| stats_o/std                    | 0.040249277 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.94742   |
| test/Q_plus_P                  | -39.94742   |
| test/reward_per_eps            | -40         |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.40836605 |
| stats_o/std                    | 0.04025037 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.208     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.928875 |
| test/Q_plus_P                  | -39.928875 |
| test/reward_per_eps            | -40        |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.40836772  |
| stats_o/std                    | 0.040254313 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -39.94231   |
| test/Q_plus_P                  | -39.94231   |
| test/reward_per_eps            | -40         |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.40837643 |
| stats_o/std                    | 0.04025834 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -39.903343 |
| test/Q_plus_P                  | -39.903343 |
| test/reward_per_eps            | -40        |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.40838718  |
| stats_o/std                    | 0.040259007 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.961376  |
| test/Q_plus_P                  | -39.961376  |
| test/reward_per_eps            | -40         |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.40838733  |
| stats_o/std                    | 0.040255956 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.9479    |
| test/Q_plus_P                  | -39.9479    |
| test/reward_per_eps            | -40         |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.40839672  |
| stats_o/std                    | 0.040264104 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.960377  |
| test/Q_plus_P                  | -39.960377  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.40839967  |
| stats_o/std                    | 0.040267676 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -39.95494   |
| test/Q_plus_P                  | -39.95494   |
| test/reward_per_eps            | -40         |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.40839055  |
| stats_o/std                    | 0.040271357 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.923107  |
| test/Q_plus_P                  | -39.923107  |
| test/reward_per_eps            | -40         |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.40838602 |
| stats_o/std                    | 0.04027399 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -40.038788 |
| test/Q_plus_P                  | -40.038788 |
| test/reward_per_eps            | -40        |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.40838525 |
| stats_o/std                    | 0.0402743  |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.9614   |
| test/Q_plus_P                  | -39.9614   |
| test/reward_per_eps            | -40        |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.40839425  |
| stats_o/std                    | 0.040276784 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.945232  |
| test/Q_plus_P                  | -39.945232  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.40840992  |
| stats_o/std                    | 0.040278744 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.905174  |
| test/Q_plus_P                  | -39.905174  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.40841746 |
| stats_o/std                    | 0.04028467 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -39.9505   |
| test/Q_plus_P                  | -39.9505   |
| test/reward_per_eps            | -40        |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.40843782  |
| stats_o/std                    | 0.040288623 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.95192   |
| test/Q_plus_P                  | -39.95192   |
| test/reward_per_eps            | -40         |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.40845525  |
| stats_o/std                    | 0.040286824 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.95083   |
| test/Q_plus_P                  | -39.95083   |
| test/reward_per_eps            | -40         |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.40847054  |
| stats_o/std                    | 0.040287368 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.916336  |
| test/Q_plus_P                  | -39.916336  |
| test/reward_per_eps            | -40         |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.40847388 |
| stats_o/std                    | 0.04029097 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -39.952892 |
| test/Q_plus_P                  | -39.952892 |
| test/reward_per_eps            | -40        |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.40849373 |
| stats_o/std                    | 0.04029351 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.207     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -39.949486 |
| test/Q_plus_P                  | -39.949486 |
| test/reward_per_eps            | -40        |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.4084954   |
| stats_o/std                    | 0.040294066 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.97571   |
| test/Q_plus_P                  | -39.97571   |
| test/reward_per_eps            | -40         |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.40849063  |
| stats_o/std                    | 0.040299956 |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.97489   |
| test/Q_plus_P                  | -39.97489   |
| test/reward_per_eps            | -40         |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
