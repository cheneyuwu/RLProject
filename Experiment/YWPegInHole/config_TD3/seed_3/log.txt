Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3/seed_3
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.43124208  |
| stats_o/std                    | 0.061552238 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.127      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.479      |
| test/Q                         | -1.3436074  |
| test/Q_plus_P                  | -1.3436074  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.269      |
| train/info_shaping_reward_min  | -0.45       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.42395672  |
| stats_o/std                    | 0.053168755 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -1.6449171  |
| test/Q_plus_P                  | -1.6449171  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.4191618  |
| stats_o/std                    | 0.05053745 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -1.9554083 |
| test/Q_plus_P                  | -1.9554083 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.417164    |
| stats_o/std                    | 0.047469627 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -2.3287144  |
| test/Q_plus_P                  | -2.3287144  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.41515115 |
| stats_o/std                    | 0.0450875  |
| test/episodes                  | 50         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -2.741546  |
| test/Q_plus_P                  | -2.741546  |
| test/reward_per_eps            | -40        |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.4140661  |
| stats_o/std                    | 0.04320137 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -3.1439033 |
| test/Q_plus_P                  | -3.1439033 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.18      |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.295     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.41361728 |
| stats_o/std                    | 0.04200332 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -3.5617719 |
| test/Q_plus_P                  | -3.5617719 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.41366324  |
| stats_o/std                    | 0.040906038 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -3.967771   |
| test/Q_plus_P                  | -3.967771   |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.181      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.41325685  |
| stats_o/std                    | 0.039875884 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -4.3835793  |
| test/Q_plus_P                  | -4.3835793  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.41299298  |
| stats_o/std                    | 0.038943585 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -4.7871666  |
| test/Q_plus_P                  | -4.7871666  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.4125555   |
| stats_o/std                    | 0.038561475 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -5.220507   |
| test/Q_plus_P                  | -5.220507   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.4127486   |
| stats_o/std                    | 0.038092013 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -5.599419   |
| test/Q_plus_P                  | -5.599419   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.4127479   |
| stats_o/std                    | 0.037628878 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -6.0069866  |
| test/Q_plus_P                  | -6.0069866  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.41275835  |
| stats_o/std                    | 0.037450653 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -6.396289   |
| test/Q_plus_P                  | -6.396289   |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.4127487   |
| stats_o/std                    | 0.037288852 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -6.792903   |
| test/Q_plus_P                  | -6.792903   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.41282198  |
| stats_o/std                    | 0.037007656 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -7.1559014  |
| test/Q_plus_P                  | -7.1559014  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.4129176   |
| stats_o/std                    | 0.036620606 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -7.565344   |
| test/Q_plus_P                  | -7.565344   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.41312623  |
| stats_o/std                    | 0.036424305 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -7.9602523  |
| test/Q_plus_P                  | -7.9602523  |
| test/reward_per_eps            | -40         |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.41309214 |
| stats_o/std                    | 0.0362869  |
| test/episodes                  | 190        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -8.319995  |
| test/Q_plus_P                  | -8.319995  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.41328606  |
| stats_o/std                    | 0.036146376 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -8.687636   |
| test/Q_plus_P                  | -8.687636   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.41333497 |
| stats_o/std                    | 0.03595813 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -9.068371  |
| test/Q_plus_P                  | -9.068371  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.41343486 |
| stats_o/std                    | 0.03582387 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -9.434953  |
| test/Q_plus_P                  | -9.434953  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.41348353 |
| stats_o/std                    | 0.03558682 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -9.806812  |
| test/Q_plus_P                  | -9.806812  |
| test/reward_per_eps            | -40        |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.4135431   |
| stats_o/std                    | 0.035460852 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -10.150011  |
| test/Q_plus_P                  | -10.150011  |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.4135686   |
| stats_o/std                    | 0.035404302 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -10.524671  |
| test/Q_plus_P                  | -10.524671  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.41364536  |
| stats_o/std                    | 0.035351668 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -10.873887  |
| test/Q_plus_P                  | -10.873887  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.4134685   |
| stats_o/std                    | 0.035374973 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -11.211538  |
| test/Q_plus_P                  | -11.211538  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.4134867  |
| stats_o/std                    | 0.03530885 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -11.555471 |
| test/Q_plus_P                  | -11.555471 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.41327393 |
| stats_o/std                    | 0.0352297  |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.209     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -11.894846 |
| test/Q_plus_P                  | -11.894846 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.4131935  |
| stats_o/std                    | 0.03518874 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -12.250204 |
| test/Q_plus_P                  | -12.250204 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.413209    |
| stats_o/std                    | 0.035175446 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -12.581805  |
| test/Q_plus_P                  | -12.581805  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.413303   |
| stats_o/std                    | 0.03509802 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -12.909443 |
| test/Q_plus_P                  | -12.909443 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.4132929   |
| stats_o/std                    | 0.035053395 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -13.235068  |
| test/Q_plus_P                  | -13.235068  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.41319135  |
| stats_o/std                    | 0.034962866 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -13.5599985 |
| test/Q_plus_P                  | -13.5599985 |
| test/reward_per_eps            | -40         |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.41313758 |
| stats_o/std                    | 0.03501473 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.209     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -13.887686 |
| test/Q_plus_P                  | -13.887686 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.4130188  |
| stats_o/std                    | 0.03501551 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -14.196536 |
| test/Q_plus_P                  | -14.196536 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.41308978 |
| stats_o/std                    | 0.03501452 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.198     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -14.498846 |
| test/Q_plus_P                  | -14.498846 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.41309193  |
| stats_o/std                    | 0.034971155 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -14.81583   |
| test/Q_plus_P                  | -14.81583   |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.41310382 |
| stats_o/std                    | 0.03498467 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -15.126534 |
| test/Q_plus_P                  | -15.126534 |
| test/reward_per_eps            | -40        |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.4129578   |
| stats_o/std                    | 0.035001863 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -15.403271  |
| test/Q_plus_P                  | -15.403271  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.41276017 |
| stats_o/std                    | 0.03496781 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -15.717688 |
| test/Q_plus_P                  | -15.717688 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.298     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.4127153   |
| stats_o/std                    | 0.034992125 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -16.004475  |
| test/Q_plus_P                  | -16.004475  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.41278946  |
| stats_o/std                    | 0.034969352 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -16.306154  |
| test/Q_plus_P                  | -16.306154  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.4126754  |
| stats_o/std                    | 0.03496684 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -16.601866 |
| test/Q_plus_P                  | -16.601866 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 44         |
| stats_o/mean                   | 0.41272047 |
| stats_o/std                    | 0.03501909 |
| test/episodes                  | 450        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -16.888237 |
| test/Q_plus_P                  | -16.888237 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18000      |
| train/episodes                 | 1800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 72000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.4127728   |
| stats_o/std                    | 0.034990076 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -17.208374  |
| test/Q_plus_P                  | -17.208374  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.211      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.412766    |
| stats_o/std                    | 0.035027314 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -17.467379  |
| test/Q_plus_P                  | -17.467379  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.41271612  |
| stats_o/std                    | 0.035095543 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -17.72045   |
| test/Q_plus_P                  | -17.72045   |
| test/reward_per_eps            | -40         |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.41253185 |
| stats_o/std                    | 0.03509931 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -17.987288 |
| test/Q_plus_P                  | -17.987288 |
| test/reward_per_eps            | -40        |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.4125      |
| stats_o/std                    | 0.035200704 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -18.252602  |
| test/Q_plus_P                  | -18.252602  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.41255367 |
| stats_o/std                    | 0.03527182 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -18.539625 |
| test/Q_plus_P                  | -18.539625 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.4124874  |
| stats_o/std                    | 0.03527419 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -18.771387 |
| test/Q_plus_P                  | -18.771387 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.41238198 |
| stats_o/std                    | 0.03532974 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -19.069572 |
| test/Q_plus_P                  | -19.069572 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.41224596  |
| stats_o/std                    | 0.035328235 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -19.315071  |
| test/Q_plus_P                  | -19.315071  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.41224185 |
| stats_o/std                    | 0.03537481 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -19.561184 |
| test/Q_plus_P                  | -19.561184 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.4122105  |
| stats_o/std                    | 0.03539768 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -19.7993   |
| test/Q_plus_P                  | -19.7993   |
| test/reward_per_eps            | -40        |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.41210377 |
| stats_o/std                    | 0.03538727 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -20.053204 |
| test/Q_plus_P                  | -20.053204 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.4119905   |
| stats_o/std                    | 0.035414588 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -20.308867  |
| test/Q_plus_P                  | -20.308867  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.4119282   |
| stats_o/std                    | 0.035410676 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -20.56495   |
| test/Q_plus_P                  | -20.56495   |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.4118793   |
| stats_o/std                    | 0.035492226 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -20.794775  |
| test/Q_plus_P                  | -20.794775  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.41189408  |
| stats_o/std                    | 0.035567302 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -21.03583   |
| test/Q_plus_P                  | -21.03583   |
| test/reward_per_eps            | -40         |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.4118042   |
| stats_o/std                    | 0.035637088 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -21.234663  |
| test/Q_plus_P                  | -21.234663  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.41171834  |
| stats_o/std                    | 0.035689566 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -21.469624  |
| test/Q_plus_P                  | -21.469624  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.4116569   |
| stats_o/std                    | 0.035721965 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -21.707117  |
| test/Q_plus_P                  | -21.707117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.41168186 |
| stats_o/std                    | 0.03574041 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -21.947393 |
| test/Q_plus_P                  | -21.947393 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.112     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.41159225  |
| stats_o/std                    | 0.035778474 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -22.137558  |
| test/Q_plus_P                  | -22.137558  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.41149604  |
| stats_o/std                    | 0.035907228 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -22.379517  |
| test/Q_plus_P                  | -22.379517  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.41145205 |
| stats_o/std                    | 0.03593299 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.138     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -22.562433 |
| test/Q_plus_P                  | -22.562433 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.41139463 |
| stats_o/std                    | 0.03598803 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -22.815971 |
| test/Q_plus_P                  | -22.815971 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.41136757  |
| stats_o/std                    | 0.036107067 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.122      |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -23.012623  |
| test/Q_plus_P                  | -23.012623  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.41132626 |
| stats_o/std                    | 0.03620466 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -23.219805 |
| test/Q_plus_P                  | -23.219805 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.41132173  |
| stats_o/std                    | 0.036235254 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -23.430496  |
| test/Q_plus_P                  | -23.430496  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.4113551   |
| stats_o/std                    | 0.036285065 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -23.627949  |
| test/Q_plus_P                  | -23.627949  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.41132137 |
| stats_o/std                    | 0.03629342 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -23.843794 |
| test/Q_plus_P                  | -23.843794 |
| test/reward_per_eps            | -40        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.4112843   |
| stats_o/std                    | 0.036316656 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -24.037727  |
| test/Q_plus_P                  | -24.037727  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.41122997 |
| stats_o/std                    | 0.03636806 |
| test/episodes                  | 760        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -24.216534 |
| test/Q_plus_P                  | -24.216534 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.41113234  |
| stats_o/std                    | 0.036398023 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -24.415274  |
| test/Q_plus_P                  | -24.415274  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.41120014 |
| stats_o/std                    | 0.03643224 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -24.647192 |
| test/Q_plus_P                  | -24.647192 |
| test/reward_per_eps            | -40        |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.41111484  |
| stats_o/std                    | 0.036495563 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -24.807117  |
| test/Q_plus_P                  | -24.807117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.41105422 |
| stats_o/std                    | 0.03651736 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -25.017778 |
| test/Q_plus_P                  | -25.017778 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.41104344  |
| stats_o/std                    | 0.036575247 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -25.159151  |
| test/Q_plus_P                  | -25.159151  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.41102362 |
| stats_o/std                    | 0.03662157 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -25.367016 |
| test/Q_plus_P                  | -25.367016 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.41099396  |
| stats_o/std                    | 0.036691632 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -25.541601  |
| test/Q_plus_P                  | -25.541601  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.41098967  |
| stats_o/std                    | 0.036712635 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -25.7321    |
| test/Q_plus_P                  | -25.7321    |
| test/reward_per_eps            | -40         |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.41094175  |
| stats_o/std                    | 0.036764037 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -25.924988  |
| test/Q_plus_P                  | -25.924988  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.41084707  |
| stats_o/std                    | 0.036793076 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -26.071575  |
| test/Q_plus_P                  | -26.071575  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.4108435   |
| stats_o/std                    | 0.036788527 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -26.221687  |
| test/Q_plus_P                  | -26.221687  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.41088042 |
| stats_o/std                    | 0.03683913 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -26.431347 |
| test/Q_plus_P                  | -26.431347 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.4108992  |
| stats_o/std                    | 0.0368719  |
| test/episodes                  | 890        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -26.592382 |
| test/Q_plus_P                  | -26.592382 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.41099688  |
| stats_o/std                    | 0.036921117 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -26.817837  |
| test/Q_plus_P                  | -26.817837  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.41094303 |
| stats_o/std                    | 0.03698948 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -26.904486 |
| test/Q_plus_P                  | -26.904486 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.41087654  |
| stats_o/std                    | 0.036979128 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -27.083853  |
| test/Q_plus_P                  | -27.083853  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.4108654  |
| stats_o/std                    | 0.03702611 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -27.252268 |
| test/Q_plus_P                  | -27.252268 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.410774    |
| stats_o/std                    | 0.037050758 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -27.41654   |
| test/Q_plus_P                  | -27.41654   |
| test/reward_per_eps            | -40         |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.4107056   |
| stats_o/std                    | 0.037080284 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -27.565159  |
| test/Q_plus_P                  | -27.565159  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 95         |
| stats_o/mean                   | 0.4106988  |
| stats_o/std                    | 0.03712651 |
| test/episodes                  | 960        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -27.742823 |
| test/Q_plus_P                  | -27.742823 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38400      |
| train/episodes                 | 3840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 153600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.4106226   |
| stats_o/std                    | 0.037148442 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -27.866524  |
| test/Q_plus_P                  | -27.866524  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.41062275 |
| stats_o/std                    | 0.03717949 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -28.006193 |
| test/Q_plus_P                  | -28.006193 |
| test/reward_per_eps            | -40        |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.41062573  |
| stats_o/std                    | 0.037196107 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -28.1457    |
| test/Q_plus_P                  | -28.1457    |
| test/reward_per_eps            | -40         |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.41067967 |
| stats_o/std                    | 0.03725661 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -28.286812 |
| test/Q_plus_P                  | -28.286812 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.41066742 |
| stats_o/std                    | 0.0372682  |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -28.471352 |
| test/Q_plus_P                  | -28.471352 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.41058877  |
| stats_o/std                    | 0.037293408 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -28.578056  |
| test/Q_plus_P                  | -28.578056  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.410619   |
| stats_o/std                    | 0.03733294 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.227     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -28.724072 |
| test/Q_plus_P                  | -28.724072 |
| test/reward_per_eps            | -40        |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.4105779   |
| stats_o/std                    | 0.037340865 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -28.861866  |
| test/Q_plus_P                  | -28.861866  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.4105872  |
| stats_o/std                    | 0.03736837 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -28.98823  |
| test/Q_plus_P                  | -28.98823  |
| test/reward_per_eps            | -40        |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.410606   |
| stats_o/std                    | 0.03739385 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.188     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -29.120367 |
| test/Q_plus_P                  | -29.120367 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.41061187 |
| stats_o/std                    | 0.03739956 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.335     |
| test/Q                         | -29.265282 |
| test/Q_plus_P                  | -29.265282 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.4105995   |
| stats_o/std                    | 0.037487198 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.208      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -29.417583  |
| test/Q_plus_P                  | -29.417583  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.269      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.41065776 |
| stats_o/std                    | 0.03752445 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -29.574608 |
| test/Q_plus_P                  | -29.574608 |
| test/reward_per_eps            | -40        |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.41062653 |
| stats_o/std                    | 0.03756683 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -29.620972 |
| test/Q_plus_P                  | -29.620972 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.4106369   |
| stats_o/std                    | 0.037582193 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -29.768126  |
| test/Q_plus_P                  | -29.768126  |
| test/reward_per_eps            | -40         |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.4106345   |
| stats_o/std                    | 0.037587352 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -29.90449   |
| test/Q_plus_P                  | -29.90449   |
| test/reward_per_eps            | -40         |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.41064024  |
| stats_o/std                    | 0.037620097 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -30.034458  |
| test/Q_plus_P                  | -30.034458  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.41061652  |
| stats_o/std                    | 0.037650246 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -30.176685  |
| test/Q_plus_P                  | -30.176685  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.4105811   |
| stats_o/std                    | 0.037670024 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -30.315298  |
| test/Q_plus_P                  | -30.315298  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.4106671   |
| stats_o/std                    | 0.037706856 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -30.402992  |
| test/Q_plus_P                  | -30.402992  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.41066626 |
| stats_o/std                    | 0.03772998 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -30.52694  |
| test/Q_plus_P                  | -30.52694  |
| test/reward_per_eps            | -40        |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.4106426  |
| stats_o/std                    | 0.03776223 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -30.65605  |
| test/Q_plus_P                  | -30.65605  |
| test/reward_per_eps            | -40        |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.41057685 |
| stats_o/std                    | 0.03781385 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -30.752525 |
| test/Q_plus_P                  | -30.752525 |
| test/reward_per_eps            | -40        |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.41058132  |
| stats_o/std                    | 0.037839666 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -30.879414  |
| test/Q_plus_P                  | -30.879414  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.41051564  |
| stats_o/std                    | 0.037864167 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -30.978786  |
| test/Q_plus_P                  | -30.978786  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.4104762  |
| stats_o/std                    | 0.0379061  |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -31.055777 |
| test/Q_plus_P                  | -31.055777 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.41040882 |
| stats_o/std                    | 0.03790998 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.355     |
| test/Q                         | -31.173891 |
| test/Q_plus_P                  | -31.173891 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.4104085  |
| stats_o/std                    | 0.03794611 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.189     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -31.2992   |
| test/Q_plus_P                  | -31.2992   |
| test/reward_per_eps            | -40        |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.41041028  |
| stats_o/std                    | 0.037966993 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -31.423906  |
| test/Q_plus_P                  | -31.423906  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.41044533  |
| stats_o/std                    | 0.038005784 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -31.503897  |
| test/Q_plus_P                  | -31.503897  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.4104446   |
| stats_o/std                    | 0.038021386 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -31.635103  |
| test/Q_plus_P                  | -31.635103  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.41042113  |
| stats_o/std                    | 0.038022198 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -31.696894  |
| test/Q_plus_P                  | -31.696894  |
| test/reward_per_eps            | -40         |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.41040698 |
| stats_o/std                    | 0.03803306 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -31.922285 |
| test/Q_plus_P                  | -31.922285 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.41042212  |
| stats_o/std                    | 0.038081452 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -31.973398  |
| test/Q_plus_P                  | -31.973398  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.41039824  |
| stats_o/std                    | 0.038105614 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -32.040394  |
| test/Q_plus_P                  | -32.040394  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.4103965   |
| stats_o/std                    | 0.038151734 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -32.121723  |
| test/Q_plus_P                  | -32.121723  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.4103644   |
| stats_o/std                    | 0.038150944 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -32.21376   |
| test/Q_plus_P                  | -32.21376   |
| test/reward_per_eps            | -40         |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.4103262  |
| stats_o/std                    | 0.03815926 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.321     |
| test/Q                         | -32.305893 |
| test/Q_plus_P                  | -32.305893 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.41033614  |
| stats_o/std                    | 0.038162816 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -32.392174  |
| test/Q_plus_P                  | -32.392174  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.4102744   |
| stats_o/std                    | 0.038186453 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -32.507877  |
| test/Q_plus_P                  | -32.507877  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.179      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.41027212 |
| stats_o/std                    | 0.03820594 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -32.61156  |
| test/Q_plus_P                  | -32.61156  |
| test/reward_per_eps            | -40        |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.41020927  |
| stats_o/std                    | 0.038228255 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -32.737186  |
| test/Q_plus_P                  | -32.737186  |
| test/reward_per_eps            | -40         |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.41020676 |
| stats_o/std                    | 0.03823056 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -32.771114 |
| test/Q_plus_P                  | -32.771114 |
| test/reward_per_eps            | -40        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.4102457   |
| stats_o/std                    | 0.038232964 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -32.888924  |
| test/Q_plus_P                  | -32.888924  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.41019395  |
| stats_o/std                    | 0.038263272 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -32.967613  |
| test/Q_plus_P                  | -32.967613  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.179      |
| train/info_shaping_reward_mean | -0.262      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.41019675 |
| stats_o/std                    | 0.03828155 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -33.065556 |
| test/Q_plus_P                  | -33.065556 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.41022548  |
| stats_o/std                    | 0.038323667 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -33.16302   |
| test/Q_plus_P                  | -33.16302   |
| test/reward_per_eps            | -40         |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.41019693  |
| stats_o/std                    | 0.038350936 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -33.216446  |
| test/Q_plus_P                  | -33.216446  |
| test/reward_per_eps            | -40         |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.4101785   |
| stats_o/std                    | 0.038363624 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -33.25894   |
| test/Q_plus_P                  | -33.25894   |
| test/reward_per_eps            | -40         |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.41013458  |
| stats_o/std                    | 0.038362667 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -33.372196  |
| test/Q_plus_P                  | -33.372196  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.4100988  |
| stats_o/std                    | 0.03838325 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -33.461254 |
| test/Q_plus_P                  | -33.461254 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.4100996   |
| stats_o/std                    | 0.038374115 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -33.55952   |
| test/Q_plus_P                  | -33.55952   |
| test/reward_per_eps            | -40         |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.41011545  |
| stats_o/std                    | 0.038401596 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -33.63076   |
| test/Q_plus_P                  | -33.63076   |
| test/reward_per_eps            | -40         |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.41013017  |
| stats_o/std                    | 0.038424697 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -33.68263   |
| test/Q_plus_P                  | -33.68263   |
| test/reward_per_eps            | -40         |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.41009986 |
| stats_o/std                    | 0.03842823 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -33.81772  |
| test/Q_plus_P                  | -33.81772  |
| test/reward_per_eps            | -40        |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.41006675 |
| stats_o/std                    | 0.03844685 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -33.956715 |
| test/Q_plus_P                  | -33.956715 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.41008472 |
| stats_o/std                    | 0.03845529 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -33.92396  |
| test/Q_plus_P                  | -33.92396  |
| test/reward_per_eps            | -40        |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.41009042  |
| stats_o/std                    | 0.038464513 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -34.009495  |
| test/Q_plus_P                  | -34.009495  |
| test/reward_per_eps            | -40         |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.41010603  |
| stats_o/std                    | 0.038472675 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -34.092308  |
| test/Q_plus_P                  | -34.092308  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.410086   |
| stats_o/std                    | 0.03851674 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -34.171112 |
| test/Q_plus_P                  | -34.171112 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.41007796 |
| stats_o/std                    | 0.03852162 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -34.24822  |
| test/Q_plus_P                  | -34.24822  |
| test/reward_per_eps            | -40        |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.4100803   |
| stats_o/std                    | 0.038548596 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -34.28698   |
| test/Q_plus_P                  | -34.28698   |
| test/reward_per_eps            | -40         |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 158        |
| stats_o/mean                   | 0.410094   |
| stats_o/std                    | 0.03856979 |
| test/episodes                  | 1590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -34.39564  |
| test/Q_plus_P                  | -34.39564  |
| test/reward_per_eps            | -40        |
| test/steps                     | 63600      |
| train/episodes                 | 6360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 254400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.4100802  |
| stats_o/std                    | 0.03857729 |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.204     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -34.458298 |
| test/Q_plus_P                  | -34.458298 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.41005957  |
| stats_o/std                    | 0.038613062 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -34.518364  |
| test/Q_plus_P                  | -34.518364  |
| test/reward_per_eps            | -40         |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 161        |
| stats_o/mean                   | 0.41005194 |
| stats_o/std                    | 0.03862779 |
| test/episodes                  | 1620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.197     |
| test/info_shaping_reward_mean  | -0.273     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -34.5965   |
| test/Q_plus_P                  | -34.5965   |
| test/reward_per_eps            | -40        |
| test/steps                     | 64800      |
| train/episodes                 | 6480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 259200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.41006002  |
| stats_o/std                    | 0.038665686 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.355      |
| test/Q                         | -34.71082   |
| test/Q_plus_P                  | -34.71082   |
| test/reward_per_eps            | -40         |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.4100561  |
| stats_o/std                    | 0.03868932 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.355     |
| test/Q                         | -34.72423  |
| test/Q_plus_P                  | -34.72423  |
| test/reward_per_eps            | -40        |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.177     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.41002858 |
| stats_o/std                    | 0.03871694 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -34.79299  |
| test/Q_plus_P                  | -34.79299  |
| test/reward_per_eps            | -40        |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.263     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.41003954  |
| stats_o/std                    | 0.038741104 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -34.803726  |
| test/Q_plus_P                  | -34.803726  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.41000584  |
| stats_o/std                    | 0.038763877 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.352      |
| test/Q                         | -34.876522  |
| test/Q_plus_P                  | -34.876522  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.269      |
| train/info_shaping_reward_min  | -0.389      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.40999007  |
| stats_o/std                    | 0.038773317 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -34.986187  |
| test/Q_plus_P                  | -34.986187  |
| test/reward_per_eps            | -40         |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.410004    |
| stats_o/std                    | 0.038802337 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -35.041573  |
| test/Q_plus_P                  | -35.041573  |
| test/reward_per_eps            | -40         |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.4100246   |
| stats_o/std                    | 0.038814966 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -35.06859   |
| test/Q_plus_P                  | -35.06859   |
| test/reward_per_eps            | -40         |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.41006854  |
| stats_o/std                    | 0.038842026 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -35.17025   |
| test/Q_plus_P                  | -35.17025   |
| test/reward_per_eps            | -40         |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.4100401   |
| stats_o/std                    | 0.038851377 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -35.21124   |
| test/Q_plus_P                  | -35.21124   |
| test/reward_per_eps            | -40         |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.41003326 |
| stats_o/std                    | 0.03886034 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -35.27405  |
| test/Q_plus_P                  | -35.27405  |
| test/reward_per_eps            | -40        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.175     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.41005337 |
| stats_o/std                    | 0.0388801  |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -35.331802 |
| test/Q_plus_P                  | -35.331802 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.377     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.4100504   |
| stats_o/std                    | 0.038912404 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -35.4002    |
| test/Q_plus_P                  | -35.4002    |
| test/reward_per_eps            | -40         |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.4100274   |
| stats_o/std                    | 0.038900223 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -35.445675  |
| test/Q_plus_P                  | -35.445675  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.40999737  |
| stats_o/std                    | 0.038889926 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.373      |
| test/Q                         | -35.514282  |
| test/Q_plus_P                  | -35.514282  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.40996626  |
| stats_o/std                    | 0.038882803 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -35.592545  |
| test/Q_plus_P                  | -35.592545  |
| test/reward_per_eps            | -40         |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.40995082  |
| stats_o/std                    | 0.038887702 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -35.6552    |
| test/Q_plus_P                  | -35.6552    |
| test/reward_per_eps            | -40         |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.40992698  |
| stats_o/std                    | 0.038884085 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.339      |
| test/Q                         | -35.68152   |
| test/Q_plus_P                  | -35.68152   |
| test/reward_per_eps            | -40         |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.409937   |
| stats_o/std                    | 0.03890607 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -35.665283 |
| test/Q_plus_P                  | -35.665283 |
| test/reward_per_eps            | -40        |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.40995848  |
| stats_o/std                    | 0.038915906 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.28       |
| test/info_shaping_reward_min   | -0.358      |
| test/Q                         | -35.824673  |
| test/Q_plus_P                  | -35.824673  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.4099499   |
| stats_o/std                    | 0.038928334 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -35.912613  |
| test/Q_plus_P                  | -35.912613  |
| test/reward_per_eps            | -40         |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.40996262  |
| stats_o/std                    | 0.038934425 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -35.890175  |
| test/Q_plus_P                  | -35.890175  |
| test/reward_per_eps            | -40         |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.40994278  |
| stats_o/std                    | 0.038944248 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -35.94286   |
| test/Q_plus_P                  | -35.94286   |
| test/reward_per_eps            | -40         |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.381      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.4098945  |
| stats_o/std                    | 0.03897886 |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -36.007935 |
| test/Q_plus_P                  | -36.007935 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.261     |
| train/info_shaping_reward_min  | -0.382     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.40985468 |
| stats_o/std                    | 0.03899371 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.327     |
| test/Q                         | -36.021584 |
| test/Q_plus_P                  | -36.021584 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.40981746 |
| stats_o/std                    | 0.03898417 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.268     |
| test/info_shaping_reward_min   | -0.343     |
| test/Q                         | -36.06623  |
| test/Q_plus_P                  | -36.06623  |
| test/reward_per_eps            | -40        |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.40981618 |
| stats_o/std                    | 0.0389946  |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.343     |
| test/Q                         | -36.136112 |
| test/Q_plus_P                  | -36.136112 |
| test/reward_per_eps            | -40        |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.40979874 |
| stats_o/std                    | 0.03899875 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -36.219776 |
| test/Q_plus_P                  | -36.219776 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.40977833 |
| stats_o/std                    | 0.03901468 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -36.223385 |
| test/Q_plus_P                  | -36.223385 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.40980604 |
| stats_o/std                    | 0.03903313 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -36.25636  |
| test/Q_plus_P                  | -36.25636  |
| test/reward_per_eps            | -40        |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.40980217  |
| stats_o/std                    | 0.039052624 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -36.322227  |
| test/Q_plus_P                  | -36.322227  |
| test/reward_per_eps            | -40         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.4098228  |
| stats_o/std                    | 0.03905763 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.334     |
| test/Q                         | -36.366745 |
| test/Q_plus_P                  | -36.366745 |
| test/reward_per_eps            | -40        |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.40983835 |
| stats_o/std                    | 0.03905405 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -36.37355  |
| test/Q_plus_P                  | -36.37355  |
| test/reward_per_eps            | -40        |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 195       |
| stats_o/mean                   | 0.4098027 |
| stats_o/std                    | 0.0390621 |
| test/episodes                  | 1960      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.145    |
| test/info_shaping_reward_mean  | -0.221    |
| test/info_shaping_reward_min   | -0.281    |
| test/Q                         | -36.46427 |
| test/Q_plus_P                  | -36.46427 |
| test/reward_per_eps            | -40       |
| test/steps                     | 78400     |
| train/episodes                 | 7840      |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.155    |
| train/info_shaping_reward_mean | -0.249    |
| train/info_shaping_reward_min  | -0.335    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 313600    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.40976322 |
| stats_o/std                    | 0.03908246 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -36.5474   |
| test/Q_plus_P                  | -36.5474   |
| test/reward_per_eps            | -40        |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.176     |
| train/info_shaping_reward_mean | -0.263     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.40973723  |
| stats_o/std                    | 0.039077956 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -36.58279   |
| test/Q_plus_P                  | -36.58279   |
| test/reward_per_eps            | -40         |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.40973878  |
| stats_o/std                    | 0.039090466 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -36.62628   |
| test/Q_plus_P                  | -36.62628   |
| test/reward_per_eps            | -40         |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.40974852 |
| stats_o/std                    | 0.03908607 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.201     |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -36.673622 |
| test/Q_plus_P                  | -36.673622 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.40970623 |
| stats_o/std                    | 0.03911616 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.261     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -36.757572 |
| test/Q_plus_P                  | -36.757572 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.181     |
| train/info_shaping_reward_mean | -0.27      |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.40972996 |
| stats_o/std                    | 0.03911893 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -36.71522  |
| test/Q_plus_P                  | -36.71522  |
| test/reward_per_eps            | -40        |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.4096931   |
| stats_o/std                    | 0.039150696 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -36.73684   |
| test/Q_plus_P                  | -36.73684   |
| test/reward_per_eps            | -40         |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.381      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.4096589   |
| stats_o/std                    | 0.039161824 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -36.852646  |
| test/Q_plus_P                  | -36.852646  |
| test/reward_per_eps            | -40         |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.40965208  |
| stats_o/std                    | 0.039170835 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -36.720776  |
| test/Q_plus_P                  | -36.720776  |
| test/reward_per_eps            | -40         |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.4096756   |
| stats_o/std                    | 0.039179005 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -36.91985   |
| test/Q_plus_P                  | -36.91985   |
| test/reward_per_eps            | -40         |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.40964213  |
| stats_o/std                    | 0.039188046 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -36.9682    |
| test/Q_plus_P                  | -36.9682    |
| test/reward_per_eps            | -40         |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.4096192  |
| stats_o/std                    | 0.03919436 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -36.93386  |
| test/Q_plus_P                  | -36.93386  |
| test/reward_per_eps            | -40        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.4096031   |
| stats_o/std                    | 0.039190583 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.36       |
| test/Q                         | -37.014328  |
| test/Q_plus_P                  | -37.014328  |
| test/reward_per_eps            | -40         |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.40957332  |
| stats_o/std                    | 0.039222922 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -37.007328  |
| test/Q_plus_P                  | -37.007328  |
| test/reward_per_eps            | -40         |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.27       |
| train/info_shaping_reward_min  | -0.402      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.40953365 |
| stats_o/std                    | 0.03923196 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.334     |
| test/Q                         | -37.102516 |
| test/Q_plus_P                  | -37.102516 |
| test/reward_per_eps            | -40        |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.40951076  |
| stats_o/std                    | 0.039242823 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.269      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -37.068455  |
| test/Q_plus_P                  | -37.068455  |
| test/reward_per_eps            | -40         |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.40947628  |
| stats_o/std                    | 0.039252546 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.203      |
| test/info_shaping_reward_mean  | -0.285      |
| test/info_shaping_reward_min   | -0.337      |
| test/Q                         | -37.10164   |
| test/Q_plus_P                  | -37.10164   |
| test/reward_per_eps            | -40         |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.379      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.40947768  |
| stats_o/std                    | 0.039257634 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.264      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -37.144886  |
| test/Q_plus_P                  | -37.144886  |
| test/reward_per_eps            | -40         |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.40942442 |
| stats_o/std                    | 0.03926063 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.355     |
| test/Q                         | -37.17369  |
| test/Q_plus_P                  | -37.17369  |
| test/reward_per_eps            | -40        |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.181     |
| train/info_shaping_reward_mean | -0.268     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.40938663  |
| stats_o/std                    | 0.039267413 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -37.197716  |
| test/Q_plus_P                  | -37.197716  |
| test/reward_per_eps            | -40         |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.40935883 |
| stats_o/std                    | 0.03927957 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.349     |
| test/Q                         | -37.240826 |
| test/Q_plus_P                  | -37.240826 |
| test/reward_per_eps            | -40        |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.40934512  |
| stats_o/std                    | 0.039286654 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -37.282677  |
| test/Q_plus_P                  | -37.282677  |
| test/reward_per_eps            | -40         |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.40933284  |
| stats_o/std                    | 0.039297685 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -37.27487   |
| test/Q_plus_P                  | -37.27487   |
| test/reward_per_eps            | -40         |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.40929243  |
| stats_o/std                    | 0.039299536 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -37.356213  |
| test/Q_plus_P                  | -37.356213  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.181      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.40926978 |
| stats_o/std                    | 0.03932113 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.254     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -37.379704 |
| test/Q_plus_P                  | -37.379704 |
| test/reward_per_eps            | -40        |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.27      |
| train/info_shaping_reward_min  | -0.378     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.40925404  |
| stats_o/std                    | 0.039320916 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.362      |
| test/Q                         | -37.424435  |
| test/Q_plus_P                  | -37.424435  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.40922984  |
| stats_o/std                    | 0.039317083 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.372      |
| test/Q                         | -37.409252  |
| test/Q_plus_P                  | -37.409252  |
| test/reward_per_eps            | -40         |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.4092115   |
| stats_o/std                    | 0.039326183 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -37.523075  |
| test/Q_plus_P                  | -37.523075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.183      |
| train/info_shaping_reward_mean | -0.267      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.4091942  |
| stats_o/std                    | 0.0393313  |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.274     |
| test/info_shaping_reward_min   | -0.359     |
| test/Q                         | -37.533653 |
| test/Q_plus_P                  | -37.533653 |
| test/reward_per_eps            | -40        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.40920427  |
| stats_o/std                    | 0.039334398 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -37.54852   |
| test/Q_plus_P                  | -37.54852   |
| test/reward_per_eps            | -40         |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.40919116  |
| stats_o/std                    | 0.039342757 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.207      |
| test/info_shaping_reward_mean  | -0.29       |
| test/info_shaping_reward_min   | -0.362      |
| test/Q                         | -37.58368   |
| test/Q_plus_P                  | -37.58368   |
| test/reward_per_eps            | -40         |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.40917215 |
| stats_o/std                    | 0.03936019 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.271     |
| test/info_shaping_reward_min   | -0.364     |
| test/Q                         | -37.59021  |
| test/Q_plus_P                  | -37.59021  |
| test/reward_per_eps            | -40        |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.18      |
| train/info_shaping_reward_mean | -0.269     |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.40913585  |
| stats_o/std                    | 0.039382707 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.37       |
| test/Q                         | -37.6435    |
| test/Q_plus_P                  | -37.6435    |
| test/reward_per_eps            | -40         |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.194      |
| train/info_shaping_reward_mean | -0.276      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.409104   |
| stats_o/std                    | 0.0393803  |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.271     |
| test/info_shaping_reward_min   | -0.354     |
| test/Q                         | -37.647194 |
| test/Q_plus_P                  | -37.647194 |
| test/reward_per_eps            | -40        |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.40905726 |
| stats_o/std                    | 0.03939294 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -37.748955 |
| test/Q_plus_P                  | -37.748955 |
| test/reward_per_eps            | -40        |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.179     |
| train/info_shaping_reward_mean | -0.261     |
| train/info_shaping_reward_min  | -0.369     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.40902507  |
| stats_o/std                    | 0.039392922 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -37.7312    |
| test/Q_plus_P                  | -37.7312    |
| test/reward_per_eps            | -40         |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.4089769   |
| stats_o/std                    | 0.039397504 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -37.7631    |
| test/Q_plus_P                  | -37.7631    |
| test/reward_per_eps            | -40         |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.4089475   |
| stats_o/std                    | 0.039400145 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.221      |
| test/info_shaping_reward_mean  | -0.284      |
| test/info_shaping_reward_min   | -0.367      |
| test/Q                         | -37.839115  |
| test/Q_plus_P                  | -37.839115  |
| test/reward_per_eps            | -40         |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.40892336 |
| stats_o/std                    | 0.03940272 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.327     |
| test/Q                         | -37.866505 |
| test/Q_plus_P                  | -37.866505 |
| test/reward_per_eps            | -40        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.40890515  |
| stats_o/std                    | 0.039399695 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.272      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -37.84635   |
| test/Q_plus_P                  | -37.84635   |
| test/reward_per_eps            | -40         |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.40890715  |
| stats_o/std                    | 0.039406355 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -37.912674  |
| test/Q_plus_P                  | -37.912674  |
| test/reward_per_eps            | -40         |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.40889156 |
| stats_o/std                    | 0.03941602 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.268     |
| test/info_shaping_reward_min   | -0.355     |
| test/Q                         | -37.816578 |
| test/Q_plus_P                  | -37.816578 |
| test/reward_per_eps            | -40        |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.40884933  |
| stats_o/std                    | 0.039426208 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.36       |
| test/Q                         | -38.031094  |
| test/Q_plus_P                  | -38.031094  |
| test/reward_per_eps            | -40         |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.182      |
| train/info_shaping_reward_mean | -0.271      |
| train/info_shaping_reward_min  | -0.377      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.4088243  |
| stats_o/std                    | 0.039432   |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.292     |
| test/info_shaping_reward_min   | -0.408     |
| test/Q                         | -37.904205 |
| test/Q_plus_P                  | -37.904205 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.378     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.4087747  |
| stats_o/std                    | 0.03944423 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.361     |
| test/Q                         | -37.992455 |
| test/Q_plus_P                  | -37.992455 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.268     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.408744    |
| stats_o/std                    | 0.039443128 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.344      |
| test/Q                         | -37.94507   |
| test/Q_plus_P                  | -37.94507   |
| test/reward_per_eps            | -40         |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.262      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.40872768  |
| stats_o/std                    | 0.039449863 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.39       |
| test/Q                         | -38.036366  |
| test/Q_plus_P                  | -38.036366  |
| test/reward_per_eps            | -40         |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.4086831  |
| stats_o/std                    | 0.0394415  |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -38.067005 |
| test/Q_plus_P                  | -38.067005 |
| test/reward_per_eps            | -40        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.4086804  |
| stats_o/std                    | 0.03944185 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -38.079544 |
| test/Q_plus_P                  | -38.079544 |
| test/reward_per_eps            | -40        |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.40865967  |
| stats_o/std                    | 0.039459508 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.271      |
| test/info_shaping_reward_min   | -0.348      |
| test/Q                         | -38.114357  |
| test/Q_plus_P                  | -38.114357  |
| test/reward_per_eps            | -40         |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.189      |
| train/info_shaping_reward_mean | -0.274      |
| train/info_shaping_reward_min  | -0.39       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.4086393   |
| stats_o/std                    | 0.039454516 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -38.17665   |
| test/Q_plus_P                  | -38.17665   |
| test/reward_per_eps            | -40         |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.40860403 |
| stats_o/std                    | 0.03946813 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -38.136295 |
| test/Q_plus_P                  | -38.136295 |
| test/reward_per_eps            | -40        |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.40857944 |
| stats_o/std                    | 0.03946858 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -38.22239  |
| test/Q_plus_P                  | -38.22239  |
| test/reward_per_eps            | -40        |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.4085492  |
| stats_o/std                    | 0.03947813 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -38.21622  |
| test/Q_plus_P                  | -38.21622  |
| test/reward_per_eps            | -40        |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.40852356  |
| stats_o/std                    | 0.039480533 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -38.210033  |
| test/Q_plus_P                  | -38.210033  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.40849182  |
| stats_o/std                    | 0.039487988 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -38.26985   |
| test/Q_plus_P                  | -38.26985   |
| test/reward_per_eps            | -40         |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.40849328 |
| stats_o/std                    | 0.03950088 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.195     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -38.31955  |
| test/Q_plus_P                  | -38.31955  |
| test/reward_per_eps            | -40        |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.40846443  |
| stats_o/std                    | 0.039503064 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.297      |
| test/info_shaping_reward_min   | -0.375      |
| test/Q                         | -38.28721   |
| test/Q_plus_P                  | -38.28721   |
| test/reward_per_eps            | -40         |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.40840268  |
| stats_o/std                    | 0.039503086 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.264      |
| test/info_shaping_reward_min   | -0.359      |
| test/Q                         | -38.342403  |
| test/Q_plus_P                  | -38.342403  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.4083718  |
| stats_o/std                    | 0.03952647 |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -38.306488 |
| test/Q_plus_P                  | -38.306488 |
| test/reward_per_eps            | -40        |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.182     |
| train/info_shaping_reward_mean | -0.274     |
| train/info_shaping_reward_min  | -0.382     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.40833429 |
| stats_o/std                    | 0.03952637 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -38.378258 |
| test/Q_plus_P                  | -38.378258 |
| test/reward_per_eps            | -40        |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.40833172 |
| stats_o/std                    | 0.03952096 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.374     |
| test/Q                         | -38.339333 |
| test/Q_plus_P                  | -38.339333 |
| test/reward_per_eps            | -40        |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.40831637  |
| stats_o/std                    | 0.039526723 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -38.402206  |
| test/Q_plus_P                  | -38.402206  |
| test/reward_per_eps            | -40         |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.408298   |
| stats_o/std                    | 0.03953646 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.374     |
| test/Q                         | -38.308548 |
| test/Q_plus_P                  | -38.308548 |
| test/reward_per_eps            | -40        |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.4082757   |
| stats_o/std                    | 0.039532077 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -38.428776  |
| test/Q_plus_P                  | -38.428776  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.4082571   |
| stats_o/std                    | 0.039536025 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -38.489532  |
| test/Q_plus_P                  | -38.489532  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.40824267  |
| stats_o/std                    | 0.039545834 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -38.44388   |
| test/Q_plus_P                  | -38.44388   |
| test/reward_per_eps            | -40         |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.40823445  |
| stats_o/std                    | 0.039541103 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -38.497036  |
| test/Q_plus_P                  | -38.497036  |
| test/reward_per_eps            | -40         |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.40820143 |
| stats_o/std                    | 0.03954586 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.326     |
| test/Q                         | -38.524456 |
| test/Q_plus_P                  | -38.524456 |
| test/reward_per_eps            | -40        |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.40817288 |
| stats_o/std                    | 0.03955833 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.322     |
| test/Q                         | -38.54373  |
| test/Q_plus_P                  | -38.54373  |
| test/reward_per_eps            | -40        |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.40814137  |
| stats_o/std                    | 0.039567687 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -38.61226   |
| test/Q_plus_P                  | -38.61226   |
| test/reward_per_eps            | -40         |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.40812734 |
| stats_o/std                    | 0.03956608 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -38.61612  |
| test/Q_plus_P                  | -38.61612  |
| test/reward_per_eps            | -40        |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.40809044  |
| stats_o/std                    | 0.039577324 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -38.592064  |
| test/Q_plus_P                  | -38.592064  |
| test/reward_per_eps            | -40         |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.276      |
| train/info_shaping_reward_min  | -0.391      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.40806356 |
| stats_o/std                    | 0.03958808 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -38.61741  |
| test/Q_plus_P                  | -38.61741  |
| test/reward_per_eps            | -40        |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.178     |
| train/info_shaping_reward_mean | -0.265     |
| train/info_shaping_reward_min  | -0.386     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.40803483  |
| stats_o/std                    | 0.039598532 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -38.6324    |
| test/Q_plus_P                  | -38.6324    |
| test/reward_per_eps            | -40         |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.40801182  |
| stats_o/std                    | 0.039607372 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -38.637447  |
| test/Q_plus_P                  | -38.637447  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.377      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.4080286   |
| stats_o/std                    | 0.039611943 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -38.665226  |
| test/Q_plus_P                  | -38.665226  |
| test/reward_per_eps            | -40         |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.40799165 |
| stats_o/std                    | 0.03961851 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.226     |
| test/info_shaping_reward_mean  | -0.282     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -38.723812 |
| test/Q_plus_P                  | -38.723812 |
| test/reward_per_eps            | -40        |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.40799606 |
| stats_o/std                    | 0.03962702 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.208     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -38.684444 |
| test/Q_plus_P                  | -38.684444 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.40798047  |
| stats_o/std                    | 0.039636314 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.357      |
| test/Q                         | -38.7082    |
| test/Q_plus_P                  | -38.7082    |
| test/reward_per_eps            | -40         |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.381      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.4079654  |
| stats_o/std                    | 0.03963825 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.207     |
| test/info_shaping_reward_mean  | -0.288     |
| test/info_shaping_reward_min   | -0.364     |
| test/Q                         | -38.73409  |
| test/Q_plus_P                  | -38.73409  |
| test/reward_per_eps            | -40        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.40794337 |
| stats_o/std                    | 0.03964249 |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.356     |
| test/Q                         | -38.75248  |
| test/Q_plus_P                  | -38.75248  |
| test/reward_per_eps            | -40        |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.175     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.40792787 |
| stats_o/std                    | 0.03965956 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.194     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -38.773026 |
| test/Q_plus_P                  | -38.773026 |
| test/reward_per_eps            | -40        |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.27      |
| train/info_shaping_reward_min  | -0.398     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.40789855  |
| stats_o/std                    | 0.039658155 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.353      |
| test/Q                         | -38.779163  |
| test/Q_plus_P                  | -38.779163  |
| test/reward_per_eps            | -40         |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.40786517 |
| stats_o/std                    | 0.03968058 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -38.819008 |
| test/Q_plus_P                  | -38.819008 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.183     |
| train/info_shaping_reward_mean | -0.269     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.40784958 |
| stats_o/std                    | 0.03968338 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.204     |
| test/info_shaping_reward_mean  | -0.273     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -38.794155 |
| test/Q_plus_P                  | -38.794155 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.183     |
| train/info_shaping_reward_mean | -0.265     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.4078593  |
| stats_o/std                    | 0.03968812 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.266     |
| test/info_shaping_reward_min   | -0.343     |
| test/Q                         | -38.854828 |
| test/Q_plus_P                  | -38.854828 |
| test/reward_per_eps            | -40        |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.40785697 |
| stats_o/std                    | 0.03968663 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -38.823116 |
| test/Q_plus_P                  | -38.823116 |
| test/reward_per_eps            | -40        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.4078499  |
| stats_o/std                    | 0.03968491 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -38.869724 |
| test/Q_plus_P                  | -38.869724 |
| test/reward_per_eps            | -40        |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.40784916  |
| stats_o/std                    | 0.039681133 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -38.86721   |
| test/Q_plus_P                  | -38.86721   |
| test/reward_per_eps            | -40         |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.40786052 |
| stats_o/std                    | 0.03967888 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -38.931217 |
| test/Q_plus_P                  | -38.931217 |
| test/reward_per_eps            | -40        |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.4078499  |
| stats_o/std                    | 0.03967512 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -38.89874  |
| test/Q_plus_P                  | -38.89874  |
| test/reward_per_eps            | -40        |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.4078598   |
| stats_o/std                    | 0.039682236 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -38.862747  |
| test/Q_plus_P                  | -38.862747  |
| test/reward_per_eps            | -40         |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.40786543  |
| stats_o/std                    | 0.039682794 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.348      |
| test/Q                         | -38.906296  |
| test/Q_plus_P                  | -38.906296  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.40786052 |
| stats_o/std                    | 0.03969301 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.344     |
| test/Q                         | -38.944405 |
| test/Q_plus_P                  | -38.944405 |
| test/reward_per_eps            | -40        |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.261     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.40785637  |
| stats_o/std                    | 0.039694883 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -38.972874  |
| test/Q_plus_P                  | -38.972874  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.4078374   |
| stats_o/std                    | 0.039692286 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -38.93269   |
| test/Q_plus_P                  | -38.93269   |
| test/reward_per_eps            | -40         |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.40785646  |
| stats_o/std                    | 0.039702833 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -38.97996   |
| test/Q_plus_P                  | -38.97996   |
| test/reward_per_eps            | -40         |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.40785122 |
| stats_o/std                    | 0.03971406 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.08974  |
| test/Q_plus_P                  | -39.08974  |
| test/reward_per_eps            | -40        |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.183     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.4078144   |
| stats_o/std                    | 0.039720338 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.01266   |
| test/Q_plus_P                  | -39.01266   |
| test/reward_per_eps            | -40         |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.40781906  |
| stats_o/std                    | 0.039718248 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -38.92577   |
| test/Q_plus_P                  | -38.92577   |
| test/reward_per_eps            | -40         |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.407814   |
| stats_o/std                    | 0.0397233  |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.019337 |
| test/Q_plus_P                  | -39.019337 |
| test/reward_per_eps            | -40        |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.40780023  |
| stats_o/std                    | 0.039725836 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.022434  |
| test/Q_plus_P                  | -39.022434  |
| test/reward_per_eps            | -40         |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.40780112  |
| stats_o/std                    | 0.039727394 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.042557  |
| test/Q_plus_P                  | -39.042557  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.40780762  |
| stats_o/std                    | 0.039738413 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.362      |
| test/Q                         | -39.04808   |
| test/Q_plus_P                  | -39.04808   |
| test/reward_per_eps            | -40         |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.383      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.4078121  |
| stats_o/std                    | 0.03974385 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.271     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -39.07402  |
| test/Q_plus_P                  | -39.07402  |
| test/reward_per_eps            | -40        |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.40781465  |
| stats_o/std                    | 0.039744385 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.08068   |
| test/Q_plus_P                  | -39.08068   |
| test/reward_per_eps            | -40         |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.4078114  |
| stats_o/std                    | 0.03973851 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -39.036434 |
| test/Q_plus_P                  | -39.036434 |
| test/reward_per_eps            | -40        |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 304        |
| stats_o/mean                   | 0.4078108  |
| stats_o/std                    | 0.03973751 |
| test/episodes                  | 3050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.353     |
| test/Q                         | -39.100597 |
| test/Q_plus_P                  | -39.100597 |
| test/reward_per_eps            | -40        |
| test/steps                     | 122000     |
| train/episodes                 | 12200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 488000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.4078174   |
| stats_o/std                    | 0.039738405 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -39.105244  |
| test/Q_plus_P                  | -39.105244  |
| test/reward_per_eps            | -40         |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.40782347  |
| stats_o/std                    | 0.039745104 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.04023   |
| test/Q_plus_P                  | -39.04023   |
| test/reward_per_eps            | -40         |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.40781832 |
| stats_o/std                    | 0.03973921 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.31099  |
| test/Q_plus_P                  | -39.31099  |
| test/reward_per_eps            | -40        |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.40781426  |
| stats_o/std                    | 0.039734248 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.12509   |
| test/Q_plus_P                  | -39.12509   |
| test/reward_per_eps            | -40         |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.40782365  |
| stats_o/std                    | 0.039726183 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.192738  |
| test/Q_plus_P                  | -39.192738  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.40782118  |
| stats_o/std                    | 0.039727245 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.224228  |
| test/Q_plus_P                  | -39.224228  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.4078044   |
| stats_o/std                    | 0.039737158 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -39.193813  |
| test/Q_plus_P                  | -39.193813  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.40783504 |
| stats_o/std                    | 0.03974723 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.208878 |
| test/Q_plus_P                  | -39.208878 |
| test/reward_per_eps            | -40        |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.407856   |
| stats_o/std                    | 0.03974887 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.194786 |
| test/Q_plus_P                  | -39.194786 |
| test/reward_per_eps            | -40        |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.40787074  |
| stats_o/std                    | 0.039760392 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -39.25522   |
| test/Q_plus_P                  | -39.25522   |
| test/reward_per_eps            | -40         |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.40787628  |
| stats_o/std                    | 0.039758645 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -39.21391   |
| test/Q_plus_P                  | -39.21391   |
| test/reward_per_eps            | -40         |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.4078908   |
| stats_o/std                    | 0.039758142 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.34359   |
| test/Q_plus_P                  | -39.34359   |
| test/reward_per_eps            | -40         |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.4078882   |
| stats_o/std                    | 0.039759804 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.348      |
| test/Q                         | -39.211945  |
| test/Q_plus_P                  | -39.211945  |
| test/reward_per_eps            | -40         |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.4078991  |
| stats_o/std                    | 0.03976312 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.274     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -39.15721  |
| test/Q_plus_P                  | -39.15721  |
| test/reward_per_eps            | -40        |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.4078815   |
| stats_o/std                    | 0.039758045 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.367      |
| test/Q                         | -39.253677  |
| test/Q_plus_P                  | -39.253677  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.40786958  |
| stats_o/std                    | 0.039751682 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.272186  |
| test/Q_plus_P                  | -39.272186  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.4078766   |
| stats_o/std                    | 0.039754227 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.37       |
| test/Q                         | -39.270947  |
| test/Q_plus_P                  | -39.270947  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.40787902  |
| stats_o/std                    | 0.039750364 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.271      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -39.26918   |
| test/Q_plus_P                  | -39.26918   |
| test/reward_per_eps            | -40         |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.40786502  |
| stats_o/std                    | 0.039747242 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -39.222652  |
| test/Q_plus_P                  | -39.222652  |
| test/reward_per_eps            | -40         |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.40786085  |
| stats_o/std                    | 0.039753377 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.265      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -39.267532  |
| test/Q_plus_P                  | -39.267532  |
| test/reward_per_eps            | -40         |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.377      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.4078507   |
| stats_o/std                    | 0.039759442 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.31784   |
| test/Q_plus_P                  | -39.31784   |
| test/reward_per_eps            | -40         |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.40784204  |
| stats_o/std                    | 0.039760817 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.26032   |
| test/Q_plus_P                  | -39.26032   |
| test/reward_per_eps            | -40         |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.4078412   |
| stats_o/std                    | 0.039765287 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.262375  |
| test/Q_plus_P                  | -39.262375  |
| test/reward_per_eps            | -40         |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.4078624   |
| stats_o/std                    | 0.039773505 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.31205   |
| test/Q_plus_P                  | -39.31205   |
| test/reward_per_eps            | -40         |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.40784624  |
| stats_o/std                    | 0.039779466 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.347256  |
| test/Q_plus_P                  | -39.347256  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.40783978  |
| stats_o/std                    | 0.039787818 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.346325  |
| test/Q_plus_P                  | -39.346325  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.40785596 |
| stats_o/std                    | 0.03979532 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.344296 |
| test/Q_plus_P                  | -39.344296 |
| test/reward_per_eps            | -40        |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 332        |
| stats_o/mean                   | 0.40786457 |
| stats_o/std                    | 0.03978583 |
| test/episodes                  | 3330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.357086 |
| test/Q_plus_P                  | -39.357086 |
| test/reward_per_eps            | -40        |
| test/steps                     | 133200     |
| train/episodes                 | 13320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 532800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.40785047 |
| stats_o/std                    | 0.03979761 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -39.343964 |
| test/Q_plus_P                  | -39.343964 |
| test/reward_per_eps            | -40        |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.176     |
| train/info_shaping_reward_mean | -0.269     |
| train/info_shaping_reward_min  | -0.399     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.40782514 |
| stats_o/std                    | 0.03981734 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.355     |
| test/Q                         | -39.356632 |
| test/Q_plus_P                  | -39.356632 |
| test/reward_per_eps            | -40        |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.202     |
| train/info_shaping_reward_mean | -0.286     |
| train/info_shaping_reward_min  | -0.391     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.4078152   |
| stats_o/std                    | 0.039812602 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.36005   |
| test/Q_plus_P                  | -39.36005   |
| test/reward_per_eps            | -40         |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.4078084   |
| stats_o/std                    | 0.039811205 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.39669   |
| test/Q_plus_P                  | -39.39669   |
| test/reward_per_eps            | -40         |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.40778217  |
| stats_o/std                    | 0.039813545 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.271      |
| test/info_shaping_reward_min   | -0.349      |
| test/Q                         | -39.37929   |
| test/Q_plus_P                  | -39.37929   |
| test/reward_per_eps            | -40         |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.40779993  |
| stats_o/std                    | 0.039818157 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.412514  |
| test/Q_plus_P                  | -39.412514  |
| test/reward_per_eps            | -40         |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.40778264  |
| stats_o/std                    | 0.039829325 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.401318  |
| test/Q_plus_P                  | -39.401318  |
| test/reward_per_eps            | -40         |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.377      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.40777096 |
| stats_o/std                    | 0.03982958 |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -39.427643 |
| test/Q_plus_P                  | -39.427643 |
| test/reward_per_eps            | -40        |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.40776846 |
| stats_o/std                    | 0.03983133 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.355     |
| test/Q                         | -39.40128  |
| test/Q_plus_P                  | -39.40128  |
| test/reward_per_eps            | -40        |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.4077656  |
| stats_o/std                    | 0.03983981 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.421947 |
| test/Q_plus_P                  | -39.421947 |
| test/reward_per_eps            | -40        |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.40775278  |
| stats_o/std                    | 0.039846312 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.429493  |
| test/Q_plus_P                  | -39.429493  |
| test/reward_per_eps            | -40         |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.4077401   |
| stats_o/std                    | 0.039854012 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.284      |
| test/info_shaping_reward_min   | -0.362      |
| test/Q                         | -39.421528  |
| test/Q_plus_P                  | -39.421528  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.375      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.4077482  |
| stats_o/std                    | 0.03985594 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.43843  |
| test/Q_plus_P                  | -39.43843  |
| test/reward_per_eps            | -40        |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.4077485  |
| stats_o/std                    | 0.03986262 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.44923  |
| test/Q_plus_P                  | -39.44923  |
| test/reward_per_eps            | -40        |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.40775415  |
| stats_o/std                    | 0.039866824 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.42756   |
| test/Q_plus_P                  | -39.42756   |
| test/reward_per_eps            | -40         |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 348        |
| stats_o/mean                   | 0.40775922 |
| stats_o/std                    | 0.03986517 |
| test/episodes                  | 3490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -39.462025 |
| test/Q_plus_P                  | -39.462025 |
| test/reward_per_eps            | -40        |
| test/steps                     | 139600     |
| train/episodes                 | 13960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 558400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.40776744  |
| stats_o/std                    | 0.039871577 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.445427  |
| test/Q_plus_P                  | -39.445427  |
| test/reward_per_eps            | -40         |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.4077617   |
| stats_o/std                    | 0.039873466 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.48383   |
| test/Q_plus_P                  | -39.48383   |
| test/reward_per_eps            | -40         |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.4077718   |
| stats_o/std                    | 0.039881986 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -39.486164  |
| test/Q_plus_P                  | -39.486164  |
| test/reward_per_eps            | -40         |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.40778324  |
| stats_o/std                    | 0.039884117 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.49525   |
| test/Q_plus_P                  | -39.49525   |
| test/reward_per_eps            | -40         |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.4078057  |
| stats_o/std                    | 0.03988451 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.50577  |
| test/Q_plus_P                  | -39.50577  |
| test/reward_per_eps            | -40        |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.40781233  |
| stats_o/std                    | 0.039890785 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.5313    |
| test/Q_plus_P                  | -39.5313    |
| test/reward_per_eps            | -40         |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.40781984  |
| stats_o/std                    | 0.039888404 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.501083  |
| test/Q_plus_P                  | -39.501083  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.40782404  |
| stats_o/std                    | 0.039890274 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.502377  |
| test/Q_plus_P                  | -39.502377  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.40782169  |
| stats_o/std                    | 0.039889216 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.4634    |
| test/Q_plus_P                  | -39.4634    |
| test/reward_per_eps            | -40         |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.4078231   |
| stats_o/std                    | 0.039891493 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.520912  |
| test/Q_plus_P                  | -39.520912  |
| test/reward_per_eps            | -40         |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.40783194  |
| stats_o/std                    | 0.039898183 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -39.507065  |
| test/Q_plus_P                  | -39.507065  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.40781188 |
| stats_o/std                    | 0.03989388 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.49628  |
| test/Q_plus_P                  | -39.49628  |
| test/reward_per_eps            | -40        |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.40781963 |
| stats_o/std                    | 0.03988599 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -39.529663 |
| test/Q_plus_P                  | -39.529663 |
| test/reward_per_eps            | -40        |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.40782526  |
| stats_o/std                    | 0.039892424 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.575253  |
| test/Q_plus_P                  | -39.575253  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.40783024  |
| stats_o/std                    | 0.039895494 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.542522  |
| test/Q_plus_P                  | -39.542522  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.4078151  |
| stats_o/std                    | 0.03990031 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.521896 |
| test/Q_plus_P                  | -39.521896 |
| test/reward_per_eps            | -40        |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.40782452  |
| stats_o/std                    | 0.039897606 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.524338  |
| test/Q_plus_P                  | -39.524338  |
| test/reward_per_eps            | -40         |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.40783918 |
| stats_o/std                    | 0.03989783 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -39.55307  |
| test/Q_plus_P                  | -39.55307  |
| test/reward_per_eps            | -40        |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.4078513   |
| stats_o/std                    | 0.039903294 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -39.56308   |
| test/Q_plus_P                  | -39.56308   |
| test/reward_per_eps            | -40         |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.407858    |
| stats_o/std                    | 0.039908376 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -39.486263  |
| test/Q_plus_P                  | -39.486263  |
| test/reward_per_eps            | -40         |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.40786135  |
| stats_o/std                    | 0.039916642 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -39.552982  |
| test/Q_plus_P                  | -39.552982  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.40788093  |
| stats_o/std                    | 0.039919514 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.21       |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.540268  |
| test/Q_plus_P                  | -39.540268  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.4078735   |
| stats_o/std                    | 0.039938632 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -39.58234   |
| test/Q_plus_P                  | -39.58234   |
| test/reward_per_eps            | -40         |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.40790567 |
| stats_o/std                    | 0.03994339 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -39.613346 |
| test/Q_plus_P                  | -39.613346 |
| test/reward_per_eps            | -40        |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.40792277 |
| stats_o/std                    | 0.03995308 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.572323 |
| test/Q_plus_P                  | -39.572323 |
| test/reward_per_eps            | -40        |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.4079264  |
| stats_o/std                    | 0.03995981 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.590103 |
| test/Q_plus_P                  | -39.590103 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.4079311  |
| stats_o/std                    | 0.03995948 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.67202  |
| test/Q_plus_P                  | -39.67202  |
| test/reward_per_eps            | -40        |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.40792766 |
| stats_o/std                    | 0.03996026 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -39.553463 |
| test/Q_plus_P                  | -39.553463 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.40794513  |
| stats_o/std                    | 0.039962687 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -39.595913  |
| test/Q_plus_P                  | -39.595913  |
| test/reward_per_eps            | -40         |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.40796018  |
| stats_o/std                    | 0.039963517 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.632538  |
| test/Q_plus_P                  | -39.632538  |
| test/reward_per_eps            | -40         |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.40798628 |
| stats_o/std                    | 0.03996892 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.629948 |
| test/Q_plus_P                  | -39.629948 |
| test/reward_per_eps            | -40        |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.4079878   |
| stats_o/std                    | 0.039983843 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.62678   |
| test/Q_plus_P                  | -39.62678   |
| test/reward_per_eps            | -40         |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.383      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.40799096  |
| stats_o/std                    | 0.039988406 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.61582   |
| test/Q_plus_P                  | -39.61582   |
| test/reward_per_eps            | -40         |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.4079943  |
| stats_o/std                    | 0.03999852 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.609344 |
| test/Q_plus_P                  | -39.609344 |
| test/reward_per_eps            | -40        |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.40800226  |
| stats_o/std                    | 0.039995212 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.63622   |
| test/Q_plus_P                  | -39.63622   |
| test/reward_per_eps            | -40         |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.40800914  |
| stats_o/std                    | 0.039994765 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.639328  |
| test/Q_plus_P                  | -39.639328  |
| test/reward_per_eps            | -40         |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.40800926 |
| stats_o/std                    | 0.04000149 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -39.672165 |
| test/Q_plus_P                  | -39.672165 |
| test/reward_per_eps            | -40        |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.4079993   |
| stats_o/std                    | 0.040000517 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.673847  |
| test/Q_plus_P                  | -39.673847  |
| test/reward_per_eps            | -40         |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.40798983  |
| stats_o/std                    | 0.040001985 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.672695  |
| test/Q_plus_P                  | -39.672695  |
| test/reward_per_eps            | -40         |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.40799868 |
| stats_o/std                    | 0.04000232 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.702824 |
| test/Q_plus_P                  | -39.702824 |
| test/reward_per_eps            | -40        |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.179     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.40799603 |
| stats_o/std                    | 0.03999563 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -39.673317 |
| test/Q_plus_P                  | -39.673317 |
| test/reward_per_eps            | -40        |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.40799654  |
| stats_o/std                    | 0.039996706 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.721203  |
| test/Q_plus_P                  | -39.721203  |
| test/reward_per_eps            | -40         |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.40800115  |
| stats_o/std                    | 0.039998177 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.688805  |
| test/Q_plus_P                  | -39.688805  |
| test/reward_per_eps            | -40         |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.4080156   |
| stats_o/std                    | 0.040000994 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.703022  |
| test/Q_plus_P                  | -39.703022  |
| test/reward_per_eps            | -40         |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.40801343 |
| stats_o/std                    | 0.04000214 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.326     |
| test/Q                         | -39.682022 |
| test/Q_plus_P                  | -39.682022 |
| test/reward_per_eps            | -40        |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 394        |
| stats_o/mean                   | 0.40801513 |
| stats_o/std                    | 0.04000207 |
| test/episodes                  | 3950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.69963  |
| test/Q_plus_P                  | -39.69963  |
| test/reward_per_eps            | -40        |
| test/steps                     | 158000     |
| train/episodes                 | 15800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 632000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.40800428 |
| stats_o/std                    | 0.04000378 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -39.69955  |
| test/Q_plus_P                  | -39.69955  |
| test/reward_per_eps            | -40        |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.40801585 |
| stats_o/std                    | 0.03999925 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -39.70008  |
| test/Q_plus_P                  | -39.70008  |
| test/reward_per_eps            | -40        |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.40800115 |
| stats_o/std                    | 0.04000385 |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -39.694195 |
| test/Q_plus_P                  | -39.694195 |
| test/reward_per_eps            | -40        |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.40799868  |
| stats_o/std                    | 0.040006142 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.719505  |
| test/Q_plus_P                  | -39.719505  |
| test/reward_per_eps            | -40         |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.40800294  |
| stats_o/std                    | 0.040000007 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.35       |
| test/Q                         | -39.665222  |
| test/Q_plus_P                  | -39.665222  |
| test/reward_per_eps            | -40         |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.40800157  |
| stats_o/std                    | 0.039998274 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.736176  |
| test/Q_plus_P                  | -39.736176  |
| test/reward_per_eps            | -40         |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.40798828 |
| stats_o/std                    | 0.03999723 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.134     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.71522  |
| test/Q_plus_P                  | -39.71522  |
| test/reward_per_eps            | -40        |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.40799275  |
| stats_o/std                    | 0.039993364 |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.727818  |
| test/Q_plus_P                  | -39.727818  |
| test/reward_per_eps            | -40         |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.40801254  |
| stats_o/std                    | 0.039997645 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.682163  |
| test/Q_plus_P                  | -39.682163  |
| test/reward_per_eps            | -40         |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.4080194   |
| stats_o/std                    | 0.039993983 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.730873  |
| test/Q_plus_P                  | -39.730873  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.40801418  |
| stats_o/std                    | 0.039996933 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.713753  |
| test/Q_plus_P                  | -39.713753  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.40801206 |
| stats_o/std                    | 0.04000178 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.721382 |
| test/Q_plus_P                  | -39.721382 |
| test/reward_per_eps            | -40        |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.369     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.40802994  |
| stats_o/std                    | 0.040006902 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.744522  |
| test/Q_plus_P                  | -39.744522  |
| test/reward_per_eps            | -40         |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.40803194  |
| stats_o/std                    | 0.040007282 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.807564  |
| test/Q_plus_P                  | -39.807564  |
| test/reward_per_eps            | -40         |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.408047    |
| stats_o/std                    | 0.040013213 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.76624   |
| test/Q_plus_P                  | -39.76624   |
| test/reward_per_eps            | -40         |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.40804803 |
| stats_o/std                    | 0.0400106  |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -39.747498 |
| test/Q_plus_P                  | -39.747498 |
| test/reward_per_eps            | -40        |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.40808108  |
| stats_o/std                    | 0.040021684 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.741665  |
| test/Q_plus_P                  | -39.741665  |
| test/reward_per_eps            | -40         |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.40808442  |
| stats_o/std                    | 0.040023286 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -39.739304  |
| test/Q_plus_P                  | -39.739304  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.4080802   |
| stats_o/std                    | 0.040021528 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.761646  |
| test/Q_plus_P                  | -39.761646  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.40807953 |
| stats_o/std                    | 0.04002266 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -39.779804 |
| test/Q_plus_P                  | -39.779804 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.40807807  |
| stats_o/std                    | 0.040025104 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.74491   |
| test/Q_plus_P                  | -39.74491   |
| test/reward_per_eps            | -40         |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.40806508  |
| stats_o/std                    | 0.040018078 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -39.735077  |
| test/Q_plus_P                  | -39.735077  |
| test/reward_per_eps            | -40         |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.40806663  |
| stats_o/std                    | 0.040011268 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.761112  |
| test/Q_plus_P                  | -39.761112  |
| test/reward_per_eps            | -40         |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.40809336  |
| stats_o/std                    | 0.040013302 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -39.78449   |
| test/Q_plus_P                  | -39.78449   |
| test/reward_per_eps            | -40         |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.4080998  |
| stats_o/std                    | 0.04001348 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.751427 |
| test/Q_plus_P                  | -39.751427 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.40811    |
| stats_o/std                    | 0.04001581 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.771683 |
| test/Q_plus_P                  | -39.771683 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.40810892  |
| stats_o/std                    | 0.040010568 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.21       |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.767868  |
| test/Q_plus_P                  | -39.767868  |
| test/reward_per_eps            | -40         |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.40810916  |
| stats_o/std                    | 0.040011123 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.76265   |
| test/Q_plus_P                  | -39.76265   |
| test/reward_per_eps            | -40         |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.40811312  |
| stats_o/std                    | 0.040011395 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.87925   |
| test/Q_plus_P                  | -39.87925   |
| test/reward_per_eps            | -40         |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.40812448  |
| stats_o/std                    | 0.040015154 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.783176  |
| test/Q_plus_P                  | -39.783176  |
| test/reward_per_eps            | -40         |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 425        |
| stats_o/mean                   | 0.40813836 |
| stats_o/std                    | 0.04001849 |
| test/episodes                  | 4260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -39.770298 |
| test/Q_plus_P                  | -39.770298 |
| test/reward_per_eps            | -40        |
| test/steps                     | 170400     |
| train/episodes                 | 17040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 681600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.408149    |
| stats_o/std                    | 0.040018894 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.77528   |
| test/Q_plus_P                  | -39.77528   |
| test/reward_per_eps            | -40         |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.40816578  |
| stats_o/std                    | 0.040028002 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.777466  |
| test/Q_plus_P                  | -39.777466  |
| test/reward_per_eps            | -40         |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.4081829   |
| stats_o/std                    | 0.040027373 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -39.78735   |
| test/Q_plus_P                  | -39.78735   |
| test/reward_per_eps            | -40         |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.4081966   |
| stats_o/std                    | 0.040033046 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -39.791046  |
| test/Q_plus_P                  | -39.791046  |
| test/reward_per_eps            | -40         |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.40818402  |
| stats_o/std                    | 0.040037557 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.345      |
| test/Q                         | -39.82344   |
| test/Q_plus_P                  | -39.82344   |
| test/reward_per_eps            | -40         |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.40817133  |
| stats_o/std                    | 0.040040538 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.77117   |
| test/Q_plus_P                  | -39.77117   |
| test/reward_per_eps            | -40         |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.40817288  |
| stats_o/std                    | 0.040044595 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -39.781105  |
| test/Q_plus_P                  | -39.781105  |
| test/reward_per_eps            | -40         |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.40818378 |
| stats_o/std                    | 0.04004869 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.797146 |
| test/Q_plus_P                  | -39.797146 |
| test/reward_per_eps            | -40        |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.40818837  |
| stats_o/std                    | 0.040053796 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.791122  |
| test/Q_plus_P                  | -39.791122  |
| test/reward_per_eps            | -40         |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.40819803  |
| stats_o/std                    | 0.040063605 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.79662   |
| test/Q_plus_P                  | -39.79662   |
| test/reward_per_eps            | -40         |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.40820798 |
| stats_o/std                    | 0.04006854 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -39.76174  |
| test/Q_plus_P                  | -39.76174  |
| test/reward_per_eps            | -40        |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.40822533  |
| stats_o/std                    | 0.040076535 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.772816  |
| test/Q_plus_P                  | -39.772816  |
| test/reward_per_eps            | -40         |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.4082264   |
| stats_o/std                    | 0.040078286 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.718456  |
| test/Q_plus_P                  | -39.718456  |
| test/reward_per_eps            | -40         |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.4082404  |
| stats_o/std                    | 0.04008227 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -39.780636 |
| test/Q_plus_P                  | -39.780636 |
| test/reward_per_eps            | -40        |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.40824178  |
| stats_o/std                    | 0.040081277 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.78427   |
| test/Q_plus_P                  | -39.78427   |
| test/reward_per_eps            | -40         |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.40824977 |
| stats_o/std                    | 0.04008456 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.786633 |
| test/Q_plus_P                  | -39.786633 |
| test/reward_per_eps            | -40        |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.40826288  |
| stats_o/std                    | 0.040080007 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.806385  |
| test/Q_plus_P                  | -39.806385  |
| test/reward_per_eps            | -40         |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 443        |
| stats_o/mean                   | 0.40826485 |
| stats_o/std                    | 0.04008608 |
| test/episodes                  | 4440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -39.82214  |
| test/Q_plus_P                  | -39.82214  |
| test/reward_per_eps            | -40        |
| test/steps                     | 177600     |
| train/episodes                 | 17760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 710400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.408275   |
| stats_o/std                    | 0.04009149 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -39.769962 |
| test/Q_plus_P                  | -39.769962 |
| test/reward_per_eps            | -40        |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.40828478 |
| stats_o/std                    | 0.04009525 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.134     |
| test/info_shaping_reward_mean  | -0.191     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -39.802197 |
| test/Q_plus_P                  | -39.802197 |
| test/reward_per_eps            | -40        |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.40829596 |
| stats_o/std                    | 0.04009464 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.124     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.79147  |
| test/Q_plus_P                  | -39.79147  |
| test/reward_per_eps            | -40        |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.4083016   |
| stats_o/std                    | 0.040090855 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.792374  |
| test/Q_plus_P                  | -39.792374  |
| test/reward_per_eps            | -40         |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.40831336  |
| stats_o/std                    | 0.040092338 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.79015   |
| test/Q_plus_P                  | -39.79015   |
| test/reward_per_eps            | -40         |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.4083246  |
| stats_o/std                    | 0.04008771 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.137     |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -39.808    |
| test/Q_plus_P                  | -39.808    |
| test/reward_per_eps            | -40        |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.40833154 |
| stats_o/std                    | 0.04008973 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.804947 |
| test/Q_plus_P                  | -39.804947 |
| test/reward_per_eps            | -40        |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.40833864  |
| stats_o/std                    | 0.040095728 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.856625  |
| test/Q_plus_P                  | -39.856625  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.4083388   |
| stats_o/std                    | 0.040092614 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.79435   |
| test/Q_plus_P                  | -39.79435   |
| test/reward_per_eps            | -40         |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.40833876 |
| stats_o/std                    | 0.04009342 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.82301  |
| test/Q_plus_P                  | -39.82301  |
| test/reward_per_eps            | -40        |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.40835622  |
| stats_o/std                    | 0.040094357 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.127      |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.813644  |
| test/Q_plus_P                  | -39.813644  |
| test/reward_per_eps            | -40         |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.40837178  |
| stats_o/std                    | 0.040098466 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.848312  |
| test/Q_plus_P                  | -39.848312  |
| test/reward_per_eps            | -40         |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.40837482  |
| stats_o/std                    | 0.040110856 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.8311    |
| test/Q_plus_P                  | -39.8311    |
| test/reward_per_eps            | -40         |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.40839043  |
| stats_o/std                    | 0.040111583 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.82184   |
| test/Q_plus_P                  | -39.82184   |
| test/reward_per_eps            | -40         |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.40839127  |
| stats_o/std                    | 0.040119227 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -39.837402  |
| test/Q_plus_P                  | -39.837402  |
| test/reward_per_eps            | -40         |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.4084054  |
| stats_o/std                    | 0.04012021 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.835358 |
| test/Q_plus_P                  | -39.835358 |
| test/reward_per_eps            | -40        |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.40841427  |
| stats_o/std                    | 0.040120292 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.829895  |
| test/Q_plus_P                  | -39.829895  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.40841067  |
| stats_o/std                    | 0.040120546 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.813828  |
| test/Q_plus_P                  | -39.813828  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.40842748 |
| stats_o/std                    | 0.0401277  |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -39.815926 |
| test/Q_plus_P                  | -39.815926 |
| test/reward_per_eps            | -40        |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.187     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.40842673 |
| stats_o/std                    | 0.04012378 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.201     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.824894 |
| test/Q_plus_P                  | -39.824894 |
| test/reward_per_eps            | -40        |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.40843686  |
| stats_o/std                    | 0.040120803 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -39.822742  |
| test/Q_plus_P                  | -39.822742  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.40844187 |
| stats_o/std                    | 0.0401158  |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.813988 |
| test/Q_plus_P                  | -39.813988 |
| test/reward_per_eps            | -40        |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.40844414 |
| stats_o/std                    | 0.04011531 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.932743 |
| test/Q_plus_P                  | -39.932743 |
| test/reward_per_eps            | -40        |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.40844333 |
| stats_o/std                    | 0.04011967 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.802433 |
| test/Q_plus_P                  | -39.802433 |
| test/reward_per_eps            | -40        |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.176     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.4084356  |
| stats_o/std                    | 0.04011113 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.267     |
| test/info_shaping_reward_min   | -0.362     |
| test/Q                         | -39.819286 |
| test/Q_plus_P                  | -39.819286 |
| test/reward_per_eps            | -40        |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.40843126  |
| stats_o/std                    | 0.040109884 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.83991   |
| test/Q_plus_P                  | -39.83991   |
| test/reward_per_eps            | -40         |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.40842137  |
| stats_o/std                    | 0.040117983 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.828068  |
| test/Q_plus_P                  | -39.828068  |
| test/reward_per_eps            | -40         |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.382      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.40842342 |
| stats_o/std                    | 0.04011429 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -39.858727 |
| test/Q_plus_P                  | -39.858727 |
| test/reward_per_eps            | -40        |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.40842697 |
| stats_o/std                    | 0.04011682 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.778057 |
| test/Q_plus_P                  | -39.778057 |
| test/reward_per_eps            | -40        |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.40843037 |
| stats_o/std                    | 0.04011822 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -39.82521  |
| test/Q_plus_P                  | -39.82521  |
| test/reward_per_eps            | -40        |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.40843162  |
| stats_o/std                    | 0.040120717 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.828217  |
| test/Q_plus_P                  | -39.828217  |
| test/reward_per_eps            | -40         |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.4084257   |
| stats_o/std                    | 0.040111016 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.827442  |
| test/Q_plus_P                  | -39.827442  |
| test/reward_per_eps            | -40         |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.40842092  |
| stats_o/std                    | 0.040110435 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.838673  |
| test/Q_plus_P                  | -39.838673  |
| test/reward_per_eps            | -40         |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.4084257  |
| stats_o/std                    | 0.04011569 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -39.82452  |
| test/Q_plus_P                  | -39.82452  |
| test/reward_per_eps            | -40        |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.4084215   |
| stats_o/std                    | 0.040109567 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.832275  |
| test/Q_plus_P                  | -39.832275  |
| test/reward_per_eps            | -40         |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.4084226   |
| stats_o/std                    | 0.040115308 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -39.83752   |
| test/Q_plus_P                  | -39.83752   |
| test/reward_per_eps            | -40         |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.40843305  |
| stats_o/std                    | 0.040120825 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.822247  |
| test/Q_plus_P                  | -39.822247  |
| test/reward_per_eps            | -40         |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.40842888 |
| stats_o/std                    | 0.04013057 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.839954 |
| test/Q_plus_P                  | -39.839954 |
| test/reward_per_eps            | -40        |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.4084277  |
| stats_o/std                    | 0.04013253 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.206     |
| test/info_shaping_reward_mean  | -0.265     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -39.831383 |
| test/Q_plus_P                  | -39.831383 |
| test/reward_per_eps            | -40        |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.4084253  |
| stats_o/std                    | 0.04013888 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.204     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.85135  |
| test/Q_plus_P                  | -39.85135  |
| test/reward_per_eps            | -40        |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.40843463  |
| stats_o/std                    | 0.040142395 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.84729   |
| test/Q_plus_P                  | -39.84729   |
| test/reward_per_eps            | -40         |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.40844718  |
| stats_o/std                    | 0.040141534 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.853596  |
| test/Q_plus_P                  | -39.853596  |
| test/reward_per_eps            | -40         |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.40845394  |
| stats_o/std                    | 0.040142175 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.849674  |
| test/Q_plus_P                  | -39.849674  |
| test/reward_per_eps            | -40         |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.40845132  |
| stats_o/std                    | 0.040145982 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.844566  |
| test/Q_plus_P                  | -39.844566  |
| test/reward_per_eps            | -40         |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.4084547  |
| stats_o/std                    | 0.04014991 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.817764 |
| test/Q_plus_P                  | -39.817764 |
| test/reward_per_eps            | -40        |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.40846172 |
| stats_o/std                    | 0.04014976 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.848354 |
| test/Q_plus_P                  | -39.848354 |
| test/reward_per_eps            | -40        |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.40846086 |
| stats_o/std                    | 0.04014777 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.817516 |
| test/Q_plus_P                  | -39.817516 |
| test/reward_per_eps            | -40        |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.4084631   |
| stats_o/std                    | 0.040145237 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.85043   |
| test/Q_plus_P                  | -39.85043   |
| test/reward_per_eps            | -40         |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.40846765 |
| stats_o/std                    | 0.04014601 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -39.83269  |
| test/Q_plus_P                  | -39.83269  |
| test/reward_per_eps            | -40        |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.40846896  |
| stats_o/std                    | 0.040144388 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.84758   |
| test/Q_plus_P                  | -39.84758   |
| test/reward_per_eps            | -40         |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.40847102  |
| stats_o/std                    | 0.040144015 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.848278  |
| test/Q_plus_P                  | -39.848278  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.40847802  |
| stats_o/std                    | 0.040145334 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.840332  |
| test/Q_plus_P                  | -39.840332  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.40847394  |
| stats_o/std                    | 0.040141348 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.84729   |
| test/Q_plus_P                  | -39.84729   |
| test/reward_per_eps            | -40         |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.40846524  |
| stats_o/std                    | 0.040137302 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.84043   |
| test/Q_plus_P                  | -39.84043   |
| test/reward_per_eps            | -40         |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.40847182  |
| stats_o/std                    | 0.040146627 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.70552   |
| test/Q_plus_P                  | -39.70552   |
| test/reward_per_eps            | -40         |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.4084665   |
| stats_o/std                    | 0.040142965 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.828377  |
| test/Q_plus_P                  | -39.828377  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.181      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.40847743  |
| stats_o/std                    | 0.040138714 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.82398   |
| test/Q_plus_P                  | -39.82398   |
| test/reward_per_eps            | -40         |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.40847912  |
| stats_o/std                    | 0.040136565 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.83388   |
| test/Q_plus_P                  | -39.83388   |
| test/reward_per_eps            | -40         |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.4084833   |
| stats_o/std                    | 0.040135697 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.215      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.853035  |
| test/Q_plus_P                  | -39.853035  |
| test/reward_per_eps            | -40         |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.40848994  |
| stats_o/std                    | 0.040131938 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -39.85828   |
| test/Q_plus_P                  | -39.85828   |
| test/reward_per_eps            | -40         |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.40848342  |
| stats_o/std                    | 0.040125072 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.867092  |
| test/Q_plus_P                  | -39.867092  |
| test/reward_per_eps            | -40         |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.40849137  |
| stats_o/std                    | 0.040127534 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.827564  |
| test/Q_plus_P                  | -39.827564  |
| test/reward_per_eps            | -40         |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 506       |
| stats_o/mean                   | 0.4085069 |
| stats_o/std                    | 0.0401277 |
| test/episodes                  | 5070      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.144    |
| test/info_shaping_reward_mean  | -0.214    |
| test/info_shaping_reward_min   | -0.266    |
| test/Q                         | -39.84212 |
| test/Q_plus_P                  | -39.84212 |
| test/reward_per_eps            | -40       |
| test/steps                     | 202800    |
| train/episodes                 | 20280     |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.146    |
| train/info_shaping_reward_mean | -0.241    |
| train/info_shaping_reward_min  | -0.346    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 811200    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.4085058  |
| stats_o/std                    | 0.04013102 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -39.861046 |
| test/Q_plus_P                  | -39.861046 |
| test/reward_per_eps            | -40        |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.4085059   |
| stats_o/std                    | 0.040133134 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.82626   |
| test/Q_plus_P                  | -39.82626   |
| test/reward_per_eps            | -40         |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.40850592  |
| stats_o/std                    | 0.040133815 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.8706    |
| test/Q_plus_P                  | -39.8706    |
| test/reward_per_eps            | -40         |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.40849915  |
| stats_o/std                    | 0.040133785 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.844894  |
| test/Q_plus_P                  | -39.844894  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.40850487  |
| stats_o/std                    | 0.040134456 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.842407  |
| test/Q_plus_P                  | -39.842407  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.40851608 |
| stats_o/std                    | 0.04012872 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.850163 |
| test/Q_plus_P                  | -39.850163 |
| test/reward_per_eps            | -40        |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.4085052   |
| stats_o/std                    | 0.040127028 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.874737  |
| test/Q_plus_P                  | -39.874737  |
| test/reward_per_eps            | -40         |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.40851268  |
| stats_o/std                    | 0.040127303 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.825985  |
| test/Q_plus_P                  | -39.825985  |
| test/reward_per_eps            | -40         |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.40851542  |
| stats_o/std                    | 0.040124636 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.205      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.81831   |
| test/Q_plus_P                  | -39.81831   |
| test/reward_per_eps            | -40         |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.40851346 |
| stats_o/std                    | 0.04012803 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.203     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.83578  |
| test/Q_plus_P                  | -39.83578  |
| test/reward_per_eps            | -40        |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.40851188 |
| stats_o/std                    | 0.04012536 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -39.814247 |
| test/Q_plus_P                  | -39.814247 |
| test/reward_per_eps            | -40        |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.40851328  |
| stats_o/std                    | 0.040125925 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.864906  |
| test/Q_plus_P                  | -39.864906  |
| test/reward_per_eps            | -40         |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.40850803 |
| stats_o/std                    | 0.04012663 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.89962  |
| test/Q_plus_P                  | -39.89962  |
| test/reward_per_eps            | -40        |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.40851462  |
| stats_o/std                    | 0.040126715 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -39.84166   |
| test/Q_plus_P                  | -39.84166   |
| test/reward_per_eps            | -40         |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.40851486  |
| stats_o/std                    | 0.040126834 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.201      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.862946  |
| test/Q_plus_P                  | -39.862946  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.40850243  |
| stats_o/std                    | 0.040122714 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.87533   |
| test/Q_plus_P                  | -39.87533   |
| test/reward_per_eps            | -40         |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.4085008  |
| stats_o/std                    | 0.04012691 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.865566 |
| test/Q_plus_P                  | -39.865566 |
| test/reward_per_eps            | -40        |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.40850332  |
| stats_o/std                    | 0.040125754 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -39.872574  |
| test/Q_plus_P                  | -39.872574  |
| test/reward_per_eps            | -40         |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.40852305 |
| stats_o/std                    | 0.04013365 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.8608   |
| test/Q_plus_P                  | -39.8608   |
| test/reward_per_eps            | -40        |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.40850767  |
| stats_o/std                    | 0.040137697 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -39.85265   |
| test/Q_plus_P                  | -39.85265   |
| test/reward_per_eps            | -40         |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.4085052  |
| stats_o/std                    | 0.04013865 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -39.88433  |
| test/Q_plus_P                  | -39.88433  |
| test/reward_per_eps            | -40        |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.40850675  |
| stats_o/std                    | 0.040142383 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.874203  |
| test/Q_plus_P                  | -39.874203  |
| test/reward_per_eps            | -40         |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.4084938   |
| stats_o/std                    | 0.040144306 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.873047  |
| test/Q_plus_P                  | -39.873047  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.40849802 |
| stats_o/std                    | 0.04014633 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.195     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -39.909786 |
| test/Q_plus_P                  | -39.909786 |
| test/reward_per_eps            | -40        |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.4085035  |
| stats_o/std                    | 0.0401468  |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.893917 |
| test/Q_plus_P                  | -39.893917 |
| test/reward_per_eps            | -40        |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.40849897  |
| stats_o/std                    | 0.040150363 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.8852    |
| test/Q_plus_P                  | -39.8852    |
| test/reward_per_eps            | -40         |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.4085034   |
| stats_o/std                    | 0.040154018 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.90297   |
| test/Q_plus_P                  | -39.90297   |
| test/reward_per_eps            | -40         |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.40849945  |
| stats_o/std                    | 0.040156413 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.890614  |
| test/Q_plus_P                  | -39.890614  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.40849033  |
| stats_o/std                    | 0.040154662 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.90177   |
| test/Q_plus_P                  | -39.90177   |
| test/reward_per_eps            | -40         |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.40848374  |
| stats_o/std                    | 0.040154163 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.895195  |
| test/Q_plus_P                  | -39.895195  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.40847751  |
| stats_o/std                    | 0.040152553 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.203      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.896946  |
| test/Q_plus_P                  | -39.896946  |
| test/reward_per_eps            | -40         |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.40846547 |
| stats_o/std                    | 0.04015022 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.895702 |
| test/Q_plus_P                  | -39.895702 |
| test/reward_per_eps            | -40        |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.40844762 |
| stats_o/std                    | 0.04015445 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -39.918602 |
| test/Q_plus_P                  | -39.918602 |
| test/reward_per_eps            | -40        |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.40846     |
| stats_o/std                    | 0.040160175 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.884823  |
| test/Q_plus_P                  | -39.884823  |
| test/reward_per_eps            | -40         |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.4084653   |
| stats_o/std                    | 0.040161047 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.86934   |
| test/Q_plus_P                  | -39.86934   |
| test/reward_per_eps            | -40         |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.4084803   |
| stats_o/std                    | 0.040161654 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.90152   |
| test/Q_plus_P                  | -39.90152   |
| test/reward_per_eps            | -40         |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.4084805   |
| stats_o/std                    | 0.040169705 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -39.895313  |
| test/Q_plus_P                  | -39.895313  |
| test/reward_per_eps            | -40         |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.40847656  |
| stats_o/std                    | 0.040174507 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.91947   |
| test/Q_plus_P                  | -39.91947   |
| test/reward_per_eps            | -40         |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.40847075 |
| stats_o/std                    | 0.04017805 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -39.906025 |
| test/Q_plus_P                  | -39.906025 |
| test/reward_per_eps            | -40        |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.40846622  |
| stats_o/std                    | 0.040178496 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.89155   |
| test/Q_plus_P                  | -39.89155   |
| test/reward_per_eps            | -40         |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 547        |
| stats_o/mean                   | 0.40847746 |
| stats_o/std                    | 0.04018223 |
| test/episodes                  | 5480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -39.91201  |
| test/Q_plus_P                  | -39.91201  |
| test/reward_per_eps            | -40        |
| test/steps                     | 219200     |
| train/episodes                 | 21920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 876800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.40848127  |
| stats_o/std                    | 0.040180784 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.90067   |
| test/Q_plus_P                  | -39.90067   |
| test/reward_per_eps            | -40         |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.4084824   |
| stats_o/std                    | 0.040177513 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -39.866016  |
| test/Q_plus_P                  | -39.866016  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.40847254 |
| stats_o/std                    | 0.04018121 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.87834  |
| test/Q_plus_P                  | -39.87834  |
| test/reward_per_eps            | -40        |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.4084761   |
| stats_o/std                    | 0.040179696 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.8388    |
| test/Q_plus_P                  | -39.8388    |
| test/reward_per_eps            | -40         |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 552        |
| stats_o/mean                   | 0.40847707 |
| stats_o/std                    | 0.04018588 |
| test/episodes                  | 5530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -39.903378 |
| test/Q_plus_P                  | -39.903378 |
| test/reward_per_eps            | -40        |
| test/steps                     | 221200     |
| train/episodes                 | 22120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 884800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.4084818   |
| stats_o/std                    | 0.040190868 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.8785    |
| test/Q_plus_P                  | -39.8785    |
| test/reward_per_eps            | -40         |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.40847406  |
| stats_o/std                    | 0.040188137 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -39.933483  |
| test/Q_plus_P                  | -39.933483  |
| test/reward_per_eps            | -40         |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.40847096 |
| stats_o/std                    | 0.04018635 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.909897 |
| test/Q_plus_P                  | -39.909897 |
| test/reward_per_eps            | -40        |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.40848538  |
| stats_o/std                    | 0.040187724 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.205      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.905575  |
| test/Q_plus_P                  | -39.905575  |
| test/reward_per_eps            | -40         |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.40849015 |
| stats_o/std                    | 0.04018913 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.86281  |
| test/Q_plus_P                  | -39.86281  |
| test/reward_per_eps            | -40        |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.40848258  |
| stats_o/std                    | 0.040185574 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.93209   |
| test/Q_plus_P                  | -39.93209   |
| test/reward_per_eps            | -40         |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.40849212  |
| stats_o/std                    | 0.040184375 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.218      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.907585  |
| test/Q_plus_P                  | -39.907585  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.40849474  |
| stats_o/std                    | 0.040184688 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.890026  |
| test/Q_plus_P                  | -39.890026  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.40849772  |
| stats_o/std                    | 0.040185742 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.212      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -39.922832  |
| test/Q_plus_P                  | -39.922832  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.4084877  |
| stats_o/std                    | 0.04018348 |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -39.93665  |
| test/Q_plus_P                  | -39.93665  |
| test/reward_per_eps            | -40        |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.4084753  |
| stats_o/std                    | 0.04018196 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.275     |
| test/info_shaping_reward_min   | -0.348     |
| test/Q                         | -39.93394  |
| test/Q_plus_P                  | -39.93394  |
| test/reward_per_eps            | -40        |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.408468    |
| stats_o/std                    | 0.040182594 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.96105   |
| test/Q_plus_P                  | -39.96105   |
| test/reward_per_eps            | -40         |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.40846205  |
| stats_o/std                    | 0.040180482 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.906242  |
| test/Q_plus_P                  | -39.906242  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.40846953 |
| stats_o/std                    | 0.0401829  |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.926945 |
| test/Q_plus_P                  | -39.926945 |
| test/reward_per_eps            | -40        |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.4084797   |
| stats_o/std                    | 0.040184967 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.94155   |
| test/Q_plus_P                  | -39.94155   |
| test/reward_per_eps            | -40         |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.4084786  |
| stats_o/std                    | 0.04018168 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.922337 |
| test/Q_plus_P                  | -39.922337 |
| test/reward_per_eps            | -40        |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.40847465  |
| stats_o/std                    | 0.040183447 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.840954  |
| test/Q_plus_P                  | -39.840954  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.40848473  |
| stats_o/std                    | 0.040182386 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.945454  |
| test/Q_plus_P                  | -39.945454  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 571        |
| stats_o/mean                   | 0.40849808 |
| stats_o/std                    | 0.04018178 |
| test/episodes                  | 5720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.109     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -39.930607 |
| test/Q_plus_P                  | -39.930607 |
| test/reward_per_eps            | -40        |
| test/steps                     | 228800     |
| train/episodes                 | 22880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 915200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.40850553 |
| stats_o/std                    | 0.04018787 |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.11      |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.914806 |
| test/Q_plus_P                  | -39.914806 |
| test/reward_per_eps            | -40        |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.40850866 |
| stats_o/std                    | 0.04019361 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.895    |
| test/Q_plus_P                  | -39.895    |
| test/reward_per_eps            | -40        |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.40850496 |
| stats_o/std                    | 0.04019695 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -39.905506 |
| test/Q_plus_P                  | -39.905506 |
| test/reward_per_eps            | -40        |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.4085144   |
| stats_o/std                    | 0.040197708 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.92644   |
| test/Q_plus_P                  | -39.92644   |
| test/reward_per_eps            | -40         |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.40851688  |
| stats_o/std                    | 0.040202904 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.12       |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.955883  |
| test/Q_plus_P                  | -39.955883  |
| test/reward_per_eps            | -40         |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.40853164 |
| stats_o/std                    | 0.04020526 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.942028 |
| test/Q_plus_P                  | -39.942028 |
| test/reward_per_eps            | -40        |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.4085271  |
| stats_o/std                    | 0.04020669 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.93591  |
| test/Q_plus_P                  | -39.93591  |
| test/reward_per_eps            | -40        |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.40852794  |
| stats_o/std                    | 0.040201716 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.938145  |
| test/Q_plus_P                  | -39.938145  |
| test/reward_per_eps            | -40         |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.40854523  |
| stats_o/std                    | 0.040206034 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.953293  |
| test/Q_plus_P                  | -39.953293  |
| test/reward_per_eps            | -40         |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.40855584 |
| stats_o/std                    | 0.04020364 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.92598  |
| test/Q_plus_P                  | -39.92598  |
| test/reward_per_eps            | -40        |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.40856144  |
| stats_o/std                    | 0.040202364 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.99693   |
| test/Q_plus_P                  | -39.99693   |
| test/reward_per_eps            | -40         |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.40856358 |
| stats_o/std                    | 0.04020321 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -39.94415  |
| test/Q_plus_P                  | -39.94415  |
| test/reward_per_eps            | -40        |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.4085621  |
| stats_o/std                    | 0.04019918 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.950817 |
| test/Q_plus_P                  | -39.950817 |
| test/reward_per_eps            | -40        |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.40855643  |
| stats_o/std                    | 0.040199928 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.925156  |
| test/Q_plus_P                  | -39.925156  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.40855357  |
| stats_o/std                    | 0.040197168 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.963364  |
| test/Q_plus_P                  | -39.963364  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.40855038 |
| stats_o/std                    | 0.04019906 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -40.030975 |
| test/Q_plus_P                  | -40.030975 |
| test/reward_per_eps            | -40        |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 588        |
| stats_o/mean                   | 0.40855357 |
| stats_o/std                    | 0.04019971 |
| test/episodes                  | 5890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -39.908722 |
| test/Q_plus_P                  | -39.908722 |
| test/reward_per_eps            | -40        |
| test/steps                     | 235600     |
| train/episodes                 | 23560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 942400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.40855482  |
| stats_o/std                    | 0.040199406 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.949448  |
| test/Q_plus_P                  | -39.949448  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.40855685  |
| stats_o/std                    | 0.040194448 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.93136   |
| test/Q_plus_P                  | -39.93136   |
| test/reward_per_eps            | -40         |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.40856132 |
| stats_o/std                    | 0.04019544 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.984932 |
| test/Q_plus_P                  | -39.984932 |
| test/reward_per_eps            | -40        |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.4085594   |
| stats_o/std                    | 0.040191088 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.938725  |
| test/Q_plus_P                  | -39.938725  |
| test/reward_per_eps            | -40         |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 593        |
| stats_o/mean                   | 0.40855992 |
| stats_o/std                    | 0.04019248 |
| test/episodes                  | 5940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.975502 |
| test/Q_plus_P                  | -39.975502 |
| test/reward_per_eps            | -40        |
| test/steps                     | 237600     |
| train/episodes                 | 23760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 950400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.4085561   |
| stats_o/std                    | 0.040192425 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.95795   |
| test/Q_plus_P                  | -39.95795   |
| test/reward_per_eps            | -40         |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.4085594   |
| stats_o/std                    | 0.040194463 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -39.95588   |
| test/Q_plus_P                  | -39.95588   |
| test/reward_per_eps            | -40         |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.40855655  |
| stats_o/std                    | 0.040192842 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.96781   |
| test/Q_plus_P                  | -39.96781   |
| test/reward_per_eps            | -40         |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.40856507  |
| stats_o/std                    | 0.040192213 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.218      |
| test/info_shaping_reward_mean  | -0.265      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.915806  |
| test/Q_plus_P                  | -39.915806  |
| test/reward_per_eps            | -40         |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.4085648   |
| stats_o/std                    | 0.040191147 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.921585  |
| test/Q_plus_P                  | -39.921585  |
| test/reward_per_eps            | -40         |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.40856752 |
| stats_o/std                    | 0.04019196 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.95134  |
| test/Q_plus_P                  | -39.95134  |
| test/reward_per_eps            | -40        |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
