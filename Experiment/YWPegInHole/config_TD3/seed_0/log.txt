Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3/seed_0
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.42011866  |
| stats_o/std                    | 0.047104392 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0726     |
| test/info_shaping_reward_mean  | -0.151      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -1.3077033  |
| test/Q_plus_P                  | -1.3077033  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0862     |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.42631248  |
| stats_o/std                    | 0.045309972 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.11       |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -1.6793504  |
| test/Q_plus_P                  | -1.6793504  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0984     |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.42664614  |
| stats_o/std                    | 0.043502074 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.114      |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -1.978692   |
| test/Q_plus_P                  | -1.978692   |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.105      |
| train/info_shaping_reward_mean | -0.199      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.42792174 |
| stats_o/std                    | 0.04066363 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.119     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.343     |
| test/Q                         | -2.3996873 |
| test/Q_plus_P                  | -2.3996873 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0894    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.42759946  |
| stats_o/std                    | 0.038612798 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.105      |
| test/info_shaping_reward_mean  | -0.171      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -2.7558966  |
| test/Q_plus_P                  | -2.7558966  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.1        |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.283      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.42815807  |
| stats_o/std                    | 0.037141223 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.156      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -3.138299   |
| test/Q_plus_P                  | -3.138299   |
| test/reward_per_eps            | -40         |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0935     |
| train/info_shaping_reward_mean | -0.164      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.42852625 |
| stats_o/std                    | 0.03586259 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.131     |
| test/info_shaping_reward_mean  | -0.156     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -3.5507536 |
| test/Q_plus_P                  | -3.5507536 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.104     |
| train/info_shaping_reward_mean | -0.168     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.4284216   |
| stats_o/std                    | 0.034897856 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.158      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -3.9692261  |
| test/Q_plus_P                  | -3.9692261  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.11       |
| train/info_shaping_reward_mean | -0.17       |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.42778325 |
| stats_o/std                    | 0.03421291 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -4.3520637 |
| test/Q_plus_P                  | -4.3520637 |
| test/reward_per_eps            | -40        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.174     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.42743957  |
| stats_o/std                    | 0.033645663 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -4.779598   |
| test/Q_plus_P                  | -4.779598   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.18       |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.42714563 |
| stats_o/std                    | 0.0333303  |
| test/episodes                  | 110        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.166     |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -5.178103  |
| test/Q_plus_P                  | -5.178103  |
| test/reward_per_eps            | -40        |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.42659    |
| stats_o/std                    | 0.03309904 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.177     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -5.5838804 |
| test/Q_plus_P                  | -5.5838804 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.188     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.4262173   |
| stats_o/std                    | 0.032875113 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -5.9784203  |
| test/Q_plus_P                  | -5.9784203  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.102      |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.42585763  |
| stats_o/std                    | 0.032549866 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.103      |
| test/info_shaping_reward_mean  | -0.166      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -6.3746448  |
| test/Q_plus_P                  | -6.3746448  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0992     |
| train/info_shaping_reward_mean | -0.175      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.4255855  |
| stats_o/std                    | 0.03236218 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.117     |
| test/info_shaping_reward_mean  | -0.192     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -6.7961802 |
| test/Q_plus_P                  | -6.7961802 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.107     |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.4251854   |
| stats_o/std                    | 0.032364376 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.188      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -7.177589   |
| test/Q_plus_P                  | -7.177589   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.107      |
| train/info_shaping_reward_mean | -0.185      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.42468396  |
| stats_o/std                    | 0.032357756 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.188      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -7.562831   |
| test/Q_plus_P                  | -7.562831   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.42431808  |
| stats_o/std                    | 0.032362666 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.121      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -7.920127   |
| test/Q_plus_P                  | -7.920127   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.112      |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.42390248 |
| stats_o/std                    | 0.03231666 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.133     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -8.299417  |
| test/Q_plus_P                  | -8.299417  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.11      |
| train/info_shaping_reward_mean | -0.189     |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.4235078   |
| stats_o/std                    | 0.032349117 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -8.679658   |
| test/Q_plus_P                  | -8.679658   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.42308822 |
| stats_o/std                    | 0.03256959 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -9.068985  |
| test/Q_plus_P                  | -9.068985  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.204     |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.42276227  |
| stats_o/std                    | 0.032751907 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -9.440708   |
| test/Q_plus_P                  | -9.440708   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.206      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.42246878  |
| stats_o/std                    | 0.032786246 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.181      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -9.779961   |
| test/Q_plus_P                  | -9.779961   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.42219374 |
| stats_o/std                    | 0.03282576 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -10.13458  |
| test/Q_plus_P                  | -10.13458  |
| test/reward_per_eps            | -40        |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.196     |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.42197588  |
| stats_o/std                    | 0.032809038 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -10.505969  |
| test/Q_plus_P                  | -10.505969  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.11       |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.42164698  |
| stats_o/std                    | 0.032829914 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.12       |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -10.847312  |
| test/Q_plus_P                  | -10.847312  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.2        |
| train/info_shaping_reward_min  | -0.286      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.42120764  |
| stats_o/std                    | 0.032947306 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -11.226804  |
| test/Q_plus_P                  | -11.226804  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.198      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.42082855  |
| stats_o/std                    | 0.03308185  |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.184      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -11.5419035 |
| test/Q_plus_P                  | -11.5419035 |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.208      |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.4205078   |
| stats_o/std                    | 0.033165324 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -11.88551   |
| test/Q_plus_P                  | -11.88551   |
| test/reward_per_eps            | -40         |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.209      |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.42019418  |
| stats_o/std                    | 0.033244565 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -12.229029  |
| test/Q_plus_P                  | -12.229029  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.208      |
| train/info_shaping_reward_min  | -0.298      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.41988942  |
| stats_o/std                    | 0.033395525 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -12.562207  |
| test/Q_plus_P                  | -12.562207  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.3        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.41961992  |
| stats_o/std                    | 0.033426236 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.192      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -12.887639  |
| test/Q_plus_P                  | -12.887639  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.109      |
| train/info_shaping_reward_mean | -0.201      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.41952705  |
| stats_o/std                    | 0.033488203 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -13.217038  |
| test/Q_plus_P                  | -13.217038  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.207      |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.4192524   |
| stats_o/std                    | 0.033482365 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.127      |
| test/info_shaping_reward_mean  | -0.182      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -13.539659  |
| test/Q_plus_P                  | -13.539659  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.204      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.41914782  |
| stats_o/std                    | 0.033551145 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.177      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -13.863865  |
| test/Q_plus_P                  | -13.863865  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.208      |
| train/info_shaping_reward_min  | -0.294      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.4189752   |
| stats_o/std                    | 0.033485107 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.185      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -14.197322  |
| test/Q_plus_P                  | -14.197322  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.4188452   |
| stats_o/std                    | 0.033503477 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -14.496695  |
| test/Q_plus_P                  | -14.496695  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.278      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.4186845   |
| stats_o/std                    | 0.033517987 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -14.81865   |
| test/Q_plus_P                  | -14.81865   |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.205      |
| train/info_shaping_reward_min  | -0.286      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.41842553 |
| stats_o/std                    | 0.03363547 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.135     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -15.100572 |
| test/Q_plus_P                  | -15.100572 |
| test/reward_per_eps            | -40        |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.289     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.4182082   |
| stats_o/std                    | 0.033695545 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -15.420327  |
| test/Q_plus_P                  | -15.420327  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.41797674 |
| stats_o/std                    | 0.03369112 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -15.722134 |
| test/Q_plus_P                  | -15.722134 |
| test/reward_per_eps            | -40        |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.21      |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.41788423  |
| stats_o/std                    | 0.033707213 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -15.999079  |
| test/Q_plus_P                  | -15.999079  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.208      |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.41769838 |
| stats_o/std                    | 0.03377536 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -16.284847 |
| test/Q_plus_P                  | -16.284847 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.41745916 |
| stats_o/std                    | 0.03379115 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -16.572876 |
| test/Q_plus_P                  | -16.572876 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.286     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.41734982  |
| stats_o/std                    | 0.033837747 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -16.866854  |
| test/Q_plus_P                  | -16.866854  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.291      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.4172032  |
| stats_o/std                    | 0.0338953  |
| test/episodes                  | 460        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -17.159779 |
| test/Q_plus_P                  | -17.159779 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.298     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.41702485  |
| stats_o/std                    | 0.033956274 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -17.418951  |
| test/Q_plus_P                  | -17.418951  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.4169346  |
| stats_o/std                    | 0.03404667 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -17.71078  |
| test/Q_plus_P                  | -17.71078  |
| test/reward_per_eps            | -40        |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.41679084  |
| stats_o/std                    | 0.034062345 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -17.973717  |
| test/Q_plus_P                  | -17.973717  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.288      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.41666672 |
| stats_o/std                    | 0.03412559 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.133     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -18.259392 |
| test/Q_plus_P                  | -18.259392 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.214     |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.41653296  |
| stats_o/std                    | 0.034215607 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -18.513117  |
| test/Q_plus_P                  | -18.513117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.298      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.41648614 |
| stats_o/std                    | 0.0342742  |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -18.766756 |
| test/Q_plus_P                  | -18.766756 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.41641212 |
| stats_o/std                    | 0.03437602 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.117     |
| test/info_shaping_reward_mean  | -0.194     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -19.038359 |
| test/Q_plus_P                  | -19.038359 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.41636017  |
| stats_o/std                    | 0.034407627 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -19.28058   |
| test/Q_plus_P                  | -19.28058   |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.294      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.4161956  |
| stats_o/std                    | 0.03447965 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -19.533062 |
| test/Q_plus_P                  | -19.533062 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.41606143  |
| stats_o/std                    | 0.034563545 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -19.768448  |
| test/Q_plus_P                  | -19.768448  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.41588727 |
| stats_o/std                    | 0.03467861 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.131     |
| test/info_shaping_reward_mean  | -0.184     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -20.048883 |
| test/Q_plus_P                  | -20.048883 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.41577125 |
| stats_o/std                    | 0.03476708 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -20.26606  |
| test/Q_plus_P                  | -20.26606  |
| test/reward_per_eps            | -40        |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.41567895  |
| stats_o/std                    | 0.034869697 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -20.509418  |
| test/Q_plus_P                  | -20.509418  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.41562927 |
| stats_o/std                    | 0.03493266 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -20.776201 |
| test/Q_plus_P                  | -20.776201 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.41552997 |
| stats_o/std                    | 0.03498831 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.114     |
| test/info_shaping_reward_mean  | -0.201     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -20.995747 |
| test/Q_plus_P                  | -20.995747 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.296     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.4154972  |
| stats_o/std                    | 0.03503396 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -21.227356 |
| test/Q_plus_P                  | -21.227356 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.4154463   |
| stats_o/std                    | 0.035114523 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -21.451574  |
| test/Q_plus_P                  | -21.451574  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.41533104 |
| stats_o/std                    | 0.03514349 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -21.706762 |
| test/Q_plus_P                  | -21.706762 |
| test/reward_per_eps            | -40        |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.41523495 |
| stats_o/std                    | 0.03518082 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.127     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -21.91788  |
| test/Q_plus_P                  | -21.91788  |
| test/reward_per_eps            | -40        |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.41511092  |
| stats_o/std                    | 0.035272397 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -22.139639  |
| test/Q_plus_P                  | -22.139639  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.41500318  |
| stats_o/std                    | 0.035340194 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -22.354193  |
| test/Q_plus_P                  | -22.354193  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.41495296  |
| stats_o/std                    | 0.035382662 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -22.566185  |
| test/Q_plus_P                  | -22.566185  |
| test/reward_per_eps            | -40         |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.41493484  |
| stats_o/std                    | 0.035475828 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -22.797207  |
| test/Q_plus_P                  | -22.797207  |
| test/reward_per_eps            | -40         |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.41486612  |
| stats_o/std                    | 0.035584193 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -22.974043  |
| test/Q_plus_P                  | -22.974043  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.4148154   |
| stats_o/std                    | 0.035616864 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.205      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -23.20059   |
| test/Q_plus_P                  | -23.20059   |
| test/reward_per_eps            | -40         |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.41471758  |
| stats_o/std                    | 0.035657763 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -23.405884  |
| test/Q_plus_P                  | -23.405884  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.41461754  |
| stats_o/std                    | 0.035705455 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -23.600952  |
| test/Q_plus_P                  | -23.600952  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.41456512  |
| stats_o/std                    | 0.035749223 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -23.797997  |
| test/Q_plus_P                  | -23.797997  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.4144841   |
| stats_o/std                    | 0.035815623 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -24.027119  |
| test/Q_plus_P                  | -24.027119  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.41433737  |
| stats_o/std                    | 0.035876416 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -24.208305  |
| test/Q_plus_P                  | -24.208305  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.41428196  |
| stats_o/std                    | 0.035952616 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -24.380953  |
| test/Q_plus_P                  | -24.380953  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.4142163  |
| stats_o/std                    | 0.03601304 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -24.593971 |
| test/Q_plus_P                  | -24.593971 |
| test/reward_per_eps            | -40        |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.41416523  |
| stats_o/std                    | 0.036058705 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -24.778217  |
| test/Q_plus_P                  | -24.778217  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.41412795 |
| stats_o/std                    | 0.0360944  |
| test/episodes                  | 800        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -24.946688 |
| test/Q_plus_P                  | -24.946688 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.41408    |
| stats_o/std                    | 0.03616043 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -25.142727 |
| test/Q_plus_P                  | -25.142727 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.117     |
| train/info_shaping_reward_mean | -0.214     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.41401294  |
| stats_o/std                    | 0.036185417 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -25.335386  |
| test/Q_plus_P                  | -25.335386  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.41392767  |
| stats_o/std                    | 0.036241706 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -25.520298  |
| test/Q_plus_P                  | -25.520298  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.41378763  |
| stats_o/std                    | 0.036302935 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -25.683052  |
| test/Q_plus_P                  | -25.683052  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.41372895  |
| stats_o/std                    | 0.036367167 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -25.865215  |
| test/Q_plus_P                  | -25.865215  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.4136661   |
| stats_o/std                    | 0.036404368 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -26.03037   |
| test/Q_plus_P                  | -26.03037   |
| test/reward_per_eps            | -40         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.4136423   |
| stats_o/std                    | 0.036444966 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -26.200474  |
| test/Q_plus_P                  | -26.200474  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.41354516  |
| stats_o/std                    | 0.036451567 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.13       |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -26.374249  |
| test/Q_plus_P                  | -26.374249  |
| test/reward_per_eps            | -40         |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.4135233   |
| stats_o/std                    | 0.036514726 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -26.555483  |
| test/Q_plus_P                  | -26.555483  |
| test/reward_per_eps            | -40         |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.4134775  |
| stats_o/std                    | 0.03654902 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.196     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -26.707685 |
| test/Q_plus_P                  | -26.707685 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 90         |
| stats_o/mean                   | 0.41337523 |
| stats_o/std                    | 0.03657666 |
| test/episodes                  | 910        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -26.88291  |
| test/Q_plus_P                  | -26.88291  |
| test/reward_per_eps            | -40        |
| test/steps                     | 36400      |
| train/episodes                 | 3640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 145600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.41330984 |
| stats_o/std                    | 0.03664558 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.118     |
| test/info_shaping_reward_mean  | -0.197     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -27.022486 |
| test/Q_plus_P                  | -27.022486 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.4132733   |
| stats_o/std                    | 0.036650073 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -27.201117  |
| test/Q_plus_P                  | -27.201117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.41319248 |
| stats_o/std                    | 0.0366892  |
| test/episodes                  | 940        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -27.38332  |
| test/Q_plus_P                  | -27.38332  |
| test/reward_per_eps            | -40        |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.4132152  |
| stats_o/std                    | 0.0367496  |
| test/episodes                  | 950        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.11      |
| test/info_shaping_reward_mean  | -0.192     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -27.473282 |
| test/Q_plus_P                  | -27.473282 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.41317344  |
| stats_o/std                    | 0.036788795 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -27.670378  |
| test/Q_plus_P                  | -27.670378  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.41313553  |
| stats_o/std                    | 0.036821432 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.114      |
| test/info_shaping_reward_mean  | -0.191      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -27.851778  |
| test/Q_plus_P                  | -27.851778  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.41304842  |
| stats_o/std                    | 0.036887664 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -27.993223  |
| test/Q_plus_P                  | -27.993223  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.4130063   |
| stats_o/std                    | 0.036944684 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -28.090155  |
| test/Q_plus_P                  | -28.090155  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.41291606  |
| stats_o/std                    | 0.036986906 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -28.280312  |
| test/Q_plus_P                  | -28.280312  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.41285408  |
| stats_o/std                    | 0.037001662 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -28.401623  |
| test/Q_plus_P                  | -28.401623  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.41281173  |
| stats_o/std                    | 0.037032623 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -28.56136   |
| test/Q_plus_P                  | -28.56136   |
| test/reward_per_eps            | -40         |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.41278264  |
| stats_o/std                    | 0.037046917 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -28.678513  |
| test/Q_plus_P                  | -28.678513  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.41268066 |
| stats_o/std                    | 0.03709352 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -28.831602 |
| test/Q_plus_P                  | -28.831602 |
| test/reward_per_eps            | -40        |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.4125776   |
| stats_o/std                    | 0.037151843 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -28.93028   |
| test/Q_plus_P                  | -28.93028   |
| test/reward_per_eps            | -40         |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.4125547   |
| stats_o/std                    | 0.037165273 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -29.064123  |
| test/Q_plus_P                  | -29.064123  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.41251853 |
| stats_o/std                    | 0.03719043 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.188     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -29.27668  |
| test/Q_plus_P                  | -29.27668  |
| test/reward_per_eps            | -40        |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.41253123 |
| stats_o/std                    | 0.03723187 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -29.342138 |
| test/Q_plus_P                  | -29.342138 |
| test/reward_per_eps            | -40        |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.41249058  |
| stats_o/std                    | 0.037274994 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -29.510874  |
| test/Q_plus_P                  | -29.510874  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 109       |
| stats_o/mean                   | 0.4124807 |
| stats_o/std                    | 0.0372969 |
| test/episodes                  | 1100      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.165    |
| test/info_shaping_reward_mean  | -0.234    |
| test/info_shaping_reward_min   | -0.334    |
| test/Q                         | -29.61062 |
| test/Q_plus_P                  | -29.61062 |
| test/reward_per_eps            | -40       |
| test/steps                     | 44000     |
| train/episodes                 | 4400      |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.156    |
| train/info_shaping_reward_mean | -0.241    |
| train/info_shaping_reward_min  | -0.321    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 176000    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.41239497 |
| stats_o/std                    | 0.03731179 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -29.765713 |
| test/Q_plus_P                  | -29.765713 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.4123112  |
| stats_o/std                    | 0.03735702 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -29.909155 |
| test/Q_plus_P                  | -29.909155 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.41227677 |
| stats_o/std                    | 0.0373792  |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -30.016909 |
| test/Q_plus_P                  | -30.016909 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.41224864  |
| stats_o/std                    | 0.037416346 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -30.148365  |
| test/Q_plus_P                  | -30.148365  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.41224086  |
| stats_o/std                    | 0.037502848 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -30.275576  |
| test/Q_plus_P                  | -30.275576  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.41220227  |
| stats_o/std                    | 0.037538566 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -30.387558  |
| test/Q_plus_P                  | -30.387558  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.378      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.4121716   |
| stats_o/std                    | 0.037563365 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -30.490005  |
| test/Q_plus_P                  | -30.490005  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.41215527  |
| stats_o/std                    | 0.037599355 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -30.607464  |
| test/Q_plus_P                  | -30.607464  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.41212055 |
| stats_o/std                    | 0.03764514 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -30.737534 |
| test/Q_plus_P                  | -30.737534 |
| test/reward_per_eps            | -40        |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.41206884  |
| stats_o/std                    | 0.037669443 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -30.865269  |
| test/Q_plus_P                  | -30.865269  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.41201833  |
| stats_o/std                    | 0.037680496 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -30.928783  |
| test/Q_plus_P                  | -30.928783  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.4119638  |
| stats_o/std                    | 0.03770872 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -31.06466  |
| test/Q_plus_P                  | -31.06466  |
| test/reward_per_eps            | -40        |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.41191927 |
| stats_o/std                    | 0.03772584 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.206     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -31.205164 |
| test/Q_plus_P                  | -31.205164 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.4118818   |
| stats_o/std                    | 0.037709933 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -31.319073  |
| test/Q_plus_P                  | -31.319073  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.41182873  |
| stats_o/std                    | 0.037791763 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -31.411192  |
| test/Q_plus_P                  | -31.411192  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.274      |
| train/info_shaping_reward_min  | -0.385      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.41177538  |
| stats_o/std                    | 0.037822325 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.272      |
| test/info_shaping_reward_min   | -0.338      |
| test/Q                         | -31.504192  |
| test/Q_plus_P                  | -31.504192  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.41178074 |
| stats_o/std                    | 0.03783937 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -31.64094  |
| test/Q_plus_P                  | -31.64094  |
| test/reward_per_eps            | -40        |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 127        |
| stats_o/mean                   | 0.41172948 |
| stats_o/std                    | 0.03784385 |
| test/episodes                  | 1280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.208     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -31.72585  |
| test/Q_plus_P                  | -31.72585  |
| test/reward_per_eps            | -40        |
| test/steps                     | 51200      |
| train/episodes                 | 5120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 204800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.4117186  |
| stats_o/std                    | 0.03786453 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.269     |
| test/info_shaping_reward_min   | -0.344     |
| test/Q                         | -31.820585 |
| test/Q_plus_P                  | -31.820585 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.41162524  |
| stats_o/std                    | 0.037899736 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -31.953682  |
| test/Q_plus_P                  | -31.953682  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.268      |
| train/info_shaping_reward_min  | -0.381      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.4115672  |
| stats_o/std                    | 0.03794713 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.262     |
| test/info_shaping_reward_min   | -0.329     |
| test/Q                         | -32.041847 |
| test/Q_plus_P                  | -32.041847 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.269     |
| train/info_shaping_reward_min  | -0.384     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 131       |
| stats_o/mean                   | 0.4114958 |
| stats_o/std                    | 0.0379693 |
| test/episodes                  | 1320      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.173    |
| test/info_shaping_reward_mean  | -0.236    |
| test/info_shaping_reward_min   | -0.325    |
| test/Q                         | -32.13225 |
| test/Q_plus_P                  | -32.13225 |
| test/reward_per_eps            | -40       |
| test/steps                     | 52800     |
| train/episodes                 | 5280      |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.162    |
| train/info_shaping_reward_mean | -0.254    |
| train/info_shaping_reward_min  | -0.354    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 211200    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.41145685  |
| stats_o/std                    | 0.037987307 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -32.228256  |
| test/Q_plus_P                  | -32.228256  |
| test/reward_per_eps            | -40         |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.41142765  |
| stats_o/std                    | 0.038003735 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -32.38002   |
| test/Q_plus_P                  | -32.38002   |
| test/reward_per_eps            | -40         |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.41140738  |
| stats_o/std                    | 0.038006406 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -32.428726  |
| test/Q_plus_P                  | -32.428726  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.41137245 |
| stats_o/std                    | 0.03800527 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -32.535683 |
| test/Q_plus_P                  | -32.535683 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.4113638   |
| stats_o/std                    | 0.038039424 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -32.608177  |
| test/Q_plus_P                  | -32.608177  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.41135958 |
| stats_o/std                    | 0.03803399 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -32.649975 |
| test/Q_plus_P                  | -32.649975 |
| test/reward_per_eps            | -40        |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.41133633  |
| stats_o/std                    | 0.038036283 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -32.784874  |
| test/Q_plus_P                  | -32.784874  |
| test/reward_per_eps            | -40         |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.41135678 |
| stats_o/std                    | 0.03805622 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -32.86103  |
| test/Q_plus_P                  | -32.86103  |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.41129747 |
| stats_o/std                    | 0.03808218 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -32.961445 |
| test/Q_plus_P                  | -32.961445 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.41127262  |
| stats_o/std                    | 0.038085483 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -33.034367  |
| test/Q_plus_P                  | -33.034367  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.4111906  |
| stats_o/std                    | 0.03812251 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.344     |
| test/Q                         | -33.112953 |
| test/Q_plus_P                  | -33.112953 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.261     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.4112027  |
| stats_o/std                    | 0.03815073 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -33.19476  |
| test/Q_plus_P                  | -33.19476  |
| test/reward_per_eps            | -40        |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.41120175  |
| stats_o/std                    | 0.038160946 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -33.300014  |
| test/Q_plus_P                  | -33.300014  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.41117427 |
| stats_o/std                    | 0.03818294 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.278     |
| test/info_shaping_reward_min   | -0.358     |
| test/Q                         | -33.37135  |
| test/Q_plus_P                  | -33.37135  |
| test/reward_per_eps            | -40        |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.4111352   |
| stats_o/std                    | 0.038204577 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.271      |
| test/info_shaping_reward_min   | -0.374      |
| test/Q                         | -33.44517   |
| test/Q_plus_P                  | -33.44517   |
| test/reward_per_eps            | -40         |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.41110405  |
| stats_o/std                    | 0.038228583 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -33.564194  |
| test/Q_plus_P                  | -33.564194  |
| test/reward_per_eps            | -40         |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.4111192   |
| stats_o/std                    | 0.038235303 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -33.58766   |
| test/Q_plus_P                  | -33.58766   |
| test/reward_per_eps            | -40         |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.41110268  |
| stats_o/std                    | 0.038250428 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -33.687294  |
| test/Q_plus_P                  | -33.687294  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.41106078  |
| stats_o/std                    | 0.038267594 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -33.7586    |
| test/Q_plus_P                  | -33.7586    |
| test/reward_per_eps            | -40         |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.41107073 |
| stats_o/std                    | 0.03828286 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -33.85374  |
| test/Q_plus_P                  | -33.85374  |
| test/reward_per_eps            | -40        |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.41106442 |
| stats_o/std                    | 0.03829315 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -33.925488 |
| test/Q_plus_P                  | -33.925488 |
| test/reward_per_eps            | -40        |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.41107714  |
| stats_o/std                    | 0.038291626 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -33.972984  |
| test/Q_plus_P                  | -33.972984  |
| test/reward_per_eps            | -40         |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.4110534  |
| stats_o/std                    | 0.03830486 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -34.07999  |
| test/Q_plus_P                  | -34.07999  |
| test/reward_per_eps            | -40        |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.41104683  |
| stats_o/std                    | 0.038325515 |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -34.153988  |
| test/Q_plus_P                  | -34.153988  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.4109893  |
| stats_o/std                    | 0.03832931 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -34.213913 |
| test/Q_plus_P                  | -34.213913 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.41095746  |
| stats_o/std                    | 0.038362946 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.27       |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -34.283455  |
| test/Q_plus_P                  | -34.283455  |
| test/reward_per_eps            | -40         |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.41094026  |
| stats_o/std                    | 0.038382497 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -34.43828   |
| test/Q_plus_P                  | -34.43828   |
| test/reward_per_eps            | -40         |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.41092443  |
| stats_o/std                    | 0.038399857 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -34.43368   |
| test/Q_plus_P                  | -34.43368   |
| test/reward_per_eps            | -40         |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.4108684   |
| stats_o/std                    | 0.038432155 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -34.52536   |
| test/Q_plus_P                  | -34.52536   |
| test/reward_per_eps            | -40         |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.41086757  |
| stats_o/std                    | 0.038440987 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -34.595848  |
| test/Q_plus_P                  | -34.595848  |
| test/reward_per_eps            | -40         |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.41089717 |
| stats_o/std                    | 0.03843664 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -34.630188 |
| test/Q_plus_P                  | -34.630188 |
| test/reward_per_eps            | -40        |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.41087687 |
| stats_o/std                    | 0.0384571  |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.335     |
| test/Q                         | -34.67085  |
| test/Q_plus_P                  | -34.67085  |
| test/reward_per_eps            | -40        |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.4108814   |
| stats_o/std                    | 0.038469035 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -34.786636  |
| test/Q_plus_P                  | -34.786636  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.41085473  |
| stats_o/std                    | 0.038487777 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.338      |
| test/Q                         | -34.856373  |
| test/Q_plus_P                  | -34.856373  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.4108337   |
| stats_o/std                    | 0.038514912 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -34.883068  |
| test/Q_plus_P                  | -34.883068  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.4108026  |
| stats_o/std                    | 0.03853075 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -34.954212 |
| test/Q_plus_P                  | -34.954212 |
| test/reward_per_eps            | -40        |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.41083947  |
| stats_o/std                    | 0.038556106 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -35.02012   |
| test/Q_plus_P                  | -35.02012   |
| test/reward_per_eps            | -40         |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.41083696  |
| stats_o/std                    | 0.038576566 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -35.084312  |
| test/Q_plus_P                  | -35.084312  |
| test/reward_per_eps            | -40         |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.41081297 |
| stats_o/std                    | 0.03858101 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -35.16206  |
| test/Q_plus_P                  | -35.16206  |
| test/reward_per_eps            | -40        |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.41079614  |
| stats_o/std                    | 0.038602505 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -35.21335   |
| test/Q_plus_P                  | -35.21335   |
| test/reward_per_eps            | -40         |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.4108024   |
| stats_o/std                    | 0.038621224 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -35.25259   |
| test/Q_plus_P                  | -35.25259   |
| test/reward_per_eps            | -40         |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.41080633  |
| stats_o/std                    | 0.038633864 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -35.26377   |
| test/Q_plus_P                  | -35.26377   |
| test/reward_per_eps            | -40         |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.41079554  |
| stats_o/std                    | 0.038670328 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -35.36498   |
| test/Q_plus_P                  | -35.36498   |
| test/reward_per_eps            | -40         |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.268      |
| train/info_shaping_reward_min  | -0.375      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.410777   |
| stats_o/std                    | 0.03869107 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.36      |
| test/Q                         | -35.453434 |
| test/Q_plus_P                  | -35.453434 |
| test/reward_per_eps            | -40        |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.41075996  |
| stats_o/std                    | 0.038712297 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.358      |
| test/Q                         | -35.510143  |
| test/Q_plus_P                  | -35.510143  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.4107647  |
| stats_o/std                    | 0.03869864 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.254     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -35.51922  |
| test/Q_plus_P                  | -35.51922  |
| test/reward_per_eps            | -40        |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.41076204  |
| stats_o/std                    | 0.038718637 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -35.598866  |
| test/Q_plus_P                  | -35.598866  |
| test/reward_per_eps            | -40         |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.41075417 |
| stats_o/std                    | 0.03872549 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -35.66949  |
| test/Q_plus_P                  | -35.66949  |
| test/reward_per_eps            | -40        |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.41074148  |
| stats_o/std                    | 0.038740303 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -35.785114  |
| test/Q_plus_P                  | -35.785114  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.4107449   |
| stats_o/std                    | 0.038738772 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -35.78107   |
| test/Q_plus_P                  | -35.78107   |
| test/reward_per_eps            | -40         |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.4107872   |
| stats_o/std                    | 0.038762268 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -35.781788  |
| test/Q_plus_P                  | -35.781788  |
| test/reward_per_eps            | -40         |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.41080606 |
| stats_o/std                    | 0.03877702 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -35.840633 |
| test/Q_plus_P                  | -35.840633 |
| test/reward_per_eps            | -40        |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.41081563  |
| stats_o/std                    | 0.038781095 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -35.931908  |
| test/Q_plus_P                  | -35.931908  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.41085005  |
| stats_o/std                    | 0.038801543 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -35.97674   |
| test/Q_plus_P                  | -35.97674   |
| test/reward_per_eps            | -40         |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.41084898 |
| stats_o/std                    | 0.0388092  |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -36.01113  |
| test/Q_plus_P                  | -36.01113  |
| test/reward_per_eps            | -40        |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.4108708   |
| stats_o/std                    | 0.038827848 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -36.076675  |
| test/Q_plus_P                  | -36.076675  |
| test/reward_per_eps            | -40         |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.41087925  |
| stats_o/std                    | 0.038831737 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -36.12916   |
| test/Q_plus_P                  | -36.12916   |
| test/reward_per_eps            | -40         |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.41090167  |
| stats_o/std                    | 0.038838554 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.115      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -36.15654   |
| test/Q_plus_P                  | -36.15654   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.4109284  |
| stats_o/std                    | 0.03885879 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.197     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -36.217342 |
| test/Q_plus_P                  | -36.217342 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.41092896  |
| stats_o/std                    | 0.038870666 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.268      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -36.245716  |
| test/Q_plus_P                  | -36.245716  |
| test/reward_per_eps            | -40         |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.41092432  |
| stats_o/std                    | 0.038895942 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -36.303356  |
| test/Q_plus_P                  | -36.303356  |
| test/reward_per_eps            | -40         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.4109302   |
| stats_o/std                    | 0.038932223 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.203      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -36.31173   |
| test/Q_plus_P                  | -36.31173   |
| test/reward_per_eps            | -40         |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.41092274 |
| stats_o/std                    | 0.03896318 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.36      |
| test/Q                         | -36.370506 |
| test/Q_plus_P                  | -36.370506 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.41091117 |
| stats_o/std                    | 0.03897274 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.348     |
| test/Q                         | -36.453934 |
| test/Q_plus_P                  | -36.453934 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.41089976  |
| stats_o/std                    | 0.038987055 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -36.472042  |
| test/Q_plus_P                  | -36.472042  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.41089272  |
| stats_o/std                    | 0.038999613 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -36.48745   |
| test/Q_plus_P                  | -36.48745   |
| test/reward_per_eps            | -40         |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.41089365 |
| stats_o/std                    | 0.03902321 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -36.535263 |
| test/Q_plus_P                  | -36.535263 |
| test/reward_per_eps            | -40        |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.4109256  |
| stats_o/std                    | 0.03903281 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -36.593536 |
| test/Q_plus_P                  | -36.593536 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.4109266  |
| stats_o/std                    | 0.03906589 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -36.627724 |
| test/Q_plus_P                  | -36.627724 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.175     |
| train/info_shaping_reward_mean | -0.266     |
| train/info_shaping_reward_min  | -0.382     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.41093013  |
| stats_o/std                    | 0.039073538 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -36.669365  |
| test/Q_plus_P                  | -36.669365  |
| test/reward_per_eps            | -40         |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.4109299  |
| stats_o/std                    | 0.03910959 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -36.736942 |
| test/Q_plus_P                  | -36.736942 |
| test/reward_per_eps            | -40        |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.385     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.4109335   |
| stats_o/std                    | 0.039126933 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -36.734203  |
| test/Q_plus_P                  | -36.734203  |
| test/reward_per_eps            | -40         |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.41094673  |
| stats_o/std                    | 0.039129473 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -36.786915  |
| test/Q_plus_P                  | -36.786915  |
| test/reward_per_eps            | -40         |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.4109949  |
| stats_o/std                    | 0.03914374 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -36.833603 |
| test/Q_plus_P                  | -36.833603 |
| test/reward_per_eps            | -40        |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.4110146  |
| stats_o/std                    | 0.03916144 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.269     |
| test/info_shaping_reward_min   | -0.35      |
| test/Q                         | -36.82512  |
| test/Q_plus_P                  | -36.82512  |
| test/reward_per_eps            | -40        |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 207         |
| stats_o/mean                   | 0.41101146  |
| stats_o/std                    | 0.039173186 |
| test/episodes                  | 2080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -36.844303  |
| test/Q_plus_P                  | -36.844303  |
| test/reward_per_eps            | -40         |
| test/steps                     | 83200       |
| train/episodes                 | 8320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 332800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.41101685  |
| stats_o/std                    | 0.039194252 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -36.891357  |
| test/Q_plus_P                  | -36.891357  |
| test/reward_per_eps            | -40         |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.41104075  |
| stats_o/std                    | 0.039218098 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.273      |
| test/info_shaping_reward_min   | -0.383      |
| test/Q                         | -36.943005  |
| test/Q_plus_P                  | -36.943005  |
| test/reward_per_eps            | -40         |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.4110733   |
| stats_o/std                    | 0.039242815 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.344      |
| test/Q                         | -36.974808  |
| test/Q_plus_P                  | -36.974808  |
| test/reward_per_eps            | -40         |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.41106072  |
| stats_o/std                    | 0.039268747 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.352      |
| test/Q                         | -37.057003  |
| test/Q_plus_P                  | -37.057003  |
| test/reward_per_eps            | -40         |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.41104093  |
| stats_o/std                    | 0.039259087 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -37.059822  |
| test/Q_plus_P                  | -37.059822  |
| test/reward_per_eps            | -40         |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.41104993 |
| stats_o/std                    | 0.0392776  |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -37.10831  |
| test/Q_plus_P                  | -37.10831  |
| test/reward_per_eps            | -40        |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 214        |
| stats_o/mean                   | 0.41103968 |
| stats_o/std                    | 0.03929912 |
| test/episodes                  | 2150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -37.13473  |
| test/Q_plus_P                  | -37.13473  |
| test/reward_per_eps            | -40        |
| test/steps                     | 86000      |
| train/episodes                 | 8600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 344000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.4110762   |
| stats_o/std                    | 0.039322954 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -37.16098   |
| test/Q_plus_P                  | -37.16098   |
| test/reward_per_eps            | -40         |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.41107488  |
| stats_o/std                    | 0.039333384 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.352      |
| test/Q                         | -37.194412  |
| test/Q_plus_P                  | -37.194412  |
| test/reward_per_eps            | -40         |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.4110867   |
| stats_o/std                    | 0.039349202 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -37.24312   |
| test/Q_plus_P                  | -37.24312   |
| test/reward_per_eps            | -40         |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.41111526  |
| stats_o/std                    | 0.039359238 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -37.28305   |
| test/Q_plus_P                  | -37.28305   |
| test/reward_per_eps            | -40         |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.41113165  |
| stats_o/std                    | 0.039361577 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -37.30206   |
| test/Q_plus_P                  | -37.30206   |
| test/reward_per_eps            | -40         |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.41113272 |
| stats_o/std                    | 0.03937069 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.398     |
| test/Q                         | -37.327446 |
| test/Q_plus_P                  | -37.327446 |
| test/reward_per_eps            | -40        |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.41111147  |
| stats_o/std                    | 0.039372485 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -37.375984  |
| test/Q_plus_P                  | -37.375984  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.41111782 |
| stats_o/std                    | 0.03938609 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -37.44832  |
| test/Q_plus_P                  | -37.44832  |
| test/reward_per_eps            | -40        |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.41110024 |
| stats_o/std                    | 0.03939182 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -37.462242 |
| test/Q_plus_P                  | -37.462242 |
| test/reward_per_eps            | -40        |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.4110752   |
| stats_o/std                    | 0.039398048 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -37.482437  |
| test/Q_plus_P                  | -37.482437  |
| test/reward_per_eps            | -40         |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.4111003   |
| stats_o/std                    | 0.039422125 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.366      |
| test/Q                         | -37.53404   |
| test/Q_plus_P                  | -37.53404   |
| test/reward_per_eps            | -40         |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.4110789   |
| stats_o/std                    | 0.039444383 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -37.545204  |
| test/Q_plus_P                  | -37.545204  |
| test/reward_per_eps            | -40         |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.269      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.41107985 |
| stats_o/std                    | 0.03945556 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.363     |
| test/Q                         | -37.582317 |
| test/Q_plus_P                  | -37.582317 |
| test/reward_per_eps            | -40        |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.41108644  |
| stats_o/std                    | 0.039456416 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -37.60423   |
| test/Q_plus_P                  | -37.60423   |
| test/reward_per_eps            | -40         |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.41109344 |
| stats_o/std                    | 0.03948347 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.361     |
| test/Q                         | -37.65068  |
| test/Q_plus_P                  | -37.65068  |
| test/reward_per_eps            | -40        |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.266     |
| train/info_shaping_reward_min  | -0.383     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.41105795 |
| stats_o/std                    | 0.03949935 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.349     |
| test/Q                         | -37.61066  |
| test/Q_plus_P                  | -37.61066  |
| test/reward_per_eps            | -40        |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.267     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.41106355  |
| stats_o/std                    | 0.039510503 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -37.667988  |
| test/Q_plus_P                  | -37.667988  |
| test/reward_per_eps            | -40         |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 232        |
| stats_o/mean                   | 0.41109022 |
| stats_o/std                    | 0.0395214  |
| test/episodes                  | 2330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -37.70321  |
| test/Q_plus_P                  | -37.70321  |
| test/reward_per_eps            | -40        |
| test/steps                     | 93200      |
| train/episodes                 | 9320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 372800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.41111326 |
| stats_o/std                    | 0.0395268  |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -37.72271  |
| test/Q_plus_P                  | -37.72271  |
| test/reward_per_eps            | -40        |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.41111115 |
| stats_o/std                    | 0.03953503 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -37.733974 |
| test/Q_plus_P                  | -37.733974 |
| test/reward_per_eps            | -40        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 235        |
| stats_o/mean                   | 0.4111067  |
| stats_o/std                    | 0.03953922 |
| test/episodes                  | 2360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.341     |
| test/Q                         | -37.78871  |
| test/Q_plus_P                  | -37.78871  |
| test/reward_per_eps            | -40        |
| test/steps                     | 94400      |
| train/episodes                 | 9440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 377600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.41110945  |
| stats_o/std                    | 0.039534446 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -37.82701   |
| test/Q_plus_P                  | -37.82701   |
| test/reward_per_eps            | -40         |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.4110975   |
| stats_o/std                    | 0.039538108 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.264      |
| test/info_shaping_reward_min   | -0.365      |
| test/Q                         | -37.808628  |
| test/Q_plus_P                  | -37.808628  |
| test/reward_per_eps            | -40         |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.41108373  |
| stats_o/std                    | 0.039551757 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.348      |
| test/Q                         | -37.89666   |
| test/Q_plus_P                  | -37.89666   |
| test/reward_per_eps            | -40         |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.41106823 |
| stats_o/std                    | 0.03957611 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -37.961933 |
| test/Q_plus_P                  | -37.961933 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.41108128  |
| stats_o/std                    | 0.039588097 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -37.947865  |
| test/Q_plus_P                  | -37.947865  |
| test/reward_per_eps            | -40         |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.41109136 |
| stats_o/std                    | 0.03961495 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -37.93745  |
| test/Q_plus_P                  | -37.93745  |
| test/reward_per_eps            | -40        |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.274     |
| train/info_shaping_reward_min  | -0.39      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.4110831  |
| stats_o/std                    | 0.03962178 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.266     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -37.98503  |
| test/Q_plus_P                  | -37.98503  |
| test/reward_per_eps            | -40        |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.41110238 |
| stats_o/std                    | 0.03960845 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -37.999374 |
| test/Q_plus_P                  | -37.999374 |
| test/reward_per_eps            | -40        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.41106758  |
| stats_o/std                    | 0.039624415 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -38.01142   |
| test/Q_plus_P                  | -38.01142   |
| test/reward_per_eps            | -40         |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.379      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.41110292  |
| stats_o/std                    | 0.039633043 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -38.03608   |
| test/Q_plus_P                  | -38.03608   |
| test/reward_per_eps            | -40         |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.41111064  |
| stats_o/std                    | 0.039646935 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -38.04597   |
| test/Q_plus_P                  | -38.04597   |
| test/reward_per_eps            | -40         |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.4111236  |
| stats_o/std                    | 0.03965402 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -38.022415 |
| test/Q_plus_P                  | -38.022415 |
| test/reward_per_eps            | -40        |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.41112912 |
| stats_o/std                    | 0.03966595 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -38.028793 |
| test/Q_plus_P                  | -38.028793 |
| test/reward_per_eps            | -40        |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.41110876 |
| stats_o/std                    | 0.03967205 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.183     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -38.147827 |
| test/Q_plus_P                  | -38.147827 |
| test/reward_per_eps            | -40        |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.41112515 |
| stats_o/std                    | 0.03966803 |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -38.12859  |
| test/Q_plus_P                  | -38.12859  |
| test/reward_per_eps            | -40        |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.4111381   |
| stats_o/std                    | 0.039663244 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -38.160435  |
| test/Q_plus_P                  | -38.160435  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.41113642 |
| stats_o/std                    | 0.03966644 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -38.208397 |
| test/Q_plus_P                  | -38.208397 |
| test/reward_per_eps            | -40        |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.41114867  |
| stats_o/std                    | 0.039662935 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -38.300594  |
| test/Q_plus_P                  | -38.300594  |
| test/reward_per_eps            | -40         |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.4111632   |
| stats_o/std                    | 0.039669704 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -38.255375  |
| test/Q_plus_P                  | -38.255375  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.41118237  |
| stats_o/std                    | 0.039683886 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -38.259254  |
| test/Q_plus_P                  | -38.259254  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.41117978 |
| stats_o/std                    | 0.03969662 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -38.297035 |
| test/Q_plus_P                  | -38.297035 |
| test/reward_per_eps            | -40        |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.41117013  |
| stats_o/std                    | 0.039687295 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -38.309715  |
| test/Q_plus_P                  | -38.309715  |
| test/reward_per_eps            | -40         |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.4111292  |
| stats_o/std                    | 0.03969056 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -38.3094   |
| test/Q_plus_P                  | -38.3094   |
| test/reward_per_eps            | -40        |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.41110444  |
| stats_o/std                    | 0.039693654 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.365      |
| test/Q                         | -38.337578  |
| test/Q_plus_P                  | -38.337578  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.4111048   |
| stats_o/std                    | 0.039687276 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -38.37707   |
| test/Q_plus_P                  | -38.37707   |
| test/reward_per_eps            | -40         |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.41110778  |
| stats_o/std                    | 0.039695363 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -38.364555  |
| test/Q_plus_P                  | -38.364555  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.41110083  |
| stats_o/std                    | 0.039712664 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.348      |
| test/Q                         | -38.375156  |
| test/Q_plus_P                  | -38.375156  |
| test/reward_per_eps            | -40         |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.384      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 263        |
| stats_o/mean                   | 0.4110932  |
| stats_o/std                    | 0.03971872 |
| test/episodes                  | 2640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -38.39654  |
| test/Q_plus_P                  | -38.39654  |
| test/reward_per_eps            | -40        |
| test/steps                     | 105600     |
| train/episodes                 | 10560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.171     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 422400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.41106427  |
| stats_o/std                    | 0.039747976 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -38.4141    |
| test/Q_plus_P                  | -38.4141    |
| test/reward_per_eps            | -40         |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.278      |
| train/info_shaping_reward_min  | -0.392      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.41105095  |
| stats_o/std                    | 0.039750237 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -38.436123  |
| test/Q_plus_P                  | -38.436123  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 266        |
| stats_o/mean                   | 0.41105214 |
| stats_o/std                    | 0.03975705 |
| test/episodes                  | 2670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -38.463985 |
| test/Q_plus_P                  | -38.463985 |
| test/reward_per_eps            | -40        |
| test/steps                     | 106800     |
| train/episodes                 | 10680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 427200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.41105762  |
| stats_o/std                    | 0.039761823 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -38.475384  |
| test/Q_plus_P                  | -38.475384  |
| test/reward_per_eps            | -40         |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.41107205  |
| stats_o/std                    | 0.039770547 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -38.473587  |
| test/Q_plus_P                  | -38.473587  |
| test/reward_per_eps            | -40         |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.4110978   |
| stats_o/std                    | 0.039774876 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -38.496746  |
| test/Q_plus_P                  | -38.496746  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.4110676   |
| stats_o/std                    | 0.039780386 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -38.566544  |
| test/Q_plus_P                  | -38.566544  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.41104332  |
| stats_o/std                    | 0.039803643 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.366      |
| test/Q                         | -38.550724  |
| test/Q_plus_P                  | -38.550724  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.4109999   |
| stats_o/std                    | 0.039806012 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -38.567265  |
| test/Q_plus_P                  | -38.567265  |
| test/reward_per_eps            | -40         |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.41097844  |
| stats_o/std                    | 0.039811734 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -38.59003   |
| test/Q_plus_P                  | -38.59003   |
| test/reward_per_eps            | -40         |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.41095912 |
| stats_o/std                    | 0.03980956 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.203     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -38.596237 |
| test/Q_plus_P                  | -38.596237 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.41096517  |
| stats_o/std                    | 0.039818954 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -38.62017   |
| test/Q_plus_P                  | -38.62017   |
| test/reward_per_eps            | -40         |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.4109588  |
| stats_o/std                    | 0.03982987 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -38.64019  |
| test/Q_plus_P                  | -38.64019  |
| test/reward_per_eps            | -40        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.173     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.41097602  |
| stats_o/std                    | 0.039840158 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -38.646305  |
| test/Q_plus_P                  | -38.646305  |
| test/reward_per_eps            | -40         |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.41097513 |
| stats_o/std                    | 0.03984844 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.339     |
| test/Q                         | -38.69929  |
| test/Q_plus_P                  | -38.69929  |
| test/reward_per_eps            | -40        |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.4109585  |
| stats_o/std                    | 0.03985604 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -38.687992 |
| test/Q_plus_P                  | -38.687992 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.41096303  |
| stats_o/std                    | 0.039866474 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -38.649834  |
| test/Q_plus_P                  | -38.649834  |
| test/reward_per_eps            | -40         |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.41094252 |
| stats_o/std                    | 0.03986745 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.261     |
| test/info_shaping_reward_min   | -0.34      |
| test/Q                         | -38.726986 |
| test/Q_plus_P                  | -38.726986 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.41091415  |
| stats_o/std                    | 0.039874993 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.362      |
| test/Q                         | -38.70091   |
| test/Q_plus_P                  | -38.70091   |
| test/reward_per_eps            | -40         |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.27       |
| train/info_shaping_reward_min  | -0.378      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.41089043 |
| stats_o/std                    | 0.03988316 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.263     |
| test/info_shaping_reward_min   | -0.394     |
| test/Q                         | -38.742043 |
| test/Q_plus_P                  | -38.742043 |
| test/reward_per_eps            | -40        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.265     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.41086686  |
| stats_o/std                    | 0.039885487 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -38.80683   |
| test/Q_plus_P                  | -38.80683   |
| test/reward_per_eps            | -40         |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.184      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.4108691  |
| stats_o/std                    | 0.03989449 |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -38.75711  |
| test/Q_plus_P                  | -38.75711  |
| test/reward_per_eps            | -40        |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.265     |
| train/info_shaping_reward_min  | -0.387     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.41086116  |
| stats_o/std                    | 0.039913706 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -38.78154   |
| test/Q_plus_P                  | -38.78154   |
| test/reward_per_eps            | -40         |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.41084513 |
| stats_o/std                    | 0.03991314 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -38.771225 |
| test/Q_plus_P                  | -38.771225 |
| test/reward_per_eps            | -40        |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.41085747  |
| stats_o/std                    | 0.039903145 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -38.764843  |
| test/Q_plus_P                  | -38.764843  |
| test/reward_per_eps            | -40         |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.4108288   |
| stats_o/std                    | 0.039897576 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -38.845055  |
| test/Q_plus_P                  | -38.845055  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.41081226  |
| stats_o/std                    | 0.039892446 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -38.881622  |
| test/Q_plus_P                  | -38.881622  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.4108113   |
| stats_o/std                    | 0.039899256 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -38.85353   |
| test/Q_plus_P                  | -38.85353   |
| test/reward_per_eps            | -40         |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.41080964  |
| stats_o/std                    | 0.039897516 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -38.92466   |
| test/Q_plus_P                  | -38.92466   |
| test/reward_per_eps            | -40         |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.4108156   |
| stats_o/std                    | 0.039900675 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -38.890743  |
| test/Q_plus_P                  | -38.890743  |
| test/reward_per_eps            | -40         |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.41079333  |
| stats_o/std                    | 0.039895017 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -38.90403   |
| test/Q_plus_P                  | -38.90403   |
| test/reward_per_eps            | -40         |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.41079774  |
| stats_o/std                    | 0.039897535 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.222      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -38.918705  |
| test/Q_plus_P                  | -38.918705  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.410808    |
| stats_o/std                    | 0.039905295 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -38.973812  |
| test/Q_plus_P                  | -38.973812  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.41080487 |
| stats_o/std                    | 0.03989465 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -38.803802 |
| test/Q_plus_P                  | -38.803802 |
| test/reward_per_eps            | -40        |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.4108086  |
| stats_o/std                    | 0.03989614 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -38.98133  |
| test/Q_plus_P                  | -38.98133  |
| test/reward_per_eps            | -40        |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.41081294 |
| stats_o/std                    | 0.03988815 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -38.931694 |
| test/Q_plus_P                  | -38.931694 |
| test/reward_per_eps            | -40        |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.41078588  |
| stats_o/std                    | 0.039894942 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -38.93352   |
| test/Q_plus_P                  | -38.93352   |
| test/reward_per_eps            | -40         |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.41079596  |
| stats_o/std                    | 0.039901774 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.131      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -38.975075  |
| test/Q_plus_P                  | -38.975075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.41083136 |
| stats_o/std                    | 0.03990805 |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -38.969505 |
| test/Q_plus_P                  | -38.969505 |
| test/reward_per_eps            | -40        |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.41083363  |
| stats_o/std                    | 0.039906327 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -38.973373  |
| test/Q_plus_P                  | -38.973373  |
| test/reward_per_eps            | -40         |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.41084087  |
| stats_o/std                    | 0.039906148 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -38.978073  |
| test/Q_plus_P                  | -38.978073  |
| test/reward_per_eps            | -40         |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.41084003 |
| stats_o/std                    | 0.03989512 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -38.99441  |
| test/Q_plus_P                  | -38.99441  |
| test/reward_per_eps            | -40        |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.4108369   |
| stats_o/std                    | 0.039887644 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.05838   |
| test/Q_plus_P                  | -39.05838   |
| test/reward_per_eps            | -40         |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.4108393  |
| stats_o/std                    | 0.03989065 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -39.042355 |
| test/Q_plus_P                  | -39.042355 |
| test/reward_per_eps            | -40        |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.41081497  |
| stats_o/std                    | 0.039891552 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.026695  |
| test/Q_plus_P                  | -39.026695  |
| test/reward_per_eps            | -40         |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.4108077  |
| stats_o/std                    | 0.03988086 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -39.041664 |
| test/Q_plus_P                  | -39.041664 |
| test/reward_per_eps            | -40        |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.41079378  |
| stats_o/std                    | 0.039890155 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.03761   |
| test/Q_plus_P                  | -39.03761   |
| test/reward_per_eps            | -40         |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.41079584 |
| stats_o/std                    | 0.03989083 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.12744  |
| test/Q_plus_P                  | -39.12744  |
| test/reward_per_eps            | -40        |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.41077626  |
| stats_o/std                    | 0.039892267 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.076958  |
| test/Q_plus_P                  | -39.076958  |
| test/reward_per_eps            | -40         |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.41075417 |
| stats_o/std                    | 0.03987949 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.097103 |
| test/Q_plus_P                  | -39.097103 |
| test/reward_per_eps            | -40        |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.41072938  |
| stats_o/std                    | 0.039904412 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.362      |
| test/Q                         | -39.123066  |
| test/Q_plus_P                  | -39.123066  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.386      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.41072682  |
| stats_o/std                    | 0.039904248 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -39.111855  |
| test/Q_plus_P                  | -39.111855  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.41070232  |
| stats_o/std                    | 0.039913896 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.129887  |
| test/Q_plus_P                  | -39.129887  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.41070172  |
| stats_o/std                    | 0.039913833 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -39.08649   |
| test/Q_plus_P                  | -39.08649   |
| test/reward_per_eps            | -40         |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 318       |
| stats_o/mean                   | 0.4106867 |
| stats_o/std                    | 0.0399198 |
| test/episodes                  | 3190      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.174    |
| test/info_shaping_reward_mean  | -0.244    |
| test/info_shaping_reward_min   | -0.3      |
| test/Q                         | -39.15767 |
| test/Q_plus_P                  | -39.15767 |
| test/reward_per_eps            | -40       |
| test/steps                     | 127600    |
| train/episodes                 | 12760     |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.178    |
| train/info_shaping_reward_mean | -0.263    |
| train/info_shaping_reward_min  | -0.365    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 510400    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.41067502  |
| stats_o/std                    | 0.039930437 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.150303  |
| test/Q_plus_P                  | -39.150303  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.4106387  |
| stats_o/std                    | 0.03992642 |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.15735  |
| test/Q_plus_P                  | -39.15735  |
| test/reward_per_eps            | -40        |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.41063365 |
| stats_o/std                    | 0.03992398 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.34      |
| test/Q                         | -39.087074 |
| test/Q_plus_P                  | -39.087074 |
| test/reward_per_eps            | -40        |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.410633   |
| stats_o/std                    | 0.03992549 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.2       |
| test/info_shaping_reward_mean  | -0.267     |
| test/info_shaping_reward_min   | -0.346     |
| test/Q                         | -39.11907  |
| test/Q_plus_P                  | -39.11907  |
| test/reward_per_eps            | -40        |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.41062382  |
| stats_o/std                    | 0.039932624 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -39.17873   |
| test/Q_plus_P                  | -39.17873   |
| test/reward_per_eps            | -40         |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.41060987 |
| stats_o/std                    | 0.03994324 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -39.200993 |
| test/Q_plus_P                  | -39.200993 |
| test/reward_per_eps            | -40        |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.41061822 |
| stats_o/std                    | 0.03994337 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.16938  |
| test/Q_plus_P                  | -39.16938  |
| test/reward_per_eps            | -40        |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.41061446  |
| stats_o/std                    | 0.039945286 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.208237  |
| test/Q_plus_P                  | -39.208237  |
| test/reward_per_eps            | -40         |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.41061547 |
| stats_o/std                    | 0.03995884 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.21549  |
| test/Q_plus_P                  | -39.21549  |
| test/reward_per_eps            | -40        |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.41060114 |
| stats_o/std                    | 0.03995919 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.217762 |
| test/Q_plus_P                  | -39.217762 |
| test/reward_per_eps            | -40        |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.41058287 |
| stats_o/std                    | 0.03995791 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -39.228397 |
| test/Q_plus_P                  | -39.228397 |
| test/reward_per_eps            | -40        |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.41057107 |
| stats_o/std                    | 0.03996221 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.255314 |
| test/Q_plus_P                  | -39.255314 |
| test/reward_per_eps            | -40        |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.41055313 |
| stats_o/std                    | 0.03996109 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.258556 |
| test/Q_plus_P                  | -39.258556 |
| test/reward_per_eps            | -40        |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.41056797  |
| stats_o/std                    | 0.039960332 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.252064  |
| test/Q_plus_P                  | -39.252064  |
| test/reward_per_eps            | -40         |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.41058138 |
| stats_o/std                    | 0.03996419 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.307327 |
| test/Q_plus_P                  | -39.307327 |
| test/reward_per_eps            | -40        |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.174     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.4105682  |
| stats_o/std                    | 0.03997125 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.326     |
| test/Q                         | -39.27502  |
| test/Q_plus_P                  | -39.27502  |
| test/reward_per_eps            | -40        |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.387     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.4105669   |
| stats_o/std                    | 0.039974198 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.264088  |
| test/Q_plus_P                  | -39.264088  |
| test/reward_per_eps            | -40         |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.41057315  |
| stats_o/std                    | 0.039981734 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.30581   |
| test/Q_plus_P                  | -39.30581   |
| test/reward_per_eps            | -40         |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.41057226  |
| stats_o/std                    | 0.039976213 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.29216   |
| test/Q_plus_P                  | -39.29216   |
| test/reward_per_eps            | -40         |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.4105729   |
| stats_o/std                    | 0.039976582 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -39.29666   |
| test/Q_plus_P                  | -39.29666   |
| test/reward_per_eps            | -40         |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.41056672  |
| stats_o/std                    | 0.039974518 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.32007   |
| test/Q_plus_P                  | -39.32007   |
| test/reward_per_eps            | -40         |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.41056624  |
| stats_o/std                    | 0.039968967 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -39.343117  |
| test/Q_plus_P                  | -39.343117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.41056597  |
| stats_o/std                    | 0.039957758 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.34168   |
| test/Q_plus_P                  | -39.34168   |
| test/reward_per_eps            | -40         |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.4105458   |
| stats_o/std                    | 0.039955676 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.270138  |
| test/Q_plus_P                  | -39.270138  |
| test/reward_per_eps            | -40         |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.4105409   |
| stats_o/std                    | 0.039953608 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.35758   |
| test/Q_plus_P                  | -39.35758   |
| test/reward_per_eps            | -40         |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.41053987  |
| stats_o/std                    | 0.039955664 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.320023  |
| test/Q_plus_P                  | -39.320023  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.41052508 |
| stats_o/std                    | 0.03995177 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -39.352898 |
| test/Q_plus_P                  | -39.352898 |
| test/reward_per_eps            | -40        |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.41052306 |
| stats_o/std                    | 0.03995313 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -39.3639   |
| test/Q_plus_P                  | -39.3639   |
| test/reward_per_eps            | -40        |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.41050756 |
| stats_o/std                    | 0.03995059 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.369843 |
| test/Q_plus_P                  | -39.369843 |
| test/reward_per_eps            | -40        |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.41051146  |
| stats_o/std                    | 0.039954223 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.40841   |
| test/Q_plus_P                  | -39.40841   |
| test/reward_per_eps            | -40         |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.41050652 |
| stats_o/std                    | 0.0399569  |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.37452  |
| test/Q_plus_P                  | -39.37452  |
| test/reward_per_eps            | -40        |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.41049242 |
| stats_o/std                    | 0.03996225 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.451313 |
| test/Q_plus_P                  | -39.451313 |
| test/reward_per_eps            | -40        |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.41049525  |
| stats_o/std                    | 0.039976265 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -39.401333  |
| test/Q_plus_P                  | -39.401333  |
| test/reward_per_eps            | -40         |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.41048643  |
| stats_o/std                    | 0.039983183 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.42676   |
| test/Q_plus_P                  | -39.42676   |
| test/reward_per_eps            | -40         |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.41047728  |
| stats_o/std                    | 0.039980803 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.204      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.544384  |
| test/Q_plus_P                  | -39.544384  |
| test/reward_per_eps            | -40         |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.41046265 |
| stats_o/std                    | 0.03998594 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.137     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.444748 |
| test/Q_plus_P                  | -39.444748 |
| test/reward_per_eps            | -40        |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 355        |
| stats_o/mean                   | 0.4104655  |
| stats_o/std                    | 0.0399916  |
| test/episodes                  | 3560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.451885 |
| test/Q_plus_P                  | -39.451885 |
| test/reward_per_eps            | -40        |
| test/steps                     | 142400     |
| train/episodes                 | 14240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 569600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.4104639  |
| stats_o/std                    | 0.03998633 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.451454 |
| test/Q_plus_P                  | -39.451454 |
| test/reward_per_eps            | -40        |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 357        |
| stats_o/mean                   | 0.41045186 |
| stats_o/std                    | 0.03998049 |
| test/episodes                  | 3580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.45025  |
| test/Q_plus_P                  | -39.45025  |
| test/reward_per_eps            | -40        |
| test/steps                     | 143200     |
| train/episodes                 | 14320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 572800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.41045403 |
| stats_o/std                    | 0.03999414 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -39.494728 |
| test/Q_plus_P                  | -39.494728 |
| test/reward_per_eps            | -40        |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.41045523  |
| stats_o/std                    | 0.039990593 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.47673   |
| test/Q_plus_P                  | -39.47673   |
| test/reward_per_eps            | -40         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.41043684 |
| stats_o/std                    | 0.0399782  |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -39.47049  |
| test/Q_plus_P                  | -39.47049  |
| test/reward_per_eps            | -40        |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.4104476   |
| stats_o/std                    | 0.039991524 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.492306  |
| test/Q_plus_P                  | -39.492306  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.41046044  |
| stats_o/std                    | 0.039991964 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.512394  |
| test/Q_plus_P                  | -39.512394  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.41046032  |
| stats_o/std                    | 0.039994366 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.501144  |
| test/Q_plus_P                  | -39.501144  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.4104428  |
| stats_o/std                    | 0.03999183 |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.195     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.511852 |
| test/Q_plus_P                  | -39.511852 |
| test/reward_per_eps            | -40        |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.41044697  |
| stats_o/std                    | 0.039995734 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.511776  |
| test/Q_plus_P                  | -39.511776  |
| test/reward_per_eps            | -40         |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.4104465   |
| stats_o/std                    | 0.039995827 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.349      |
| test/Q                         | -39.511578  |
| test/Q_plus_P                  | -39.511578  |
| test/reward_per_eps            | -40         |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.41042778  |
| stats_o/std                    | 0.039993674 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.504894  |
| test/Q_plus_P                  | -39.504894  |
| test/reward_per_eps            | -40         |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.4104351   |
| stats_o/std                    | 0.040003803 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.50724   |
| test/Q_plus_P                  | -39.50724   |
| test/reward_per_eps            | -40         |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.41043553 |
| stats_o/std                    | 0.04000255 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -39.5237   |
| test/Q_plus_P                  | -39.5237   |
| test/reward_per_eps            | -40        |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.41043034  |
| stats_o/std                    | 0.040000554 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.542545  |
| test/Q_plus_P                  | -39.542545  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.41043258  |
| stats_o/std                    | 0.039996512 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.536003  |
| test/Q_plus_P                  | -39.536003  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.41043374  |
| stats_o/std                    | 0.040002674 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.61635   |
| test/Q_plus_P                  | -39.61635   |
| test/reward_per_eps            | -40         |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.4104483   |
| stats_o/std                    | 0.040012304 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -39.5447    |
| test/Q_plus_P                  | -39.5447    |
| test/reward_per_eps            | -40         |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.41045377 |
| stats_o/std                    | 0.04002011 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.116     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.549374 |
| test/Q_plus_P                  | -39.549374 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.4104673   |
| stats_o/std                    | 0.040020224 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.531334  |
| test/Q_plus_P                  | -39.531334  |
| test/reward_per_eps            | -40         |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.41047695  |
| stats_o/std                    | 0.040013142 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.6071    |
| test/Q_plus_P                  | -39.6071    |
| test/reward_per_eps            | -40         |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.4104782   |
| stats_o/std                    | 0.040013313 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -39.56505   |
| test/Q_plus_P                  | -39.56505   |
| test/reward_per_eps            | -40         |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.4104583  |
| stats_o/std                    | 0.04001754 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.133     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.549175 |
| test/Q_plus_P                  | -39.549175 |
| test/reward_per_eps            | -40        |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.41045102  |
| stats_o/std                    | 0.040014997 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.584927  |
| test/Q_plus_P                  | -39.584927  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.4104646   |
| stats_o/std                    | 0.040013764 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.609818  |
| test/Q_plus_P                  | -39.609818  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 381       |
| stats_o/mean                   | 0.4104654 |
| stats_o/std                    | 0.040012  |
| test/episodes                  | 3820      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.185    |
| test/info_shaping_reward_mean  | -0.218    |
| test/info_shaping_reward_min   | -0.314    |
| test/Q                         | -39.58887 |
| test/Q_plus_P                  | -39.58887 |
| test/reward_per_eps            | -40       |
| test/steps                     | 152800    |
| train/episodes                 | 15280     |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.144    |
| train/info_shaping_reward_mean | -0.248    |
| train/info_shaping_reward_min  | -0.344    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 611200    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.41047084 |
| stats_o/std                    | 0.04000868 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -39.57353  |
| test/Q_plus_P                  | -39.57353  |
| test/reward_per_eps            | -40        |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.4104884  |
| stats_o/std                    | 0.04000591 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -39.7094   |
| test/Q_plus_P                  | -39.7094   |
| test/reward_per_eps            | -40        |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.4104905  |
| stats_o/std                    | 0.04000366 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.608223 |
| test/Q_plus_P                  | -39.608223 |
| test/reward_per_eps            | -40        |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.41050258  |
| stats_o/std                    | 0.040004548 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -39.610718  |
| test/Q_plus_P                  | -39.610718  |
| test/reward_per_eps            | -40         |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.41051212  |
| stats_o/std                    | 0.039997537 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.618317  |
| test/Q_plus_P                  | -39.618317  |
| test/reward_per_eps            | -40         |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.41051885 |
| stats_o/std                    | 0.03999569 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.619198 |
| test/Q_plus_P                  | -39.619198 |
| test/reward_per_eps            | -40        |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.41052008  |
| stats_o/std                    | 0.039988983 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.625633  |
| test/Q_plus_P                  | -39.625633  |
| test/reward_per_eps            | -40         |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.4105215   |
| stats_o/std                    | 0.039997693 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.578426  |
| test/Q_plus_P                  | -39.578426  |
| test/reward_per_eps            | -40         |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.41052946  |
| stats_o/std                    | 0.039999794 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.61073   |
| test/Q_plus_P                  | -39.61073   |
| test/reward_per_eps            | -40         |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.410527    |
| stats_o/std                    | 0.039989416 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -39.591076  |
| test/Q_plus_P                  | -39.591076  |
| test/reward_per_eps            | -40         |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.410534   |
| stats_o/std                    | 0.03998431 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.63147  |
| test/Q_plus_P                  | -39.63147  |
| test/reward_per_eps            | -40        |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.41054806  |
| stats_o/std                    | 0.039982863 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.65022   |
| test/Q_plus_P                  | -39.65022   |
| test/reward_per_eps            | -40         |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.41053823  |
| stats_o/std                    | 0.039980438 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -39.662613  |
| test/Q_plus_P                  | -39.662613  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.41053775  |
| stats_o/std                    | 0.039979514 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.361      |
| test/Q                         | -39.650425  |
| test/Q_plus_P                  | -39.650425  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.4105201   |
| stats_o/std                    | 0.039981987 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -39.627296  |
| test/Q_plus_P                  | -39.627296  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.41052353  |
| stats_o/std                    | 0.039981525 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.656162  |
| test/Q_plus_P                  | -39.656162  |
| test/reward_per_eps            | -40         |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.41053462  |
| stats_o/std                    | 0.039984588 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.69807   |
| test/Q_plus_P                  | -39.69807   |
| test/reward_per_eps            | -40         |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.41053095 |
| stats_o/std                    | 0.03999395 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -39.75408  |
| test/Q_plus_P                  | -39.75408  |
| test/reward_per_eps            | -40        |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.41051543  |
| stats_o/std                    | 0.039992653 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -39.67898   |
| test/Q_plus_P                  | -39.67898   |
| test/reward_per_eps            | -40         |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.4105111  |
| stats_o/std                    | 0.03999141 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.6812   |
| test/Q_plus_P                  | -39.6812   |
| test/reward_per_eps            | -40        |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.41051236 |
| stats_o/std                    | 0.0399973  |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.65091  |
| test/Q_plus_P                  | -39.65091  |
| test/reward_per_eps            | -40        |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.41052222  |
| stats_o/std                    | 0.039997842 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.67282   |
| test/Q_plus_P                  | -39.67282   |
| test/reward_per_eps            | -40         |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.41052893  |
| stats_o/std                    | 0.040000577 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.66187   |
| test/Q_plus_P                  | -39.66187   |
| test/reward_per_eps            | -40         |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.41053125  |
| stats_o/std                    | 0.039996874 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.670456  |
| test/Q_plus_P                  | -39.670456  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.41052762 |
| stats_o/std                    | 0.03999427 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.189     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -39.749203 |
| test/Q_plus_P                  | -39.749203 |
| test/reward_per_eps            | -40        |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.41052434  |
| stats_o/std                    | 0.039998133 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.67944   |
| test/Q_plus_P                  | -39.67944   |
| test/reward_per_eps            | -40         |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.41051483 |
| stats_o/std                    | 0.03999255 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.676075 |
| test/Q_plus_P                  | -39.676075 |
| test/reward_per_eps            | -40        |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.41050985  |
| stats_o/std                    | 0.039996002 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.652523  |
| test/Q_plus_P                  | -39.652523  |
| test/reward_per_eps            | -40         |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.4105061   |
| stats_o/std                    | 0.039994158 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -39.773327  |
| test/Q_plus_P                  | -39.773327  |
| test/reward_per_eps            | -40         |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.41050673  |
| stats_o/std                    | 0.039991293 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.70134   |
| test/Q_plus_P                  | -39.70134   |
| test/reward_per_eps            | -40         |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.41051248  |
| stats_o/std                    | 0.039991833 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.720924  |
| test/Q_plus_P                  | -39.720924  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.41051438  |
| stats_o/std                    | 0.039998323 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.738075  |
| test/Q_plus_P                  | -39.738075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.41051984 |
| stats_o/std                    | 0.04000544 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.694027 |
| test/Q_plus_P                  | -39.694027 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.41052997 |
| stats_o/std                    | 0.04001625 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.629284 |
| test/Q_plus_P                  | -39.629284 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.4105307  |
| stats_o/std                    | 0.04001131 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.73604  |
| test/Q_plus_P                  | -39.73604  |
| test/reward_per_eps            | -40        |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.41053662 |
| stats_o/std                    | 0.04001308 |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.680973 |
| test/Q_plus_P                  | -39.680973 |
| test/reward_per_eps            | -40        |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.41053256  |
| stats_o/std                    | 0.040025573 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -39.70804   |
| test/Q_plus_P                  | -39.70804   |
| test/reward_per_eps            | -40         |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.41053858 |
| stats_o/std                    | 0.04002983 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.686275 |
| test/Q_plus_P                  | -39.686275 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.410547   |
| stats_o/std                    | 0.04003028 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.758923 |
| test/Q_plus_P                  | -39.758923 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.41052923 |
| stats_o/std                    | 0.04003361 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.717007 |
| test/Q_plus_P                  | -39.717007 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.176     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.41053355 |
| stats_o/std                    | 0.04003006 |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.735893 |
| test/Q_plus_P                  | -39.735893 |
| test/reward_per_eps            | -40        |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.4105158   |
| stats_o/std                    | 0.040032547 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.71013   |
| test/Q_plus_P                  | -39.71013   |
| test/reward_per_eps            | -40         |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.41050103  |
| stats_o/std                    | 0.040036153 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.717358  |
| test/Q_plus_P                  | -39.717358  |
| test/reward_per_eps            | -40         |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.41049278  |
| stats_o/std                    | 0.040038746 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.207      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -39.6985    |
| test/Q_plus_P                  | -39.6985    |
| test/reward_per_eps            | -40         |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.41047934  |
| stats_o/std                    | 0.040043373 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.71621   |
| test/Q_plus_P                  | -39.71621   |
| test/reward_per_eps            | -40         |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 427        |
| stats_o/mean                   | 0.41046748 |
| stats_o/std                    | 0.04004044 |
| test/episodes                  | 4280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.212     |
| test/info_shaping_reward_mean  | -0.262     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -39.72643  |
| test/Q_plus_P                  | -39.72643  |
| test/reward_per_eps            | -40        |
| test/steps                     | 171200     |
| train/episodes                 | 17120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 684800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.41046634  |
| stats_o/std                    | 0.040042985 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.725143  |
| test/Q_plus_P                  | -39.725143  |
| test/reward_per_eps            | -40         |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.41044483  |
| stats_o/std                    | 0.040056366 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.725754  |
| test/Q_plus_P                  | -39.725754  |
| test/reward_per_eps            | -40         |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.267      |
| train/info_shaping_reward_min  | -0.381      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.41045526 |
| stats_o/std                    | 0.04005615 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -39.74125  |
| test/Q_plus_P                  | -39.74125  |
| test/reward_per_eps            | -40        |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.4104439   |
| stats_o/std                    | 0.040056396 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.361      |
| test/Q                         | -39.69208   |
| test/Q_plus_P                  | -39.69208   |
| test/reward_per_eps            | -40         |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.4104364   |
| stats_o/std                    | 0.040060062 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -39.716297  |
| test/Q_plus_P                  | -39.716297  |
| test/reward_per_eps            | -40         |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.268      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.4104259   |
| stats_o/std                    | 0.040059336 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.80488   |
| test/Q_plus_P                  | -39.80488   |
| test/reward_per_eps            | -40         |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.41042444 |
| stats_o/std                    | 0.04006117 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.659668 |
| test/Q_plus_P                  | -39.659668 |
| test/reward_per_eps            | -40        |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.4104209  |
| stats_o/std                    | 0.04006639 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.221     |
| test/info_shaping_reward_mean  | -0.273     |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -39.752502 |
| test/Q_plus_P                  | -39.752502 |
| test/reward_per_eps            | -40        |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.174     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.369     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.41039935  |
| stats_o/std                    | 0.040064298 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.717472  |
| test/Q_plus_P                  | -39.717472  |
| test/reward_per_eps            | -40         |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.383      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.41039398 |
| stats_o/std                    | 0.04005901 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.353     |
| test/Q                         | -39.719143 |
| test/Q_plus_P                  | -39.719143 |
| test/reward_per_eps            | -40        |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.410387    |
| stats_o/std                    | 0.040051147 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.725372  |
| test/Q_plus_P                  | -39.725372  |
| test/reward_per_eps            | -40         |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.41039023  |
| stats_o/std                    | 0.040054765 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.73565   |
| test/Q_plus_P                  | -39.73565   |
| test/reward_per_eps            | -40         |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.179      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.410382    |
| stats_o/std                    | 0.040054094 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -39.770653  |
| test/Q_plus_P                  | -39.770653  |
| test/reward_per_eps            | -40         |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.41036728  |
| stats_o/std                    | 0.040057775 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -39.785324  |
| test/Q_plus_P                  | -39.785324  |
| test/reward_per_eps            | -40         |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.41036156 |
| stats_o/std                    | 0.04005932 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -39.80497  |
| test/Q_plus_P                  | -39.80497  |
| test/reward_per_eps            | -40        |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.41035116  |
| stats_o/std                    | 0.040064566 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.790672  |
| test/Q_plus_P                  | -39.790672  |
| test/reward_per_eps            | -40         |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.268      |
| train/info_shaping_reward_min  | -0.394      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 444        |
| stats_o/mean                   | 0.41034484 |
| stats_o/std                    | 0.04006586 |
| test/episodes                  | 4450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.795006 |
| test/Q_plus_P                  | -39.795006 |
| test/reward_per_eps            | -40        |
| test/steps                     | 178000     |
| train/episodes                 | 17800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 712000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.41033056 |
| stats_o/std                    | 0.04006991 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.204     |
| test/info_shaping_reward_mean  | -0.265     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -39.847164 |
| test/Q_plus_P                  | -39.847164 |
| test/reward_per_eps            | -40        |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.41032243 |
| stats_o/std                    | 0.04007264 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.196     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.82024  |
| test/Q_plus_P                  | -39.82024  |
| test/reward_per_eps            | -40        |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.41031942 |
| stats_o/std                    | 0.04007546 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.817585 |
| test/Q_plus_P                  | -39.817585 |
| test/reward_per_eps            | -40        |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.41030478  |
| stats_o/std                    | 0.040078428 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -39.800694  |
| test/Q_plus_P                  | -39.800694  |
| test/reward_per_eps            | -40         |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.4102901  |
| stats_o/std                    | 0.04008269 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.329     |
| test/Q                         | -39.87863  |
| test/Q_plus_P                  | -39.87863  |
| test/reward_per_eps            | -40        |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.266     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.41028965  |
| stats_o/std                    | 0.040086802 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -39.898273  |
| test/Q_plus_P                  | -39.898273  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.173      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.41028866  |
| stats_o/std                    | 0.040084336 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.821503  |
| test/Q_plus_P                  | -39.821503  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.41027513 |
| stats_o/std                    | 0.04008632 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -39.83294  |
| test/Q_plus_P                  | -39.83294  |
| test/reward_per_eps            | -40        |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.182     |
| train/info_shaping_reward_mean | -0.263     |
| train/info_shaping_reward_min  | -0.376     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.41027418  |
| stats_o/std                    | 0.040087312 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -39.826797  |
| test/Q_plus_P                  | -39.826797  |
| test/reward_per_eps            | -40         |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.41027352  |
| stats_o/std                    | 0.040089265 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -39.82866   |
| test/Q_plus_P                  | -39.82866   |
| test/reward_per_eps            | -40         |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 455        |
| stats_o/mean                   | 0.41025475 |
| stats_o/std                    | 0.04009666 |
| test/episodes                  | 4560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -39.827957 |
| test/Q_plus_P                  | -39.827957 |
| test/reward_per_eps            | -40        |
| test/steps                     | 182400     |
| train/episodes                 | 18240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.265     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 729600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.41026774 |
| stats_o/std                    | 0.04010263 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -39.830345 |
| test/Q_plus_P                  | -39.830345 |
| test/reward_per_eps            | -40        |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.41027883  |
| stats_o/std                    | 0.040105868 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.345      |
| test/Q                         | -39.83894   |
| test/Q_plus_P                  | -39.83894   |
| test/reward_per_eps            | -40         |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.4102831   |
| stats_o/std                    | 0.040113267 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.80237   |
| test/Q_plus_P                  | -39.80237   |
| test/reward_per_eps            | -40         |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.41027784  |
| stats_o/std                    | 0.040117692 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.82726   |
| test/Q_plus_P                  | -39.82726   |
| test/reward_per_eps            | -40         |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.41026405  |
| stats_o/std                    | 0.040119898 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -39.859756  |
| test/Q_plus_P                  | -39.859756  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.41025877 |
| stats_o/std                    | 0.04011023 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.34      |
| test/Q                         | -39.876987 |
| test/Q_plus_P                  | -39.876987 |
| test/reward_per_eps            | -40        |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.41025737 |
| stats_o/std                    | 0.04011758 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.88146  |
| test/Q_plus_P                  | -39.88146  |
| test/reward_per_eps            | -40        |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.41024926  |
| stats_o/std                    | 0.040120523 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.889038  |
| test/Q_plus_P                  | -39.889038  |
| test/reward_per_eps            | -40         |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.41023424  |
| stats_o/std                    | 0.040124517 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.84494   |
| test/Q_plus_P                  | -39.84494   |
| test/reward_per_eps            | -40         |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 465        |
| stats_o/mean                   | 0.4102308  |
| stats_o/std                    | 0.04013432 |
| test/episodes                  | 4660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.85835  |
| test/Q_plus_P                  | -39.85835  |
| test/reward_per_eps            | -40        |
| test/steps                     | 186400     |
| train/episodes                 | 18640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 745600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.41022536 |
| stats_o/std                    | 0.04013468 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -39.78839  |
| test/Q_plus_P                  | -39.78839  |
| test/reward_per_eps            | -40        |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.4102325  |
| stats_o/std                    | 0.04013176 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -39.867172 |
| test/Q_plus_P                  | -39.867172 |
| test/reward_per_eps            | -40        |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.41022825 |
| stats_o/std                    | 0.04014081 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.2       |
| test/info_shaping_reward_mean  | -0.273     |
| test/info_shaping_reward_min   | -0.34      |
| test/Q                         | -39.840527 |
| test/Q_plus_P                  | -39.840527 |
| test/reward_per_eps            | -40        |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.265     |
| train/info_shaping_reward_min  | -0.383     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 469        |
| stats_o/mean                   | 0.41023305 |
| stats_o/std                    | 0.04014315 |
| test/episodes                  | 4700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.263     |
| test/info_shaping_reward_min   | -0.334     |
| test/Q                         | -39.79799  |
| test/Q_plus_P                  | -39.79799  |
| test/reward_per_eps            | -40        |
| test/steps                     | 188000     |
| train/episodes                 | 18800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 752000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.41025397  |
| stats_o/std                    | 0.040150158 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.897533  |
| test/Q_plus_P                  | -39.897533  |
| test/reward_per_eps            | -40         |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.41026136  |
| stats_o/std                    | 0.040154163 |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.9021    |
| test/Q_plus_P                  | -39.9021    |
| test/reward_per_eps            | -40         |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.410261    |
| stats_o/std                    | 0.040155917 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.895416  |
| test/Q_plus_P                  | -39.895416  |
| test/reward_per_eps            | -40         |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.4102504   |
| stats_o/std                    | 0.040155068 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.337      |
| test/Q                         | -39.82788   |
| test/Q_plus_P                  | -39.82788   |
| test/reward_per_eps            | -40         |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.41023263  |
| stats_o/std                    | 0.040152498 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.89528   |
| test/Q_plus_P                  | -39.89528   |
| test/reward_per_eps            | -40         |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.41022754  |
| stats_o/std                    | 0.040157642 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.95359   |
| test/Q_plus_P                  | -39.95359   |
| test/reward_per_eps            | -40         |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.41022244  |
| stats_o/std                    | 0.040152006 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.21       |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.94728   |
| test/Q_plus_P                  | -39.94728   |
| test/reward_per_eps            | -40         |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 477        |
| stats_o/mean                   | 0.41023615 |
| stats_o/std                    | 0.04014732 |
| test/episodes                  | 4780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.86947  |
| test/Q_plus_P                  | -39.86947  |
| test/reward_per_eps            | -40        |
| test/steps                     | 191200     |
| train/episodes                 | 19120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 764800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.41023675  |
| stats_o/std                    | 0.040146355 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -39.889736  |
| test/Q_plus_P                  | -39.889736  |
| test/reward_per_eps            | -40         |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.4102396  |
| stats_o/std                    | 0.04015198 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.195     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.879807 |
| test/Q_plus_P                  | -39.879807 |
| test/reward_per_eps            | -40        |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.41024268 |
| stats_o/std                    | 0.04015815 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -39.88609  |
| test/Q_plus_P                  | -39.88609  |
| test/reward_per_eps            | -40        |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.41023362  |
| stats_o/std                    | 0.040157583 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.899845  |
| test/Q_plus_P                  | -39.899845  |
| test/reward_per_eps            | -40         |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.41023585  |
| stats_o/std                    | 0.040157747 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.88023   |
| test/Q_plus_P                  | -39.88023   |
| test/reward_per_eps            | -40         |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.4102534   |
| stats_o/std                    | 0.040159028 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.903187  |
| test/Q_plus_P                  | -39.903187  |
| test/reward_per_eps            | -40         |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.41024968 |
| stats_o/std                    | 0.0401619  |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.219     |
| test/info_shaping_reward_mean  | -0.276     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -39.903343 |
| test/Q_plus_P                  | -39.903343 |
| test/reward_per_eps            | -40        |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.41024613  |
| stats_o/std                    | 0.040168576 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -39.91593   |
| test/Q_plus_P                  | -39.91593   |
| test/reward_per_eps            | -40         |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.41024005 |
| stats_o/std                    | 0.04016419 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.91104  |
| test/Q_plus_P                  | -39.91104  |
| test/reward_per_eps            | -40        |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.41022956  |
| stats_o/std                    | 0.040172584 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.89424   |
| test/Q_plus_P                  | -39.89424   |
| test/reward_per_eps            | -40         |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.272      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.41023803 |
| stats_o/std                    | 0.04017059 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.345     |
| test/Q                         | -39.902008 |
| test/Q_plus_P                  | -39.902008 |
| test/reward_per_eps            | -40        |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.4102249   |
| stats_o/std                    | 0.040175106 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.86358   |
| test/Q_plus_P                  | -39.86358   |
| test/reward_per_eps            | -40         |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.41021788 |
| stats_o/std                    | 0.04017909 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -39.898968 |
| test/Q_plus_P                  | -39.898968 |
| test/reward_per_eps            | -40        |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.41021     |
| stats_o/std                    | 0.040179364 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.205      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.894585  |
| test/Q_plus_P                  | -39.894585  |
| test/reward_per_eps            | -40         |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.41020048 |
| stats_o/std                    | 0.04018358 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -39.905884 |
| test/Q_plus_P                  | -39.905884 |
| test/reward_per_eps            | -40        |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.4102033  |
| stats_o/std                    | 0.04018768 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -39.892353 |
| test/Q_plus_P                  | -39.892353 |
| test/reward_per_eps            | -40        |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.41021898 |
| stats_o/std                    | 0.04019456 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.903446 |
| test/Q_plus_P                  | -39.903446 |
| test/reward_per_eps            | -40        |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.41022643  |
| stats_o/std                    | 0.040194534 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.881832  |
| test/Q_plus_P                  | -39.881832  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.41023192  |
| stats_o/std                    | 0.040197846 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.87006   |
| test/Q_plus_P                  | -39.87006   |
| test/reward_per_eps            | -40         |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.41023698  |
| stats_o/std                    | 0.040197086 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.90683   |
| test/Q_plus_P                  | -39.90683   |
| test/reward_per_eps            | -40         |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.41024277 |
| stats_o/std                    | 0.04019239 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.89411  |
| test/Q_plus_P                  | -39.89411  |
| test/reward_per_eps            | -40        |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.41025403  |
| stats_o/std                    | 0.040200654 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.921177  |
| test/Q_plus_P                  | -39.921177  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.41026095 |
| stats_o/std                    | 0.04020424 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.897335 |
| test/Q_plus_P                  | -39.897335 |
| test/reward_per_eps            | -40        |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.41027263 |
| stats_o/std                    | 0.04020331 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.916416 |
| test/Q_plus_P                  | -39.916416 |
| test/reward_per_eps            | -40        |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.41026697  |
| stats_o/std                    | 0.040213224 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.19       |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.899124  |
| test/Q_plus_P                  | -39.899124  |
| test/reward_per_eps            | -40         |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.4102566   |
| stats_o/std                    | 0.040215757 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.811417  |
| test/Q_plus_P                  | -39.811417  |
| test/reward_per_eps            | -40         |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.41027403  |
| stats_o/std                    | 0.040218335 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.124      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.865746  |
| test/Q_plus_P                  | -39.865746  |
| test/reward_per_eps            | -40         |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.4102681  |
| stats_o/std                    | 0.04021994 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -39.90321  |
| test/Q_plus_P                  | -39.90321  |
| test/reward_per_eps            | -40        |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.41026983  |
| stats_o/std                    | 0.040226027 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.893833  |
| test/Q_plus_P                  | -39.893833  |
| test/reward_per_eps            | -40         |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.4102771   |
| stats_o/std                    | 0.040227666 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.927425  |
| test/Q_plus_P                  | -39.927425  |
| test/reward_per_eps            | -40         |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.41027173  |
| stats_o/std                    | 0.040223874 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.8806    |
| test/Q_plus_P                  | -39.8806    |
| test/reward_per_eps            | -40         |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.4102703   |
| stats_o/std                    | 0.040226903 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.894135  |
| test/Q_plus_P                  | -39.894135  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.41027835  |
| stats_o/std                    | 0.040231552 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.89551   |
| test/Q_plus_P                  | -39.89551   |
| test/reward_per_eps            | -40         |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.41027692  |
| stats_o/std                    | 0.040232945 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.874657  |
| test/Q_plus_P                  | -39.874657  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.4102925   |
| stats_o/std                    | 0.040238887 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.943222  |
| test/Q_plus_P                  | -39.943222  |
| test/reward_per_eps            | -40         |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.41028237  |
| stats_o/std                    | 0.040237144 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.878437  |
| test/Q_plus_P                  | -39.878437  |
| test/reward_per_eps            | -40         |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.41030872  |
| stats_o/std                    | 0.040242236 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.918564  |
| test/Q_plus_P                  | -39.918564  |
| test/reward_per_eps            | -40         |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.4103136  |
| stats_o/std                    | 0.04024448 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.108     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.906162 |
| test/Q_plus_P                  | -39.906162 |
| test/reward_per_eps            | -40        |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.41032776 |
| stats_o/std                    | 0.04024164 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -39.89856  |
| test/Q_plus_P                  | -39.89856  |
| test/reward_per_eps            | -40        |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.41034257  |
| stats_o/std                    | 0.040243845 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.90102   |
| test/Q_plus_P                  | -39.90102   |
| test/reward_per_eps            | -40         |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.41034842  |
| stats_o/std                    | 0.040249445 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.898922  |
| test/Q_plus_P                  | -39.898922  |
| test/reward_per_eps            | -40         |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.41035697  |
| stats_o/std                    | 0.040252555 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.89507   |
| test/Q_plus_P                  | -39.89507   |
| test/reward_per_eps            | -40         |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.410368    |
| stats_o/std                    | 0.040251393 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -39.896698  |
| test/Q_plus_P                  | -39.896698  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.41037366  |
| stats_o/std                    | 0.040243868 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.896633  |
| test/Q_plus_P                  | -39.896633  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.41038075 |
| stats_o/std                    | 0.04025196 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.904846 |
| test/Q_plus_P                  | -39.904846 |
| test/reward_per_eps            | -40        |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.41038796  |
| stats_o/std                    | 0.040257294 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.912315  |
| test/Q_plus_P                  | -39.912315  |
| test/reward_per_eps            | -40         |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.410381    |
| stats_o/std                    | 0.040259928 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.906876  |
| test/Q_plus_P                  | -39.906876  |
| test/reward_per_eps            | -40         |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 525        |
| stats_o/mean                   | 0.41037655 |
| stats_o/std                    | 0.04026074 |
| test/episodes                  | 5260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.892212 |
| test/Q_plus_P                  | -39.892212 |
| test/reward_per_eps            | -40        |
| test/steps                     | 210400     |
| train/episodes                 | 21040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 841600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.41036856 |
| stats_o/std                    | 0.04026216 |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -39.90819  |
| test/Q_plus_P                  | -39.90819  |
| test/reward_per_eps            | -40        |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 527        |
| stats_o/mean                   | 0.41036725 |
| stats_o/std                    | 0.04026105 |
| test/episodes                  | 5280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.908936 |
| test/Q_plus_P                  | -39.908936 |
| test/reward_per_eps            | -40        |
| test/steps                     | 211200     |
| train/episodes                 | 21120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 844800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.41036177 |
| stats_o/std                    | 0.04026211 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -39.919437 |
| test/Q_plus_P                  | -39.919437 |
| test/reward_per_eps            | -40        |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.165     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.4103895   |
| stats_o/std                    | 0.040274806 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.104      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.90582   |
| test/Q_plus_P                  | -39.90582   |
| test/reward_per_eps            | -40         |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.41040203 |
| stats_o/std                    | 0.04027969 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.909546 |
| test/Q_plus_P                  | -39.909546 |
| test/reward_per_eps            | -40        |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.41041347  |
| stats_o/std                    | 0.040283363 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.131      |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.904774  |
| test/Q_plus_P                  | -39.904774  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 532        |
| stats_o/mean                   | 0.41041481 |
| stats_o/std                    | 0.04027893 |
| test/episodes                  | 5330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.899097 |
| test/Q_plus_P                  | -39.899097 |
| test/reward_per_eps            | -40        |
| test/steps                     | 213200     |
| train/episodes                 | 21320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 852800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.41040775  |
| stats_o/std                    | 0.040279653 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.860863  |
| test/Q_plus_P                  | -39.860863  |
| test/reward_per_eps            | -40         |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.4104115  |
| stats_o/std                    | 0.04027852 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -39.9032   |
| test/Q_plus_P                  | -39.9032   |
| test/reward_per_eps            | -40        |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.41041005  |
| stats_o/std                    | 0.040274788 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.913963  |
| test/Q_plus_P                  | -39.913963  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.41041303 |
| stats_o/std                    | 0.04027705 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -39.93205  |
| test/Q_plus_P                  | -39.93205  |
| test/reward_per_eps            | -40        |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.41041127  |
| stats_o/std                    | 0.040281337 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.196      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -39.872364  |
| test/Q_plus_P                  | -39.872364  |
| test/reward_per_eps            | -40         |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.41041198  |
| stats_o/std                    | 0.040279474 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.126      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.87877   |
| test/Q_plus_P                  | -39.87877   |
| test/reward_per_eps            | -40         |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.213      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.41041386 |
| stats_o/std                    | 0.040277   |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.89623  |
| test/Q_plus_P                  | -39.89623  |
| test/reward_per_eps            | -40        |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.410422   |
| stats_o/std                    | 0.04027295 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -39.90574  |
| test/Q_plus_P                  | -39.90574  |
| test/reward_per_eps            | -40        |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.117     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.41042864  |
| stats_o/std                    | 0.040275607 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.131      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.896046  |
| test/Q_plus_P                  | -39.896046  |
| test/reward_per_eps            | -40         |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.4104288  |
| stats_o/std                    | 0.04027158 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.884247 |
| test/Q_plus_P                  | -39.884247 |
| test/reward_per_eps            | -40        |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.4104304  |
| stats_o/std                    | 0.04027346 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.109     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -39.903004 |
| test/Q_plus_P                  | -39.903004 |
| test/reward_per_eps            | -40        |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.113     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 544        |
| stats_o/mean                   | 0.41043118 |
| stats_o/std                    | 0.04027672 |
| test/episodes                  | 5450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.201     |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -39.879074 |
| test/Q_plus_P                  | -39.879074 |
| test/reward_per_eps            | -40        |
| test/steps                     | 218000     |
| train/episodes                 | 21800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 872000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.41043043  |
| stats_o/std                    | 0.040274255 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.87784   |
| test/Q_plus_P                  | -39.87784   |
| test/reward_per_eps            | -40         |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.41044155  |
| stats_o/std                    | 0.040271576 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0999     |
| test/info_shaping_reward_mean  | -0.202      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -39.983566  |
| test/Q_plus_P                  | -39.983566  |
| test/reward_per_eps            | -40         |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.4104605   |
| stats_o/std                    | 0.040280115 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.115      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.896915  |
| test/Q_plus_P                  | -39.896915  |
| test/reward_per_eps            | -40         |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.41047525  |
| stats_o/std                    | 0.040279534 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.891563  |
| test/Q_plus_P                  | -39.891563  |
| test/reward_per_eps            | -40         |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.41047874  |
| stats_o/std                    | 0.040274538 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.892567  |
| test/Q_plus_P                  | -39.892567  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.41048643  |
| stats_o/std                    | 0.040279146 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -39.904163  |
| test/Q_plus_P                  | -39.904163  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.4104899   |
| stats_o/std                    | 0.040276084 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.122      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.868095  |
| test/Q_plus_P                  | -39.868095  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.41048625  |
| stats_o/std                    | 0.040278327 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.899395  |
| test/Q_plus_P                  | -39.899395  |
| test/reward_per_eps            | -40         |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.363      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.41048315 |
| stats_o/std                    | 0.04027549 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.135     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.89833  |
| test/Q_plus_P                  | -39.89833  |
| test/reward_per_eps            | -40        |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.41049388  |
| stats_o/std                    | 0.040281378 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.90698   |
| test/Q_plus_P                  | -39.90698   |
| test/reward_per_eps            | -40         |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 555        |
| stats_o/mean                   | 0.41048405 |
| stats_o/std                    | 0.04028236 |
| test/episodes                  | 5560       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.915462 |
| test/Q_plus_P                  | -39.915462 |
| test/reward_per_eps            | -40        |
| test/steps                     | 222400     |
| train/episodes                 | 22240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 889600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.41048655  |
| stats_o/std                    | 0.040278517 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.96244   |
| test/Q_plus_P                  | -39.96244   |
| test/reward_per_eps            | -40         |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.41048682 |
| stats_o/std                    | 0.04027758 |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.95256  |
| test/Q_plus_P                  | -39.95256  |
| test/reward_per_eps            | -40        |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.4105021  |
| stats_o/std                    | 0.0402805  |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.911312 |
| test/Q_plus_P                  | -39.911312 |
| test/reward_per_eps            | -40        |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 559        |
| stats_o/mean                   | 0.41051054 |
| stats_o/std                    | 0.04028556 |
| test/episodes                  | 5600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.254     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.95553  |
| test/Q_plus_P                  | -39.95553  |
| test/reward_per_eps            | -40        |
| test/steps                     | 224000     |
| train/episodes                 | 22400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 896000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.41051963  |
| stats_o/std                    | 0.040288877 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.918953  |
| test/Q_plus_P                  | -39.918953  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.41052946  |
| stats_o/std                    | 0.040282197 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -39.954914  |
| test/Q_plus_P                  | -39.954914  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.4105296   |
| stats_o/std                    | 0.040284965 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.92721   |
| test/Q_plus_P                  | -39.92721   |
| test/reward_per_eps            | -40         |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.4105405   |
| stats_o/std                    | 0.040288735 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.936207  |
| test/Q_plus_P                  | -39.936207  |
| test/reward_per_eps            | -40         |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.41054657  |
| stats_o/std                    | 0.040293664 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.94034   |
| test/Q_plus_P                  | -39.94034   |
| test/reward_per_eps            | -40         |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00313     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.41054597 |
| stats_o/std                    | 0.04028554 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -39.94103  |
| test/Q_plus_P                  | -39.94103  |
| test/reward_per_eps            | -40        |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 566        |
| stats_o/mean                   | 0.4105554  |
| stats_o/std                    | 0.04028196 |
| test/episodes                  | 5670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.202     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -39.95349  |
| test/Q_plus_P                  | -39.95349  |
| test/reward_per_eps            | -40        |
| test/steps                     | 226800     |
| train/episodes                 | 22680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 907200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.4105624   |
| stats_o/std                    | 0.040289573 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.87728   |
| test/Q_plus_P                  | -39.87728   |
| test/reward_per_eps            | -40         |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.41056764  |
| stats_o/std                    | 0.040290933 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.948204  |
| test/Q_plus_P                  | -39.948204  |
| test/reward_per_eps            | -40         |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.4105678   |
| stats_o/std                    | 0.040293813 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.92594   |
| test/Q_plus_P                  | -39.92594   |
| test/reward_per_eps            | -40         |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.41057536  |
| stats_o/std                    | 0.040292356 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.89598   |
| test/Q_plus_P                  | -39.89598   |
| test/reward_per_eps            | -40         |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.41057602  |
| stats_o/std                    | 0.040292535 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.928593  |
| test/Q_plus_P                  | -39.928593  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.41058162 |
| stats_o/std                    | 0.04030151 |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.262     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.912857 |
| test/Q_plus_P                  | -39.912857 |
| test/reward_per_eps            | -40        |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.4105846  |
| stats_o/std                    | 0.04030363 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.962467 |
| test/Q_plus_P                  | -39.962467 |
| test/reward_per_eps            | -40        |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.41059533 |
| stats_o/std                    | 0.04030322 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.99993  |
| test/Q_plus_P                  | -39.99993  |
| test/reward_per_eps            | -40        |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.41059756 |
| stats_o/std                    | 0.04030534 |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.201     |
| test/info_shaping_reward_mean  | -0.248     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.89796  |
| test/Q_plus_P                  | -39.89796  |
| test/reward_per_eps            | -40        |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.41060278  |
| stats_o/std                    | 0.040305395 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.94791   |
| test/Q_plus_P                  | -39.94791   |
| test/reward_per_eps            | -40         |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.41061887  |
| stats_o/std                    | 0.040302675 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.936493  |
| test/Q_plus_P                  | -39.936493  |
| test/reward_per_eps            | -40         |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.4106314  |
| stats_o/std                    | 0.04030865 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.262     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -39.965942 |
| test/Q_plus_P                  | -39.965942 |
| test/reward_per_eps            | -40        |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.41063276 |
| stats_o/std                    | 0.0403149  |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -39.939625 |
| test/Q_plus_P                  | -39.939625 |
| test/reward_per_eps            | -40        |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.41063377  |
| stats_o/std                    | 0.040314544 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.948395  |
| test/Q_plus_P                  | -39.948395  |
| test/reward_per_eps            | -40         |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.41063693  |
| stats_o/std                    | 0.040320132 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.930668  |
| test/Q_plus_P                  | -39.930668  |
| test/reward_per_eps            | -40         |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.4106382   |
| stats_o/std                    | 0.040322922 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.95321   |
| test/Q_plus_P                  | -39.95321   |
| test/reward_per_eps            | -40         |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.4106501   |
| stats_o/std                    | 0.040325433 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.938236  |
| test/Q_plus_P                  | -39.938236  |
| test/reward_per_eps            | -40         |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.41063812 |
| stats_o/std                    | 0.04033414 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.9269   |
| test/Q_plus_P                  | -39.9269   |
| test/reward_per_eps            | -40        |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.410643   |
| stats_o/std                    | 0.04033312 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -40.037365 |
| test/Q_plus_P                  | -40.037365 |
| test/reward_per_eps            | -40        |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.41064855  |
| stats_o/std                    | 0.040334262 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.97759   |
| test/Q_plus_P                  | -39.97759   |
| test/reward_per_eps            | -40         |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.4106536  |
| stats_o/std                    | 0.04033717 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -39.97241  |
| test/Q_plus_P                  | -39.97241  |
| test/reward_per_eps            | -40        |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.41066572  |
| stats_o/std                    | 0.040339474 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.956657  |
| test/Q_plus_P                  | -39.956657  |
| test/reward_per_eps            | -40         |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.41067326  |
| stats_o/std                    | 0.040342603 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.947968  |
| test/Q_plus_P                  | -39.947968  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.41067258  |
| stats_o/std                    | 0.040343095 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -39.944103  |
| test/Q_plus_P                  | -39.944103  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.41067532  |
| stats_o/std                    | 0.040345546 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -40.030567  |
| test/Q_plus_P                  | -40.030567  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.410683   |
| stats_o/std                    | 0.04035221 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.917065 |
| test/Q_plus_P                  | -39.917065 |
| test/reward_per_eps            | -40        |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.41068813  |
| stats_o/std                    | 0.040347718 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.975815  |
| test/Q_plus_P                  | -39.975815  |
| test/reward_per_eps            | -40         |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.41069686  |
| stats_o/std                    | 0.040351063 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.96023   |
| test/Q_plus_P                  | -39.96023   |
| test/reward_per_eps            | -40         |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.41069818  |
| stats_o/std                    | 0.040354718 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.8901    |
| test/Q_plus_P                  | -39.8901    |
| test/reward_per_eps            | -40         |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.41069642 |
| stats_o/std                    | 0.04035824 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.916927 |
| test/Q_plus_P                  | -39.916927 |
| test/reward_per_eps            | -40        |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.41069105  |
| stats_o/std                    | 0.040356796 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.944054  |
| test/Q_plus_P                  | -39.944054  |
| test/reward_per_eps            | -40         |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.4107001  |
| stats_o/std                    | 0.04035622 |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.917656 |
| test/Q_plus_P                  | -39.917656 |
| test/reward_per_eps            | -40        |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.41070795 |
| stats_o/std                    | 0.04035264 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.92903  |
| test/Q_plus_P                  | -39.92903  |
| test/reward_per_eps            | -40        |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
