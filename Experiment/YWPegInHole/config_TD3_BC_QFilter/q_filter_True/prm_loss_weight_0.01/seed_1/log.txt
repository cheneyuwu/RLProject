Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC_QFilter/q_filter_True/prm_loss_weight_0.01/seed_1
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.43316078 |
| stats_o/std                    | 0.06313241 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.364     |
| test/info_shaping_reward_min   | -0.846     |
| test/Q                         | -1.3197906 |
| test/Q_plus_P                  | -1.3197906 |
| test/reward_per_eps            | -40        |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.313     |
| train/info_shaping_reward_min  | -0.618     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.42146054  |
| stats_o/std                    | 0.064019196 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.359      |
| test/info_shaping_reward_min   | -0.632      |
| test/Q                         | -1.5861211  |
| test/Q_plus_P                  | -1.5861211  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.33       |
| train/info_shaping_reward_min  | -0.571      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.41161144  |
| stats_o/std                    | 0.064881384 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.372      |
| test/info_shaping_reward_min   | -0.669      |
| test/Q                         | -1.9272394  |
| test/Q_plus_P                  | -1.9272394  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.347      |
| train/info_shaping_reward_min  | -0.639      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.40906814 |
| stats_o/std                    | 0.06388267 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0549    |
| test/info_shaping_reward_mean  | -0.29      |
| test/info_shaping_reward_min   | -0.629     |
| test/Q                         | -2.3444846 |
| test/Q_plus_P                  | -2.3444846 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.295     |
| train/info_shaping_reward_min  | -0.46      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.40932825 |
| stats_o/std                    | 0.06433447 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0649    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -2.6496584 |
| test/Q_plus_P                  | -2.6496584 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0748    |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.451     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.4114269  |
| stats_o/std                    | 0.06357648 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0678    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -3.0539422 |
| test/Q_plus_P                  | -3.0539422 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0654    |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.4131787  |
| stats_o/std                    | 0.06298024 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0685    |
| test/info_shaping_reward_mean  | -0.158     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -3.449303  |
| test/Q_plus_P                  | -3.449303  |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0735    |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.383     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.41436064  |
| stats_o/std                    | 0.062149156 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0673     |
| test/info_shaping_reward_mean  | -0.116      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -3.868637   |
| test/Q_plus_P                  | -3.868637   |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0644     |
| train/info_shaping_reward_mean | -0.177      |
| train/info_shaping_reward_min  | -0.386      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.41614482 |
| stats_o/std                    | 0.06108741 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0225    |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -4.1646104 |
| test/Q_plus_P                  | -4.1646104 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00187    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0608    |
| train/info_shaping_reward_mean | -0.156     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.41682994 |
| stats_o/std                    | 0.05982609 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0711    |
| test/info_shaping_reward_mean  | -0.145     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -4.564848  |
| test/Q_plus_P                  | -4.564848  |
| test/reward_per_eps            | -40        |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0585    |
| train/info_shaping_reward_mean | -0.145     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.4179943   |
| stats_o/std                    | 0.058862302 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.11        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.026      |
| test/info_shaping_reward_mean  | -0.0927     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -4.730683   |
| test/Q_plus_P                  | -4.730683   |
| test/reward_per_eps            | -35.6       |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0597     |
| train/info_shaping_reward_mean | -0.144      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.41993567  |
| stats_o/std                    | 0.058166277 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0682     |
| test/info_shaping_reward_mean  | -0.0885     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -5.4374905  |
| test/Q_plus_P                  | -5.4374905  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0156      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0559     |
| train/info_shaping_reward_mean | -0.118      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.42151764  |
| stats_o/std                    | 0.057430636 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0675      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0311     |
| test/info_shaping_reward_mean  | -0.087      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -5.5809617  |
| test/Q_plus_P                  | -5.5809617  |
| test/reward_per_eps            | -37.3       |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00375     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0587     |
| train/info_shaping_reward_mean | -0.134      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.42286715  |
| stats_o/std                    | 0.056606755 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0633     |
| test/info_shaping_reward_mean  | -0.0876     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -6.219409   |
| test/Q_plus_P                  | -6.219409   |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0631     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.42430794  |
| stats_o/std                    | 0.055903584 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0698     |
| test/info_shaping_reward_mean  | -0.0867     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -6.519695   |
| test/Q_plus_P                  | -6.519695   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00187     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0597     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.4253229  |
| stats_o/std                    | 0.05514607 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0389    |
| test/info_shaping_reward_mean  | -0.0882    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -6.6249046 |
| test/Q_plus_P                  | -6.6249046 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0597    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.42597818 |
| stats_o/std                    | 0.05437478 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0646    |
| test/info_shaping_reward_mean  | -0.105     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -7.4301424 |
| test/Q_plus_P                  | -7.4301424 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0578    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.278     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.42686883 |
| stats_o/std                    | 0.05380963 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0198    |
| test/info_shaping_reward_mean  | -0.0882    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -7.429783  |
| test/Q_plus_P                  | -7.429783  |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0651    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.42774448  |
| stats_o/std                    | 0.053163126 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.125       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0427     |
| test/info_shaping_reward_mean  | -0.0863     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -7.0518007  |
| test/Q_plus_P                  | -7.0518007  |
| test/reward_per_eps            | -35         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0629     |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.4284257  |
| stats_o/std                    | 0.05256578 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0342    |
| test/info_shaping_reward_mean  | -0.0876    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -7.969531  |
| test/Q_plus_P                  | -7.969531  |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0633    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.4290289   |
| stats_o/std                    | 0.052017275 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0701     |
| test/info_shaping_reward_mean  | -0.0891     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -8.938409   |
| test/Q_plus_P                  | -8.938409   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0612     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.42934993  |
| stats_o/std                    | 0.051422235 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.035       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0377     |
| test/info_shaping_reward_mean  | -0.0895     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -8.277426   |
| test/Q_plus_P                  | -8.277426   |
| test/reward_per_eps            | -38.6       |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0593     |
| train/info_shaping_reward_mean | -0.118      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.42977324  |
| stats_o/std                    | 0.050907616 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.177       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.022      |
| test/info_shaping_reward_mean  | -0.0786     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -7.6374526  |
| test/Q_plus_P                  | -7.6374526  |
| test/reward_per_eps            | -32.9       |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0614     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.42977682 |
| stats_o/std                    | 0.05032972 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0925     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0115    |
| test/info_shaping_reward_mean  | -0.0843    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -9.159754  |
| test/Q_plus_P                  | -9.159754  |
| test/reward_per_eps            | -36.3      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0598    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.4301777   |
| stats_o/std                    | 0.049880102 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.135       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0803     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -8.52208    |
| test/Q_plus_P                  | -8.52208    |
| test/reward_per_eps            | -34.6       |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.06       |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.43061495  |
| stats_o/std                    | 0.049429685 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.018      |
| test/info_shaping_reward_mean  | -0.0877     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -9.776959   |
| test/Q_plus_P                  | -9.776959   |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0138      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0562     |
| train/info_shaping_reward_mean | -0.126      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.4310486   |
| stats_o/std                    | 0.049099416 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.107       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0292     |
| test/info_shaping_reward_mean  | -0.0865     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -9.650907   |
| test/Q_plus_P                  | -9.650907   |
| test/reward_per_eps            | -35.7       |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0581     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.43143466 |
| stats_o/std                    | 0.04867279 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0603    |
| test/info_shaping_reward_mean  | -0.09      |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -10.689715 |
| test/Q_plus_P                  | -10.689715 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.000625   |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0616    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.43183532  |
| stats_o/std                    | 0.048309103 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.128       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0255     |
| test/info_shaping_reward_mean  | -0.0794     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -9.675401   |
| test/Q_plus_P                  | -9.675401   |
| test/reward_per_eps            | -34.9       |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0469      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0461     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.1       |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.43205312  |
| stats_o/std                    | 0.047915027 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.287       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00951    |
| test/info_shaping_reward_mean  | -0.0781     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -8.757488   |
| test/Q_plus_P                  | -8.757488   |
| test/reward_per_eps            | -28.5       |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0212      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0525     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.43239608 |
| stats_o/std                    | 0.04748854 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.17       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00161   |
| test/info_shaping_reward_mean  | -0.0799    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -10.07798  |
| test/Q_plus_P                  | -10.07798  |
| test/reward_per_eps            | -33.2      |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0225     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.049     |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.4325579  |
| stats_o/std                    | 0.04794754 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.312      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00758   |
| test/info_shaping_reward_mean  | -0.0736    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -8.670499  |
| test/Q_plus_P                  | -8.670499  |
| test/reward_per_eps            | -27.5      |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0319     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0558    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.422     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 32        |
| stats_o/mean                   | 0.432722  |
| stats_o/std                    | 0.0475451 |
| test/episodes                  | 330       |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.29      |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00954  |
| test/info_shaping_reward_mean  | -0.0774   |
| test/info_shaping_reward_min   | -0.259    |
| test/Q                         | -8.929609 |
| test/Q_plus_P                  | -8.929609 |
| test/reward_per_eps            | -28.4     |
| test/steps                     | 13200     |
| train/episodes                 | 1320      |
| train/info_is_success_max      | 0.3       |
| train/info_is_success_mean     | 0.0231    |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.0473   |
| train/info_shaping_reward_mean | -0.122    |
| train/info_shaping_reward_min  | -0.269    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -39.1     |
| train/steps                    | 52800     |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.4328988   |
| stats_o/std                    | 0.047293548 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.285       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.014      |
| test/info_shaping_reward_mean  | -0.0771     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -8.058283   |
| test/Q_plus_P                  | -8.058283   |
| test/reward_per_eps            | -28.6       |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0475      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0351     |
| train/info_shaping_reward_mean | -0.118      |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.1       |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.43318927 |
| stats_o/std                    | 0.04693335 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0664    |
| test/info_shaping_reward_mean  | -0.0868    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -11.35252  |
| test/Q_plus_P                  | -11.35252  |
| test/reward_per_eps            | -40        |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0625     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0377    |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.5      |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.43351388  |
| stats_o/std                    | 0.046579033 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.28        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0262     |
| test/info_shaping_reward_mean  | -0.073      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -5.9762926  |
| test/Q_plus_P                  | -5.9762926  |
| test/reward_per_eps            | -28.8       |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.158       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0297     |
| train/info_shaping_reward_mean | -0.0961     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.7       |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.43386987 |
| stats_o/std                    | 0.0462672  |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.39       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00707   |
| test/info_shaping_reward_mean  | -0.0679    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -5.7222524 |
| test/Q_plus_P                  | -5.7222524 |
| test/reward_per_eps            | -24.4      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.152      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0285    |
| train/info_shaping_reward_mean | -0.0981    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.9      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.43422344  |
| stats_o/std                    | 0.045973357 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.28        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0197     |
| test/info_shaping_reward_mean  | -0.0696     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -6.2775025  |
| test/Q_plus_P                  | -6.2775025  |
| test/reward_per_eps            | -28.8       |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.216       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0256     |
| train/info_shaping_reward_mean | -0.0892     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.4       |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.43461975 |
| stats_o/std                    | 0.04568131 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.188      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0185    |
| test/info_shaping_reward_mean  | -0.0773    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -7.242992  |
| test/Q_plus_P                  | -7.242992  |
| test/reward_per_eps            | -32.5      |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.127      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.028     |
| train/info_shaping_reward_mean | -0.093     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.9      |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.43498787 |
| stats_o/std                    | 0.04541415 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.333      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0236    |
| test/info_shaping_reward_mean  | -0.0742    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -5.964784  |
| test/Q_plus_P                  | -5.964784  |
| test/reward_per_eps            | -26.7      |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.181      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0236    |
| train/info_shaping_reward_mean | -0.0873    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -32.8      |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.43539038  |
| stats_o/std                    | 0.045201983 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.438       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0653     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -5.1583996  |
| test/Q_plus_P                  | -5.1583996  |
| test/reward_per_eps            | -22.5       |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.173       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0165     |
| train/info_shaping_reward_mean | -0.0919     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.1       |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.43575194  |
| stats_o/std                    | 0.044948354 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.36        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0205     |
| test/info_shaping_reward_mean  | -0.0673     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -5.109753   |
| test/Q_plus_P                  | -5.109753   |
| test/reward_per_eps            | -25.6       |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.125       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0206     |
| train/info_shaping_reward_mean | -0.087      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.4360412  |
| stats_o/std                    | 0.04469386 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0436    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -3.7056067 |
| test/Q_plus_P                  | -3.7056067 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.275      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00775   |
| train/info_shaping_reward_mean | -0.0786    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29        |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.436286   |
| stats_o/std                    | 0.04445283 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.583      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00462   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -4.321125  |
| test/Q_plus_P                  | -4.321125  |
| test/reward_per_eps            | -16.7      |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.32       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.0756    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.2      |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.43650743  |
| stats_o/std                    | 0.044235338 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00319    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -3.6640356  |
| test/Q_plus_P                  | -3.6640356  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.406       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.8       |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.4366862   |
| stats_o/std                    | 0.044010118 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -3.4145691  |
| test/Q_plus_P                  | -3.4145691  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.491       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.43686318  |
| stats_o/std                    | 0.043784637 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -2.9486039  |
| test/Q_plus_P                  | -2.9486039  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.43701825  |
| stats_o/std                    | 0.043572534 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0332     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -3.112721   |
| test/Q_plus_P                  | -3.112721   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.538       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.43716446  |
| stats_o/std                    | 0.043369826 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00311    |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.8498528  |
| test/Q_plus_P                  | -2.8498528  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.493       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.43728867  |
| stats_o/std                    | 0.043172326 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0328     |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -3.0257325  |
| test/Q_plus_P                  | -3.0257325  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.522       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.43740633  |
| stats_o/std                    | 0.042975787 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00314    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -2.9784539  |
| test/Q_plus_P                  | -2.9784539  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.553       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.4375497  |
| stats_o/std                    | 0.04276756 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00328   |
| test/info_shaping_reward_mean  | -0.0355    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -2.8195245 |
| test/Q_plus_P                  | -2.8195245 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0598    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.4376643  |
| stats_o/std                    | 0.04257765 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0322    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -2.6265142 |
| test/Q_plus_P                  | -2.6265142 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0592    |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.43780705 |
| stats_o/std                    | 0.04239038 |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00243   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -3.1306677 |
| test/Q_plus_P                  | -3.1306677 |
| test/reward_per_eps            | -14        |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.489      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.43793914  |
| stats_o/std                    | 0.042199243 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -3.1987414  |
| test/Q_plus_P                  | -3.1987414  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.489       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.43805453  |
| stats_o/std                    | 0.042010516 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -2.649079   |
| test/Q_plus_P                  | -2.649079   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.495       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0643     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.4381832   |
| stats_o/std                    | 0.041824754 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00288    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -2.6733494  |
| test/Q_plus_P                  | -2.6733494  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.43829036  |
| stats_o/std                    | 0.041656885 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -3.0385294  |
| test/Q_plus_P                  | -3.0385294  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.43841136  |
| stats_o/std                    | 0.041487265 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -2.7688935  |
| test/Q_plus_P                  | -2.7688935  |
| test/reward_per_eps            | -10         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.511       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.43850932 |
| stats_o/std                    | 0.04132773 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0356    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -2.6249037 |
| test/Q_plus_P                  | -2.6249037 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.543      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.43862548  |
| stats_o/std                    | 0.041176263 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -2.627329   |
| test/Q_plus_P                  | -2.627329   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.486       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.43869224  |
| stats_o/std                    | 0.041045334 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0337     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -2.4455156  |
| test/Q_plus_P                  | -2.4455156  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.43879747  |
| stats_o/std                    | 0.040902007 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -2.2649312  |
| test/Q_plus_P                  | -2.2649312  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.4388896   |
| stats_o/std                    | 0.040771235 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.652       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -2.6453543  |
| test/Q_plus_P                  | -2.6453543  |
| test/reward_per_eps            | -13.9       |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00198    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.43899536  |
| stats_o/std                    | 0.040646113 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00246    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -2.295049   |
| test/Q_plus_P                  | -2.295049   |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.4391071   |
| stats_o/std                    | 0.040516462 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0029     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.8601131  |
| test/Q_plus_P                  | -2.8601131  |
| test/reward_per_eps            | -13         |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.43919384  |
| stats_o/std                    | 0.040408663 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000743   |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -2.1672812  |
| test/Q_plus_P                  | -2.1672812  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.43928304  |
| stats_o/std                    | 0.040272225 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000524   |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -2.2696042  |
| test/Q_plus_P                  | -2.2696042  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00242    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.43935204  |
| stats_o/std                    | 0.040158965 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.414047   |
| test/Q_plus_P                  | -2.414047   |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00255    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.43941963  |
| stats_o/std                    | 0.040034186 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00319    |
| test/info_shaping_reward_mean  | -0.0331     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -2.222509   |
| test/Q_plus_P                  | -2.222509   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.43948576  |
| stats_o/std                    | 0.039899368 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00154    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -2.1141179  |
| test/Q_plus_P                  | -2.1141179  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.43955207  |
| stats_o/std                    | 0.039772004 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00306    |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -2.2679965  |
| test/Q_plus_P                  | -2.2679965  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00216    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.43961284  |
| stats_o/std                    | 0.039654177 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -2.3826637  |
| test/Q_plus_P                  | -2.3826637  |
| test/reward_per_eps            | -10         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.43968117 |
| stats_o/std                    | 0.03953403 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000966  |
| test/info_shaping_reward_mean  | -0.0299    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -2.207566  |
| test/Q_plus_P                  | -2.207566  |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0024    |
| train/info_shaping_reward_mean | -0.0607    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.43974105 |
| stats_o/std                    | 0.03941149 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0388    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -2.501542  |
| test/Q_plus_P                  | -2.501542  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00257   |
| train/info_shaping_reward_mean | -0.0523    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.4397944   |
| stats_o/std                    | 0.039295036 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -2.1411288  |
| test/Q_plus_P                  | -2.1411288  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0024     |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.43984196 |
| stats_o/std                    | 0.03917402 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.0331    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -2.3122013 |
| test/Q_plus_P                  | -2.3122013 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00218   |
| train/info_shaping_reward_mean | -0.0493    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.43989548 |
| stats_o/std                    | 0.03905858 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0324    |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -2.0887194 |
| test/Q_plus_P                  | -2.0887194 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00302   |
| train/info_shaping_reward_mean | -0.0533    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.43995082  |
| stats_o/std                    | 0.038941823 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0323     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -2.0927355  |
| test/Q_plus_P                  | -2.0927355  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.43999162  |
| stats_o/std                    | 0.038833383 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00391    |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -2.044641   |
| test/Q_plus_P                  | -2.044641   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0495     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.4400339  |
| stats_o/std                    | 0.03872062 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000576  |
| test/info_shaping_reward_mean  | -0.029     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -1.8504926 |
| test/Q_plus_P                  | -1.8504926 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.693      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00223   |
| train/info_shaping_reward_mean | -0.0494    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.440069    |
| stats_o/std                    | 0.038620833 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -2.1752043  |
| test/Q_plus_P                  | -2.1752043  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.44012204  |
| stats_o/std                    | 0.038515262 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -2.1366794  |
| test/Q_plus_P                  | -2.1366794  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.595       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.44017026  |
| stats_o/std                    | 0.038415793 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.099923   |
| test/Q_plus_P                  | -2.099923   |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.44020954  |
| stats_o/std                    | 0.038332243 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.9502329  |
| test/Q_plus_P                  | -1.9502329  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.44025156  |
| stats_o/std                    | 0.038247716 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.8703809  |
| test/Q_plus_P                  | -1.8703809  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.6         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.4402928  |
| stats_o/std                    | 0.03814735 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.0328    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.8635881 |
| test/Q_plus_P                  | -1.8635881 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00234   |
| train/info_shaping_reward_mean | -0.0551    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.4403374   |
| stats_o/std                    | 0.038048733 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00041    |
| test/info_shaping_reward_mean  | -0.0316     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.8811344  |
| test/Q_plus_P                  | -1.8811344  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.4403771   |
| stats_o/std                    | 0.037952848 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.0039597  |
| test/Q_plus_P                  | -2.0039597  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.44042298  |
| stats_o/std                    | 0.037859734 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0308     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.8721942  |
| test/Q_plus_P                  | -1.8721942  |
| test/reward_per_eps            | -8          |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.44046044  |
| stats_o/std                    | 0.037771758 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0294     |
| test/info_shaping_reward_min   | -0.203      |
| test/Q                         | -1.7912167  |
| test/Q_plus_P                  | -1.7912167  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00188    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.44050848 |
| stats_o/std                    | 0.03768511 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0348    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -1.7484325 |
| test/Q_plus_P                  | -1.7484325 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00283   |
| train/info_shaping_reward_mean | -0.0533    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.4405537   |
| stats_o/std                    | 0.037603233 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0017     |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.7312224  |
| test/Q_plus_P                  | -1.7312224  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00216    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.44059232  |
| stats_o/std                    | 0.037527695 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0299     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.6917194  |
| test/Q_plus_P                  | -1.6917194  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00218    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.44063732  |
| stats_o/std                    | 0.037444316 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.7082016  |
| test/Q_plus_P                  | -1.7082016  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.44067952  |
| stats_o/std                    | 0.037361298 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00303    |
| test/info_shaping_reward_mean  | -0.031      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.6695727  |
| test/Q_plus_P                  | -1.6695727  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.44072118  |
| stats_o/std                    | 0.037289705 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.0306     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.5906826  |
| test/Q_plus_P                  | -1.5906826  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.4407622   |
| stats_o/std                    | 0.037206795 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00575    |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.5993458  |
| test/Q_plus_P                  | -1.5993458  |
| test/reward_per_eps            | -7          |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.44079265  |
| stats_o/std                    | 0.037126634 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0261     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.493662   |
| test/Q_plus_P                  | -1.493662   |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0508     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.44081986  |
| stats_o/std                    | 0.037048068 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000751   |
| test/info_shaping_reward_mean  | -0.026      |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.5362462  |
| test/Q_plus_P                  | -1.5362462  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.44084945  |
| stats_o/std                    | 0.036973838 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00271    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.6741365  |
| test/Q_plus_P                  | -1.6741365  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00212    |
| train/info_shaping_reward_mean | -0.0511     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.44088423  |
| stats_o/std                    | 0.036891326 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0339     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.4872783  |
| test/Q_plus_P                  | -1.4872783  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.4409151   |
| stats_o/std                    | 0.036811303 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0291     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.4481425  |
| test/Q_plus_P                  | -1.4481425  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0505     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.44094923  |
| stats_o/std                    | 0.036740396 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000897   |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.8214244  |
| test/Q_plus_P                  | -1.8214244  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.44098032 |
| stats_o/std                    | 0.03666882 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0414    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -1.904396  |
| test/Q_plus_P                  | -1.904396  |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0024    |
| train/info_shaping_reward_mean | -0.0574    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.44102263  |
| stats_o/std                    | 0.036584802 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.5681734  |
| test/Q_plus_P                  | -1.5681734  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.44105914  |
| stats_o/std                    | 0.036506746 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0334     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.5385998  |
| test/Q_plus_P                  | -1.5385998  |
| test/reward_per_eps            | -8          |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.44108304  |
| stats_o/std                    | 0.036432143 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.4748371  |
| test/Q_plus_P                  | -1.4748371  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00209    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.44110358  |
| stats_o/std                    | 0.036359992 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.5392102  |
| test/Q_plus_P                  | -1.5392102  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00196    |
| train/info_shaping_reward_mean | -0.0507     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.44112977 |
| stats_o/std                    | 0.03628986 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0293    |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -1.3893692 |
| test/Q_plus_P                  | -1.3893692 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0026    |
| train/info_shaping_reward_mean | -0.0509    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.4411472   |
| stats_o/std                    | 0.036222447 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.5626053  |
| test/Q_plus_P                  | -1.5626053  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.44116497 |
| stats_o/std                    | 0.03615601 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000788  |
| test/info_shaping_reward_mean  | -0.0315    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -1.449961  |
| test/Q_plus_P                  | -1.449961  |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.686      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00233   |
| train/info_shaping_reward_mean | -0.051     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.44118235  |
| stats_o/std                    | 0.036086906 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0314     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.400079   |
| test/Q_plus_P                  | -1.400079   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0494     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.4412047   |
| stats_o/std                    | 0.036031447 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.5191128  |
| test/Q_plus_P                  | -1.5191128  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.44122854  |
| stats_o/std                    | 0.035965737 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000757   |
| test/info_shaping_reward_mean  | -0.0309     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.4586655  |
| test/Q_plus_P                  | -1.4586655  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.44125223  |
| stats_o/std                    | 0.035894923 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.5540789  |
| test/Q_plus_P                  | -1.5540789  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.44126832 |
| stats_o/std                    | 0.03583042 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00293   |
| test/info_shaping_reward_mean  | -0.0341    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -1.4249187 |
| test/Q_plus_P                  | -1.4249187 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00296   |
| train/info_shaping_reward_mean | -0.0567    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.44128633  |
| stats_o/std                    | 0.035766054 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0323     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.4538361  |
| test/Q_plus_P                  | -1.4538361  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.44129547  |
| stats_o/std                    | 0.035702784 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000827   |
| test/info_shaping_reward_mean  | -0.029      |
| test/info_shaping_reward_min   | -0.209      |
| test/Q                         | -1.31432    |
| test/Q_plus_P                  | -1.31432    |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0507     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.44130477 |
| stats_o/std                    | 0.03564288 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0324    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.3136656 |
| test/Q_plus_P                  | -1.3136656 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.681      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0521    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.44131628 |
| stats_o/std                    | 0.03557932 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000452  |
| test/info_shaping_reward_mean  | -0.0357    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.4115415 |
| test/Q_plus_P                  | -1.4115415 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00207   |
| train/info_shaping_reward_mean | -0.0501    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.44132963 |
| stats_o/std                    | 0.03551677 |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0329    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.3135966 |
| test/Q_plus_P                  | -1.3135966 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.731      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00305   |
| train/info_shaping_reward_mean | -0.048     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -10.8      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.44134068  |
| stats_o/std                    | 0.035457682 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -1.3277352  |
| test/Q_plus_P                  | -1.3277352  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.44136748 |
| stats_o/std                    | 0.03539467 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000359  |
| test/info_shaping_reward_mean  | -0.0312    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -1.29251   |
| test/Q_plus_P                  | -1.29251   |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00258   |
| train/info_shaping_reward_mean | -0.0529    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.44138876  |
| stats_o/std                    | 0.035334576 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.5277727  |
| test/Q_plus_P                  | -1.5277727  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 125       |
| stats_o/mean                   | 0.4414114 |
| stats_o/std                    | 0.0352764 |
| test/episodes                  | 1260      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.8       |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.00149  |
| test/info_shaping_reward_mean  | -0.0347   |
| test/info_shaping_reward_min   | -0.239    |
| test/Q                         | -1.361764 |
| test/Q_plus_P                  | -1.361764 |
| test/reward_per_eps            | -8        |
| test/steps                     | 50400     |
| train/episodes                 | 5040      |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.674     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00261  |
| train/info_shaping_reward_mean | -0.0524   |
| train/info_shaping_reward_min  | -0.236    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -13.1     |
| train/steps                    | 201600    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.44142985  |
| stats_o/std                    | 0.035219494 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00435    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.386136   |
| test/Q_plus_P                  | -1.386136   |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.44144773  |
| stats_o/std                    | 0.035162102 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0288     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.2943506  |
| test/Q_plus_P                  | -1.2943506  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0504     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.44145858 |
| stats_o/std                    | 0.03510555 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0309    |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -1.2039233 |
| test/Q_plus_P                  | -1.2039233 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.716      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00278   |
| train/info_shaping_reward_mean | -0.0493    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.441478   |
| stats_o/std                    | 0.03504776 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.036     |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.3061637 |
| test/Q_plus_P                  | -1.3061637 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.714      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00275   |
| train/info_shaping_reward_mean | -0.0502    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.44149497 |
| stats_o/std                    | 0.03500155 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0336    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -1.1983104 |
| test/Q_plus_P                  | -1.1983104 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00297   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.44151112 |
| stats_o/std                    | 0.03494284 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00523   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.218     |
| test/Q                         | -1.150936  |
| test/Q_plus_P                  | -1.150936  |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.674      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00277   |
| train/info_shaping_reward_mean | -0.053     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.441524    |
| stats_o/std                    | 0.034896582 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0322     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.2371628  |
| test/Q_plus_P                  | -1.2371628  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.44154584 |
| stats_o/std                    | 0.03484982 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00686   |
| test/info_shaping_reward_mean  | -0.0408    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -1.3542812 |
| test/Q_plus_P                  | -1.3542812 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.44156453 |
| stats_o/std                    | 0.03480551 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0302    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -1.2762082 |
| test/Q_plus_P                  | -1.2762082 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00266   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.44157386  |
| stats_o/std                    | 0.034764845 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000782   |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.2710655  |
| test/Q_plus_P                  | -1.2710655  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00204    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.4415821   |
| stats_o/std                    | 0.034717496 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.4247478  |
| test/Q_plus_P                  | -1.4247478  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.44160292 |
| stats_o/std                    | 0.03466897 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00379   |
| test/info_shaping_reward_mean  | -0.0366    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.1418796 |
| test/Q_plus_P                  | -1.1418796 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.673      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0575    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.4416201  |
| stats_o/std                    | 0.03461949 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00496   |
| test/info_shaping_reward_mean  | -0.0367    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -1.1708913 |
| test/Q_plus_P                  | -1.1708913 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.441633    |
| stats_o/std                    | 0.034580808 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.1471276  |
| test/Q_plus_P                  | -1.1471276  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.44165194 |
| stats_o/std                    | 0.03453493 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0014    |
| test/info_shaping_reward_mean  | -0.034     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.1732674 |
| test/Q_plus_P                  | -1.1732674 |
| test/reward_per_eps            | -7         |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.681      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0549    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.441668    |
| stats_o/std                    | 0.034487784 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0997576  |
| test/Q_plus_P                  | -1.0997576  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0497     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.44167295 |
| stats_o/std                    | 0.03444244 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00132   |
| test/info_shaping_reward_mean  | -0.0349    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.2346383 |
| test/Q_plus_P                  | -1.2346383 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.724      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00371   |
| train/info_shaping_reward_mean | -0.0505    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.1      |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.4416813  |
| stats_o/std                    | 0.03440279 |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.0352    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -1.1219547 |
| test/Q_plus_P                  | -1.1219547 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.691      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0036    |
| train/info_shaping_reward_mean | -0.0535    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.4416969   |
| stats_o/std                    | 0.034360968 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00507    |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.1838642  |
| test/Q_plus_P                  | -1.1838642  |
| test/reward_per_eps            | -7          |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 145        |
| stats_o/mean                   | 0.44172022 |
| stats_o/std                    | 0.03432555 |
| test/episodes                  | 1460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00421   |
| test/info_shaping_reward_mean  | -0.0481    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.5520557 |
| test/Q_plus_P                  | -1.5520557 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 58400      |
| train/episodes                 | 5840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 233600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.44174662  |
| stats_o/std                    | 0.034290086 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000602   |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.4568814  |
| test/Q_plus_P                  | -1.4568814  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.482       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.4417611   |
| stats_o/std                    | 0.034241978 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.3054448  |
| test/Q_plus_P                  | -1.3054448  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.44177353 |
| stats_o/std                    | 0.03419851 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.0372    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -1.2567879 |
| test/Q_plus_P                  | -1.2567879 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00283   |
| train/info_shaping_reward_mean | -0.0524    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.44178903  |
| stats_o/std                    | 0.034147706 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00282    |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.211      |
| test/Q                         | -1.1171626  |
| test/Q_plus_P                  | -1.1171626  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.4418     |
| stats_o/std                    | 0.03410445 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0302    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -1.1705556 |
| test/Q_plus_P                  | -1.1705556 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.0525    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.44180146 |
| stats_o/std                    | 0.03406432 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00274   |
| test/info_shaping_reward_mean  | -0.034     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -1.2460568 |
| test/Q_plus_P                  | -1.2460568 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.674      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0514    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.4418098   |
| stats_o/std                    | 0.034025874 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00398    |
| test/info_shaping_reward_mean  | -0.0308     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.1016983  |
| test/Q_plus_P                  | -1.1016983  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.0488     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.4418222   |
| stats_o/std                    | 0.033981983 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0291     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.0143913  |
| test/Q_plus_P                  | -1.0143913  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.0505     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.44182822 |
| stats_o/std                    | 0.03394061 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.031     |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -1.1418271 |
| test/Q_plus_P                  | -1.1418271 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.677      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00234   |
| train/info_shaping_reward_mean | -0.0515    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.44184175  |
| stats_o/std                    | 0.03390651  |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0305     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.94757307 |
| test/Q_plus_P                  | -0.94757307 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.44185054 |
| stats_o/std                    | 0.03386588 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00311   |
| test/info_shaping_reward_mean  | -0.0375    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -1.1278889 |
| test/Q_plus_P                  | -1.1278889 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00272   |
| train/info_shaping_reward_mean | -0.0518    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 157        |
| stats_o/mean                   | 0.44185996 |
| stats_o/std                    | 0.03383107 |
| test/episodes                  | 1580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0328    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.1321173 |
| test/Q_plus_P                  | -1.1321173 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 63200      |
| train/episodes                 | 6320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00263   |
| train/info_shaping_reward_mean | -0.0567    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 252800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.4418635   |
| stats_o/std                    | 0.033791687 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.0757823  |
| test/Q_plus_P                  | -1.0757823  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.44187185  |
| stats_o/std                    | 0.033745486 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0302     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.97371274 |
| test/Q_plus_P                  | -0.97371274 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.44188383 |
| stats_o/std                    | 0.03370584 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0315    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -0.9651094 |
| test/Q_plus_P                  | -0.9651094 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.69       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00294   |
| train/info_shaping_reward_mean | -0.0501    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 161        |
| stats_o/mean                   | 0.44189653 |
| stats_o/std                    | 0.03366048 |
| test/episodes                  | 1620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00308   |
| test/info_shaping_reward_mean  | -0.0288    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -1.0013539 |
| test/Q_plus_P                  | -1.0013539 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 64800      |
| train/episodes                 | 6480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.706      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0498    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.8      |
| train/steps                    | 259200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.44190565  |
| stats_o/std                    | 0.033628706 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00337    |
| test/info_shaping_reward_mean  | -0.0307     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.9453113  |
| test/Q_plus_P                  | -0.9453113  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.44191602  |
| stats_o/std                    | 0.033586264 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.0949475  |
| test/Q_plus_P                  | -1.0949475  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 164        |
| stats_o/mean                   | 0.44192472 |
| stats_o/std                    | 0.03354802 |
| test/episodes                  | 1650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0337    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.1381993 |
| test/Q_plus_P                  | -1.1381993 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 66000      |
| train/episodes                 | 6600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00246   |
| train/info_shaping_reward_mean | -0.0525    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 264000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.44193253 |
| stats_o/std                    | 0.03351058 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000626  |
| test/info_shaping_reward_mean  | -0.034     |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -1.0527492 |
| test/Q_plus_P                  | -1.0527492 |
| test/reward_per_eps            | -7         |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.66       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00289   |
| train/info_shaping_reward_mean | -0.0541    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.44194055  |
| stats_o/std                    | 0.033472385 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000678   |
| test/info_shaping_reward_mean  | -0.0311     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.0152373  |
| test/Q_plus_P                  | -1.0152373  |
| test/reward_per_eps            | -7          |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.44194946  |
| stats_o/std                    | 0.033438284 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.030723   |
| test/Q_plus_P                  | -1.030723   |
| test/reward_per_eps            | -7          |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.44196224  |
| stats_o/std                    | 0.033398025 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00405    |
| test/info_shaping_reward_mean  | -0.0378     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.0930381  |
| test/Q_plus_P                  | -1.0930381  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.44197252  |
| stats_o/std                    | 0.033360783 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0301     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.8874329  |
| test/Q_plus_P                  | -0.8874329  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.4419787  |
| stats_o/std                    | 0.03332298 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00585   |
| test/info_shaping_reward_mean  | -0.0372    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -0.8881714 |
| test/Q_plus_P                  | -0.8881714 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.691      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00233   |
| train/info_shaping_reward_mean | -0.0518    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.4419842   |
| stats_o/std                    | 0.033288535 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00295    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.056631   |
| test/Q_plus_P                  | -1.056631   |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.44199753 |
| stats_o/std                    | 0.03324985 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000724  |
| test/info_shaping_reward_mean  | -0.0333    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.0168778 |
| test/Q_plus_P                  | -1.0168778 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00308   |
| train/info_shaping_reward_mean | -0.0559    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.4420047   |
| stats_o/std                    | 0.033212006 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0303149  |
| test/Q_plus_P                  | -1.0303149  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0509     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.44201398 |
| stats_o/std                    | 0.03317602 |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0078    |
| test/info_shaping_reward_mean  | -0.0375    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -0.9097827 |
| test/Q_plus_P                  | -0.9097827 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.0553    |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.4420228   |
| stats_o/std                    | 0.033141825 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00234    |
| test/info_shaping_reward_mean  | -0.0298     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.8907007  |
| test/Q_plus_P                  | -0.8907007  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00468    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.4420303   |
| stats_o/std                    | 0.033105183 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0312     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.93532544 |
| test/Q_plus_P                  | -0.93532544 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.44203737  |
| stats_o/std                    | 0.033075493 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000636   |
| test/info_shaping_reward_mean  | -0.0301     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.9642546  |
| test/Q_plus_P                  | -0.9642546  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.44204792  |
| stats_o/std                    | 0.033040237 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.9510411  |
| test/Q_plus_P                  | -0.9510411  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.44206175  |
| stats_o/std                    | 0.032997996 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0017     |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.0570432  |
| test/Q_plus_P                  | -1.0570432  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.44207048 |
| stats_o/std                    | 0.03296129 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0331    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -0.986039  |
| test/Q_plus_P                  | -0.986039  |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.694      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00318   |
| train/info_shaping_reward_mean | -0.0517    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.4420822   |
| stats_o/std                    | 0.032923687 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00085    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.96226776 |
| test/Q_plus_P                  | -0.96226776 |
| test/reward_per_eps            | -7          |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.44209316  |
| stats_o/std                    | 0.032887504 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00171    |
| test/info_shaping_reward_mean  | -0.0342     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.0050292  |
| test/Q_plus_P                  | -1.0050292  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0509     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.44210425  |
| stats_o/std                    | 0.032849345 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0109552  |
| test/Q_plus_P                  | -1.0109552  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.44211236 |
| stats_o/std                    | 0.03281546 |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000678  |
| test/info_shaping_reward_mean  | -0.0362    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.8703025 |
| test/Q_plus_P                  | -0.8703025 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.056     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.44212165  |
| stats_o/std                    | 0.032781728 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.92690307 |
| test/Q_plus_P                  | -0.92690307 |
| test/reward_per_eps            | -7          |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.4421313  |
| stats_o/std                    | 0.03274713 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00757   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -1.0143131 |
| test/Q_plus_P                  | -1.0143131 |
| test/reward_per_eps            | -8         |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00281   |
| train/info_shaping_reward_mean | -0.0525    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.44213974  |
| stats_o/std                    | 0.032713078 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00397    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.94712394 |
| test/Q_plus_P                  | -0.94712394 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00503    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.44214666  |
| stats_o/std                    | 0.03268591  |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00615    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.87291354 |
| test/Q_plus_P                  | -0.87291354 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.44215977 |
| stats_o/std                    | 0.032651   |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00541   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.0656384 |
| test/Q_plus_P                  | -1.0656384 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.668      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00407   |
| train/info_shaping_reward_mean | -0.0559    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.44216862  |
| stats_o/std                    | 0.032625783 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000654   |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.95436114 |
| test/Q_plus_P                  | -0.95436114 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00359    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.44217837  |
| stats_o/std                    | 0.032594834 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00283    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9025618  |
| test/Q_plus_P                  | -0.9025618  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.44219127 |
| stats_o/std                    | 0.03256461 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00348   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -1.1404574 |
| test/Q_plus_P                  | -1.1404574 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00466   |
| train/info_shaping_reward_mean | -0.0611    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.44220617  |
| stats_o/std                    | 0.032528665 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.9515206  |
| test/Q_plus_P                  | -0.9515206  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.4422128   |
| stats_o/std                    | 0.032499913 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00412    |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.92036134 |
| test/Q_plus_P                  | -0.92036134 |
| test/reward_per_eps            | -7          |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.4422165  |
| stats_o/std                    | 0.03247228 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.003     |
| test/info_shaping_reward_mean  | -0.035     |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -0.9263816 |
| test/Q_plus_P                  | -0.9263816 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0531    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.4422251  |
| stats_o/std                    | 0.03243995 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.845      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00611   |
| test/info_shaping_reward_mean  | -0.034     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -0.8658177 |
| test/Q_plus_P                  | -0.8658177 |
| test/reward_per_eps            | -6.2       |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.681      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0525    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.44223115  |
| stats_o/std                    | 0.032409485 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.8849365  |
| test/Q_plus_P                  | -0.8849365  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.44223702 |
| stats_o/std                    | 0.03238067 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.0357    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -0.9113015 |
| test/Q_plus_P                  | -0.9113015 |
| test/reward_per_eps            | -7         |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.726      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.0519    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -10.9      |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.44224355  |
| stats_o/std                    | 0.03234915  |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0298     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.84524935 |
| test/Q_plus_P                  | -0.84524935 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.44225442 |
| stats_o/std                    | 0.03231865 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00151   |
| test/info_shaping_reward_mean  | -0.0326    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -0.8740566 |
| test/Q_plus_P                  | -0.8740566 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.693      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0526    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.4422594   |
| stats_o/std                    | 0.032290958 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0339     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.8281975  |
| test/Q_plus_P                  | -0.8281975  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0505     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.44226813  |
| stats_o/std                    | 0.032256342 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000386   |
| test/info_shaping_reward_mean  | -0.0316     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.8736364  |
| test/Q_plus_P                  | -0.8736364  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.715       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0487     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.44227612 |
| stats_o/std                    | 0.03222787 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0017    |
| test/info_shaping_reward_mean  | -0.036     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -0.8990681 |
| test/Q_plus_P                  | -0.8990681 |
| test/reward_per_eps            | -7         |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00216   |
| train/info_shaping_reward_mean | -0.0533    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.4422823   |
| stats_o/std                    | 0.0321969   |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00273    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.96874297 |
| test/Q_plus_P                  | -0.96874297 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.4422897   |
| stats_o/std                    | 0.03216557  |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000998   |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -0.92529917 |
| test/Q_plus_P                  | -0.92529917 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.715       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.44229332  |
| stats_o/std                    | 0.032139678 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00385    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.8622455  |
| test/Q_plus_P                  | -0.8622455  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.44230297 |
| stats_o/std                    | 0.03211393 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0362    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -0.8666288 |
| test/Q_plus_P                  | -0.8666288 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00324   |
| train/info_shaping_reward_mean | -0.056     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.44230604  |
| stats_o/std                    | 0.032091897 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00358    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.9185317  |
| test/Q_plus_P                  | -0.9185317  |
| test/reward_per_eps            | -7          |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.4423125   |
| stats_o/std                    | 0.032069482 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0343     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.84339297 |
| test/Q_plus_P                  | -0.84339297 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.44231918  |
| stats_o/std                    | 0.032041743 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9079496  |
| test/Q_plus_P                  | -0.9079496  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00226    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.44232634  |
| stats_o/std                    | 0.03201645  |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0337     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.87126654 |
| test/Q_plus_P                  | -0.87126654 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.44233286  |
| stats_o/std                    | 0.031989094 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.9208954  |
| test/Q_plus_P                  | -0.9208954  |
| test/reward_per_eps            | -7          |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.44234028  |
| stats_o/std                    | 0.031962216 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.90159696 |
| test/Q_plus_P                  | -0.90159696 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.44234613  |
| stats_o/std                    | 0.03193308  |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.88959634 |
| test/Q_plus_P                  | -0.88959634 |
| test/reward_per_eps            | -7          |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.44235328  |
| stats_o/std                    | 0.031907443 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -0.81015104 |
| test/Q_plus_P                  | -0.81015104 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.44235757  |
| stats_o/std                    | 0.031889666 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.89168847 |
| test/Q_plus_P                  | -0.89168847 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.442363    |
| stats_o/std                    | 0.031864267 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0056     |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.9273451  |
| test/Q_plus_P                  | -0.9273451  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.44236925  |
| stats_o/std                    | 0.03184143  |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00381    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.81391966 |
| test/Q_plus_P                  | -0.81391966 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.4423749   |
| stats_o/std                    | 0.031814173 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0031     |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.78305155 |
| test/Q_plus_P                  | -0.78305155 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.4423788  |
| stats_o/std                    | 0.031788   |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0398    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -0.8859953 |
| test/Q_plus_P                  | -0.8859953 |
| test/reward_per_eps            | -7         |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.698      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00308   |
| train/info_shaping_reward_mean | -0.053     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.44238976  |
| stats_o/std                    | 0.031757686 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00373    |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.8283689  |
| test/Q_plus_P                  | -0.8283689  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.44239572  |
| stats_o/std                    | 0.031729326 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00398    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.8516853  |
| test/Q_plus_P                  | -0.8516853  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.4424019   |
| stats_o/std                    | 0.0317032   |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00939    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.79182833 |
| test/Q_plus_P                  | -0.79182833 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00426    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.44241166 |
| stats_o/std                    | 0.03167394 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0112    |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -1.1141086 |
| test/Q_plus_P                  | -1.1141086 |
| test/reward_per_eps            | -11        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.695      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00368   |
| train/info_shaping_reward_mean | -0.0527    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.4424155  |
| stats_o/std                    | 0.03164781 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00355   |
| test/info_shaping_reward_mean  | -0.0373    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -0.845296  |
| test/Q_plus_P                  | -0.845296  |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0539    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.44241822  |
| stats_o/std                    | 0.03162771  |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00444    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.95712554 |
| test/Q_plus_P                  | -0.95712554 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.44242406 |
| stats_o/std                    | 0.03160641 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000798  |
| test/info_shaping_reward_mean  | -0.0361    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -0.8143871 |
| test/Q_plus_P                  | -0.8143871 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.687      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0028    |
| train/info_shaping_reward_mean | -0.0541    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.44243225  |
| stats_o/std                    | 0.031582076 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.8070019  |
| test/Q_plus_P                  | -0.8070019  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.44243953 |
| stats_o/std                    | 0.03155783 |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000966  |
| test/info_shaping_reward_mean  | -0.0338    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -0.9795572 |
| test/Q_plus_P                  | -0.9795572 |
| test/reward_per_eps            | -8         |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.687      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00253   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.44244602  |
| stats_o/std                    | 0.031534553 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.92202973 |
| test/Q_plus_P                  | -0.92202973 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.44245127 |
| stats_o/std                    | 0.03150621 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0341    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -0.8254904 |
| test/Q_plus_P                  | -0.8254904 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.716      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00297   |
| train/info_shaping_reward_mean | -0.0519    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.44245705  |
| stats_o/std                    | 0.031483468 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000957   |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.8290124  |
| test/Q_plus_P                  | -0.8290124  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.44246432  |
| stats_o/std                    | 0.031460762 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.77798235 |
| test/Q_plus_P                  | -0.77798235 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.4424689  |
| stats_o/std                    | 0.03143856 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.0362    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.8170482 |
| test/Q_plus_P                  | -0.8170482 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.4424741   |
| stats_o/std                    | 0.031413034 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000813   |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7571367  |
| test/Q_plus_P                  | -0.7571367  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0504     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.44247746  |
| stats_o/std                    | 0.031390715 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.853       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0314     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.6553563  |
| test/Q_plus_P                  | -0.6553563  |
| test/reward_per_eps            | -5.9        |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00188    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.44248483  |
| stats_o/std                    | 0.031371597 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.81756175 |
| test/Q_plus_P                  | -0.81756175 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00224    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.44248983  |
| stats_o/std                    | 0.031348303 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.7446539  |
| test/Q_plus_P                  | -0.7446539  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.4424994  |
| stats_o/std                    | 0.03132681 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0374    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -0.7867792 |
| test/Q_plus_P                  | -0.7867792 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0538    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.4425043   |
| stats_o/std                    | 0.031301122 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.751918   |
| test/Q_plus_P                  | -0.751918   |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0508     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.44250795 |
| stats_o/std                    | 0.03128239 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00365   |
| test/info_shaping_reward_mean  | -0.0406    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -0.8315905 |
| test/Q_plus_P                  | -0.8315905 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00305   |
| train/info_shaping_reward_mean | -0.0544    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.44251645  |
| stats_o/std                    | 0.031260367 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000765   |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.76092935 |
| test/Q_plus_P                  | -0.76092935 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.718       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00479    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.4425198   |
| stats_o/std                    | 0.031240897 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00282    |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.8272894  |
| test/Q_plus_P                  | -0.8272894  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.4425262   |
| stats_o/std                    | 0.031219816 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0051     |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.7844315  |
| test/Q_plus_P                  | -0.7844315  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00412    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.44253004  |
| stats_o/std                    | 0.031201681 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.7080545  |
| test/Q_plus_P                  | -0.7080545  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.44253233  |
| stats_o/std                    | 0.031182198 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00665    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.80716753 |
| test/Q_plus_P                  | -0.80716753 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.44254196  |
| stats_o/std                    | 0.03116001  |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.81769824 |
| test/Q_plus_P                  | -0.81769824 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00432    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.44254467 |
| stats_o/std                    | 0.03114372 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0371    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -0.7940298 |
| test/Q_plus_P                  | -0.7940298 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.702      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.44255033  |
| stats_o/std                    | 0.03112229  |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00362    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.77720475 |
| test/Q_plus_P                  | -0.77720475 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.715       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.44255587  |
| stats_o/std                    | 0.031100594 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00249    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.8431993  |
| test/Q_plus_P                  | -0.8431993  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.44256413  |
| stats_o/std                    | 0.031079331 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00546    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.7809221  |
| test/Q_plus_P                  | -0.7809221  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.4425672   |
| stats_o/std                    | 0.031060755 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00555    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.706876   |
| test/Q_plus_P                  | -0.706876   |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.715       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.44257486  |
| stats_o/std                    | 0.031040715 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.8720831  |
| test/Q_plus_P                  | -0.8720831  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.44258228  |
| stats_o/std                    | 0.031022303 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.8525924  |
| test/Q_plus_P                  | -0.8525924  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.44258454  |
| stats_o/std                    | 0.031000787 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.7804513  |
| test/Q_plus_P                  | -0.7804513  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.719       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0509     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.44258997  |
| stats_o/std                    | 0.030979127 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.87435097 |
| test/Q_plus_P                  | -0.87435097 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0509     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 257        |
| stats_o/mean                   | 0.44259557 |
| stats_o/std                    | 0.03095933 |
| test/episodes                  | 2580       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00426   |
| test/info_shaping_reward_mean  | -0.0396    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -0.7807553 |
| test/Q_plus_P                  | -0.7807553 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 103200     |
| train/episodes                 | 10320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.698      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0546    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 412800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.44259754  |
| stats_o/std                    | 0.03094201  |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0347     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.75236404 |
| test/Q_plus_P                  | -0.75236404 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.4425994   |
| stats_o/std                    | 0.030921495 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00571    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.81944394 |
| test/Q_plus_P                  | -0.81944394 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.44260553 |
| stats_o/std                    | 0.03089939 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0364    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -0.7636773 |
| test/Q_plus_P                  | -0.7636773 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.703      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00329   |
| train/info_shaping_reward_mean | -0.0508    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.4426085   |
| stats_o/std                    | 0.030883128 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00537    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7821432  |
| test/Q_plus_P                  | -0.7821432  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.4426162   |
| stats_o/std                    | 0.030863956 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0112     |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.7351572  |
| test/Q_plus_P                  | -0.7351572  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.44262025  |
| stats_o/std                    | 0.030845156 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00355    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.7462742  |
| test/Q_plus_P                  | -0.7462742  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.44262636  |
| stats_o/std                    | 0.030825296 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00586    |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.71848893 |
| test/Q_plus_P                  | -0.71848893 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.44263104  |
| stats_o/std                    | 0.030809559 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00453    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.9154492  |
| test/Q_plus_P                  | -0.9154492  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.44263387  |
| stats_o/std                    | 0.030788826 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00724    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.8682167  |
| test/Q_plus_P                  | -0.8682167  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0491     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.4426347   |
| stats_o/std                    | 0.030779034 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00571    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.71943694 |
| test/Q_plus_P                  | -0.71943694 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.44264016  |
| stats_o/std                    | 0.030757813 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00648    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.758394   |
| test/Q_plus_P                  | -0.758394   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.719       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0502     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.44264162  |
| stats_o/std                    | 0.030740118 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.65933305 |
| test/Q_plus_P                  | -0.65933305 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00391    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.44264737  |
| stats_o/std                    | 0.030719325 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00638    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.7891577  |
| test/Q_plus_P                  | -0.7891577  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.44264984  |
| stats_o/std                    | 0.030700572 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0037     |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.78004956 |
| test/Q_plus_P                  | -0.78004956 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.44265354  |
| stats_o/std                    | 0.030682487 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00244    |
| test/info_shaping_reward_mean  | -0.0342     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.70344466 |
| test/Q_plus_P                  | -0.70344466 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.4426584   |
| stats_o/std                    | 0.030661555 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.751931   |
| test/Q_plus_P                  | -0.751931   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.4426624   |
| stats_o/std                    | 0.030646034 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7310746  |
| test/Q_plus_P                  | -0.7310746  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.44266787 |
| stats_o/std                    | 0.030629   |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0349    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -0.7333445 |
| test/Q_plus_P                  | -0.7333445 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.44266787  |
| stats_o/std                    | 0.03061588  |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.76509076 |
| test/Q_plus_P                  | -0.76509076 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.44267157  |
| stats_o/std                    | 0.030602572 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.77596664 |
| test/Q_plus_P                  | -0.77596664 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.44267944  |
| stats_o/std                    | 0.030581271 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0038     |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7528226  |
| test/Q_plus_P                  | -0.7528226  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.44268295  |
| stats_o/std                    | 0.03056817  |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00826    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.73859715 |
| test/Q_plus_P                  | -0.73859715 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.4426875   |
| stats_o/std                    | 0.030549688 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000684   |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.8352546  |
| test/Q_plus_P                  | -0.8352546  |
| test/reward_per_eps            | -7          |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.44268727  |
| stats_o/std                    | 0.030538693 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000563   |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.74485105 |
| test/Q_plus_P                  | -0.74485105 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.44269297 |
| stats_o/std                    | 0.03052075 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000878  |
| test/info_shaping_reward_mean  | -0.041     |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -0.7535722 |
| test/Q_plus_P                  | -0.7535722 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.0563    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.44269875  |
| stats_o/std                    | 0.030506095 |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00347    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.6992577  |
| test/Q_plus_P                  | -0.6992577  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.44270334  |
| stats_o/std                    | 0.030489534 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000596   |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.6471099  |
| test/Q_plus_P                  | -0.6471099  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.4427078   |
| stats_o/std                    | 0.030472888 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.75679123 |
| test/Q_plus_P                  | -0.75679123 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.729       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.8       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.44271287  |
| stats_o/std                    | 0.030456722 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00731    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.74709475 |
| test/Q_plus_P                  | -0.74709475 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.44271386  |
| stats_o/std                    | 0.030444302 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00885    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.81398475 |
| test/Q_plus_P                  | -0.81398475 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.4427222   |
| stats_o/std                    | 0.030425213 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.8216755  |
| test/Q_plus_P                  | -0.8216755  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00415    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.44272435  |
| stats_o/std                    | 0.030413354 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.75631297 |
| test/Q_plus_P                  | -0.75631297 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.44272932  |
| stats_o/std                    | 0.030397883 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00334    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.79676783 |
| test/Q_plus_P                  | -0.79676783 |
| test/reward_per_eps            | -7          |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.4427365   |
| stats_o/std                    | 0.030382844 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.75364363 |
| test/Q_plus_P                  | -0.75364363 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.44274136  |
| stats_o/std                    | 0.030363524 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00784    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.7192588  |
| test/Q_plus_P                  | -0.7192588  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00475    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.44274905  |
| stats_o/std                    | 0.030349517 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7736813  |
| test/Q_plus_P                  | -0.7736813  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00455    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.44274977  |
| stats_o/std                    | 0.030335179 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00643    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.7030032  |
| test/Q_plus_P                  | -0.7030032  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.44275555  |
| stats_o/std                    | 0.030322215 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00581    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.77448815 |
| test/Q_plus_P                  | -0.77448815 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.4427613   |
| stats_o/std                    | 0.030303031 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00556    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.6746967  |
| test/Q_plus_P                  | -0.6746967  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.4427661   |
| stats_o/std                    | 0.030288005 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.7597244  |
| test/Q_plus_P                  | -0.7597244  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.4427706   |
| stats_o/std                    | 0.030273253 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.6118262  |
| test/Q_plus_P                  | -0.6118262  |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.44277486  |
| stats_o/std                    | 0.030259756 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00447    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.70791894 |
| test/Q_plus_P                  | -0.70791894 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.4427799   |
| stats_o/std                    | 0.030252352 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00566    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7600542  |
| test/Q_plus_P                  | -0.7600542  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.44278634  |
| stats_o/std                    | 0.030236823 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0138     |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.73634285 |
| test/Q_plus_P                  | -0.73634285 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00498    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.4427911   |
| stats_o/std                    | 0.030223511 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.72235435 |
| test/Q_plus_P                  | -0.72235435 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.44279793 |
| stats_o/std                    | 0.03020717 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00377   |
| test/info_shaping_reward_mean  | -0.0408    |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -0.7141377 |
| test/Q_plus_P                  | -0.7141377 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0571    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.44280067  |
| stats_o/std                    | 0.030193241 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00997    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.6663693  |
| test/Q_plus_P                  | -0.6663693  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.44280505 |
| stats_o/std                    | 0.03017701 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00785   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -0.649735  |
| test/Q_plus_P                  | -0.649735  |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.714      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00508   |
| train/info_shaping_reward_mean | -0.0545    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.4428097   |
| stats_o/std                    | 0.03015797  |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0191     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.67472947 |
| test/Q_plus_P                  | -0.67472947 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.44281816  |
| stats_o/std                    | 0.030146996 |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.69889724 |
| test/Q_plus_P                  | -0.69889724 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.4428204   |
| stats_o/std                    | 0.030131409 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00391    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7946951  |
| test/Q_plus_P                  | -0.7946951  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.44282544  |
| stats_o/std                    | 0.030113941 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00631    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.7858341  |
| test/Q_plus_P                  | -0.7858341  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.44283047  |
| stats_o/std                    | 0.030101037 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00355    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.81034327 |
| test/Q_plus_P                  | -0.81034327 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.4428334  |
| stats_o/std                    | 0.03008773 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0388    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -0.7156528 |
| test/Q_plus_P                  | -0.7156528 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0563    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.44283485  |
| stats_o/std                    | 0.030077575 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00568    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7558275  |
| test/Q_plus_P                  | -0.7558275  |
| test/reward_per_eps            | -7          |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.44284007  |
| stats_o/std                    | 0.030059734 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00535    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.67802966 |
| test/Q_plus_P                  | -0.67802966 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.4428446   |
| stats_o/std                    | 0.030044392 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0144     |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.7096667  |
| test/Q_plus_P                  | -0.7096667  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.4428488   |
| stats_o/std                    | 0.030030988 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00407    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.7708249  |
| test/Q_plus_P                  | -0.7708249  |
| test/reward_per_eps            | -7          |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.4428508   |
| stats_o/std                    | 0.030020297 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00403    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7121523  |
| test/Q_plus_P                  | -0.7121523  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.44285306  |
| stats_o/std                    | 0.030006954 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.73679864 |
| test/Q_plus_P                  | -0.73679864 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00503    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.44285715  |
| stats_o/std                    | 0.029993951 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.7407242  |
| test/Q_plus_P                  | -0.7407242  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.44286156  |
| stats_o/std                    | 0.029983083 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.70914626 |
| test/Q_plus_P                  | -0.70914626 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.44286725  |
| stats_o/std                    | 0.029971689 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000759   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.736149   |
| test/Q_plus_P                  | -0.736149   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.719       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.4428726   |
| stats_o/std                    | 0.029958664 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7390651  |
| test/Q_plus_P                  | -0.7390651  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.44287884  |
| stats_o/std                    | 0.029943632 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7439884  |
| test/Q_plus_P                  | -0.7439884  |
| test/reward_per_eps            | -7          |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.4428823  |
| stats_o/std                    | 0.029932   |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.003     |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.7744186 |
| test/Q_plus_P                  | -0.7744186 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00328   |
| train/info_shaping_reward_mean | -0.0575    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.44288853  |
| stats_o/std                    | 0.029918341 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0114     |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -0.81193244 |
| test/Q_plus_P                  | -0.81193244 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 325        |
| stats_o/mean                   | 0.44289315 |
| stats_o/std                    | 0.02990785 |
| test/episodes                  | 3260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0441    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.6834554 |
| test/Q_plus_P                  | -0.6834554 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 130400     |
| train/episodes                 | 13040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.69       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00355   |
| train/info_shaping_reward_mean | -0.0557    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 521600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.4428964   |
| stats_o/std                    | 0.029891225 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.69845575 |
| test/Q_plus_P                  | -0.69845575 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.4428973   |
| stats_o/std                    | 0.029879814 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00878    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.68987674 |
| test/Q_plus_P                  | -0.68987674 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.4429014   |
| stats_o/std                    | 0.029866057 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.7193176  |
| test/Q_plus_P                  | -0.7193176  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.4429047   |
| stats_o/std                    | 0.029853195 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00321    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.7736392  |
| test/Q_plus_P                  | -0.7736392  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.739       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.4       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.44290748  |
| stats_o/std                    | 0.029840989 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0037     |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.70181876 |
| test/Q_plus_P                  | -0.70181876 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.726       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.9       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.44291124  |
| stats_o/std                    | 0.029827446 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00718    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.6642336  |
| test/Q_plus_P                  | -0.6642336  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.44291863  |
| stats_o/std                    | 0.02981398  |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00915    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.66698426 |
| test/Q_plus_P                  | -0.66698426 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.44292402  |
| stats_o/std                    | 0.029807946 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0061     |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.71579474 |
| test/Q_plus_P                  | -0.71579474 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.44292757  |
| stats_o/std                    | 0.029791392 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00811    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.6750823  |
| test/Q_plus_P                  | -0.6750823  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.44293264  |
| stats_o/std                    | 0.029781073 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00713    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.62294096 |
| test/Q_plus_P                  | -0.62294096 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.44293642  |
| stats_o/std                    | 0.029768713 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00861    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.7473046  |
| test/Q_plus_P                  | -0.7473046  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00648    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.44294     |
| stats_o/std                    | 0.029754652 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6793654  |
| test/Q_plus_P                  | -0.6793654  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.44294205  |
| stats_o/std                    | 0.029744685 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00746    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.6774308  |
| test/Q_plus_P                  | -0.6774308  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 339        |
| stats_o/mean                   | 0.4429449  |
| stats_o/std                    | 0.02973265 |
| test/episodes                  | 3400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00806   |
| test/info_shaping_reward_mean  | -0.0451    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.6173605 |
| test/Q_plus_P                  | -0.6173605 |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 136000     |
| train/episodes                 | 13600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0577    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 544000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.44294825  |
| stats_o/std                    | 0.029720934 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00694    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.67813593 |
| test/Q_plus_P                  | -0.67813593 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.44295207  |
| stats_o/std                    | 0.029705366 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.853       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00695    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.58797336 |
| test/Q_plus_P                  | -0.58797336 |
| test/reward_per_eps            | -5.9        |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.4429551   |
| stats_o/std                    | 0.02969593  |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.75912005 |
| test/Q_plus_P                  | -0.75912005 |
| test/reward_per_eps            | -7          |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.44295895  |
| stats_o/std                    | 0.029684315 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.7560694  |
| test/Q_plus_P                  | -0.7560694  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.44296277 |
| stats_o/std                    | 0.02967166 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0138    |
| test/info_shaping_reward_mean  | -0.0468    |
| test/info_shaping_reward_min   | -0.211     |
| test/Q                         | -0.6662281 |
| test/Q_plus_P                  | -0.6662281 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.702      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0038    |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.44296753  |
| stats_o/std                    | 0.029656062 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00801    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.71657723 |
| test/Q_plus_P                  | -0.71657723 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.44297078  |
| stats_o/std                    | 0.029642226 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0181     |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.65826195 |
| test/Q_plus_P                  | -0.65826195 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.729       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.8       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.44297567  |
| stats_o/std                    | 0.029630383 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000887   |
| test/info_shaping_reward_mean  | -0.031      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -0.6900768  |
| test/Q_plus_P                  | -0.6900768  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00425    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.44297692  |
| stats_o/std                    | 0.029618451 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00336    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.6667532  |
| test/Q_plus_P                  | -0.6667532  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.4429798   |
| stats_o/std                    | 0.029606223 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00575    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7400599  |
| test/Q_plus_P                  | -0.7400599  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.44298455  |
| stats_o/std                    | 0.029594457 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00396    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.7397411  |
| test/Q_plus_P                  | -0.7397411  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.44299078  |
| stats_o/std                    | 0.029580861 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00328    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.659359   |
| test/Q_plus_P                  | -0.659359   |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.005      |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.44299194  |
| stats_o/std                    | 0.029566104 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.6658256  |
| test/Q_plus_P                  | -0.6658256  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.746       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0502     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.2       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.44299817  |
| stats_o/std                    | 0.029552713 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.64454055 |
| test/Q_plus_P                  | -0.64454055 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00516    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.44300127  |
| stats_o/std                    | 0.029542001 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0113     |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.724145   |
| test/Q_plus_P                  | -0.724145   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.721       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.4430026   |
| stats_o/std                    | 0.029532744 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0315     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.66681975 |
| test/Q_plus_P                  | -0.66681975 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.4430059  |
| stats_o/std                    | 0.02952222 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00282   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -0.7422279 |
| test/Q_plus_P                  | -0.7422279 |
| test/reward_per_eps            | -7         |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00364   |
| train/info_shaping_reward_mean | -0.0559    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.44300818  |
| stats_o/std                    | 0.029512392 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.64664733 |
| test/Q_plus_P                  | -0.64664733 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.4430132   |
| stats_o/std                    | 0.029503027 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.6422515  |
| test/Q_plus_P                  | -0.6422515  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.44301772  |
| stats_o/std                    | 0.029490888 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00318    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.730536   |
| test/Q_plus_P                  | -0.730536   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.44302106  |
| stats_o/std                    | 0.029479818 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0072     |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.59873766 |
| test/Q_plus_P                  | -0.59873766 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.44302464  |
| stats_o/std                    | 0.02946961  |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0008     |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.71242803 |
| test/Q_plus_P                  | -0.71242803 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.44302705  |
| stats_o/std                    | 0.029460916 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0067     |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.7544397  |
| test/Q_plus_P                  | -0.7544397  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.44303092  |
| stats_o/std                    | 0.029450977 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.6636356  |
| test/Q_plus_P                  | -0.6636356  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0041     |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.44303462  |
| stats_o/std                    | 0.029442243 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.6345508  |
| test/Q_plus_P                  | -0.6345508  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.44303834  |
| stats_o/std                    | 0.029432638 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0142     |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.68769616 |
| test/Q_plus_P                  | -0.68769616 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.44304037  |
| stats_o/std                    | 0.029422022 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0268     |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.691872   |
| test/Q_plus_P                  | -0.691872   |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.44304243  |
| stats_o/std                    | 0.02941517  |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0308     |
| test/info_shaping_reward_mean  | -0.0584     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.70524347 |
| test/Q_plus_P                  | -0.70524347 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.44304696  |
| stats_o/std                    | 0.029402176 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0234     |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.7375793  |
| test/Q_plus_P                  | -0.7375793  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.44304916  |
| stats_o/std                    | 0.02939347  |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.58580244 |
| test/Q_plus_P                  | -0.58580244 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.44305238  |
| stats_o/std                    | 0.029383453 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00207    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7002155  |
| test/Q_plus_P                  | -0.7002155  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.44305602  |
| stats_o/std                    | 0.029373275 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.8023725  |
| test/Q_plus_P                  | -0.8023725  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00398    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.44305727  |
| stats_o/std                    | 0.029366598 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00685    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.7848868  |
| test/Q_plus_P                  | -0.7848868  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00201    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.44305956  |
| stats_o/std                    | 0.029358022 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.7596668  |
| test/Q_plus_P                  | -0.7596668  |
| test/reward_per_eps            | -7          |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00659    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.44306406 |
| stats_o/std                    | 0.02934873 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00982   |
| test/info_shaping_reward_mean  | -0.0528    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -0.7148442 |
| test/Q_plus_P                  | -0.7148442 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.673      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00314   |
| train/info_shaping_reward_mean | -0.0556    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.44306925  |
| stats_o/std                    | 0.02933641  |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00311    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.69125485 |
| test/Q_plus_P                  | -0.69125485 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00513    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.44306862  |
| stats_o/std                    | 0.029328184 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00566    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.64064443 |
| test/Q_plus_P                  | -0.64064443 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.44307002  |
| stats_o/std                    | 0.029318245 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00424    |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6713244  |
| test/Q_plus_P                  | -0.6713244  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.44307432  |
| stats_o/std                    | 0.029309496 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00403    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.7235233  |
| test/Q_plus_P                  | -0.7235233  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.44307843  |
| stats_o/std                    | 0.029300494 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.659909   |
| test/Q_plus_P                  | -0.659909   |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00393    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.44308174  |
| stats_o/std                    | 0.029291378 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0197     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.65880716 |
| test/Q_plus_P                  | -0.65880716 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.44308504  |
| stats_o/std                    | 0.029282413 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00549    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.80145615 |
| test/Q_plus_P                  | -0.80145615 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 382         |
| stats_o/mean                   | 0.44308826  |
| stats_o/std                    | 0.029275708 |
| test/episodes                  | 3830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00542    |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.8719269  |
| test/Q_plus_P                  | -0.8719269  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 153200      |
| train/episodes                 | 15320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 612800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.44309556  |
| stats_o/std                    | 0.029265711 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00939    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.72916305 |
| test/Q_plus_P                  | -0.72916305 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.44309843  |
| stats_o/std                    | 0.029258596 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00232    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.77521574 |
| test/Q_plus_P                  | -0.77521574 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.005      |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.44310394  |
| stats_o/std                    | 0.029250957 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00394    |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.6879194  |
| test/Q_plus_P                  | -0.6879194  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00552    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.44310585  |
| stats_o/std                    | 0.029244484 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00427    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.75097203 |
| test/Q_plus_P                  | -0.75097203 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00489    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.44310856  |
| stats_o/std                    | 0.02923444  |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00925    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.71011996 |
| test/Q_plus_P                  | -0.71011996 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.721       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.44311288  |
| stats_o/std                    | 0.029224036 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00308    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.70485216 |
| test/Q_plus_P                  | -0.70485216 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00492    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.4431176   |
| stats_o/std                    | 0.029213957 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.004      |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.70548487 |
| test/Q_plus_P                  | -0.70548487 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.44311905  |
| stats_o/std                    | 0.029206263 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.71834594 |
| test/Q_plus_P                  | -0.71834594 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.4431207   |
| stats_o/std                    | 0.029194966 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00546    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.71342224 |
| test/Q_plus_P                  | -0.71342224 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.734       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.7       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.4431226   |
| stats_o/std                    | 0.029186996 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.7325541  |
| test/Q_plus_P                  | -0.7325541  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.443126    |
| stats_o/std                    | 0.029175205 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00764    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.71629554 |
| test/Q_plus_P                  | -0.71629554 |
| test/reward_per_eps            | -7          |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.44312692  |
| stats_o/std                    | 0.029168202 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0059     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.6541243  |
| test/Q_plus_P                  | -0.6541243  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.44313243  |
| stats_o/std                    | 0.029158873 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0229     |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.713289   |
| test/Q_plus_P                  | -0.713289   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00454    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.44313645  |
| stats_o/std                    | 0.029149568 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0161     |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.6560808  |
| test/Q_plus_P                  | -0.6560808  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.44313934  |
| stats_o/std                    | 0.029139312 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0303     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.66582054 |
| test/Q_plus_P                  | -0.66582054 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.44314453 |
| stats_o/std                    | 0.02913095 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0302    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -0.6766429 |
| test/Q_plus_P                  | -0.6766429 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00601   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.4431497   |
| stats_o/std                    | 0.029122232 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000979   |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.62574995 |
| test/Q_plus_P                  | -0.62574995 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.44315195  |
| stats_o/std                    | 0.029114408 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.6373701  |
| test/Q_plus_P                  | -0.6373701  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.44315302  |
| stats_o/std                    | 0.029105343 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00394    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.62875575 |
| test/Q_plus_P                  | -0.62875575 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.44315776  |
| stats_o/std                    | 0.02909656  |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0148     |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.70850784 |
| test/Q_plus_P                  | -0.70850784 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.44316173  |
| stats_o/std                    | 0.029086096 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.6585582  |
| test/Q_plus_P                  | -0.6585582  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.4431653   |
| stats_o/std                    | 0.029075837 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0247     |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.6932003  |
| test/Q_plus_P                  | -0.6932003  |
| test/reward_per_eps            | -7          |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.44316852  |
| stats_o/std                    | 0.029064426 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0224     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.79477906 |
| test/Q_plus_P                  | -0.79477906 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.443172    |
| stats_o/std                    | 0.029055998 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.6191093  |
| test/Q_plus_P                  | -0.6191093  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00446    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.44317344  |
| stats_o/std                    | 0.02904771  |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.64789873 |
| test/Q_plus_P                  | -0.64789873 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.4431746   |
| stats_o/std                    | 0.029038176 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00992    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.70291626 |
| test/Q_plus_P                  | -0.70291626 |
| test/reward_per_eps            | -7          |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00528    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.44317663  |
| stats_o/std                    | 0.029031716 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00605    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.206      |
| test/Q                         | -0.60986334 |
| test/Q_plus_P                  | -0.60986334 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00618    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.44317994  |
| stats_o/std                    | 0.029021455 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0234     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.73730546 |
| test/Q_plus_P                  | -0.73730546 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.44318238  |
| stats_o/std                    | 0.029012809 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.6735911  |
| test/Q_plus_P                  | -0.6735911  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.44318518  |
| stats_o/std                    | 0.029003127 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0219     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6709966  |
| test/Q_plus_P                  | -0.6709966  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0051     |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.44318852  |
| stats_o/std                    | 0.02899496  |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00567    |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.74429625 |
| test/Q_plus_P                  | -0.74429625 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.44319102  |
| stats_o/std                    | 0.028985696 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0082     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.7960598  |
| test/Q_plus_P                  | -0.7960598  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.44319502  |
| stats_o/std                    | 0.028975407 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.0378     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.76220536 |
| test/Q_plus_P                  | -0.76220536 |
| test/reward_per_eps            | -7          |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.44319937  |
| stats_o/std                    | 0.028965376 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0177     |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.6780186  |
| test/Q_plus_P                  | -0.6780186  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.44320258  |
| stats_o/std                    | 0.028957576 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00271    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.6776572  |
| test/Q_plus_P                  | -0.6776572  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.44320703  |
| stats_o/std                    | 0.028948603 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00404    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.6329188  |
| test/Q_plus_P                  | -0.6329188  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.44320986  |
| stats_o/std                    | 0.028942041 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.82835877 |
| test/Q_plus_P                  | -0.82835877 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.4432137   |
| stats_o/std                    | 0.028932884 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00266    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.73833567 |
| test/Q_plus_P                  | -0.73833567 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.44321737  |
| stats_o/std                    | 0.028923085 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.035      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7195445  |
| test/Q_plus_P                  | -0.7195445  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.44322094  |
| stats_o/std                    | 0.028914584 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.72486585 |
| test/Q_plus_P                  | -0.72486585 |
| test/reward_per_eps            | -7          |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.4432216   |
| stats_o/std                    | 0.028907701 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7086636  |
| test/Q_plus_P                  | -0.7086636  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00512    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.44322464  |
| stats_o/std                    | 0.02889842  |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000934   |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.59141195 |
| test/Q_plus_P                  | -0.59141195 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.44322813  |
| stats_o/std                    | 0.028889874 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.66806203 |
| test/Q_plus_P                  | -0.66806203 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00534    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.44323158  |
| stats_o/std                    | 0.028884284 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00501    |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.6707309  |
| test/Q_plus_P                  | -0.6707309  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0042     |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.44323468  |
| stats_o/std                    | 0.028877595 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.85        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0054     |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.593066   |
| test/Q_plus_P                  | -0.593066   |
| test/reward_per_eps            | -6          |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.44323632  |
| stats_o/std                    | 0.028868712 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00425    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.66708755 |
| test/Q_plus_P                  | -0.66708755 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.44323906  |
| stats_o/std                    | 0.028860925 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0257     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.674897   |
| test/Q_plus_P                  | -0.674897   |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.44324264  |
| stats_o/std                    | 0.028853446 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.67453295 |
| test/Q_plus_P                  | -0.67453295 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.4432428   |
| stats_o/std                    | 0.028849378 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0272     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.8166964  |
| test/Q_plus_P                  | -0.8166964  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.44324407  |
| stats_o/std                    | 0.028842026 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.7022554  |
| test/Q_plus_P                  | -0.7022554  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.718       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.44324788 |
| stats_o/std                    | 0.02883401 |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00447   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -0.6692576 |
| test/Q_plus_P                  | -0.6692576 |
| test/reward_per_eps            | -7         |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00267   |
| train/info_shaping_reward_mean | -0.0568    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.44324967 |
| stats_o/std                    | 0.02882968 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0269    |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.6211262 |
| test/Q_plus_P                  | -0.6211262 |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0573    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.4432538   |
| stats_o/std                    | 0.02882281  |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.66403085 |
| test/Q_plus_P                  | -0.66403085 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.4432579   |
| stats_o/std                    | 0.028815933 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000613   |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.6271601  |
| test/Q_plus_P                  | -0.6271601  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00484    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.44326246  |
| stats_o/std                    | 0.028807493 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0069     |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.6985343  |
| test/Q_plus_P                  | -0.6985343  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.4432626   |
| stats_o/std                    | 0.028803052 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0154     |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.64028907 |
| test/Q_plus_P                  | -0.64028907 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.44326484  |
| stats_o/std                    | 0.028796084 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0145     |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.204      |
| test/Q                         | -0.59202975 |
| test/Q_plus_P                  | -0.59202975 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.4432652   |
| stats_o/std                    | 0.028790882 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00606    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.6300609  |
| test/Q_plus_P                  | -0.6300609  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.44326594 |
| stats_o/std                    | 0.02878066 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0143    |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.6345222 |
| test/Q_plus_P                  | -0.6345222 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.72       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00319   |
| train/info_shaping_reward_mean | -0.0512    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.2      |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.44326857  |
| stats_o/std                    | 0.028772978 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0255     |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.7001689  |
| test/Q_plus_P                  | -0.7001689  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00432    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.4432727   |
| stats_o/std                    | 0.028761724 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0122     |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.6790669  |
| test/Q_plus_P                  | -0.6790669  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.44327554  |
| stats_o/std                    | 0.028754292 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.6763984  |
| test/Q_plus_P                  | -0.6763984  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.44327912 |
| stats_o/std                    | 0.02874712 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00207   |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -0.6539105 |
| test/Q_plus_P                  | -0.6539105 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00459   |
| train/info_shaping_reward_mean | -0.0574    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.4432821   |
| stats_o/std                    | 0.028738288 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.016      |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.691271   |
| test/Q_plus_P                  | -0.691271   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.4432865   |
| stats_o/std                    | 0.028726941 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.014      |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.6779487  |
| test/Q_plus_P                  | -0.6779487  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00564    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.4432908   |
| stats_o/std                    | 0.028720863 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0236     |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.7263079  |
| test/Q_plus_P                  | -0.7263079  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.4432933   |
| stats_o/std                    | 0.028713701 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.69586104 |
| test/Q_plus_P                  | -0.69586104 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.4432944   |
| stats_o/std                    | 0.028705867 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0156     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.74416834 |
| test/Q_plus_P                  | -0.74416834 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.44329748  |
| stats_o/std                    | 0.028695112 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00319    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.65376663 |
| test/Q_plus_P                  | -0.65376663 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00515    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.4432989   |
| stats_o/std                    | 0.028688394 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0293     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.59412736 |
| test/Q_plus_P                  | -0.59412736 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.44330177  |
| stats_o/std                    | 0.028679766 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00411    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.67477435 |
| test/Q_plus_P                  | -0.67477435 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00434    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.4433039   |
| stats_o/std                    | 0.028671892 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.717704   |
| test/Q_plus_P                  | -0.717704   |
| test/reward_per_eps            | -7          |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.4433072   |
| stats_o/std                    | 0.028664023 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00521    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.6002202  |
| test/Q_plus_P                  | -0.6002202  |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.44330895  |
| stats_o/std                    | 0.028657086 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.012      |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.69523484 |
| test/Q_plus_P                  | -0.69523484 |
| test/reward_per_eps            | -7          |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.4433123   |
| stats_o/std                    | 0.028649757 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0173     |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.6578877  |
| test/Q_plus_P                  | -0.6578877  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00478    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.4433153   |
| stats_o/std                    | 0.028642861 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00887    |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.7039642  |
| test/Q_plus_P                  | -0.7039642  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00721    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.4433181   |
| stats_o/std                    | 0.028634831 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.855       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0041     |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.53597814 |
| test/Q_plus_P                  | -0.53597814 |
| test/reward_per_eps            | -5.8        |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.44331864  |
| stats_o/std                    | 0.028629646 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.6320379  |
| test/Q_plus_P                  | -0.6320379  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.4433209  |
| stats_o/std                    | 0.02862248 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.848      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00247   |
| test/info_shaping_reward_mean  | -0.0325    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -0.579002  |
| test/Q_plus_P                  | -0.579002  |
| test/reward_per_eps            | -6.1       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.702      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.0535    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.4433237   |
| stats_o/std                    | 0.028612731 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00241    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.7292252  |
| test/Q_plus_P                  | -0.7292252  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.4433243   |
| stats_o/std                    | 0.028604649 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00246    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.59396625 |
| test/Q_plus_P                  | -0.59396625 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00446    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.44332647  |
| stats_o/std                    | 0.028595222 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0166     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.6655933  |
| test/Q_plus_P                  | -0.6655933  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00455    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.4433285   |
| stats_o/std                    | 0.028586281 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0105     |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.62118745 |
| test/Q_plus_P                  | -0.62118745 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.44333088  |
| stats_o/std                    | 0.028577566 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00772    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.6869761  |
| test/Q_plus_P                  | -0.6869761  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.44333282  |
| stats_o/std                    | 0.028570315 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00278    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.60614294 |
| test/Q_plus_P                  | -0.60614294 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00792    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.4433352   |
| stats_o/std                    | 0.028562708 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.6564356  |
| test/Q_plus_P                  | -0.6564356  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00443    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.4433382   |
| stats_o/std                    | 0.028557844 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00477    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.72302246 |
| test/Q_plus_P                  | -0.72302246 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00429    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.4433409  |
| stats_o/std                    | 0.02855009 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000974  |
| test/info_shaping_reward_mean  | -0.0409    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -0.714081  |
| test/Q_plus_P                  | -0.714081  |
| test/reward_per_eps            | -7         |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.44334367  |
| stats_o/std                    | 0.028543912 |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00908    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.6873727  |
| test/Q_plus_P                  | -0.6873727  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.44334638  |
| stats_o/std                    | 0.028538898 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00271    |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.65652245 |
| test/Q_plus_P                  | -0.65652245 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.44334856 |
| stats_o/std                    | 0.02853144 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00464   |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -0.6674598 |
| test/Q_plus_P                  | -0.6674598 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00664   |
| train/info_shaping_reward_mean | -0.0568    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.44335023 |
| stats_o/std                    | 0.02852428 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00767   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -0.7048121 |
| test/Q_plus_P                  | -0.7048121 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0581    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.44335186  |
| stats_o/std                    | 0.028518595 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.6892665  |
| test/Q_plus_P                  | -0.6892665  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00499    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.44335437  |
| stats_o/std                    | 0.028511522 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0051     |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6536704  |
| test/Q_plus_P                  | -0.6536704  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00513    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.44335583  |
| stats_o/std                    | 0.028503945 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00605    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.6252913  |
| test/Q_plus_P                  | -0.6252913  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.4433576  |
| stats_o/std                    | 0.0284967  |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.011     |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -0.6707053 |
| test/Q_plus_P                  | -0.6707053 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.708      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00452   |
| train/info_shaping_reward_mean | -0.0539    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.7      |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.4433597   |
| stats_o/std                    | 0.028491788 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00205    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.58801544 |
| test/Q_plus_P                  | -0.58801544 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00571    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.44336185  |
| stats_o/std                    | 0.028485922 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00449    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.67002296 |
| test/Q_plus_P                  | -0.67002296 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.44336426  |
| stats_o/std                    | 0.028480455 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00621    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.6536884  |
| test/Q_plus_P                  | -0.6536884  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00504    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.44336402  |
| stats_o/std                    | 0.028475238 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00429    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.73315346 |
| test/Q_plus_P                  | -0.73315346 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0504     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.44336548  |
| stats_o/std                    | 0.028465956 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.8292399  |
| test/Q_plus_P                  | -0.8292399  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.44336677  |
| stats_o/std                    | 0.028460577 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00881    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.6167965  |
| test/Q_plus_P                  | -0.6167965  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.44336855  |
| stats_o/std                    | 0.028455086 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.7039258  |
| test/Q_plus_P                  | -0.7039258  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.44337013  |
| stats_o/std                    | 0.028448187 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00428    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.67726785 |
| test/Q_plus_P                  | -0.67726785 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.4433737   |
| stats_o/std                    | 0.028441913 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0212     |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.66185004 |
| test/Q_plus_P                  | -0.66185004 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.44337514 |
| stats_o/std                    | 0.02843851 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.005     |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -0.7289304 |
| test/Q_plus_P                  | -0.7289304 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00493   |
| train/info_shaping_reward_mean | -0.0597    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.44337562  |
| stats_o/std                    | 0.028433517 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.66631037 |
| test/Q_plus_P                  | -0.66631037 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.44337714  |
| stats_o/std                    | 0.028427623 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000923   |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.71902853 |
| test/Q_plus_P                  | -0.71902853 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.44337928  |
| stats_o/std                    | 0.028420908 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00249    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.64537805 |
| test/Q_plus_P                  | -0.64537805 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00396    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.4433818  |
| stats_o/std                    | 0.02841512 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.848      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0322    |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -0.5843232 |
| test/Q_plus_P                  | -0.5843232 |
| test/reward_per_eps            | -6.1       |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00439   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.44338396  |
| stats_o/std                    | 0.028410045 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0303     |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.7418492  |
| test/Q_plus_P                  | -0.7418492  |
| test/reward_per_eps            | -7          |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.44338605  |
| stats_o/std                    | 0.02840294  |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.69471794 |
| test/Q_plus_P                  | -0.69471794 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.44338945 |
| stats_o/std                    | 0.02839758 |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0393    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.7250189 |
| test/Q_plus_P                  | -0.7250189 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.665      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0043    |
| train/info_shaping_reward_mean | -0.0575    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.4433913   |
| stats_o/std                    | 0.028391825 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00604    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.73049945 |
| test/Q_plus_P                  | -0.73049945 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.44339177  |
| stats_o/std                    | 0.028385846 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00495    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.7074673  |
| test/Q_plus_P                  | -0.7074673  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.44339237  |
| stats_o/std                    | 0.028381078 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00738    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.75996286 |
| test/Q_plus_P                  | -0.75996286 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.44339418  |
| stats_o/std                    | 0.028376019 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00371    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.66832703 |
| test/Q_plus_P                  | -0.66832703 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00495    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.44339576 |
| stats_o/std                    | 0.02836862 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0263    |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -0.7023144 |
| test/Q_plus_P                  | -0.7023144 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0571    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.44339862  |
| stats_o/std                    | 0.028362863 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.6691711  |
| test/Q_plus_P                  | -0.6691711  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.44340062  |
| stats_o/std                    | 0.028356291 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6825454  |
| test/Q_plus_P                  | -0.6825454  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.44340155  |
| stats_o/std                    | 0.028349362 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.59530807 |
| test/Q_plus_P                  | -0.59530807 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.44340226  |
| stats_o/std                    | 0.028344588 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00662    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.65111125 |
| test/Q_plus_P                  | -0.65111125 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00565    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.4434049  |
| stats_o/std                    | 0.02833735 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0271    |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.6455734 |
| test/Q_plus_P                  | -0.6455734 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.698      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00441   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.44340527  |
| stats_o/std                    | 0.028332224 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00714    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.66476464 |
| test/Q_plus_P                  | -0.66476464 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00483    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.44340706  |
| stats_o/std                    | 0.028329605 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6762584  |
| test/Q_plus_P                  | -0.6762584  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00634    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.44340846  |
| stats_o/std                    | 0.028322997 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00266    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.69335985 |
| test/Q_plus_P                  | -0.69335985 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.4434102   |
| stats_o/std                    | 0.028318664 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0214     |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.67479545 |
| test/Q_plus_P                  | -0.67479545 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00501    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.44341147 |
| stats_o/std                    | 0.02831336 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0244    |
| test/info_shaping_reward_mean  | -0.0517    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -0.6341424 |
| test/Q_plus_P                  | -0.6341424 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0591    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.44341323  |
| stats_o/std                    | 0.028306512 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00618    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.68635476 |
| test/Q_plus_P                  | -0.68635476 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.44341615  |
| stats_o/std                    | 0.028300462 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00369    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.6920976  |
| test/Q_plus_P                  | -0.6920976  |
| test/reward_per_eps            | -7          |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00579    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.44341812  |
| stats_o/std                    | 0.028296487 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0166     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.74314857 |
| test/Q_plus_P                  | -0.74314857 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00431    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.4434204   |
| stats_o/std                    | 0.028291741 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.018      |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.67608917 |
| test/Q_plus_P                  | -0.67608917 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.44342276  |
| stats_o/std                    | 0.028285204 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0242     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.6497261  |
| test/Q_plus_P                  | -0.6497261  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00425    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.4434257   |
| stats_o/std                    | 0.028276859 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0248     |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.6333361  |
| test/Q_plus_P                  | -0.6333361  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.44342843  |
| stats_o/std                    | 0.028268516 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0177     |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.6552046  |
| test/Q_plus_P                  | -0.6552046  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00516    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.44343305  |
| stats_o/std                    | 0.028261138 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0136     |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.59170675 |
| test/Q_plus_P                  | -0.59170675 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00523    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.44343522  |
| stats_o/std                    | 0.02825521  |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00553    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.55683523 |
| test/Q_plus_P                  | -0.55683523 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00489    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.44343904  |
| stats_o/std                    | 0.028247163 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00404    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.65181077 |
| test/Q_plus_P                  | -0.65181077 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.44344234  |
| stats_o/std                    | 0.028241098 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00479    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.62642145 |
| test/Q_plus_P                  | -0.62642145 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.44344604  |
| stats_o/std                    | 0.028235016 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0128     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.650346   |
| test/Q_plus_P                  | -0.650346   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00709    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.44344926  |
| stats_o/std                    | 0.028227558 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.6925035  |
| test/Q_plus_P                  | -0.6925035  |
| test/reward_per_eps            | -7          |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00534    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.44345114  |
| stats_o/std                    | 0.028221153 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00661    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.59561914 |
| test/Q_plus_P                  | -0.59561914 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00394    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.44345355  |
| stats_o/std                    | 0.028213024 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.67329997 |
| test/Q_plus_P                  | -0.67329997 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.44345438  |
| stats_o/std                    | 0.028207727 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0291     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.59821683 |
| test/Q_plus_P                  | -0.59821683 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.44345567  |
| stats_o/std                    | 0.028201351 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0276     |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.64511925 |
| test/Q_plus_P                  | -0.64511925 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.724       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.1       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.44345713  |
| stats_o/std                    | 0.028194912 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0282     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.71732163 |
| test/Q_plus_P                  | -0.71732163 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.44346035  |
| stats_o/std                    | 0.028189348 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00539    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.69829345 |
| test/Q_plus_P                  | -0.69829345 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00599    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.4434617   |
| stats_o/std                    | 0.028186398 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.7238614  |
| test/Q_plus_P                  | -0.7238614  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00661    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.44346222  |
| stats_o/std                    | 0.028183075 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0247     |
| test/info_shaping_reward_mean  | -0.0596     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.90679467 |
| test/Q_plus_P                  | -0.90679467 |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.44346634  |
| stats_o/std                    | 0.028179033 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00352    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.6722047  |
| test/Q_plus_P                  | -0.6722047  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00508    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.443467    |
| stats_o/std                    | 0.028172545 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.6837509  |
| test/Q_plus_P                  | -0.6837509  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.44346675  |
| stats_o/std                    | 0.028168276 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00358    |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.6758297  |
| test/Q_plus_P                  | -0.6758297  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00625    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.4434681   |
| stats_o/std                    | 0.028164655 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00702    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6676395  |
| test/Q_plus_P                  | -0.6676395  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.4434695   |
| stats_o/std                    | 0.028160186 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0311     |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.72025687 |
| test/Q_plus_P                  | -0.72025687 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.44346985  |
| stats_o/std                    | 0.028153881 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00572    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.6782891  |
| test/Q_plus_P                  | -0.6782891  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00471    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.44347072 |
| stats_o/std                    | 0.02815078 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00515   |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.6788072 |
| test/Q_plus_P                  | -0.6788072 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.691      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0588    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.44347334  |
| stats_o/std                    | 0.028147148 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0258     |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.7175177  |
| test/Q_plus_P                  | -0.7175177  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.44347438  |
| stats_o/std                    | 0.028140394 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0221     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.69379634 |
| test/Q_plus_P                  | -0.69379634 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.443476    |
| stats_o/std                    | 0.02813346  |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0222     |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.66337425 |
| test/Q_plus_P                  | -0.66337425 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.44347775  |
| stats_o/std                    | 0.02812802  |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.66230154 |
| test/Q_plus_P                  | -0.66230154 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.44347975  |
| stats_o/std                    | 0.028120564 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0134     |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -0.5529624  |
| test/Q_plus_P                  | -0.5529624  |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.44348255  |
| stats_o/std                    | 0.028113855 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0107     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.6915051  |
| test/Q_plus_P                  | -0.6915051  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0061     |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.44348502  |
| stats_o/std                    | 0.02810812  |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.58245707 |
| test/Q_plus_P                  | -0.58245707 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.4434879   |
| stats_o/std                    | 0.028103242 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0282     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.6545617  |
| test/Q_plus_P                  | -0.6545617  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00708    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.4434887   |
| stats_o/std                    | 0.028099447 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00779    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.6301002  |
| test/Q_plus_P                  | -0.6301002  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.44349113 |
| stats_o/std                    | 0.02809391 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.016     |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.6511252 |
| test/Q_plus_P                  | -0.6511252 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.681      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0586    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.443494    |
| stats_o/std                    | 0.028087286 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0182     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.6679892  |
| test/Q_plus_P                  | -0.6679892  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.44349563 |
| stats_o/std                    | 0.02808278 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00679   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.7085539 |
| test/Q_plus_P                  | -0.7085539 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00639   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.44349626  |
| stats_o/std                    | 0.028079664 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.659599   |
| test/Q_plus_P                  | -0.659599   |
| test/reward_per_eps            | -7          |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.44349733  |
| stats_o/std                    | 0.028074095 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00476    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.685885   |
| test/Q_plus_P                  | -0.685885   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 553        |
| stats_o/mean                   | 0.44349882 |
| stats_o/std                    | 0.02806661 |
| test/episodes                  | 5540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0201    |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -0.7074308 |
| test/Q_plus_P                  | -0.7074308 |
| test/reward_per_eps            | -7         |
| test/steps                     | 221600     |
| train/episodes                 | 22160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00295   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 886400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.44350052  |
| stats_o/std                    | 0.028061947 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.65168273 |
| test/Q_plus_P                  | -0.65168273 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.4435028   |
| stats_o/std                    | 0.028056381 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0176     |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.63188964 |
| test/Q_plus_P                  | -0.63188964 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.4435048  |
| stats_o/std                    | 0.02805431 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0234    |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -0.6538152 |
| test/Q_plus_P                  | -0.6538152 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00754   |
| train/info_shaping_reward_mean | -0.0653    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.44350728  |
| stats_o/std                    | 0.028047988 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0299     |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.6432074  |
| test/Q_plus_P                  | -0.6432074  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.44350973  |
| stats_o/std                    | 0.028043376 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0224     |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.6065807  |
| test/Q_plus_P                  | -0.6065807  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00595    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.44351313  |
| stats_o/std                    | 0.028037377 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0321     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.5610048  |
| test/Q_plus_P                  | -0.5610048  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.44351408 |
| stats_o/std                    | 0.02803345 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0258    |
| test/info_shaping_reward_mean  | -0.0546    |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -0.6568524 |
| test/Q_plus_P                  | -0.6568524 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00689   |
| train/info_shaping_reward_mean | -0.06      |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.4435166   |
| stats_o/std                    | 0.028028188 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0303     |
| test/info_shaping_reward_mean  | -0.057      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.69211257 |
| test/Q_plus_P                  | -0.69211257 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.44351828  |
| stats_o/std                    | 0.028021919 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.6418661  |
| test/Q_plus_P                  | -0.6418661  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.4435189  |
| stats_o/std                    | 0.02801916 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0149    |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -0.6660019 |
| test/Q_plus_P                  | -0.6660019 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.698      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0542    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.44351956  |
| stats_o/std                    | 0.028014423 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.013      |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.67680025 |
| test/Q_plus_P                  | -0.67680025 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00547    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.4435207   |
| stats_o/std                    | 0.028008759 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00936    |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6975472  |
| test/Q_plus_P                  | -0.6975472  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.44352427  |
| stats_o/std                    | 0.027998148 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.85        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0282     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.563048   |
| test/Q_plus_P                  | -0.563048   |
| test/reward_per_eps            | -6          |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00675    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.44352677 |
| stats_o/std                    | 0.02799225 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0319    |
| test/info_shaping_reward_mean  | -0.0575    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -0.6470921 |
| test/Q_plus_P                  | -0.6470921 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.674      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00633   |
| train/info_shaping_reward_mean | -0.0579    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.44352952  |
| stats_o/std                    | 0.027986238 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0208     |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -0.59127545 |
| test/Q_plus_P                  | -0.59127545 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00497    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.44353107  |
| stats_o/std                    | 0.027978688 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0238     |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.62786734 |
| test/Q_plus_P                  | -0.62786734 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.4435322  |
| stats_o/std                    | 0.02797194 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00697   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -0.6959393 |
| test/Q_plus_P                  | -0.6959393 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00766   |
| train/info_shaping_reward_mean | -0.0594    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.44353306  |
| stats_o/std                    | 0.027967589 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.855       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00584    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.5350008  |
| test/Q_plus_P                  | -0.5350008  |
| test/reward_per_eps            | -5.8        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0077     |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.44353545  |
| stats_o/std                    | 0.027961224 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0262     |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.655804   |
| test/Q_plus_P                  | -0.655804   |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.44353756  |
| stats_o/std                    | 0.027956648 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0265     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.6380697  |
| test/Q_plus_P                  | -0.6380697  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00398    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.44353938 |
| stats_o/std                    | 0.02795238 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.0557    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -0.7448946 |
| test/Q_plus_P                  | -0.7448946 |
| test/reward_per_eps            | -7         |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0044    |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.44354132  |
| stats_o/std                    | 0.027947562 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0246     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6062773  |
| test/Q_plus_P                  | -0.6062773  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.44354272  |
| stats_o/std                    | 0.027945118 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00818    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.6565012  |
| test/Q_plus_P                  | -0.6565012  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.44354442  |
| stats_o/std                    | 0.027937302 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00872    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.67878604 |
| test/Q_plus_P                  | -0.67878604 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.44354498  |
| stats_o/std                    | 0.027934022 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00816    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.62907785 |
| test/Q_plus_P                  | -0.62907785 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00469    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.44354558 |
| stats_o/std                    | 0.02792737 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0158    |
| test/info_shaping_reward_mean  | -0.0489    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.7018498 |
| test/Q_plus_P                  | -0.7018498 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.697      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00318   |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.44354892  |
| stats_o/std                    | 0.027920201 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.537       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.018      |
| test/info_shaping_reward_mean  | -0.0653     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.5328892  |
| test/Q_plus_P                  | -1.5328892  |
| test/reward_per_eps            | -18.5       |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0059     |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.4435492   |
| stats_o/std                    | 0.027918233 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00456    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.69900614 |
| test/Q_plus_P                  | -0.69900614 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 582        |
| stats_o/mean                   | 0.44355062 |
| stats_o/std                    | 0.02791538 |
| test/episodes                  | 5830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0147    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.7111994 |
| test/Q_plus_P                  | -0.7111994 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 233200     |
| train/episodes                 | 23320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0598    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 932800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.44355294 |
| stats_o/std                    | 0.02790946 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0188    |
| test/info_shaping_reward_mean  | -0.0576    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -0.6930567 |
| test/Q_plus_P                  | -0.6930567 |
| test/reward_per_eps            | -7         |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00374   |
| train/info_shaping_reward_mean | -0.0551    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.4435539   |
| stats_o/std                    | 0.027902879 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0202     |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.65746826 |
| test/Q_plus_P                  | -0.65746826 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.44355664  |
| stats_o/std                    | 0.027897151 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0175     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.6497293  |
| test/Q_plus_P                  | -0.6497293  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00658    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.44355723  |
| stats_o/std                    | 0.027890496 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0202     |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.6981134  |
| test/Q_plus_P                  | -0.6981134  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.748       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0507     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.1       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.4435595   |
| stats_o/std                    | 0.027887395 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0182     |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.68938655 |
| test/Q_plus_P                  | -0.68938655 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.44355965  |
| stats_o/std                    | 0.027882382 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0202     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.7023081  |
| test/Q_plus_P                  | -0.7023081  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00431    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.44356194  |
| stats_o/std                    | 0.027875545 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.63640076 |
| test/Q_plus_P                  | -0.63640076 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00794    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.4435631   |
| stats_o/std                    | 0.027871614 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0131     |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.6991081  |
| test/Q_plus_P                  | -0.6991081  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00513    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.44356355  |
| stats_o/std                    | 0.027867688 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.65904    |
| test/Q_plus_P                  | -0.65904    |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.4435654   |
| stats_o/std                    | 0.027861739 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.6933224  |
| test/Q_plus_P                  | -0.6933224  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00445    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.44356632  |
| stats_o/std                    | 0.02785673  |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00205    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.63973665 |
| test/Q_plus_P                  | -0.63973665 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.44356832  |
| stats_o/std                    | 0.027851531 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0164     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.5837894  |
| test/Q_plus_P                  | -0.5837894  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.4435697   |
| stats_o/std                    | 0.027847173 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0262     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.6296288  |
| test/Q_plus_P                  | -0.6296288  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.44357166  |
| stats_o/std                    | 0.027843116 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00626    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.6624766  |
| test/Q_plus_P                  | -0.6624766  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00618    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.44357345  |
| stats_o/std                    | 0.027837688 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0026     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.58798987 |
| test/Q_plus_P                  | -0.58798987 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.44357482  |
| stats_o/std                    | 0.027831815 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0343     |
| test/info_shaping_reward_mean  | -0.0608     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.7362828  |
| test/Q_plus_P                  | -0.7362828  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00493    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.44357613 |
| stats_o/std                    | 0.02782824 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.848      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0321    |
| test/info_shaping_reward_mean  | -0.0533    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -0.5674397 |
| test/Q_plus_P                  | -0.5674397 |
| test/reward_per_eps            | -6.1       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00595   |
| train/info_shaping_reward_mean | -0.0618    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
