Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC_QFilter/q_filter_True/prm_loss_weight_0.01/seed_0
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.444791   |
| stats_o/std                    | 0.03863553 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.074     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.505     |
| test/Q                         | -1.3482759 |
| test/Q_plus_P                  | -1.3482759 |
| test/reward_per_eps            | -40        |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0775    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.44574222  |
| stats_o/std                    | 0.039859153 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0629     |
| test/info_shaping_reward_mean  | -0.122      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -1.6074332  |
| test/Q_plus_P                  | -1.6074332  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0667     |
| train/info_shaping_reward_mean | -0.161      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.44401613  |
| stats_o/std                    | 0.040223453 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.076      |
| test/info_shaping_reward_mean  | -0.133      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -1.9474499  |
| test/Q_plus_P                  | -1.9474499  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0644     |
| train/info_shaping_reward_mean | -0.156      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.44401217 |
| stats_o/std                    | 0.04017671 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.22       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00503   |
| test/info_shaping_reward_mean  | -0.0809    |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -2.0887284 |
| test/Q_plus_P                  | -2.0887284 |
| test/reward_per_eps            | -31.2      |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0175     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0562    |
| train/info_shaping_reward_mean | -0.146     |
| train/info_shaping_reward_min  | -0.29      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 4         |
| stats_o/mean                   | 0.4436226 |
| stats_o/std                    | 0.0392476 |
| test/episodes                  | 50        |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.0687   |
| test/info_shaping_reward_mean  | -0.0892   |
| test/info_shaping_reward_min   | -0.275    |
| test/Q                         | -2.628081 |
| test/Q_plus_P                  | -2.628081 |
| test/reward_per_eps            | -40       |
| test/steps                     | 2000      |
| train/episodes                 | 200       |
| train/info_is_success_max      | 0.2       |
| train/info_is_success_mean     | 0.0125    |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.0511   |
| train/info_shaping_reward_mean | -0.118    |
| train/info_shaping_reward_min  | -0.274    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -39.5     |
| train/steps                    | 8000      |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.4435197   |
| stats_o/std                    | 0.038218614 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0703     |
| test/info_shaping_reward_mean  | -0.0913     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -3.0841994  |
| test/Q_plus_P                  | -3.0841994  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0187      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0494     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.44344547  |
| stats_o/std                    | 0.037486978 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0702     |
| test/info_shaping_reward_mean  | -0.0913     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -3.4847558  |
| test/Q_plus_P                  | -3.4847558  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.035       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0369     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.6       |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.4430623   |
| stats_o/std                    | 0.036901224 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0663     |
| test/info_shaping_reward_mean  | -0.089      |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -3.89547    |
| test/Q_plus_P                  | -3.89547    |
| test/reward_per_eps            | -40         |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0587     |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.44304022  |
| stats_o/std                    | 0.036392905 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0719     |
| test/info_shaping_reward_mean  | -0.0946     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -4.326804   |
| test/Q_plus_P                  | -4.326804   |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0231      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.041      |
| train/info_shaping_reward_mean | -0.104      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.44276068  |
| stats_o/std                    | 0.035857756 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0545     |
| test/info_shaping_reward_mean  | -0.0871     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -4.7414002  |
| test/Q_plus_P                  | -4.7414002  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0287      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0489     |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.4425592   |
| stats_o/std                    | 0.035326626 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0655     |
| test/info_shaping_reward_mean  | -0.0936     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -5.118959   |
| test/Q_plus_P                  | -5.118959   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0546     |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.44247565  |
| stats_o/std                    | 0.034921344 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0717     |
| test/info_shaping_reward_mean  | -0.0951     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -5.502129   |
| test/Q_plus_P                  | -5.502129   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0025      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0541     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 12         |
| stats_o/mean                   | 0.44248056 |
| stats_o/std                    | 0.03464881 |
| test/episodes                  | 130        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0139    |
| test/info_shaping_reward_mean  | -0.0911    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -5.6529627 |
| test/Q_plus_P                  | -5.6529627 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 5200       |
| train/episodes                 | 520        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0594    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 20800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.44260952  |
| stats_o/std                    | 0.034387518 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0657     |
| test/info_shaping_reward_mean  | -0.0841     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -6.3593626  |
| test/Q_plus_P                  | -6.3593626  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0287      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0501     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.44264662 |
| stats_o/std                    | 0.03418866 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0686    |
| test/info_shaping_reward_mean  | -0.0871    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -6.6746125 |
| test/Q_plus_P                  | -6.6746125 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0394     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0454    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.4      |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.442717    |
| stats_o/std                    | 0.033968925 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0689     |
| test/info_shaping_reward_mean  | -0.0868     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -7.0624     |
| test/Q_plus_P                  | -7.0624     |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.005       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0548     |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.44272593  |
| stats_o/std                    | 0.033852432 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0673     |
| test/info_shaping_reward_mean  | -0.0902     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -7.428312   |
| test/Q_plus_P                  | -7.428312   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0582     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.44268623 |
| stats_o/std                    | 0.03375871 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0646    |
| test/info_shaping_reward_mean  | -0.0884    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -7.7908726 |
| test/Q_plus_P                  | -7.7908726 |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00375    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0538    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.44284365 |
| stats_o/std                    | 0.03363211 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.065      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00903   |
| test/info_shaping_reward_mean  | -0.084     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -7.6703997 |
| test/Q_plus_P                  | -7.6703997 |
| test/reward_per_eps            | -37.4      |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00875    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0491    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.44305468 |
| stats_o/std                    | 0.03350772 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0573    |
| test/info_shaping_reward_mean  | -0.0877    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -8.620649  |
| test/Q_plus_P                  | -8.620649  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0583    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.44317928  |
| stats_o/std                    | 0.033337917 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.237       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0141     |
| test/info_shaping_reward_mean  | -0.123      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -6.763918   |
| test/Q_plus_P                  | -6.763918   |
| test/reward_per_eps            | -30.5       |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0613     |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.4433528  |
| stats_o/std                    | 0.0333042  |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.453      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.0608    |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -5.4200463 |
| test/Q_plus_P                  | -5.4200463 |
| test/reward_per_eps            | -21.9      |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0156     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0505    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.44349286  |
| stats_o/std                    | 0.033234008 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.28        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00351    |
| test/info_shaping_reward_mean  | -0.0746     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -7.020165   |
| test/Q_plus_P                  | -7.020165   |
| test/reward_per_eps            | -28.8       |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0275      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0521     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.44367763 |
| stats_o/std                    | 0.0333104  |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.172      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00503   |
| test/info_shaping_reward_mean  | -0.0858    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -8.140388  |
| test/Q_plus_P                  | -8.140388  |
| test/reward_per_eps            | -33.1      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0187     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0535    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.44402492  |
| stats_o/std                    | 0.033262465 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.223       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000919   |
| test/info_shaping_reward_mean  | -0.0758     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -8.121338   |
| test/Q_plus_P                  | -8.121338   |
| test/reward_per_eps            | -31.1       |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00313     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0565     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.44433746 |
| stats_o/std                    | 0.03326964 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.215      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0034    |
| test/info_shaping_reward_mean  | -0.0818    |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -7.9225745 |
| test/Q_plus_P                  | -7.9225745 |
| test/reward_per_eps            | -31.4      |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00187    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0666    |
| train/info_shaping_reward_mean | -0.135     |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.9      |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.44460642  |
| stats_o/std                    | 0.033226587 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.323       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.072      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -7.140149   |
| test/Q_plus_P                  | -7.140149   |
| test/reward_per_eps            | -27.1       |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00625     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0608     |
| train/info_shaping_reward_mean | -0.127      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 27         |
| stats_o/mean                   | 0.44476524 |
| stats_o/std                    | 0.03311851 |
| test/episodes                  | 280        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.477      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00263   |
| test/info_shaping_reward_mean  | -0.0601    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -5.904314  |
| test/Q_plus_P                  | -5.904314  |
| test/reward_per_eps            | -20.9      |
| test/steps                     | 11200      |
| train/episodes                 | 1120       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0568    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 44800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.4450051   |
| stats_o/std                    | 0.033133928 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.41        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.063      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -6.4921193  |
| test/Q_plus_P                  | -6.4921193  |
| test/reward_per_eps            | -23.6       |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.025       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0493     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.282      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.44514832  |
| stats_o/std                    | 0.033142086 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -3.592351   |
| test/Q_plus_P                  | -3.592351   |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.127       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0217     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.9       |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.44520736  |
| stats_o/std                    | 0.033000454 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00235    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -3.9581473  |
| test/Q_plus_P                  | -3.9581473  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.184       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0883     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.6       |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.44521764  |
| stats_o/std                    | 0.032865737 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00274    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -4.0281453  |
| test/Q_plus_P                  | -4.0281453  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.332       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00551    |
| train/info_shaping_reward_mean | -0.0761     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.7       |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.4452844   |
| stats_o/std                    | 0.032771517 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000693   |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -3.3481092  |
| test/Q_plus_P                  | -3.3481092  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.313       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.08       |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.5       |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.445339   |
| stats_o/std                    | 0.03263098 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0271    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -2.8404455 |
| test/Q_plus_P                  | -2.8404455 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.381      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00422   |
| train/info_shaping_reward_mean | -0.0732    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.8      |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.44535443 |
| stats_o/std                    | 0.03251385 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00444   |
| test/info_shaping_reward_mean  | -0.0331    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -2.9378781 |
| test/Q_plus_P                  | -2.9378781 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.394      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00677   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.2      |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.4453344  |
| stats_o/std                    | 0.03244413 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.398      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -4.679876  |
| test/Q_plus_P                  | -4.679876  |
| test/reward_per_eps            | -24.1      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00208   |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.4453189   |
| stats_o/std                    | 0.032360535 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -2.8159902  |
| test/Q_plus_P                  | -2.8159902  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.4453113   |
| stats_o/std                    | 0.032290496 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00537    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -2.8577194  |
| test/Q_plus_P                  | -2.8577194  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.44532144  |
| stats_o/std                    | 0.032226566 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -2.6008058  |
| test/Q_plus_P                  | -2.6008058  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.473       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.44533876  |
| stats_o/std                    | 0.032156985 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00233    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -2.363178   |
| test/Q_plus_P                  | -2.363178   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 40         |
| stats_o/mean                   | 0.44533    |
| stats_o/std                    | 0.03208324 |
| test/episodes                  | 410        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00272   |
| test/info_shaping_reward_mean  | -0.0311    |
| test/info_shaping_reward_min   | -0.207     |
| test/Q                         | -2.2721922 |
| test/Q_plus_P                  | -2.2721922 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 16400      |
| train/episodes                 | 1640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.058     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 65600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.44533002  |
| stats_o/std                    | 0.032009497 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0293     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -2.149363   |
| test/Q_plus_P                  | -2.149363   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.44532645  |
| stats_o/std                    | 0.031915087 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000421   |
| test/info_shaping_reward_mean  | -0.0293     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -2.108029   |
| test/Q_plus_P                  | -2.108029   |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00206    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.4453062  |
| stats_o/std                    | 0.03183834 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.032     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -1.963808  |
| test/Q_plus_P                  | -1.963808  |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00178   |
| train/info_shaping_reward_mean | -0.0521    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.44529116  |
| stats_o/std                    | 0.031770375 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.035      |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -2.1719224  |
| test/Q_plus_P                  | -2.1719224  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.44532776  |
| stats_o/std                    | 0.031703155 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00166    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -2.393763   |
| test/Q_plus_P                  | -2.393763   |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.464       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.4       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.4453269  |
| stats_o/std                    | 0.03162987 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0363    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -2.1827636 |
| test/Q_plus_P                  | -2.1827636 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00187   |
| train/info_shaping_reward_mean | -0.061     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.44530323 |
| stats_o/std                    | 0.03156994 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000543  |
| test/info_shaping_reward_mean  | -0.0297    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -1.8313144 |
| test/Q_plus_P                  | -1.8313144 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00234   |
| train/info_shaping_reward_mean | -0.0566    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.44525316 |
| stats_o/std                    | 0.03152401 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000562  |
| test/info_shaping_reward_mean  | -0.0309    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.9159997 |
| test/Q_plus_P                  | -1.9159997 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00265   |
| train/info_shaping_reward_mean | -0.0548    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.4452276  |
| stats_o/std                    | 0.03146154 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0299    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.759274  |
| test/Q_plus_P                  | -1.759274  |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00215   |
| train/info_shaping_reward_mean | -0.0502    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.4452097   |
| stats_o/std                    | 0.031406622 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0291     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.7611922  |
| test/Q_plus_P                  | -1.7611922  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00197    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.44519877 |
| stats_o/std                    | 0.03134529 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.0329    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -1.7723144 |
| test/Q_plus_P                  | -1.7723144 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00198   |
| train/info_shaping_reward_mean | -0.0506    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.44517717  |
| stats_o/std                    | 0.031283397 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000521   |
| test/info_shaping_reward_mean  | -0.0276     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.4260986  |
| test/Q_plus_P                  | -1.4260986  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.44515696  |
| stats_o/std                    | 0.031228548 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0298     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.454767   |
| test/Q_plus_P                  | -1.454767   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00205    |
| train/info_shaping_reward_mean | -0.0497     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.4451371  |
| stats_o/std                    | 0.03119669 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.68       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000959  |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -1.9216341 |
| test/Q_plus_P                  | -1.9216341 |
| test/reward_per_eps            | -12.8      |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00211   |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.44514012 |
| stats_o/std                    | 0.03116286 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000744  |
| test/info_shaping_reward_mean  | -0.037     |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.5968281 |
| test/Q_plus_P                  | -1.5968281 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.499      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00257   |
| train/info_shaping_reward_mean | -0.0684    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.44514573  |
| stats_o/std                    | 0.031112509 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.713871   |
| test/Q_plus_P                  | -1.713871   |
| test/reward_per_eps            | -11         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.492       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.44514927  |
| stats_o/std                    | 0.031057358 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.8290532  |
| test/Q_plus_P                  | -1.8290532  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.507       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.7       |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.44515935  |
| stats_o/std                    | 0.031013722 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.8073361  |
| test/Q_plus_P                  | -1.8073361  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.531       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.4451505  |
| stats_o/std                    | 0.03097484 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.0295    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -1.4452264 |
| test/Q_plus_P                  | -1.4452264 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.533      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00274   |
| train/info_shaping_reward_mean | -0.0594    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.7      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.4451315   |
| stats_o/std                    | 0.030928977 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00432    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.5700687  |
| test/Q_plus_P                  | -1.5700687  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00195    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.44510314  |
| stats_o/std                    | 0.030898565 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0334     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.6334828  |
| test/Q_plus_P                  | -1.6334828  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00222    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.44509766  |
| stats_o/std                    | 0.030851787 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000677   |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.6246033  |
| test/Q_plus_P                  | -1.6246033  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.4450924   |
| stats_o/std                    | 0.030792817 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000761   |
| test/info_shaping_reward_mean  | -0.0331     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.596926   |
| test/Q_plus_P                  | -1.596926   |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.44506428 |
| stats_o/std                    | 0.03075478 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000465  |
| test/info_shaping_reward_mean  | -0.0293    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -1.4510411 |
| test/Q_plus_P                  | -1.4510411 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.678      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0025    |
| train/info_shaping_reward_mean | -0.0504    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.44504395  |
| stats_o/std                    | 0.030707903 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000602   |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.6731274  |
| test/Q_plus_P                  | -1.6731274  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00197    |
| train/info_shaping_reward_mean | -0.0489     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.4450178   |
| stats_o/std                    | 0.030663831 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0316     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.4038867  |
| test/Q_plus_P                  | -1.4038867  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.44500193 |
| stats_o/std                    | 0.03061019 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0291    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -1.3439965 |
| test/Q_plus_P                  | -1.3439965 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00262   |
| train/info_shaping_reward_mean | -0.0495    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.44499063  |
| stats_o/std                    | 0.030562548 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000753   |
| test/info_shaping_reward_mean  | -0.0287     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.3790257  |
| test/Q_plus_P                  | -1.3790257  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.44497025  |
| stats_o/std                    | 0.030524313 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0275     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.349039   |
| test/Q_plus_P                  | -1.349039   |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.44494835 |
| stats_o/std                    | 0.03050526 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0367    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.5979723 |
| test/Q_plus_P                  | -1.5979723 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00205   |
| train/info_shaping_reward_mean | -0.0533    |
| train/info_shaping_reward_min  | -0.277     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.44493222  |
| stats_o/std                    | 0.030456664 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00062    |
| test/info_shaping_reward_mean  | -0.0293     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.3789651  |
| test/Q_plus_P                  | -1.3789651  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0504     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.4449233   |
| stats_o/std                    | 0.030406123 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.028      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.3174576  |
| test/Q_plus_P                  | -1.3174576  |
| test/reward_per_eps            | -7          |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00213    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.444905    |
| stats_o/std                    | 0.030360214 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00074    |
| test/info_shaping_reward_mean  | -0.0288     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.3369595  |
| test/Q_plus_P                  | -1.3369595  |
| test/reward_per_eps            | -7          |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00201    |
| train/info_shaping_reward_mean | -0.0484     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.44488022 |
| stats_o/std                    | 0.03032866 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0279    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -1.2669066 |
| test/Q_plus_P                  | -1.2669066 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00208   |
| train/info_shaping_reward_mean | -0.0493    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.44487438 |
| stats_o/std                    | 0.03028418 |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0301    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -1.258556  |
| test/Q_plus_P                  | -1.258556  |
| test/reward_per_eps            | -7         |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00229   |
| train/info_shaping_reward_mean | -0.047     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.44485596  |
| stats_o/std                    | 0.030237218 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00426    |
| test/info_shaping_reward_mean  | -0.0286     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -1.2072766  |
| test/Q_plus_P                  | -1.2072766  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00185    |
| train/info_shaping_reward_mean | -0.0476     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.44483724  |
| stats_o/std                    | 0.030201355 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000431   |
| test/info_shaping_reward_mean  | -0.0314     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.3493439  |
| test/Q_plus_P                  | -1.3493439  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0488     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.44481742 |
| stats_o/std                    | 0.03015243 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00538   |
| test/info_shaping_reward_mean  | -0.0357    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -1.4111147 |
| test/Q_plus_P                  | -1.4111147 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.701      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00232   |
| train/info_shaping_reward_mean | -0.0466    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.4448237   |
| stats_o/std                    | 0.030101292 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0303     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.3620286  |
| test/Q_plus_P                  | -1.3620286  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.44481054  |
| stats_o/std                    | 0.030054389 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00202    |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.3510345  |
| test/Q_plus_P                  | -1.3510345  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00206    |
| train/info_shaping_reward_mean | -0.0479     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.44478595  |
| stats_o/std                    | 0.030020079 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000687   |
| test/info_shaping_reward_mean  | -0.0315     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.365362   |
| test/Q_plus_P                  | -1.365362   |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.44476342  |
| stats_o/std                    | 0.029988838 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0026     |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.3577428  |
| test/Q_plus_P                  | -1.3577428  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0491     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.44474712  |
| stats_o/std                    | 0.029969716 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0283     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.1469107  |
| test/Q_plus_P                  | -1.1469107  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.4447401   |
| stats_o/std                    | 0.029930664 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000616   |
| test/info_shaping_reward_mean  | -0.0278     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.1766173  |
| test/Q_plus_P                  | -1.1766173  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.44472933  |
| stats_o/std                    | 0.029889092 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0304     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.2322124  |
| test/Q_plus_P                  | -1.2322124  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00226    |
| train/info_shaping_reward_mean | -0.0497     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 86         |
| stats_o/mean                   | 0.4447159  |
| stats_o/std                    | 0.02986174 |
| test/episodes                  | 870        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0296    |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -1.1839331 |
| test/Q_plus_P                  | -1.1839331 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 34800      |
| train/episodes                 | 3480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00219   |
| train/info_shaping_reward_mean | -0.0513    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 139200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.44470724  |
| stats_o/std                    | 0.029827004 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0318     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.4383975  |
| test/Q_plus_P                  | -1.4383975  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00222    |
| train/info_shaping_reward_mean | -0.0484     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.44469     |
| stats_o/std                    | 0.029786894 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000779   |
| test/info_shaping_reward_mean  | -0.0295     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.2099353  |
| test/Q_plus_P                  | -1.2099353  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00181    |
| train/info_shaping_reward_mean | -0.0446     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.44467613 |
| stats_o/std                    | 0.02974102 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0269    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.0578561 |
| test/Q_plus_P                  | -1.0578561 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.714      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00169   |
| train/info_shaping_reward_mean | -0.0439    |
| train/info_shaping_reward_min  | -0.232     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.44466054  |
| stats_o/std                    | 0.029718801 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0283     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.1020234  |
| test/Q_plus_P                  | -1.1020234  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00172    |
| train/info_shaping_reward_mean | -0.0485     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.44464502  |
| stats_o/std                    | 0.029693238 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000842   |
| test/info_shaping_reward_mean  | -0.0282     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -1.1078193  |
| test/Q_plus_P                  | -1.1078193  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0467     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.44462836  |
| stats_o/std                    | 0.029672528 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0303     |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -1.1985706  |
| test/Q_plus_P                  | -1.1985706  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.049      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.4446198   |
| stats_o/std                    | 0.029644378 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000867   |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.319214   |
| test/Q_plus_P                  | -1.319214   |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00221    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.44460788  |
| stats_o/std                    | 0.029620789 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0302     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.1338636  |
| test/Q_plus_P                  | -1.1338636  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.44460103  |
| stats_o/std                    | 0.029589027 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00258    |
| test/info_shaping_reward_mean  | -0.0312     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.1928657  |
| test/Q_plus_P                  | -1.1928657  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.4445888   |
| stats_o/std                    | 0.029558731 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0278     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.0837268  |
| test/Q_plus_P                  | -1.0837268  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00198    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.4445878   |
| stats_o/std                    | 0.029522672 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000751   |
| test/info_shaping_reward_mean  | -0.0321     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.1842667  |
| test/Q_plus_P                  | -1.1842667  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00215    |
| train/info_shaping_reward_mean | -0.0483     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 98         |
| stats_o/mean                   | 0.44456813 |
| stats_o/std                    | 0.02950496 |
| test/episodes                  | 990        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0327    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -1.1776357 |
| test/Q_plus_P                  | -1.1776357 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 39600      |
| train/episodes                 | 3960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.721      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00287   |
| train/info_shaping_reward_mean | -0.0477    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.2      |
| train/steps                    | 158400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.4445617   |
| stats_o/std                    | 0.029479623 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00274    |
| test/info_shaping_reward_mean  | -0.0284     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.0323491  |
| test/Q_plus_P                  | -1.0323491  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0474     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.4445591   |
| stats_o/std                    | 0.029454805 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0287     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.0399966  |
| test/Q_plus_P                  | -1.0399966  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00215    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.44456577  |
| stats_o/std                    | 0.029438647 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00295    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.4275731  |
| test/Q_plus_P                  | -1.4275731  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.542       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.44457018 |
| stats_o/std                    | 0.02942524 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000894  |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -1.5889679 |
| test/Q_plus_P                  | -1.5889679 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00267   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.44456327 |
| stats_o/std                    | 0.02939099 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00103   |
| test/info_shaping_reward_mean  | -0.0315    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.2010046 |
| test/Q_plus_P                  | -1.2010046 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00207   |
| train/info_shaping_reward_mean | -0.052     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.4445528  |
| stats_o/std                    | 0.02936037 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0286    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.1055051 |
| test/Q_plus_P                  | -1.1055051 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00245   |
| train/info_shaping_reward_mean | -0.0486    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.444544   |
| stats_o/std                    | 0.02933377 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0313    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -1.1407287 |
| test/Q_plus_P                  | -1.1407287 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00287   |
| train/info_shaping_reward_mean | -0.0504    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.4445299   |
| stats_o/std                    | 0.029301748 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0282     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -1.1170834  |
| test/Q_plus_P                  | -1.1170834  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00187    |
| train/info_shaping_reward_mean | -0.0472     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.44451725  |
| stats_o/std                    | 0.029281251 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00331    |
| test/info_shaping_reward_mean  | -0.0297     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.0559715  |
| test/Q_plus_P                  | -1.0559715  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0466     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.4445063   |
| stats_o/std                    | 0.029247478 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00215    |
| test/info_shaping_reward_mean  | -0.0304     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.1359739  |
| test/Q_plus_P                  | -1.1359739  |
| test/reward_per_eps            | -7          |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0466     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.4445003   |
| stats_o/std                    | 0.029220039 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0272     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.0187383  |
| test/Q_plus_P                  | -1.0187383  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00197    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.44449535  |
| stats_o/std                    | 0.029186768 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00285    |
| test/info_shaping_reward_mean  | -0.03       |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.0716107  |
| test/Q_plus_P                  | -1.0716107  |
| test/reward_per_eps            | -7          |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.049      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.44447884  |
| stats_o/std                    | 0.029168546 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000882   |
| test/info_shaping_reward_mean  | -0.0301     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.0290303  |
| test/Q_plus_P                  | -1.0290303  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.44446447  |
| stats_o/std                    | 0.029146722 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0302     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0739671  |
| test/Q_plus_P                  | -1.0739671  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0468     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.4444498   |
| stats_o/std                    | 0.029119587 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00253    |
| test/info_shaping_reward_mean  | -0.0278     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.04414    |
| test/Q_plus_P                  | -1.04414    |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0466     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.44443557  |
| stats_o/std                    | 0.029097857 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0307     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.0711758  |
| test/Q_plus_P                  | -1.0711758  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0487     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.4444269  |
| stats_o/std                    | 0.02907802 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00198   |
| test/info_shaping_reward_mean  | -0.0301    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -1.045547  |
| test/Q_plus_P                  | -1.045547  |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.685      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00229   |
| train/info_shaping_reward_mean | -0.0495    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.44442222 |
| stats_o/std                    | 0.02905821 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0289    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.0744526 |
| test/Q_plus_P                  | -1.0744526 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00271   |
| train/info_shaping_reward_mean | -0.0544    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.44442567  |
| stats_o/std                    | 0.029033473 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.5082948  |
| test/Q_plus_P                  | -1.5082948  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.44442186  |
| stats_o/std                    | 0.029023418 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00243    |
| test/info_shaping_reward_mean  | -0.0312     |
| test/info_shaping_reward_min   | -0.218      |
| test/Q                         | -1.1403837  |
| test/Q_plus_P                  | -1.1403837  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00214    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.44441494  |
| stats_o/std                    | 0.029002061 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0343     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.2125915  |
| test/Q_plus_P                  | -1.2125915  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00226    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.44440672  |
| stats_o/std                    | 0.028982684 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0342     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.2392168  |
| test/Q_plus_P                  | -1.2392168  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00255    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.44440532  |
| stats_o/std                    | 0.028963832 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0323     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.1612716  |
| test/Q_plus_P                  | -1.1612716  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.44439712 |
| stats_o/std                    | 0.02894262 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00271   |
| test/info_shaping_reward_mean  | -0.0302    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.0784057 |
| test/Q_plus_P                  | -1.0784057 |
| test/reward_per_eps            | -7         |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0023    |
| train/info_shaping_reward_mean | -0.0515    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.4443971   |
| stats_o/std                    | 0.028912866 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0311     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.1117498  |
| test/Q_plus_P                  | -1.1117498  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0505     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.44439614  |
| stats_o/std                    | 0.028887577 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0331     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.100676   |
| test/Q_plus_P                  | -1.100676   |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.44438902  |
| stats_o/std                    | 0.028863288 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.029      |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.95294034 |
| test/Q_plus_P                  | -0.95294034 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0507     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.44438633 |
| stats_o/std                    | 0.02884235 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.0348    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.0546026 |
| test/Q_plus_P                  | -1.0546026 |
| test/reward_per_eps            | -7         |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.722      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00259   |
| train/info_shaping_reward_mean | -0.048     |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.1      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.44437742  |
| stats_o/std                    | 0.02882857  |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0255     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -0.86748064 |
| test/Q_plus_P                  | -0.86748064 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.44436613  |
| stats_o/std                    | 0.028816836 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00432    |
| test/info_shaping_reward_mean  | -0.034      |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.0956004  |
| test/Q_plus_P                  | -1.0956004  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.722       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00209    |
| train/info_shaping_reward_mean | -0.0471     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.1       |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.44436395  |
| stats_o/std                    | 0.028793247 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0036     |
| test/info_shaping_reward_mean  | -0.031      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.95256394 |
| test/Q_plus_P                  | -0.95256394 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.4443643   |
| stats_o/std                    | 0.028778404 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000763   |
| test/info_shaping_reward_mean  | -0.0322     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0600015  |
| test/Q_plus_P                  | -1.0600015  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.44436708  |
| stats_o/std                    | 0.028755955 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.9820219  |
| test/Q_plus_P                  | -0.9820219  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.444363    |
| stats_o/std                    | 0.028737927 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.0265492  |
| test/Q_plus_P                  | -1.0265492  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.4443532  |
| stats_o/std                    | 0.02872593 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000372  |
| test/info_shaping_reward_mean  | -0.0301    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -0.9391752 |
| test/Q_plus_P                  | -0.9391752 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00211   |
| train/info_shaping_reward_mean | -0.0541    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.44435608  |
| stats_o/std                    | 0.028701818 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.9055069  |
| test/Q_plus_P                  | -0.9055069  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.44435585  |
| stats_o/std                    | 0.02868867  |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00391    |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.97534037 |
| test/Q_plus_P                  | -0.97534037 |
| test/reward_per_eps            | -7          |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00215    |
| train/info_shaping_reward_mean | -0.0523     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.44435787  |
| stats_o/std                    | 0.028668992 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000812   |
| test/info_shaping_reward_mean  | -0.0334     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.022769   |
| test/Q_plus_P                  | -1.022769   |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00217    |
| train/info_shaping_reward_mean | -0.0503     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.4443574   |
| stats_o/std                    | 0.028647473 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000793   |
| test/info_shaping_reward_mean  | -0.0321     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -0.9609058  |
| test/Q_plus_P                  | -0.9609058  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.44435585  |
| stats_o/std                    | 0.028630635 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00215    |
| test/info_shaping_reward_mean  | -0.0267     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.85918653 |
| test/Q_plus_P                  | -0.85918653 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.44434997  |
| stats_o/std                    | 0.028620074 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00361    |
| test/info_shaping_reward_mean  | -0.0331     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.99840224 |
| test/Q_plus_P                  | -0.99840224 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.44435015 |
| stats_o/std                    | 0.02860018 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0347    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.0121247 |
| test/Q_plus_P                  | -1.0121247 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00213   |
| train/info_shaping_reward_mean | -0.0541    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.4443545   |
| stats_o/std                    | 0.028575407 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000676   |
| test/info_shaping_reward_mean  | -0.029      |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.92919195 |
| test/Q_plus_P                  | -0.92919195 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0487     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.4443443   |
| stats_o/std                    | 0.028555615 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000828   |
| test/info_shaping_reward_mean  | -0.0306     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.0091628  |
| test/Q_plus_P                  | -1.0091628  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.721       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.049      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.4443392   |
| stats_o/std                    | 0.028535487 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.9689145  |
| test/Q_plus_P                  | -0.9689145  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.44433594  |
| stats_o/std                    | 0.02851744  |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.031      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.90873307 |
| test/Q_plus_P                  | -0.90873307 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.44433013  |
| stats_o/std                    | 0.028498841 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0220337  |
| test/Q_plus_P                  | -1.0220337  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.44432855  |
| stats_o/std                    | 0.028477496 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.8768593  |
| test/Q_plus_P                  | -0.8768593  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00222    |
| train/info_shaping_reward_mean | -0.0498     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.44432223  |
| stats_o/std                    | 0.02846239  |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000677   |
| test/info_shaping_reward_mean  | -0.0281     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.88617724 |
| test/Q_plus_P                  | -0.88617724 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.44431135  |
| stats_o/std                    | 0.028458765 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0328     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.0155647  |
| test/Q_plus_P                  | -1.0155647  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0498     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.44430235  |
| stats_o/std                    | 0.028443074 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00154    |
| test/info_shaping_reward_mean  | -0.0309     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.87344205 |
| test/Q_plus_P                  | -0.87344205 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0479     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.44429937  |
| stats_o/std                    | 0.0284212   |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0288     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.87961245 |
| test/Q_plus_P                  | -0.87961245 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0484     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 151        |
| stats_o/mean                   | 0.44429266 |
| stats_o/std                    | 0.02840437 |
| test/episodes                  | 1520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000573  |
| test/info_shaping_reward_mean  | -0.0294    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -0.7887417 |
| test/Q_plus_P                  | -0.7887417 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 60800      |
| train/episodes                 | 6080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.69       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00239   |
| train/info_shaping_reward_mean | -0.0522    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 243200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.44428527  |
| stats_o/std                    | 0.028391348 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000961   |
| test/info_shaping_reward_mean  | -0.0313     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.9566365  |
| test/Q_plus_P                  | -0.9566365  |
| test/reward_per_eps            | -7          |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0475     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.444285    |
| stats_o/std                    | 0.028379226 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000625   |
| test/info_shaping_reward_mean  | -0.0313     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.89560616 |
| test/Q_plus_P                  | -0.89560616 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.4442811  |
| stats_o/std                    | 0.02836849 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0012    |
| test/info_shaping_reward_mean  | -0.0333    |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -1.0258198 |
| test/Q_plus_P                  | -1.0258198 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00283   |
| train/info_shaping_reward_mean | -0.0538    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.4442792   |
| stats_o/std                    | 0.02836211  |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000895   |
| test/info_shaping_reward_mean  | -0.029      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.85897756 |
| test/Q_plus_P                  | -0.85897756 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.44427717  |
| stats_o/std                    | 0.028344417 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00172    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.97599024 |
| test/Q_plus_P                  | -0.97599024 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.44427446  |
| stats_o/std                    | 0.028333737 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000787   |
| test/info_shaping_reward_mean  | -0.0332     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.90116173 |
| test/Q_plus_P                  | -0.90116173 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.44427362  |
| stats_o/std                    | 0.028320504 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000582   |
| test/info_shaping_reward_mean  | -0.0299     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.854336   |
| test/Q_plus_P                  | -0.854336   |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.44427046  |
| stats_o/std                    | 0.028305039 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0342     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.9519677  |
| test/Q_plus_P                  | -0.9519677  |
| test/reward_per_eps            | -7          |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.4442717   |
| stats_o/std                    | 0.028286057 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.032      |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.88493806 |
| test/Q_plus_P                  | -0.88493806 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00218    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.44426915  |
| stats_o/std                    | 0.028268596 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.9711669  |
| test/Q_plus_P                  | -0.9711669  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.44426075  |
| stats_o/std                    | 0.0282636   |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0316     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.84802127 |
| test/Q_plus_P                  | -0.84802127 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.44425973  |
| stats_o/std                    | 0.028250337 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000933   |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.9014337  |
| test/Q_plus_P                  | -0.9014337  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.44425353  |
| stats_o/std                    | 0.028240586 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.93797517 |
| test/Q_plus_P                  | -0.93797517 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.735       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00338    |
| train/info_shaping_reward_mean | -0.0489     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.6       |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.44425622  |
| stats_o/std                    | 0.028228229 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0323     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.8375847  |
| test/Q_plus_P                  | -0.8375847  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.44425842 |
| stats_o/std                    | 0.02820689 |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.037     |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.000353  |
| test/Q_plus_P                  | -1.000353  |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.0506    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.44425258  |
| stats_o/std                    | 0.028196074 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0033     |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.87607604 |
| test/Q_plus_P                  | -0.87607604 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.444247    |
| stats_o/std                    | 0.028181784 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00166    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.86013985 |
| test/Q_plus_P                  | -0.86013985 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.44425073 |
| stats_o/std                    | 0.02817156 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0016    |
| test/info_shaping_reward_mean  | -0.0363    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.1054081 |
| test/Q_plus_P                  | -1.1054081 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00289   |
| train/info_shaping_reward_mean | -0.0542    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.44424537  |
| stats_o/std                    | 0.02816292  |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.97613806 |
| test/Q_plus_P                  | -0.97613806 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.44424006  |
| stats_o/std                    | 0.028145798 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00319    |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.195      |
| test/Q                         | -0.8075468  |
| test/Q_plus_P                  | -0.8075468  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0495     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.44423315  |
| stats_o/std                    | 0.02813694  |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.89554715 |
| test/Q_plus_P                  | -0.89554715 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.44423556  |
| stats_o/std                    | 0.028118791 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0314     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.8732972  |
| test/Q_plus_P                  | -0.8732972  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.44423488  |
| stats_o/std                    | 0.028100269 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00273    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.9486644  |
| test/Q_plus_P                  | -0.9486644  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.4442327   |
| stats_o/std                    | 0.028086314 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0337     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.9130851  |
| test/Q_plus_P                  | -0.9130851  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0501     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.44423237  |
| stats_o/std                    | 0.028072745 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0315     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.9344008  |
| test/Q_plus_P                  | -0.9344008  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0494     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.44422922  |
| stats_o/std                    | 0.028054902 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00247    |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.9324362  |
| test/Q_plus_P                  | -0.9324362  |
| test/reward_per_eps            | -7          |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0508     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.44422826  |
| stats_o/std                    | 0.028039021 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00249    |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.8353981  |
| test/Q_plus_P                  | -0.8353981  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.722       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0504     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.1       |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.44422936  |
| stats_o/std                    | 0.028019207 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0313     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.78850365 |
| test/Q_plus_P                  | -0.78850365 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0511     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.4442248   |
| stats_o/std                    | 0.02800479  |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00324    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.86492646 |
| test/Q_plus_P                  | -0.86492646 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.44422483  |
| stats_o/std                    | 0.027992332 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000359   |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.84680974 |
| test/Q_plus_P                  | -0.84680974 |
| test/reward_per_eps            | -7          |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.44422388  |
| stats_o/std                    | 0.027984725 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.0850629  |
| test/Q_plus_P                  | -1.0850629  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.44421887  |
| stats_o/std                    | 0.027974816 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000438   |
| test/info_shaping_reward_mean  | -0.0334     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.94703895 |
| test/Q_plus_P                  | -0.94703895 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.4442164   |
| stats_o/std                    | 0.027962528 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0299     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.81009704 |
| test/Q_plus_P                  | -0.81009704 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.4442161   |
| stats_o/std                    | 0.027949499 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0339     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -0.8534145  |
| test/Q_plus_P                  | -0.8534145  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.4442155   |
| stats_o/std                    | 0.027933592 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.88934654 |
| test/Q_plus_P                  | -0.88934654 |
| test/reward_per_eps            | -7          |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.4442099   |
| stats_o/std                    | 0.027924938 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0318     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.83139783 |
| test/Q_plus_P                  | -0.83139783 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0476     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.4442011   |
| stats_o/std                    | 0.027914636 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00448    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.8728926  |
| test/Q_plus_P                  | -0.8728926  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.718       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00215    |
| train/info_shaping_reward_mean | -0.05       |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.44419745  |
| stats_o/std                    | 0.027902963 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.8602597  |
| test/Q_plus_P                  | -0.8602597  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.732       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0487     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.7       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.44419682  |
| stats_o/std                    | 0.027896397 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00398    |
| test/info_shaping_reward_mean  | -0.0337     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.82653445 |
| test/Q_plus_P                  | -0.82653445 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.44419673  |
| stats_o/std                    | 0.027886359 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.032      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.78107065 |
| test/Q_plus_P                  | -0.78107065 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0503     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.44419518  |
| stats_o/std                    | 0.027877415 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000403   |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.9239086  |
| test/Q_plus_P                  | -0.9239086  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.4441915   |
| stats_o/std                    | 0.027863408 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000603   |
| test/info_shaping_reward_mean  | -0.03       |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.7289299  |
| test/Q_plus_P                  | -0.7289299  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.4441888   |
| stats_o/std                    | 0.027849667 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.0461701  |
| test/Q_plus_P                  | -1.0461701  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0492     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 195        |
| stats_o/mean                   | 0.4441862  |
| stats_o/std                    | 0.02783774 |
| test/episodes                  | 1960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.0346    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.8866918 |
| test/Q_plus_P                  | -0.8866918 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 78400      |
| train/episodes                 | 7840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00229   |
| train/info_shaping_reward_mean | -0.0531    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 313600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.44417742  |
| stats_o/std                    | 0.027832106 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00293    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.95185435 |
| test/Q_plus_P                  | -0.95185435 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.4441721   |
| stats_o/std                    | 0.02782534  |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0055     |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.76442933 |
| test/Q_plus_P                  | -0.76442933 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.4441718   |
| stats_o/std                    | 0.027818816 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.88424575 |
| test/Q_plus_P                  | -0.88424575 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.4441719  |
| stats_o/std                    | 0.02781188 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0414    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -0.815077  |
| test/Q_plus_P                  | -0.815077  |
| test/reward_per_eps            | -7         |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.676      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00243   |
| train/info_shaping_reward_mean | -0.053     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.44416714  |
| stats_o/std                    | 0.027801469 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00089    |
| test/info_shaping_reward_mean  | -0.0289     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.75598425 |
| test/Q_plus_P                  | -0.75598425 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0498     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.44416857  |
| stats_o/std                    | 0.027789362 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000689   |
| test/info_shaping_reward_mean  | -0.0279     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.79946524 |
| test/Q_plus_P                  | -0.79946524 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.44416532  |
| stats_o/std                    | 0.027782695 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0309     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.8092245  |
| test/Q_plus_P                  | -0.8092245  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.44416723  |
| stats_o/std                    | 0.027776748 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.8882068  |
| test/Q_plus_P                  | -0.8882068  |
| test/reward_per_eps            | -7          |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.44416642  |
| stats_o/std                    | 0.027763227 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0316     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.7782671  |
| test/Q_plus_P                  | -0.7782671  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.722       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0489     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.1       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.44416797  |
| stats_o/std                    | 0.027752176 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.032      |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.77593696 |
| test/Q_plus_P                  | -0.77593696 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.44416785  |
| stats_o/std                    | 0.027740339 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000361   |
| test/info_shaping_reward_mean  | -0.0316     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.78214735 |
| test/Q_plus_P                  | -0.78214735 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.4441717  |
| stats_o/std                    | 0.02772858 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0335    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -0.764906  |
| test/Q_plus_P                  | -0.764906  |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.694      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00265   |
| train/info_shaping_reward_mean | -0.0519    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.4441723  |
| stats_o/std                    | 0.02772358 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.0315    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -0.7741618 |
| test/Q_plus_P                  | -0.7741618 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.678      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0567    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.4441739   |
| stats_o/std                    | 0.027719853 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0023     |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.7863187  |
| test/Q_plus_P                  | -0.7863187  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.44417486  |
| stats_o/std                    | 0.027713977 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00306    |
| test/info_shaping_reward_mean  | -0.032      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.71551466 |
| test/Q_plus_P                  | -0.71551466 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.444175    |
| stats_o/std                    | 0.027703809 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000608   |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.80243003 |
| test/Q_plus_P                  | -0.80243003 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.44417107  |
| stats_o/std                    | 0.027701199 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00076    |
| test/info_shaping_reward_mean  | -0.0266     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.73605514 |
| test/Q_plus_P                  | -0.73605514 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00412    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 213        |
| stats_o/mean                   | 0.44417033 |
| stats_o/std                    | 0.02769655 |
| test/episodes                  | 2140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0358    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -0.8077448 |
| test/Q_plus_P                  | -0.8077448 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 85600      |
| train/episodes                 | 8560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00297   |
| train/info_shaping_reward_mean | -0.0533    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 342400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.44416872  |
| stats_o/std                    | 0.027694851 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00393    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.70504844 |
| test/Q_plus_P                  | -0.70504844 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.44417092  |
| stats_o/std                    | 0.027676873 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.7818156  |
| test/Q_plus_P                  | -0.7818156  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0503     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.44417048  |
| stats_o/std                    | 0.027663842 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00457    |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.7267408  |
| test/Q_plus_P                  | -0.7267408  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.44416976  |
| stats_o/std                    | 0.027655065 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00351    |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.78262    |
| test/Q_plus_P                  | -0.78262    |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.4441695  |
| stats_o/std                    | 0.02764296 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.848      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0317    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -0.6769908 |
| test/Q_plus_P                  | -0.6769908 |
| test/reward_per_eps            | -6.1       |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.71       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00308   |
| train/info_shaping_reward_mean | -0.0509    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.6      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.44417465  |
| stats_o/std                    | 0.02763548  |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00402    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.80858016 |
| test/Q_plus_P                  | -0.80858016 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.44417349  |
| stats_o/std                    | 0.027624773 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00081    |
| test/info_shaping_reward_mean  | -0.0328     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.8227513  |
| test/Q_plus_P                  | -0.8227513  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.44417647  |
| stats_o/std                    | 0.02761343  |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.73926383 |
| test/Q_plus_P                  | -0.73926383 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.719       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0511     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.44417712 |
| stats_o/std                    | 0.0276094  |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.036     |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.7785037 |
| test/Q_plus_P                  | -0.7785037 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.676      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00323   |
| train/info_shaping_reward_mean | -0.0567    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.44417915  |
| stats_o/std                    | 0.027600003 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00375    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7701901  |
| test/Q_plus_P                  | -0.7701901  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.44418088  |
| stats_o/std                    | 0.027589232 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00441    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.78414947 |
| test/Q_plus_P                  | -0.78414947 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.44418278  |
| stats_o/std                    | 0.027579976 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00421    |
| test/info_shaping_reward_mean  | -0.0396     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.84658617 |
| test/Q_plus_P                  | -0.84658617 |
| test/reward_per_eps            | -7          |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.721       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00499    |
| train/info_shaping_reward_mean | -0.0523     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.44418135  |
| stats_o/std                    | 0.027572513 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.80056655 |
| test/Q_plus_P                  | -0.80056655 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.44418082  |
| stats_o/std                    | 0.027567064 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00361    |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.74678475 |
| test/Q_plus_P                  | -0.74678475 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.4441841   |
| stats_o/std                    | 0.027561784 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.7452782  |
| test/Q_plus_P                  | -0.7452782  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.44418314  |
| stats_o/std                    | 0.027548617 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00752    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7200321  |
| test/Q_plus_P                  | -0.7200321  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.4441847   |
| stats_o/std                    | 0.027541881 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7696335  |
| test/Q_plus_P                  | -0.7696335  |
| test/reward_per_eps            | -7          |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.44418225  |
| stats_o/std                    | 0.027540253 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000268   |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.7404488  |
| test/Q_plus_P                  | -0.7404488  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.44418195  |
| stats_o/std                    | 0.027531654 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.75964737 |
| test/Q_plus_P                  | -0.75964737 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.44418457  |
| stats_o/std                    | 0.027529426 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0017     |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.9347659  |
| test/Q_plus_P                  | -0.9347659  |
| test/reward_per_eps            | -9          |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 234         |
| stats_o/mean                   | 0.44418344  |
| stats_o/std                    | 0.027524067 |
| test/episodes                  | 2350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000849   |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.1559067  |
| test/Q_plus_P                  | -1.1559067  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 94000       |
| train/episodes                 | 9400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 376000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.44418108  |
| stats_o/std                    | 0.027519932 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000642   |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.0590022  |
| test/Q_plus_P                  | -1.0590022  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.44417945  |
| stats_o/std                    | 0.027513215 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.8638083  |
| test/Q_plus_P                  | -0.8638083  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.44417962  |
| stats_o/std                    | 0.027501518 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0321     |
| test/info_shaping_reward_min   | -0.208      |
| test/Q                         | -0.71729684 |
| test/Q_plus_P                  | -0.71729684 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.44417855  |
| stats_o/std                    | 0.027494451 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0032     |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.793944   |
| test/Q_plus_P                  | -0.793944   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.44417763  |
| stats_o/std                    | 0.027481703 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00432    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.7605948  |
| test/Q_plus_P                  | -0.7605948  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.44417942  |
| stats_o/std                    | 0.027475126 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.74259186 |
| test/Q_plus_P                  | -0.74259186 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.44417885  |
| stats_o/std                    | 0.027465722 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.76028365 |
| test/Q_plus_P                  | -0.76028365 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.44417801 |
| stats_o/std                    | 0.02745152 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.845      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00303   |
| test/info_shaping_reward_mean  | -0.0335    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -0.66578   |
| test/Q_plus_P                  | -0.66578   |
| test/reward_per_eps            | -6.2       |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.701      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00276   |
| train/info_shaping_reward_mean | -0.0516    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.4441793   |
| stats_o/std                    | 0.027445033 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000724   |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.84649605 |
| test/Q_plus_P                  | -0.84649605 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.4441787   |
| stats_o/std                    | 0.027438434 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.6826229  |
| test/Q_plus_P                  | -0.6826229  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.44418165  |
| stats_o/std                    | 0.027424993 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00556    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.6601089  |
| test/Q_plus_P                  | -0.6601089  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00505    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.44418225  |
| stats_o/std                    | 0.027414806 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00793    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.79332614 |
| test/Q_plus_P                  | -0.79332614 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.44418505 |
| stats_o/std                    | 0.02740539 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0029    |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.7243944 |
| test/Q_plus_P                  | -0.7243944 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.695      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0556    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.44418228  |
| stats_o/std                    | 0.027396105 |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.7340325  |
| test/Q_plus_P                  | -0.7340325  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0502     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.44418037  |
| stats_o/std                    | 0.027392058 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000714   |
| test/info_shaping_reward_mean  | -0.0338     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.7776617  |
| test/Q_plus_P                  | -0.7776617  |
| test/reward_per_eps            | -7          |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.44417813  |
| stats_o/std                    | 0.027388101 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000874   |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.70464724 |
| test/Q_plus_P                  | -0.70464724 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.44418037  |
| stats_o/std                    | 0.027378513 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.68415964 |
| test/Q_plus_P                  | -0.68415964 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.44418097  |
| stats_o/std                    | 0.027371274 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.853       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00244    |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.6358286  |
| test/Q_plus_P                  | -0.6358286  |
| test/reward_per_eps            | -5.9        |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.44418272  |
| stats_o/std                    | 0.027359096 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000763   |
| test/info_shaping_reward_mean  | -0.0311     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.6944329  |
| test/Q_plus_P                  | -0.6944329  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.4441822   |
| stats_o/std                    | 0.027348852 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000689   |
| test/info_shaping_reward_mean  | -0.0307     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.7474458  |
| test/Q_plus_P                  | -0.7474458  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.44418523  |
| stats_o/std                    | 0.027342742 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00195    |
| test/info_shaping_reward_mean  | -0.0332     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7021364  |
| test/Q_plus_P                  | -0.7021364  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00394    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.4441837   |
| stats_o/std                    | 0.027335552 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00395    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.7598002  |
| test/Q_plus_P                  | -0.7598002  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.4441843   |
| stats_o/std                    | 0.027330391 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00433    |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.72984046 |
| test/Q_plus_P                  | -0.72984046 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.4441874   |
| stats_o/std                    | 0.027326861 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.63582236 |
| test/Q_plus_P                  | -0.63582236 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.4441919   |
| stats_o/std                    | 0.027318358 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.7161282  |
| test/Q_plus_P                  | -0.7161282  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00393    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.44419006  |
| stats_o/std                    | 0.027311295 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0185     |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.70276207 |
| test/Q_plus_P                  | -0.70276207 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.44418958  |
| stats_o/std                    | 0.027309315 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.75926346 |
| test/Q_plus_P                  | -0.75926346 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.44418526  |
| stats_o/std                    | 0.027303712 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.8005177  |
| test/Q_plus_P                  | -0.8005177  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.721       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.44418526  |
| stats_o/std                    | 0.027298547 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000443   |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7150114  |
| test/Q_plus_P                  | -0.7150114  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.44418475  |
| stats_o/std                    | 0.027288197 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.7347597  |
| test/Q_plus_P                  | -0.7347597  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.4441882   |
| stats_o/std                    | 0.027277894 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00446    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.72275233 |
| test/Q_plus_P                  | -0.72275233 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.44418716  |
| stats_o/std                    | 0.027269274 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.78885347 |
| test/Q_plus_P                  | -0.78885347 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.4441851   |
| stats_o/std                    | 0.027265942 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0049     |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.71374655 |
| test/Q_plus_P                  | -0.71374655 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.715       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0504     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.44418624  |
| stats_o/std                    | 0.027261822 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00756    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -0.83142245 |
| test/Q_plus_P                  | -0.83142245 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00493    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.4441923   |
| stats_o/std                    | 0.027258346 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.9705687  |
| test/Q_plus_P                  | -0.9705687  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.55        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00412    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.44419196  |
| stats_o/std                    | 0.027250255 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.7881485  |
| test/Q_plus_P                  | -0.7881485  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.4441911   |
| stats_o/std                    | 0.027245702 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00934    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.8412767  |
| test/Q_plus_P                  | -0.8412767  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.44419074  |
| stats_o/std                    | 0.027235625 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -0.7720252  |
| test/Q_plus_P                  | -0.7720252  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.4441879   |
| stats_o/std                    | 0.027231736 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.8154403  |
| test/Q_plus_P                  | -0.8154403  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.44418946  |
| stats_o/std                    | 0.027221622 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00389    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.85260063 |
| test/Q_plus_P                  | -0.85260063 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0508     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.44418648 |
| stats_o/std                    | 0.02721434 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00957   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.8023915 |
| test/Q_plus_P                  | -0.8023915 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.053     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.4441854   |
| stats_o/std                    | 0.027207272 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000888   |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.7942597  |
| test/Q_plus_P                  | -0.7942597  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00445    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.44418702  |
| stats_o/std                    | 0.027205786 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00646    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.73985934 |
| test/Q_plus_P                  | -0.73985934 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.44418505  |
| stats_o/std                    | 0.027197354 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000678   |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.6902807  |
| test/Q_plus_P                  | -0.6902807  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.4441814   |
| stats_o/std                    | 0.027194375 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.6833118  |
| test/Q_plus_P                  | -0.6833118  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.44418395  |
| stats_o/std                    | 0.027185256 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.77353644 |
| test/Q_plus_P                  | -0.77353644 |
| test/reward_per_eps            | -7          |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.44418427  |
| stats_o/std                    | 0.027177505 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00388    |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.71055526 |
| test/Q_plus_P                  | -0.71055526 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.44418526  |
| stats_o/std                    | 0.027170079 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.6869074  |
| test/Q_plus_P                  | -0.6869074  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.44418564  |
| stats_o/std                    | 0.027165024 |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00224    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7355769  |
| test/Q_plus_P                  | -0.7355769  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.44418538  |
| stats_o/std                    | 0.027156679 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00439    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6525779  |
| test/Q_plus_P                  | -0.6525779  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.44418705  |
| stats_o/std                    | 0.027150763 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.7656119  |
| test/Q_plus_P                  | -0.7656119  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.44418657  |
| stats_o/std                    | 0.027143538 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000608   |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.735065   |
| test/Q_plus_P                  | -0.735065   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.44418636 |
| stats_o/std                    | 0.02714102 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00357   |
| test/info_shaping_reward_mean  | -0.034     |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.6305933 |
| test/Q_plus_P                  | -0.6305933 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.703      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00242   |
| train/info_shaping_reward_mean | -0.0539    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.44418737  |
| stats_o/std                    | 0.027132543 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00172    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.71231276 |
| test/Q_plus_P                  | -0.71231276 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.44418326  |
| stats_o/std                    | 0.027130164 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.6949277  |
| test/Q_plus_P                  | -0.6949277  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.44418624  |
| stats_o/std                    | 0.027121149 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0334     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.73443377 |
| test/Q_plus_P                  | -0.73443377 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00359    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.4441863   |
| stats_o/std                    | 0.027117096 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00333    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.77830553 |
| test/Q_plus_P                  | -0.77830553 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.44418845 |
| stats_o/std                    | 0.02710835 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00394   |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -0.7209075 |
| test/Q_plus_P                  | -0.7209075 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.701      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0035    |
| train/info_shaping_reward_mean | -0.0533    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12        |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.44418785  |
| stats_o/std                    | 0.027100615 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000994   |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7471423  |
| test/Q_plus_P                  | -0.7471423  |
| test/reward_per_eps            | -7          |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.444186    |
| stats_o/std                    | 0.027092086 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.034      |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -0.667601   |
| test/Q_plus_P                  | -0.667601   |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.729       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.049      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.8       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.44418374  |
| stats_o/std                    | 0.027083976 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.6369621  |
| test/Q_plus_P                  | -0.6369621  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0495     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.44418368  |
| stats_o/std                    | 0.027076619 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000629   |
| test/info_shaping_reward_mean  | -0.0281     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.6644538  |
| test/Q_plus_P                  | -0.6644538  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.44418287  |
| stats_o/std                    | 0.027073292 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00363    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.6570599  |
| test/Q_plus_P                  | -0.6570599  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 298        |
| stats_o/mean                   | 0.44418213 |
| stats_o/std                    | 0.02706325 |
| test/episodes                  | 2990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0332    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -0.6790415 |
| test/Q_plus_P                  | -0.6790415 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 119600     |
| train/episodes                 | 11960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.713      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00351   |
| train/info_shaping_reward_mean | -0.0495    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.5      |
| train/steps                    | 478400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.44418344  |
| stats_o/std                    | 0.027053796 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.73098403 |
| test/Q_plus_P                  | -0.73098403 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.4441817   |
| stats_o/std                    | 0.027046131 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00483    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7023839  |
| test/Q_plus_P                  | -0.7023839  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0523     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.44418013  |
| stats_o/std                    | 0.027040392 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.6682437  |
| test/Q_plus_P                  | -0.6682437  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00488    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.44417942  |
| stats_o/std                    | 0.027032336 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00393    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.73128754 |
| test/Q_plus_P                  | -0.73128754 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.44418013 |
| stats_o/std                    | 0.02702461 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00306   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -0.7623114 |
| test/Q_plus_P                  | -0.7623114 |
| test/reward_per_eps            | -7         |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.676      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00282   |
| train/info_shaping_reward_mean | -0.0528    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.44417903  |
| stats_o/std                    | 0.02701621  |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.71338844 |
| test/Q_plus_P                  | -0.71338844 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.44417822  |
| stats_o/std                    | 0.027007086 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.68873125 |
| test/Q_plus_P                  | -0.68873125 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.4441779   |
| stats_o/std                    | 0.02699872  |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00343    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.68322015 |
| test/Q_plus_P                  | -0.68322015 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00218    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.44417706 |
| stats_o/std                    | 0.02699522 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00582   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -0.6983338 |
| test/Q_plus_P                  | -0.6983338 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0576    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.44417343  |
| stats_o/std                    | 0.026993668 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00964    |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.997527   |
| test/Q_plus_P                  | -0.997527   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.44417477 |
| stats_o/std                    | 0.02698951 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.0378    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -0.7123404 |
| test/Q_plus_P                  | -0.7123404 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.44417313  |
| stats_o/std                    | 0.026985114 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00304    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.73764664 |
| test/Q_plus_P                  | -0.73764664 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.44417152  |
| stats_o/std                    | 0.026976183 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000541   |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.71663016 |
| test/Q_plus_P                  | -0.71663016 |
| test/reward_per_eps            | -7          |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.752       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0474     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -9.9        |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.44417354  |
| stats_o/std                    | 0.026967583 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000419   |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.7348153  |
| test/Q_plus_P                  | -0.7348153  |
| test/reward_per_eps            | -7          |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00443    |
| train/info_shaping_reward_mean | -0.0523     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.44416964  |
| stats_o/std                    | 0.026963077 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.7291503  |
| test/Q_plus_P                  | -0.7291503  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.44416746 |
| stats_o/std                    | 0.02695592 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00227   |
| test/info_shaping_reward_mean  | -0.0275    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -0.6578119 |
| test/Q_plus_P                  | -0.6578119 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0517    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.44416404  |
| stats_o/std                    | 0.026950426 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000615   |
| test/info_shaping_reward_mean  | -0.0299     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.74959093 |
| test/Q_plus_P                  | -0.74959093 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0476     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.4441603   |
| stats_o/std                    | 0.026943402 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0318     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.672675   |
| test/Q_plus_P                  | -0.672675   |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.715       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00194    |
| train/info_shaping_reward_mean | -0.0464     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.44415817  |
| stats_o/std                    | 0.026935099 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000568   |
| test/info_shaping_reward_mean  | -0.0299     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.65141267 |
| test/Q_plus_P                  | -0.65141267 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0462     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.44415763  |
| stats_o/std                    | 0.026927726 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000677   |
| test/info_shaping_reward_mean  | -0.0288     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.70247245 |
| test/Q_plus_P                  | -0.70247245 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0508     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.44415733  |
| stats_o/std                    | 0.026918953 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000896   |
| test/info_shaping_reward_mean  | -0.0297     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.66531074 |
| test/Q_plus_P                  | -0.66531074 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0491     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.44415608  |
| stats_o/std                    | 0.026914328 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000945   |
| test/info_shaping_reward_mean  | -0.0285     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.66770345 |
| test/Q_plus_P                  | -0.66770345 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.44415283  |
| stats_o/std                    | 0.026911547 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.74876314 |
| test/Q_plus_P                  | -0.74876314 |
| test/reward_per_eps            | -7          |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.44415307  |
| stats_o/std                    | 0.026902286 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0322     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -0.68880033 |
| test/Q_plus_P                  | -0.68880033 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0491     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.44415173  |
| stats_o/std                    | 0.02689767  |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.77089685 |
| test/Q_plus_P                  | -0.77089685 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.4441491   |
| stats_o/std                    | 0.026898554 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.84038955 |
| test/Q_plus_P                  | -0.84038955 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.4441459   |
| stats_o/std                    | 0.026896454 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0292     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.6510483  |
| test/Q_plus_P                  | -0.6510483  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0023     |
| train/info_shaping_reward_mean | -0.0511     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.4441426   |
| stats_o/std                    | 0.026893126 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0337     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.7190675  |
| test/Q_plus_P                  | -0.7190675  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.4441419   |
| stats_o/std                    | 0.026885988 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0304     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.73217756 |
| test/Q_plus_P                  | -0.73217756 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0507     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.44413796  |
| stats_o/std                    | 0.026885169 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00213    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.7128181  |
| test/Q_plus_P                  | -0.7128181  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.44413963  |
| stats_o/std                    | 0.026878558 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.77622116 |
| test/Q_plus_P                  | -0.77622116 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.44413865  |
| stats_o/std                    | 0.026873736 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000278   |
| test/info_shaping_reward_mean  | -0.0304     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -0.6327386  |
| test/Q_plus_P                  | -0.6327386  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.44413546  |
| stats_o/std                    | 0.026870325 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00207    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.7211347  |
| test/Q_plus_P                  | -0.7211347  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.44413328  |
| stats_o/std                    | 0.026861774 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00563    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.6771372  |
| test/Q_plus_P                  | -0.6771372  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.731       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00359    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.8       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.44413593  |
| stats_o/std                    | 0.026860096 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00307    |
| test/info_shaping_reward_mean  | -0.0343     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.75216556 |
| test/Q_plus_P                  | -0.75216556 |
| test/reward_per_eps            | -7          |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00511    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.44413793 |
| stats_o/std                    | 0.02685441 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0345    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.6984518 |
| test/Q_plus_P                  | -0.6984518 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00309   |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.44413984 |
| stats_o/std                    | 0.02684679 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0319    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -0.7177908 |
| test/Q_plus_P                  | -0.7177908 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 336        |
| stats_o/mean                   | 0.44413778 |
| stats_o/std                    | 0.02684319 |
| test/episodes                  | 3370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0338    |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -0.6955086 |
| test/Q_plus_P                  | -0.6955086 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 134800     |
| train/episodes                 | 13480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00302   |
| train/info_shaping_reward_mean | -0.0523    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 539200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.4441388   |
| stats_o/std                    | 0.026838504 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.65681136 |
| test/Q_plus_P                  | -0.65681136 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.4441403  |
| stats_o/std                    | 0.0268324  |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000507  |
| test/info_shaping_reward_mean  | -0.036     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -0.7659822 |
| test/Q_plus_P                  | -0.7659822 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.742      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.0492    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -10.3      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.4441389   |
| stats_o/std                    | 0.026825318 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00257    |
| test/info_shaping_reward_mean  | -0.0334     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.6671196  |
| test/Q_plus_P                  | -0.6671196  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 340        |
| stats_o/mean                   | 0.44413865 |
| stats_o/std                    | 0.0268206  |
| test/episodes                  | 3410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00317   |
| test/info_shaping_reward_mean  | -0.0364    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -0.7357226 |
| test/Q_plus_P                  | -0.7357226 |
| test/reward_per_eps            | -7         |
| test/steps                     | 136400     |
| train/episodes                 | 13640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.704      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00333   |
| train/info_shaping_reward_mean | -0.0534    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.8      |
| train/steps                    | 545600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.4441404   |
| stats_o/std                    | 0.026813261 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000781   |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.6585265  |
| test/Q_plus_P                  | -0.6585265  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.719       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.44414094 |
| stats_o/std                    | 0.0268118  |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0381    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -0.7301128 |
| test/Q_plus_P                  | -0.7301128 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0568    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.44414315  |
| stats_o/std                    | 0.02680499  |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00206    |
| test/info_shaping_reward_mean  | -0.035      |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.66261995 |
| test/Q_plus_P                  | -0.66261995 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.44414306  |
| stats_o/std                    | 0.026802719 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.5950785  |
| test/Q_plus_P                  | -0.5950785  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.44414476  |
| stats_o/std                    | 0.026794886 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00241    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.71008646 |
| test/Q_plus_P                  | -0.71008646 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.44414338  |
| stats_o/std                    | 0.026790371 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00663    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.6426691  |
| test/Q_plus_P                  | -0.6426691  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.44414377  |
| stats_o/std                    | 0.026785525 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.718028   |
| test/Q_plus_P                  | -0.718028   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00359    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.4441428   |
| stats_o/std                    | 0.026784094 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00539    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.69340867 |
| test/Q_plus_P                  | -0.69340867 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.44414234  |
| stats_o/std                    | 0.026777672 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.6377732  |
| test/Q_plus_P                  | -0.6377732  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.44414207  |
| stats_o/std                    | 0.026771484 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000876   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.6791203  |
| test/Q_plus_P                  | -0.6791203  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.4441427   |
| stats_o/std                    | 0.026763203 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.67119277 |
| test/Q_plus_P                  | -0.67119277 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.44414318  |
| stats_o/std                    | 0.026760543 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000961   |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.68649065 |
| test/Q_plus_P                  | -0.68649065 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.44414368  |
| stats_o/std                    | 0.026752533 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00272    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.72466284 |
| test/Q_plus_P                  | -0.72466284 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.44414434 |
| stats_o/std                    | 0.02675134 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00799   |
| test/info_shaping_reward_mean  | -0.0474    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -0.7875664 |
| test/Q_plus_P                  | -0.7875664 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.719      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00387   |
| train/info_shaping_reward_mean | -0.0539    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.2      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.4441465   |
| stats_o/std                    | 0.026744904 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00267    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.6705612  |
| test/Q_plus_P                  | -0.6705612  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.44414625  |
| stats_o/std                    | 0.026740663 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.6185775  |
| test/Q_plus_P                  | -0.6185775  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.05       |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.44414362  |
| stats_o/std                    | 0.026738947 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000441   |
| test/info_shaping_reward_mean  | -0.0276     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.63058436 |
| test/Q_plus_P                  | -0.63058436 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.4441433   |
| stats_o/std                    | 0.026733348 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.63760793 |
| test/Q_plus_P                  | -0.63760793 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.44414136  |
| stats_o/std                    | 0.026727313 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.67694885 |
| test/Q_plus_P                  | -0.67694885 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.44414043  |
| stats_o/std                    | 0.026724257 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000483   |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.6927513  |
| test/Q_plus_P                  | -0.6927513  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.44414076  |
| stats_o/std                    | 0.026720298 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.6668691  |
| test/Q_plus_P                  | -0.6668691  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.44414064 |
| stats_o/std                    | 0.02671388 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0371    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -0.724074  |
| test/Q_plus_P                  | -0.724074  |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00432   |
| train/info_shaping_reward_mean | -0.0568    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.44414055  |
| stats_o/std                    | 0.026708623 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0271     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.67422724 |
| test/Q_plus_P                  | -0.67422724 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.44413862  |
| stats_o/std                    | 0.026708327 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000435   |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.6928373  |
| test/Q_plus_P                  | -0.6928373  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.44413832  |
| stats_o/std                    | 0.02670406  |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.68551326 |
| test/Q_plus_P                  | -0.68551326 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.44413888  |
| stats_o/std                    | 0.026697816 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.69276786 |
| test/Q_plus_P                  | -0.69276786 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00499    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.4441384   |
| stats_o/std                    | 0.026693655 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.66787577 |
| test/Q_plus_P                  | -0.66787577 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.4441395   |
| stats_o/std                    | 0.026691673 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00726    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.6916997  |
| test/Q_plus_P                  | -0.6916997  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.4441391   |
| stats_o/std                    | 0.026690474 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000975   |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.68522525 |
| test/Q_plus_P                  | -0.68522525 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.44413888  |
| stats_o/std                    | 0.026683874 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.702015   |
| test/Q_plus_P                  | -0.702015   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.715       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.44413853  |
| stats_o/std                    | 0.026681527 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00155    |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.69560075 |
| test/Q_plus_P                  | -0.69560075 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.44414005  |
| stats_o/std                    | 0.026676625 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.011      |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.6462068  |
| test/Q_plus_P                  | -0.6462068  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.4441407   |
| stats_o/std                    | 0.026670054 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00336    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.6939311  |
| test/Q_plus_P                  | -0.6939311  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.72        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00556    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.44414088  |
| stats_o/std                    | 0.026664706 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00647    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -0.58801186 |
| test/Q_plus_P                  | -0.58801186 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.44414183  |
| stats_o/std                    | 0.026659964 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.69259703 |
| test/Q_plus_P                  | -0.69259703 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00623    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.44414508  |
| stats_o/std                    | 0.026653992 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.730796   |
| test/Q_plus_P                  | -0.730796   |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00468    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.44414714  |
| stats_o/std                    | 0.026651302 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00596    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.68159944 |
| test/Q_plus_P                  | -0.68159944 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.44414768  |
| stats_o/std                    | 0.026645385 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00446    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.7128225  |
| test/Q_plus_P                  | -0.7128225  |
| test/reward_per_eps            | -7          |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.724       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11         |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.444147   |
| stats_o/std                    | 0.02664464 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -0.6648369 |
| test/Q_plus_P                  | -0.6648369 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.44414794  |
| stats_o/std                    | 0.026640328 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.022      |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.75601465 |
| test/Q_plus_P                  | -0.75601465 |
| test/reward_per_eps            | -7          |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.44414696  |
| stats_o/std                    | 0.026638722 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00422    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.6328931  |
| test/Q_plus_P                  | -0.6328931  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.723       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.1       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.44414762 |
| stats_o/std                    | 0.02663313 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00286   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -0.6692229 |
| test/Q_plus_P                  | -0.6692229 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.688      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0537    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.44414854  |
| stats_o/std                    | 0.026631774 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000911   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.7821678  |
| test/Q_plus_P                  | -0.7821678  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.4441479  |
| stats_o/std                    | 0.02663021 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00598   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.6832689 |
| test/Q_plus_P                  | -0.6832689 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0045    |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.44414708  |
| stats_o/std                    | 0.026628135 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0039     |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.70635504 |
| test/Q_plus_P                  | -0.70635504 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.44414875  |
| stats_o/std                    | 0.026620852 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0057     |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.73705286 |
| test/Q_plus_P                  | -0.73705286 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.44414923  |
| stats_o/std                    | 0.026617534 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7119276  |
| test/Q_plus_P                  | -0.7119276  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.44415054  |
| stats_o/std                    | 0.026612394 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0151     |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.74166405 |
| test/Q_plus_P                  | -0.74166405 |
| test/reward_per_eps            | -7          |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00497    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.44415185 |
| stats_o/std                    | 0.02660924 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00884   |
| test/info_shaping_reward_mean  | -0.0436    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -0.6804652 |
| test/Q_plus_P                  | -0.6804652 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.712      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00457   |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.5      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.44415143  |
| stats_o/std                    | 0.02660253  |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00958    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.69284594 |
| test/Q_plus_P                  | -0.69284594 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.718       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.44415167  |
| stats_o/std                    | 0.026599849 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.68619096 |
| test/Q_plus_P                  | -0.68619096 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00393    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.44414923  |
| stats_o/std                    | 0.026596924 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0102     |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.6805207  |
| test/Q_plus_P                  | -0.6805207  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.4441519  |
| stats_o/std                    | 0.0265915  |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0276    |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -0.5751735 |
| test/Q_plus_P                  | -0.5751735 |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.692      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.44415238  |
| stats_o/std                    | 0.026587658 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0193     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.7417599  |
| test/Q_plus_P                  | -0.7417599  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.4441551   |
| stats_o/std                    | 0.026582016 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00839    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.6834193  |
| test/Q_plus_P                  | -0.6834193  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 396        |
| stats_o/mean                   | 0.44415548 |
| stats_o/std                    | 0.02658228 |
| test/episodes                  | 3970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00462   |
| test/info_shaping_reward_mean  | -0.0492    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -0.6993313 |
| test/Q_plus_P                  | -0.6993313 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 158800     |
| train/episodes                 | 15880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.691      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00317   |
| train/info_shaping_reward_mean | -0.0557    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 635200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.44415578  |
| stats_o/std                    | 0.026576698 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0231     |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.8012248  |
| test/Q_plus_P                  | -0.8012248  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.4441568  |
| stats_o/std                    | 0.02657348 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.686064  |
| test/Q_plus_P                  | -0.686064  |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.705      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00333   |
| train/info_shaping_reward_mean | -0.0531    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.8      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.44415584  |
| stats_o/std                    | 0.026569074 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000612   |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.7169971  |
| test/Q_plus_P                  | -0.7169971  |
| test/reward_per_eps            | -7          |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.44415757  |
| stats_o/std                    | 0.026564263 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.61039245 |
| test/Q_plus_P                  | -0.61039245 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00502    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.4441569   |
| stats_o/std                    | 0.026563607 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00787    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.7428622  |
| test/Q_plus_P                  | -0.7428622  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.4441551   |
| stats_o/std                    | 0.026562577 |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -0.7327619  |
| test/Q_plus_P                  | -0.7327619  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.44415185  |
| stats_o/std                    | 0.026562177 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00423    |
| test/info_shaping_reward_mean  | -0.035      |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.67289513 |
| test/Q_plus_P                  | -0.67289513 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.44415095  |
| stats_o/std                    | 0.026560672 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00063    |
| test/info_shaping_reward_mean  | -0.0338     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.6655917  |
| test/Q_plus_P                  | -0.6655917  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.44415072  |
| stats_o/std                    | 0.026557019 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000681   |
| test/info_shaping_reward_mean  | -0.0307     |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -0.62377924 |
| test/Q_plus_P                  | -0.62377924 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00428    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.44414958  |
| stats_o/std                    | 0.026554316 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0322     |
| test/info_shaping_reward_min   | -0.21       |
| test/Q                         | -0.6527741  |
| test/Q_plus_P                  | -0.6527741  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.44414747  |
| stats_o/std                    | 0.026551249 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.73975396 |
| test/Q_plus_P                  | -0.73975396 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 408        |
| stats_o/mean                   | 0.4441466  |
| stats_o/std                    | 0.02654767 |
| test/episodes                  | 4090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -0.7302951 |
| test/Q_plus_P                  | -0.7302951 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 163600     |
| train/episodes                 | 16360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.705      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00268   |
| train/info_shaping_reward_mean | -0.051     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.8      |
| train/steps                    | 654400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.44414684  |
| stats_o/std                    | 0.026542602 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.71507186 |
| test/Q_plus_P                  | -0.71507186 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0495     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.4441466   |
| stats_o/std                    | 0.026538922 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000836   |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.6839993  |
| test/Q_plus_P                  | -0.6839993  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00216    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.44414893  |
| stats_o/std                    | 0.026532836 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00258    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7602985  |
| test/Q_plus_P                  | -0.7602985  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.44414756  |
| stats_o/std                    | 0.026532317 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0106     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.67273664 |
| test/Q_plus_P                  | -0.67273664 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.44414735  |
| stats_o/std                    | 0.026530394 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00431    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.6894195  |
| test/Q_plus_P                  | -0.6894195  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.44414797  |
| stats_o/std                    | 0.026526086 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000851   |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.7517915  |
| test/Q_plus_P                  | -0.7517915  |
| test/reward_per_eps            | -7          |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.4441487   |
| stats_o/std                    | 0.026523009 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0149     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.6819461  |
| test/Q_plus_P                  | -0.6819461  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.44414935  |
| stats_o/std                    | 0.026521293 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.7373661  |
| test/Q_plus_P                  | -0.7373661  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.4441506   |
| stats_o/std                    | 0.026521176 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00065    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.0236852  |
| test/Q_plus_P                  | -1.0236852  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.585       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.4441515   |
| stats_o/std                    | 0.026516488 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00269    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.8129692  |
| test/Q_plus_P                  | -0.8129692  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.44415176  |
| stats_o/std                    | 0.026511887 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.7761855  |
| test/Q_plus_P                  | -0.7761855  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.44415238 |
| stats_o/std                    | 0.02650752 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.0368    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -0.7315471 |
| test/Q_plus_P                  | -0.7315471 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.0557    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.4441521   |
| stats_o/std                    | 0.026502939 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00241    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.8733411  |
| test/Q_plus_P                  | -0.8733411  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.44415343  |
| stats_o/std                    | 0.026499147 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.6882506  |
| test/Q_plus_P                  | -0.6882506  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.4441525   |
| stats_o/std                    | 0.026497142 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.7043857  |
| test/Q_plus_P                  | -0.7043857  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.44415227  |
| stats_o/std                    | 0.026493078 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0048     |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.6607505  |
| test/Q_plus_P                  | -0.6607505  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.44415367  |
| stats_o/std                    | 0.026487665 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -0.61445326 |
| test/Q_plus_P                  | -0.61445326 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.721       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.44415474  |
| stats_o/std                    | 0.026483022 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00713    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.71606344 |
| test/Q_plus_P                  | -0.71606344 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.44415382  |
| stats_o/std                    | 0.026480338 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0149     |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.65044194 |
| test/Q_plus_P                  | -0.65044194 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.44415584  |
| stats_o/std                    | 0.026478482 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.858       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.5921148  |
| test/Q_plus_P                  | -0.5921148  |
| test/reward_per_eps            | -5.7        |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00515    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.4441557   |
| stats_o/std                    | 0.026479602 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0263     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -0.7423022  |
| test/Q_plus_P                  | -0.7423022  |
| test/reward_per_eps            | -7          |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.44415513  |
| stats_o/std                    | 0.026475586 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00376    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.6973275  |
| test/Q_plus_P                  | -0.6973275  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.44415817  |
| stats_o/std                    | 0.026472554 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00547    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.6944234  |
| test/Q_plus_P                  | -0.6944234  |
| test/reward_per_eps            | -7          |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.44415984  |
| stats_o/std                    | 0.026467117 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.77791613 |
| test/Q_plus_P                  | -0.77791613 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00479    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.44416177  |
| stats_o/std                    | 0.026467195 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00466    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.75614005 |
| test/Q_plus_P                  | -0.75614005 |
| test/reward_per_eps            | -7          |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.44416404  |
| stats_o/std                    | 0.026461326 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00627    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.69982946 |
| test/Q_plus_P                  | -0.69982946 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00475    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.44416645  |
| stats_o/std                    | 0.026457312 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00869    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.74173814 |
| test/Q_plus_P                  | -0.74173814 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.4441669  |
| stats_o/std                    | 0.02645524 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0234    |
| test/info_shaping_reward_mean  | -0.052     |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -0.6833122 |
| test/Q_plus_P                  | -0.6833122 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.65       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00495   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.44416893  |
| stats_o/std                    | 0.026453214 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00693    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.70890254 |
| test/Q_plus_P                  | -0.70890254 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.44417027  |
| stats_o/std                    | 0.026449366 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.694768   |
| test/Q_plus_P                  | -0.694768   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.4441744   |
| stats_o/std                    | 0.026446467 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0209     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.6996171  |
| test/Q_plus_P                  | -0.6996171  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00732    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.44417563  |
| stats_o/std                    | 0.026441826 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0139     |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.70840377 |
| test/Q_plus_P                  | -0.70840377 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00434    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.4441785   |
| stats_o/std                    | 0.026441025 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00946    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.6266893  |
| test/Q_plus_P                  | -0.6266893  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00636    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.44417658  |
| stats_o/std                    | 0.026442217 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0186     |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.66937125 |
| test/Q_plus_P                  | -0.66937125 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.44417775  |
| stats_o/std                    | 0.026439255 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0215     |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.66879296 |
| test/Q_plus_P                  | -0.66879296 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.44417846  |
| stats_o/std                    | 0.026438309 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0209     |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.67724144 |
| test/Q_plus_P                  | -0.67724144 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.4441787   |
| stats_o/std                    | 0.026436394 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000318   |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.72702926 |
| test/Q_plus_P                  | -0.72702926 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.4441787   |
| stats_o/std                    | 0.026432516 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.60160017 |
| test/Q_plus_P                  | -0.60160017 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00646    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.44417933  |
| stats_o/std                    | 0.026428936 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00665    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7345993  |
| test/Q_plus_P                  | -0.7345993  |
| test/reward_per_eps            | -7          |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.4441795   |
| stats_o/std                    | 0.026423827 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00914    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.66766846 |
| test/Q_plus_P                  | -0.66766846 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.44417974  |
| stats_o/std                    | 0.026420645 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0249     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7049871  |
| test/Q_plus_P                  | -0.7049871  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.44418085  |
| stats_o/std                    | 0.026416527 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00654    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.66233945 |
| test/Q_plus_P                  | -0.66233945 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.44418254  |
| stats_o/std                    | 0.026413878 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0263     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.67198837 |
| test/Q_plus_P                  | -0.67198837 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0065     |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.44418633  |
| stats_o/std                    | 0.026410328 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00535    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.77754694 |
| test/Q_plus_P                  | -0.77754694 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00643    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.4441849  |
| stats_o/std                    | 0.02640885 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00624   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -0.7762642 |
| test/Q_plus_P                  | -0.7762642 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.698      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0549    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.44418612  |
| stats_o/std                    | 0.026408173 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.7422085  |
| test/Q_plus_P                  | -0.7422085  |
| test/reward_per_eps            | -7          |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.44418898  |
| stats_o/std                    | 0.026403705 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00828    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.7426115  |
| test/Q_plus_P                  | -0.7426115  |
| test/reward_per_eps            | -7          |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00693    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.4441888   |
| stats_o/std                    | 0.02640237  |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00982    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.63007116 |
| test/Q_plus_P                  | -0.63007116 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.44418934  |
| stats_o/std                    | 0.026396768 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00915    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.73632073 |
| test/Q_plus_P                  | -0.73632073 |
| test/reward_per_eps            | -7          |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00589    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.44419158  |
| stats_o/std                    | 0.026389295 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.027      |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.8080434  |
| test/Q_plus_P                  | -0.8080434  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00492    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.4441931   |
| stats_o/std                    | 0.026384993 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00527    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.77025986 |
| test/Q_plus_P                  | -0.77025986 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.44419554  |
| stats_o/std                    | 0.026382076 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00759    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.6279004  |
| test/Q_plus_P                  | -0.6279004  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.44419488  |
| stats_o/std                    | 0.026378715 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.025      |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.6980161  |
| test/Q_plus_P                  | -0.6980161  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00589    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.44419703  |
| stats_o/std                    | 0.026376424 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0175     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.75385857 |
| test/Q_plus_P                  | -0.75385857 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00541    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.44419765  |
| stats_o/std                    | 0.026374258 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00285    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.72537184 |
| test/Q_plus_P                  | -0.72537184 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.4441987   |
| stats_o/std                    | 0.026374796 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.74148524 |
| test/Q_plus_P                  | -0.74148524 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.44420043  |
| stats_o/std                    | 0.026372515 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000889   |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.6768539  |
| test/Q_plus_P                  | -0.6768539  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.4442003   |
| stats_o/std                    | 0.026368676 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00849    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.7133445  |
| test/Q_plus_P                  | -0.7133445  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00431    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.44420215  |
| stats_o/std                    | 0.026364306 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00978    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7423169  |
| test/Q_plus_P                  | -0.7423169  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.44420326  |
| stats_o/std                    | 0.026360301 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.006      |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.6555566  |
| test/Q_plus_P                  | -0.6555566  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.44420192  |
| stats_o/std                    | 0.026357973 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00535    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.6651842  |
| test/Q_plus_P                  | -0.6651842  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.44420114  |
| stats_o/std                    | 0.026354315 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0214     |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.6868427  |
| test/Q_plus_P                  | -0.6868427  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.44420195  |
| stats_o/std                    | 0.026350342 |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00296    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.7057429  |
| test/Q_plus_P                  | -0.7057429  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.44420347  |
| stats_o/std                    | 0.026342792 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0173     |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.77040726 |
| test/Q_plus_P                  | -0.77040726 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.44420424  |
| stats_o/std                    | 0.026339145 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00209    |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.67721105 |
| test/Q_plus_P                  | -0.67721105 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00517    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.44420648  |
| stats_o/std                    | 0.026334295 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.6993228  |
| test/Q_plus_P                  | -0.6993228  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.44420788 |
| stats_o/std                    | 0.02632842 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0239    |
| test/info_shaping_reward_mean  | -0.0541    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -0.7241884 |
| test/Q_plus_P                  | -0.7241884 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.704      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.0551    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.8      |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.44420943  |
| stats_o/std                    | 0.026322817 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00861    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -0.64397466 |
| test/Q_plus_P                  | -0.64397466 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.4442093   |
| stats_o/std                    | 0.026318872 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00279    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.6517861  |
| test/Q_plus_P                  | -0.6517861  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.44420695  |
| stats_o/std                    | 0.026318155 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00958    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.69975936 |
| test/Q_plus_P                  | -0.69975936 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00472    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.4442073  |
| stats_o/std                    | 0.02631369 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00253   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -0.6454718 |
| test/Q_plus_P                  | -0.6454718 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.688      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00457   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.44420782 |
| stats_o/std                    | 0.02631078 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.845      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00853   |
| test/info_shaping_reward_mean  | -0.0406    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -0.6246653 |
| test/Q_plus_P                  | -0.6246653 |
| test/reward_per_eps            | -6.2       |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.723      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00273   |
| train/info_shaping_reward_mean | -0.0526    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.1      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.4442068  |
| stats_o/std                    | 0.02630984 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00241   |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -0.7583906 |
| test/Q_plus_P                  | -0.7583906 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.709      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00469   |
| train/info_shaping_reward_mean | -0.056     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.7      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.44420692  |
| stats_o/std                    | 0.026308602 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -0.79657906 |
| test/Q_plus_P                  | -0.79657906 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.44420388  |
| stats_o/std                    | 0.026306627 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0065     |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.7497731  |
| test/Q_plus_P                  | -0.7497731  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.4442047   |
| stats_o/std                    | 0.026303181 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000623   |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.6702344  |
| test/Q_plus_P                  | -0.6702344  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00423    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.4442052   |
| stats_o/std                    | 0.026299698 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.7645192  |
| test/Q_plus_P                  | -0.7645192  |
| test/reward_per_eps            | -7          |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.44420657  |
| stats_o/std                    | 0.026296193 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.6885145  |
| test/Q_plus_P                  | -0.6885145  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.44420603  |
| stats_o/std                    | 0.026292166 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00642    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.69721246 |
| test/Q_plus_P                  | -0.69721246 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.44420633  |
| stats_o/std                    | 0.026289375 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000912   |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.75600743 |
| test/Q_plus_P                  | -0.75600743 |
| test/reward_per_eps            | -7          |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.4442078   |
| stats_o/std                    | 0.026287625 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00857    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.7528992  |
| test/Q_plus_P                  | -0.7528992  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.4442071   |
| stats_o/std                    | 0.026286678 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00689    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.69933164 |
| test/Q_plus_P                  | -0.69933164 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.4442083   |
| stats_o/std                    | 0.026280357 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00559    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.6594707  |
| test/Q_plus_P                  | -0.6594707  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.718       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.44420826  |
| stats_o/std                    | 0.026277548 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00276    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.6722047  |
| test/Q_plus_P                  | -0.6722047  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.4442086   |
| stats_o/std                    | 0.026274994 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7296191  |
| test/Q_plus_P                  | -0.7296191  |
| test/reward_per_eps            | -7          |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.44421026  |
| stats_o/std                    | 0.026273092 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00749    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7432722  |
| test/Q_plus_P                  | -0.7432722  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0047     |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.44421148  |
| stats_o/std                    | 0.026269702 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.6714107  |
| test/Q_plus_P                  | -0.6714107  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.4442121   |
| stats_o/std                    | 0.026265832 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00547    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6976057  |
| test/Q_plus_P                  | -0.6976057  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.4442128   |
| stats_o/std                    | 0.026267825 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00571    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.6447617  |
| test/Q_plus_P                  | -0.6447617  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.44421276  |
| stats_o/std                    | 0.02626686  |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0128     |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.71336687 |
| test/Q_plus_P                  | -0.71336687 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.444212    |
| stats_o/std                    | 0.026265154 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00686    |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7834528  |
| test/Q_plus_P                  | -0.7834528  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.44421434  |
| stats_o/std                    | 0.026257642 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0213     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -0.6773876  |
| test/Q_plus_P                  | -0.6773876  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0048     |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.4442151   |
| stats_o/std                    | 0.026254443 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00741    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.6679356  |
| test/Q_plus_P                  | -0.6679356  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.44421685  |
| stats_o/std                    | 0.026252938 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.631851   |
| test/Q_plus_P                  | -0.631851   |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.4442176   |
| stats_o/std                    | 0.026251324 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0117     |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.74556684 |
| test/Q_plus_P                  | -0.74556684 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.44421902  |
| stats_o/std                    | 0.026247948 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00379    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.71032417 |
| test/Q_plus_P                  | -0.71032417 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.44421825 |
| stats_o/std                    | 0.02624765 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00683   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -0.7254441 |
| test/Q_plus_P                  | -0.7254441 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.714      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00431   |
| train/info_shaping_reward_mean | -0.0549    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.44421867  |
| stats_o/std                    | 0.026246415 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0232     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7085021  |
| test/Q_plus_P                  | -0.7085021  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.44421908  |
| stats_o/std                    | 0.026244469 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00497    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.65325326 |
| test/Q_plus_P                  | -0.65325326 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.44421816  |
| stats_o/std                    | 0.026243746 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7316147  |
| test/Q_plus_P                  | -0.7316147  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.44421816  |
| stats_o/std                    | 0.026241193 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00728    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.74471635 |
| test/Q_plus_P                  | -0.74471635 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.44421804  |
| stats_o/std                    | 0.026237702 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0205     |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.72761995 |
| test/Q_plus_P                  | -0.72761995 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00482    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.44421682  |
| stats_o/std                    | 0.026236719 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0118     |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7234162  |
| test/Q_plus_P                  | -0.7234162  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.44421637  |
| stats_o/std                    | 0.026234634 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.014      |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.70967484 |
| test/Q_plus_P                  | -0.70967484 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00583    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.4442153   |
| stats_o/std                    | 0.026231602 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.65824205 |
| test/Q_plus_P                  | -0.65824205 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.4442161  |
| stats_o/std                    | 0.02622848 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00913   |
| test/info_shaping_reward_mean  | -0.0512    |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -0.6999884 |
| test/Q_plus_P                  | -0.6999884 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.699      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0546    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.4442154  |
| stats_o/std                    | 0.02622637 |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00384   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.7279971 |
| test/Q_plus_P                  | -0.7279971 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0558    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.44421777  |
| stats_o/std                    | 0.026222765 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0108     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7361175  |
| test/Q_plus_P                  | -0.7361175  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.719       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00437    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.4442196   |
| stats_o/std                    | 0.026221765 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00928    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.7931515  |
| test/Q_plus_P                  | -0.7931515  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00547    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.44422004  |
| stats_o/std                    | 0.026217975 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0172     |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.6822589  |
| test/Q_plus_P                  | -0.6822589  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.4442204   |
| stats_o/std                    | 0.026212668 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6441245  |
| test/Q_plus_P                  | -0.6441245  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.44422016 |
| stats_o/std                    | 0.02620992 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00488   |
| test/info_shaping_reward_mean  | -0.0404    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -0.7325478 |
| test/Q_plus_P                  | -0.7325478 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00275   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.44421968  |
| stats_o/std                    | 0.026206227 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0023     |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.68387604 |
| test/Q_plus_P                  | -0.68387604 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00436    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.44422174  |
| stats_o/std                    | 0.026201235 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00401    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.67127305 |
| test/Q_plus_P                  | -0.67127305 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.44422135  |
| stats_o/std                    | 0.026201569 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00358    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.68762726 |
| test/Q_plus_P                  | -0.68762726 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.44422245 |
| stats_o/std                    | 0.02620147 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0055    |
| test/info_shaping_reward_mean  | -0.0401    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.6673931 |
| test/Q_plus_P                  | -0.6673931 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00469   |
| train/info_shaping_reward_mean | -0.0579    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.44422415  |
| stats_o/std                    | 0.026199965 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0122     |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.7772383  |
| test/Q_plus_P                  | -0.7772383  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.44422504  |
| stats_o/std                    | 0.026196584 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00606    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.6437962  |
| test/Q_plus_P                  | -0.6437962  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.44422588  |
| stats_o/std                    | 0.026192077 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0347     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.6198888  |
| test/Q_plus_P                  | -0.6198888  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.44422522  |
| stats_o/std                    | 0.026189944 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00271    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.7709588  |
| test/Q_plus_P                  | -0.7709588  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.444226    |
| stats_o/std                    | 0.026186088 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.6936041  |
| test/Q_plus_P                  | -0.6936041  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 530        |
| stats_o/mean                   | 0.4442263  |
| stats_o/std                    | 0.02618338 |
| test/episodes                  | 5310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00121   |
| test/info_shaping_reward_mean  | -0.0342    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.6548653 |
| test/Q_plus_P                  | -0.6548653 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 212400     |
| train/episodes                 | 21240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.0602    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 849600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.44422707  |
| stats_o/std                    | 0.026181726 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00323    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.66316813 |
| test/Q_plus_P                  | -0.66316813 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00523    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.44422802  |
| stats_o/std                    | 0.026180623 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00347    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.6693818  |
| test/Q_plus_P                  | -0.6693818  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00588    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.4442284   |
| stats_o/std                    | 0.026177973 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00549    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.70219046 |
| test/Q_plus_P                  | -0.70219046 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.729       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00594    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.8       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.44422886  |
| stats_o/std                    | 0.026176156 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00707    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.76159906 |
| test/Q_plus_P                  | -0.76159906 |
| test/reward_per_eps            | -7          |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00606    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.44423208  |
| stats_o/std                    | 0.026173048 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00639    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.6495502  |
| test/Q_plus_P                  | -0.6495502  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.4442328   |
| stats_o/std                    | 0.026170744 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00895    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.7291999  |
| test/Q_plus_P                  | -0.7291999  |
| test/reward_per_eps            | -7          |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00447    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.44423354  |
| stats_o/std                    | 0.026167884 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00298    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.70173544 |
| test/Q_plus_P                  | -0.70173544 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.44423398  |
| stats_o/std                    | 0.026166484 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00677    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.6903416  |
| test/Q_plus_P                  | -0.6903416  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0049     |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.4442352   |
| stats_o/std                    | 0.026165634 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00929    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.6730011  |
| test/Q_plus_P                  | -0.6730011  |
| test/reward_per_eps            | -7          |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00597    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.44423625  |
| stats_o/std                    | 0.026163267 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0303     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.712387   |
| test/Q_plus_P                  | -0.712387   |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.44423592  |
| stats_o/std                    | 0.026162142 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.593465   |
| test/Q_plus_P                  | -0.593465   |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.4442376  |
| stats_o/std                    | 0.02615909 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00462   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.6882328 |
| test/Q_plus_P                  | -0.6882328 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.691      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0046    |
| train/info_shaping_reward_mean | -0.0584    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.44423553  |
| stats_o/std                    | 0.026159791 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.664783   |
| test/Q_plus_P                  | -0.664783   |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.4442385   |
| stats_o/std                    | 0.026158765 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0102     |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.6965927  |
| test/Q_plus_P                  | -0.6965927  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00547    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.44424006  |
| stats_o/std                    | 0.026153935 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00448    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -0.6477623  |
| test/Q_plus_P                  | -0.6477623  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.729       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00425    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -10.8       |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.44424078  |
| stats_o/std                    | 0.026150389 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00705    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.62203753 |
| test/Q_plus_P                  | -0.62203753 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.444242    |
| stats_o/std                    | 0.026145326 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00605    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.70222455 |
| test/Q_plus_P                  | -0.70222455 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.4442443   |
| stats_o/std                    | 0.026142644 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0236     |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7547149  |
| test/Q_plus_P                  | -0.7547149  |
| test/reward_per_eps            | -7          |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.44424656  |
| stats_o/std                    | 0.026139049 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0128     |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.65351266 |
| test/Q_plus_P                  | -0.65351266 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00596    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.44424817  |
| stats_o/std                    | 0.02613584  |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00294    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.71356833 |
| test/Q_plus_P                  | -0.71356833 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.44424775  |
| stats_o/std                    | 0.026135439 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.64757365 |
| test/Q_plus_P                  | -0.64757365 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.44424632  |
| stats_o/std                    | 0.026133217 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00783    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.70154566 |
| test/Q_plus_P                  | -0.70154566 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.44424748  |
| stats_o/std                    | 0.026130928 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.03       |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.73483706 |
| test/Q_plus_P                  | -0.73483706 |
| test/reward_per_eps            | -7          |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00539    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.44424936  |
| stats_o/std                    | 0.026130417 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0137     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6498472  |
| test/Q_plus_P                  | -0.6498472  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00503    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.44424927  |
| stats_o/std                    | 0.026129382 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0208     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.6742368  |
| test/Q_plus_P                  | -0.6742368  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.44425073  |
| stats_o/std                    | 0.02612613  |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0249     |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.62369156 |
| test/Q_plus_P                  | -0.62369156 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00563    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.44425216  |
| stats_o/std                    | 0.026124403 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.024      |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.60029197 |
| test/Q_plus_P                  | -0.60029197 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00431    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.44425306  |
| stats_o/std                    | 0.026124932 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6924221  |
| test/Q_plus_P                  | -0.6924221  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00546    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.44425297  |
| stats_o/std                    | 0.026122747 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0262     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.6697131  |
| test/Q_plus_P                  | -0.6697131  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00585    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.4442527  |
| stats_o/std                    | 0.02612231 |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0315    |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -0.7062971 |
| test/Q_plus_P                  | -0.7062971 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00706   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.44425365  |
| stats_o/std                    | 0.026121626 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0212     |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.712676   |
| test/Q_plus_P                  | -0.712676   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00402    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 562        |
| stats_o/mean                   | 0.4442519  |
| stats_o/std                    | 0.0261216  |
| test/episodes                  | 5630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0171    |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -0.6567095 |
| test/Q_plus_P                  | -0.6567095 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 225200     |
| train/episodes                 | 22520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.694      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0553    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 900800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.44425294  |
| stats_o/std                    | 0.026117416 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0279     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.6138708  |
| test/Q_plus_P                  | -0.6138708  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00443    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.44425288  |
| stats_o/std                    | 0.026117677 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0239     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.74120826 |
| test/Q_plus_P                  | -0.74120826 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0056     |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.44425264  |
| stats_o/std                    | 0.026115403 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6598323  |
| test/Q_plus_P                  | -0.6598323  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.44425318  |
| stats_o/std                    | 0.026115103 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00923    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -0.72615224 |
| test/Q_plus_P                  | -0.72615224 |
| test/reward_per_eps            | -7          |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00579    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.44425228  |
| stats_o/std                    | 0.026113229 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0116     |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.6915412  |
| test/Q_plus_P                  | -0.6915412  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.44425222  |
| stats_o/std                    | 0.026112976 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0136     |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.63593084 |
| test/Q_plus_P                  | -0.63593084 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.4442514   |
| stats_o/std                    | 0.026116446 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00331    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.74414754 |
| test/Q_plus_P                  | -0.74414754 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.44425163  |
| stats_o/std                    | 0.026110971 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.85        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0052     |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.58486754 |
| test/Q_plus_P                  | -0.58486754 |
| test/reward_per_eps            | -6          |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.44425347  |
| stats_o/std                    | 0.026110956 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0175     |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.64779437 |
| test/Q_plus_P                  | -0.64779437 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.4442544   |
| stats_o/std                    | 0.026107354 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.70298666 |
| test/Q_plus_P                  | -0.70298666 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.4442552  |
| stats_o/std                    | 0.02610445 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0101    |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -0.7611292 |
| test/Q_plus_P                  | -0.7611292 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0564    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.44425508  |
| stats_o/std                    | 0.026105938 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00598    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.6261292  |
| test/Q_plus_P                  | -0.6261292  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.44425654  |
| stats_o/std                    | 0.026102716 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0151     |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.7703784  |
| test/Q_plus_P                  | -0.7703784  |
| test/reward_per_eps            | -7          |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.44425675  |
| stats_o/std                    | 0.026100753 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00859    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.643129   |
| test/Q_plus_P                  | -0.643129   |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.44425726  |
| stats_o/std                    | 0.026098689 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0202     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.72346115 |
| test/Q_plus_P                  | -0.72346115 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00823    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.44425645  |
| stats_o/std                    | 0.026098259 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0206     |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7221634  |
| test/Q_plus_P                  | -0.7221634  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.444258    |
| stats_o/std                    | 0.026095519 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00651    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.218      |
| test/Q                         | -0.66417855 |
| test/Q_plus_P                  | -0.66417855 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.44425902  |
| stats_o/std                    | 0.026095143 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0052     |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.6223832  |
| test/Q_plus_P                  | -0.6223832  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.44426095  |
| stats_o/std                    | 0.026093287 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00893    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.7036606  |
| test/Q_plus_P                  | -0.7036606  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00499    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.44426075  |
| stats_o/std                    | 0.026091514 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00154    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.73598844 |
| test/Q_plus_P                  | -0.73598844 |
| test/reward_per_eps            | -7          |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00521    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.44426116  |
| stats_o/std                    | 0.026091002 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00675    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.72144425 |
| test/Q_plus_P                  | -0.72144425 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.44426396  |
| stats_o/std                    | 0.026087694 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0112     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.7193619  |
| test/Q_plus_P                  | -0.7193619  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00498    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.44426373  |
| stats_o/std                    | 0.026086273 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00936    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.66939133 |
| test/Q_plus_P                  | -0.66939133 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.4442648   |
| stats_o/std                    | 0.026087321 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00813    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.72492576 |
| test/Q_plus_P                  | -0.72492576 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00454    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.4442656   |
| stats_o/std                    | 0.026088478 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00869    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.72713333 |
| test/Q_plus_P                  | -0.72713333 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00789    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.44426703  |
| stats_o/std                    | 0.026084473 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00993    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.6870777  |
| test/Q_plus_P                  | -0.6870777  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00538    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.44426736  |
| stats_o/std                    | 0.026083576 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00649    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7363208  |
| test/Q_plus_P                  | -0.7363208  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.44426942  |
| stats_o/std                    | 0.02607927  |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0107     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.76249236 |
| test/Q_plus_P                  | -0.76249236 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.44426945  |
| stats_o/std                    | 0.026076766 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0101     |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.65805435 |
| test/Q_plus_P                  | -0.65805435 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.44426847  |
| stats_o/std                    | 0.02607505  |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00987    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.67564034 |
| test/Q_plus_P                  | -0.67564034 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00606    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.44426787  |
| stats_o/std                    | 0.026076503 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0109     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.7589654  |
| test/Q_plus_P                  | -0.7589654  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00511    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.44426882  |
| stats_o/std                    | 0.026073584 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0187     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.6924254  |
| test/Q_plus_P                  | -0.6924254  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.4442694   |
| stats_o/std                    | 0.026072422 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0251     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.71666044 |
| test/Q_plus_P                  | -0.71666044 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00532    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.44426802  |
| stats_o/std                    | 0.026074164 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.76699555 |
| test/Q_plus_P                  | -0.76699555 |
| test/reward_per_eps            | -7          |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 597        |
| stats_o/mean                   | 0.44426808 |
| stats_o/std                    | 0.02607426 |
| test/episodes                  | 5980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0174    |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -0.6607009 |
| test/Q_plus_P                  | -0.6607009 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 239200     |
| train/episodes                 | 23920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00536   |
| train/info_shaping_reward_mean | -0.0606    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 956800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.4442688  |
| stats_o/std                    | 0.0260694  |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00843   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -0.7628651 |
| test/Q_plus_P                  | -0.7628651 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.694      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00365   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.44426954  |
| stats_o/std                    | 0.02606952  |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0111     |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.69069296 |
| test/Q_plus_P                  | -0.69069296 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00563    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
