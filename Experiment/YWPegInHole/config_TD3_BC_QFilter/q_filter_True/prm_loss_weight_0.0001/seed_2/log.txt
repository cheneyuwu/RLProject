Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC_QFilter/q_filter_True/prm_loss_weight_0.0001/seed_2
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.43215087 |
| stats_o/std                    | 0.05048993 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.118     |
| test/info_shaping_reward_mean  | -0.176     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -1.2962472 |
| test/Q_plus_P                  | -1.2962472 |
| test/reward_per_eps            | -40        |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.263     |
| train/info_shaping_reward_min  | -0.474     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 1          |
| stats_o/mean                   | 0.43064812 |
| stats_o/std                    | 0.05296846 |
| test/episodes                  | 20         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.6115228 |
| test/Q_plus_P                  | -1.6115228 |
| test/reward_per_eps            | -40        |
| test/steps                     | 800        |
| train/episodes                 | 80         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.434     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 3200       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.42804846  |
| stats_o/std                    | 0.051587384 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.554      |
| test/Q                         | -2.0125654  |
| test/Q_plus_P                  | -2.0125654  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.42360127  |
| stats_o/std                    | 0.051654637 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.371      |
| test/info_shaping_reward_min   | -0.6        |
| test/Q                         | -2.4511044  |
| test/Q_plus_P                  | -2.4511044  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.276      |
| train/info_shaping_reward_min  | -0.428      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.41875955  |
| stats_o/std                    | 0.052404862 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.201      |
| test/info_shaping_reward_mean  | -0.318      |
| test/info_shaping_reward_min   | -0.379      |
| test/Q                         | -2.7232897  |
| test/Q_plus_P                  | -2.7232897  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.313      |
| train/info_shaping_reward_min  | -0.46       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.41381958 |
| stats_o/std                    | 0.05737998 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.392     |
| test/Q                         | -3.1449468 |
| test/Q_plus_P                  | -3.1449468 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.339     |
| train/info_shaping_reward_min  | -0.582     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.41420707 |
| stats_o/std                    | 0.05697982 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.109     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -3.5616226 |
| test/Q_plus_P                  | -3.5616226 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0784    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.41260275 |
| stats_o/std                    | 0.05569033 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0753    |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.42      |
| test/Q                         | -3.9725702 |
| test/Q_plus_P                  | -3.9725702 |
| test/reward_per_eps            | -40        |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.082     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.41334346  |
| stats_o/std                    | 0.055304516 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.105      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.411      |
| test/Q                         | -4.4084344  |
| test/Q_plus_P                  | -4.4084344  |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0892     |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.41255865  |
| stats_o/std                    | 0.054610044 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0714     |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -4.7991486  |
| test/Q_plus_P                  | -4.7991486  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0778     |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.386      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 10         |
| stats_o/mean                   | 0.41200045 |
| stats_o/std                    | 0.05367178 |
| test/episodes                  | 110        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0587    |
| test/info_shaping_reward_mean  | -0.19      |
| test/info_shaping_reward_min   | -0.347     |
| test/Q                         | -5.242621  |
| test/Q_plus_P                  | -5.242621  |
| test/reward_per_eps            | -40        |
| test/steps                     | 4400       |
| train/episodes                 | 440        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0765    |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 17600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.41142794 |
| stats_o/std                    | 0.05294795 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0713    |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.372     |
| test/Q                         | -5.5943604 |
| test/Q_plus_P                  | -5.5943604 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0959    |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.41093302  |
| stats_o/std                    | 0.052269403 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.338      |
| test/Q                         | -5.972998   |
| test/Q_plus_P                  | -5.972998   |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0999     |
| train/info_shaping_reward_mean | -0.213      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 13         |
| stats_o/mean                   | 0.41057304 |
| stats_o/std                    | 0.0516329  |
| test/episodes                  | 140        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.062     |
| test/info_shaping_reward_mean  | -0.169     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -6.4808736 |
| test/Q_plus_P                  | -6.4808736 |
| test/reward_per_eps            | -40        |
| test/steps                     | 5600       |
| train/episodes                 | 560        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0803    |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 22400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.41032228 |
| stats_o/std                    | 0.05103677 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0668    |
| test/info_shaping_reward_mean  | -0.173     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -6.8324566 |
| test/Q_plus_P                  | -6.8324566 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0686    |
| train/info_shaping_reward_mean | -0.193     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.40968248 |
| stats_o/std                    | 0.05041513 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.101     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.326     |
| test/Q                         | -7.1775484 |
| test/Q_plus_P                  | -7.1775484 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.40926316 |
| stats_o/std                    | 0.04981007 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0865    |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.359     |
| test/Q                         | -7.58464   |
| test/Q_plus_P                  | -7.58464   |
| test/reward_per_eps            | -40        |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0835    |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 17         |
| stats_o/mean                   | 0.40889394 |
| stats_o/std                    | 0.04935667 |
| test/episodes                  | 180        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.112     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.727     |
| test/Q                         | -7.9640903 |
| test/Q_plus_P                  | -7.9640903 |
| test/reward_per_eps            | -40        |
| test/steps                     | 7200       |
| train/episodes                 | 720        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0859    |
| train/info_shaping_reward_mean | -0.205     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 28800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.4083631   |
| stats_o/std                    | 0.049085353 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.122      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.352      |
| test/Q                         | -8.436523   |
| test/Q_plus_P                  | -8.436523   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.107      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.40789977  |
| stats_o/std                    | 0.048590887 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -8.724548   |
| test/Q_plus_P                  | -8.724548   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.40662095  |
| stats_o/std                    | 0.049844634 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.315      |
| test/info_shaping_reward_min   | -0.667      |
| test/Q                         | -9.07383    |
| test/Q_plus_P                  | -9.07383    |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.304      |
| train/info_shaping_reward_min  | -0.561      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.40589988 |
| stats_o/std                    | 0.04946052 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.121     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -9.457861  |
| test/Q_plus_P                  | -9.457861  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.4052175   |
| stats_o/std                    | 0.049025763 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.126      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -9.811099   |
| test/Q_plus_P                  | -9.811099   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.4046121   |
| stats_o/std                    | 0.048713148 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.273      |
| test/info_shaping_reward_min   | -0.377      |
| test/Q                         | -10.159937  |
| test/Q_plus_P                  | -10.159937  |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.4040461  |
| stats_o/std                    | 0.04837699 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.271     |
| test/info_shaping_reward_min   | -0.355     |
| test/Q                         | -10.511205 |
| test/Q_plus_P                  | -10.511205 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.4035621   |
| stats_o/std                    | 0.047964957 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -10.868069  |
| test/Q_plus_P                  | -10.868069  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.40329337  |
| stats_o/std                    | 0.048980877 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0544     |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.366      |
| test/Q                         | -11.261443  |
| test/Q_plus_P                  | -11.261443  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0994     |
| train/info_shaping_reward_mean | -0.277      |
| train/info_shaping_reward_min  | -0.512      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.40357986  |
| stats_o/std                    | 0.049090017 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0605     |
| test/info_shaping_reward_mean  | -0.132      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -11.638392  |
| test/Q_plus_P                  | -11.638392  |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0613     |
| train/info_shaping_reward_mean | -0.187      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.40429127 |
| stats_o/std                    | 0.04916851 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0958    |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -11.982511 |
| test/Q_plus_P                  | -11.982511 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0624    |
| train/info_shaping_reward_mean | -0.136     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.40458384 |
| stats_o/std                    | 0.04906713 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0985    |
| test/info_shaping_reward_mean  | -0.151     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -12.335669 |
| test/Q_plus_P                  | -12.335669 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0772    |
| train/info_shaping_reward_mean | -0.154     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.404685   |
| stats_o/std                    | 0.04907244 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0686    |
| test/info_shaping_reward_mean  | -0.143     |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -12.650998 |
| test/Q_plus_P                  | -12.650998 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0817    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.40452635 |
| stats_o/std                    | 0.04874688 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0944    |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -12.948826 |
| test/Q_plus_P                  | -12.948826 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0986    |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.40481368  |
| stats_o/std                    | 0.048553962 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.116      |
| test/info_shaping_reward_mean  | -0.15       |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -13.300451  |
| test/Q_plus_P                  | -13.300451  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0722     |
| train/info_shaping_reward_mean | -0.147      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.4050586  |
| stats_o/std                    | 0.04822083 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0698    |
| test/info_shaping_reward_mean  | -0.121     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -13.632112 |
| test/Q_plus_P                  | -13.632112 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0807    |
| train/info_shaping_reward_mean | -0.145     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.40546775  |
| stats_o/std                    | 0.048065763 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0685     |
| test/info_shaping_reward_mean  | -0.121      |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -13.9572115 |
| test/Q_plus_P                  | -13.9572115 |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.068      |
| train/info_shaping_reward_mean | -0.133      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.40582502  |
| stats_o/std                    | 0.047803283 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0906     |
| test/info_shaping_reward_mean  | -0.134      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -14.27195   |
| test/Q_plus_P                  | -14.27195   |
| test/reward_per_eps            | -40         |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0664     |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.4060317   |
| stats_o/std                    | 0.047539454 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0604     |
| test/info_shaping_reward_mean  | -0.119      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -14.59218   |
| test/Q_plus_P                  | -14.59218   |
| test/reward_per_eps            | -40         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.066      |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.4062016  |
| stats_o/std                    | 0.04727137 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.102     |
| test/info_shaping_reward_mean  | -0.162     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -14.904114 |
| test/Q_plus_P                  | -14.904114 |
| test/reward_per_eps            | -40        |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0697    |
| train/info_shaping_reward_mean | -0.167     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.40652028  |
| stats_o/std                    | 0.047016148 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0579     |
| test/info_shaping_reward_mean  | -0.112      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -15.237593  |
| test/Q_plus_P                  | -15.237593  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0639     |
| train/info_shaping_reward_mean | -0.147      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.4071399   |
| stats_o/std                    | 0.046926145 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0714     |
| test/info_shaping_reward_mean  | -0.117      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -15.524397  |
| test/Q_plus_P                  | -15.524397  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.000625    |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0624     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.4077396   |
| stats_o/std                    | 0.046854835 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0704     |
| test/info_shaping_reward_mean  | -0.124      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -15.799812  |
| test/Q_plus_P                  | -15.799812  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0613     |
| train/info_shaping_reward_mean | -0.125      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.40814924  |
| stats_o/std                    | 0.046659317 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0563     |
| test/info_shaping_reward_mean  | -0.0941     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -16.11681   |
| test/Q_plus_P                  | -16.11681   |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0644     |
| train/info_shaping_reward_mean | -0.141      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.40862522  |
| stats_o/std                    | 0.046598013 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0578     |
| test/info_shaping_reward_mean  | -0.0894     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -16.389053  |
| test/Q_plus_P                  | -16.389053  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0749     |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.40916157  |
| stats_o/std                    | 0.046526074 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0588     |
| test/info_shaping_reward_mean  | -0.0961     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -16.667847  |
| test/Q_plus_P                  | -16.667847  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0597     |
| train/info_shaping_reward_mean | -0.132      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.40960228  |
| stats_o/std                    | 0.046372857 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0556     |
| test/info_shaping_reward_mean  | -0.0822     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -16.966412  |
| test/Q_plus_P                  | -16.966412  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0587     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.41014376  |
| stats_o/std                    | 0.046322886 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0588     |
| test/info_shaping_reward_mean  | -0.0896     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -17.247404  |
| test/Q_plus_P                  | -17.247404  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.01        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0551     |
| train/info_shaping_reward_mean | -0.134      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.41070545  |
| stats_o/std                    | 0.046317387 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0555     |
| test/info_shaping_reward_mean  | -0.0896     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -17.538542  |
| test/Q_plus_P                  | -17.538542  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0609     |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.4112546   |
| stats_o/std                    | 0.046294957 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0546     |
| test/info_shaping_reward_mean  | -0.0849     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -17.804214  |
| test/Q_plus_P                  | -17.804214  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0602     |
| train/info_shaping_reward_mean | -0.132      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.41176286 |
| stats_o/std                    | 0.04625557 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0561    |
| test/info_shaping_reward_mean  | -0.102     |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -18.045023 |
| test/Q_plus_P                  | -18.045023 |
| test/reward_per_eps            | -40        |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0555    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.41230547  |
| stats_o/std                    | 0.046276312 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0568     |
| test/info_shaping_reward_mean  | -0.0848     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -18.33525   |
| test/Q_plus_P                  | -18.33525   |
| test/reward_per_eps            | -40         |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0598     |
| train/info_shaping_reward_mean | -0.131      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.41273832  |
| stats_o/std                    | 0.046160355 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0546     |
| test/info_shaping_reward_mean  | -0.0854     |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -18.579102  |
| test/Q_plus_P                  | -18.579102  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0561     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.41320693  |
| stats_o/std                    | 0.046099406 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0566     |
| test/info_shaping_reward_mean  | -0.0831     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -18.871965  |
| test/Q_plus_P                  | -18.871965  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0597     |
| train/info_shaping_reward_mean | -0.126      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.41363874  |
| stats_o/std                    | 0.045986924 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0542     |
| test/info_shaping_reward_mean  | -0.0786     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -19.155197  |
| test/Q_plus_P                  | -19.155197  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0597     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.41399312  |
| stats_o/std                    | 0.045903478 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0544     |
| test/info_shaping_reward_mean  | -0.0774     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -19.36809   |
| test/Q_plus_P                  | -19.36809   |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0594     |
| train/info_shaping_reward_mean | -0.135      |
| train/info_shaping_reward_min  | -0.277      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.41446626  |
| stats_o/std                    | 0.045889705 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0545     |
| test/info_shaping_reward_mean  | -0.086      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -19.673609  |
| test/Q_plus_P                  | -19.673609  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0615     |
| train/info_shaping_reward_mean | -0.131      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.41492835 |
| stats_o/std                    | 0.04590765 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.117      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0839    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -19.8682   |
| test/Q_plus_P                  | -19.8682   |
| test/reward_per_eps            | -35.3      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0331     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0499    |
| train/info_shaping_reward_mean | -0.134     |
| train/info_shaping_reward_min  | -0.286     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.4154024   |
| stats_o/std                    | 0.045908112 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0564     |
| test/info_shaping_reward_mean  | -0.0883     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -20.161654  |
| test/Q_plus_P                  | -20.161654  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0636     |
| train/info_shaping_reward_mean | -0.138      |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.41582796 |
| stats_o/std                    | 0.04592645 |
| test/episodes                  | 580        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0549    |
| test/info_shaping_reward_mean  | -0.0889    |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -20.365446 |
| test/Q_plus_P                  | -20.365446 |
| test/reward_per_eps            | -40        |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0564    |
| train/info_shaping_reward_mean | -0.134     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.41623724  |
| stats_o/std                    | 0.045878027 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0558     |
| test/info_shaping_reward_mean  | -0.0832     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -20.624718  |
| test/Q_plus_P                  | -20.624718  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0583     |
| train/info_shaping_reward_mean | -0.13       |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.41665527  |
| stats_o/std                    | 0.045850236 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0151     |
| test/info_shaping_reward_mean  | -0.0828     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -20.810923  |
| test/Q_plus_P                  | -20.810923  |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0557     |
| train/info_shaping_reward_mean | -0.12       |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.41713178  |
| stats_o/std                    | 0.045867026 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0556     |
| test/info_shaping_reward_mean  | -0.0846     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -21.05458   |
| test/Q_plus_P                  | -21.05458   |
| test/reward_per_eps            | -40         |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0613     |
| train/info_shaping_reward_mean | -0.125      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.41764066 |
| stats_o/std                    | 0.04590111 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0576    |
| test/info_shaping_reward_mean  | -0.0839    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -21.32311  |
| test/Q_plus_P                  | -21.32311  |
| test/reward_per_eps            | -40        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0661    |
| train/info_shaping_reward_mean | -0.131     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.41805804 |
| stats_o/std                    | 0.04586189 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0571    |
| test/info_shaping_reward_mean  | -0.087     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -21.557432 |
| test/Q_plus_P                  | -21.557432 |
| test/reward_per_eps            | -40        |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0624    |
| train/info_shaping_reward_mean | -0.126     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.41838822  |
| stats_o/std                    | 0.045771424 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00335    |
| test/info_shaping_reward_mean  | -0.0872     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -21.706438  |
| test/Q_plus_P                  | -21.706438  |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0565     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.41875824 |
| stats_o/std                    | 0.04571755 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.315      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0702    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -21.648804 |
| test/Q_plus_P                  | -21.648804 |
| test/reward_per_eps            | -27.4      |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0611    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.4190803   |
| stats_o/std                    | 0.045642003 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.11        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0864     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -22.09159   |
| test/Q_plus_P                  | -22.09159   |
| test/reward_per_eps            | -35.6       |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0586     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 66         |
| stats_o/mean                   | 0.41943303 |
| stats_o/std                    | 0.04559845 |
| test/episodes                  | 670        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0678    |
| test/info_shaping_reward_mean  | -0.0918    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -22.375303 |
| test/Q_plus_P                  | -22.375303 |
| test/reward_per_eps            | -40        |
| test/steps                     | 26800      |
| train/episodes                 | 2680       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0206     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0493    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.2      |
| train/steps                    | 107200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.41978106 |
| stats_o/std                    | 0.04554153 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0575     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0872    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -22.417568 |
| test/Q_plus_P                  | -22.417568 |
| test/reward_per_eps            | -37.7      |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0584    |
| train/info_shaping_reward_mean | -0.124     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.42022857  |
| stats_o/std                    | 0.045797285 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0175      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0837     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -22.71915   |
| test/Q_plus_P                  | -22.71915   |
| test/reward_per_eps            | -39.3       |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0598     |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.42067742 |
| stats_o/std                    | 0.0460791  |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.045      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00482   |
| test/info_shaping_reward_mean  | -0.0853    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -22.911167 |
| test/Q_plus_P                  | -22.911167 |
| test/reward_per_eps            | -38.2      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00437    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0601    |
| train/info_shaping_reward_mean | -0.184     |
| train/info_shaping_reward_min  | -0.421     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.4210657  |
| stats_o/std                    | 0.04607832 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0675     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.0929    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -22.934351 |
| test/Q_plus_P                  | -22.934351 |
| test/reward_per_eps            | -37.3      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0587    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.42144683 |
| stats_o/std                    | 0.04605752 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.06       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0807    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -23.093765 |
| test/Q_plus_P                  | -23.093765 |
| test/reward_per_eps            | -37.6      |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0621    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.4218155   |
| stats_o/std                    | 0.046042245 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0687     |
| test/info_shaping_reward_mean  | -0.0895     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -23.192276  |
| test/Q_plus_P                  | -23.192276  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0613     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.42218438 |
| stats_o/std                    | 0.0460436  |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.05       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00526   |
| test/info_shaping_reward_mean  | -0.0897    |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -23.49546  |
| test/Q_plus_P                  | -23.49546  |
| test/reward_per_eps            | -38        |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0582    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.42255652  |
| stats_o/std                    | 0.046038043 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.115       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0815     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -23.767107  |
| test/Q_plus_P                  | -23.767107  |
| test/reward_per_eps            | -35.4       |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0075      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0568     |
| train/info_shaping_reward_mean | -0.125      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.4228995  |
| stats_o/std                    | 0.0460391  |
| test/episodes                  | 760        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00341   |
| test/info_shaping_reward_mean  | -0.0795    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -23.652178 |
| test/Q_plus_P                  | -23.652178 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00562    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0549    |
| train/info_shaping_reward_mean | -0.127     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.4232446  |
| stats_o/std                    | 0.04604347 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.117      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00331   |
| test/info_shaping_reward_mean  | -0.0842    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -23.822813 |
| test/Q_plus_P                  | -23.822813 |
| test/reward_per_eps            | -35.3      |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0594    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.42352948 |
| stats_o/std                    | 0.04600956 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.113      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0815    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -24.064451 |
| test/Q_plus_P                  | -24.064451 |
| test/reward_per_eps            | -35.5      |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0525    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.42381272  |
| stats_o/std                    | 0.045975532 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.11        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00639    |
| test/info_shaping_reward_mean  | -0.083      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -24.097095  |
| test/Q_plus_P                  | -24.097095  |
| test/reward_per_eps            | -35.6       |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0584     |
| train/info_shaping_reward_mean | -0.125      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.42412868  |
| stats_o/std                    | 0.045953903 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.095       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00556    |
| test/info_shaping_reward_mean  | -0.0808     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -24.498394  |
| test/Q_plus_P                  | -24.498394  |
| test/reward_per_eps            | -36.2       |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0119      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.055      |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.4243782   |
| stats_o/std                    | 0.045903143 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.05        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00579    |
| test/info_shaping_reward_mean  | -0.0883     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -24.749748  |
| test/Q_plus_P                  | -24.749748  |
| test/reward_per_eps            | -38         |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0558     |
| train/info_shaping_reward_mean | -0.116      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.42464375  |
| stats_o/std                    | 0.045907628 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.12        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00549    |
| test/info_shaping_reward_mean  | -0.0837     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -24.506147  |
| test/Q_plus_P                  | -24.506147  |
| test/reward_per_eps            | -35.2       |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0187      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.046      |
| train/info_shaping_reward_mean | -0.127      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.42494217  |
| stats_o/std                    | 0.045916528 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.14        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00651    |
| test/info_shaping_reward_mean  | -0.0785     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -24.60558   |
| test/Q_plus_P                  | -24.60558   |
| test/reward_per_eps            | -34.4       |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00187     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0575     |
| train/info_shaping_reward_mean | -0.13       |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.42519012  |
| stats_o/std                    | 0.045896355 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0175      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0209     |
| test/info_shaping_reward_mean  | -0.0896     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -25.098614  |
| test/Q_plus_P                  | -25.098614  |
| test/reward_per_eps            | -39.3       |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0619     |
| train/info_shaping_reward_mean | -0.133      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.42544416 |
| stats_o/std                    | 0.04588679 |
| test/episodes                  | 850        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0696    |
| test/info_shaping_reward_mean  | -0.0958    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -25.490908 |
| test/Q_plus_P                  | -25.490908 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0593    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.4256481   |
| stats_o/std                    | 0.045840215 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0684     |
| test/info_shaping_reward_mean  | -0.0971     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -25.492582  |
| test/Q_plus_P                  | -25.492582  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.049      |
| train/info_shaping_reward_mean | -0.127      |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.42587718  |
| stats_o/std                    | 0.045832798 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.07        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00405    |
| test/info_shaping_reward_mean  | -0.0877     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -25.271074  |
| test/Q_plus_P                  | -25.271074  |
| test/reward_per_eps            | -37.2       |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0578     |
| train/info_shaping_reward_mean | -0.128      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.426137   |
| stats_o/std                    | 0.04582869 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.22       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0031    |
| test/info_shaping_reward_mean  | -0.0799    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -24.588823 |
| test/Q_plus_P                  | -24.588823 |
| test/reward_per_eps            | -31.2      |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0505    |
| train/info_shaping_reward_mean | -0.129     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.42630675  |
| stats_o/std                    | 0.045777936 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0725      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00641    |
| test/info_shaping_reward_mean  | -0.0848     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -25.569458  |
| test/Q_plus_P                  | -25.569458  |
| test/reward_per_eps            | -37.1       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0156      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0523     |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.42649472 |
| stats_o/std                    | 0.0458061  |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.198      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0056    |
| test/info_shaping_reward_mean  | -0.0844    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -24.697573 |
| test/Q_plus_P                  | -24.697573 |
| test/reward_per_eps            | -32.1      |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00688    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0561    |
| train/info_shaping_reward_mean | -0.141     |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.42664883  |
| stats_o/std                    | 0.045796845 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00577    |
| test/info_shaping_reward_mean  | -0.0897     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -25.672768  |
| test/Q_plus_P                  | -25.672768  |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.059      |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.3        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.42682314  |
| stats_o/std                    | 0.045777768 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.12        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00581    |
| test/info_shaping_reward_mean  | -0.085      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -25.227903  |
| test/Q_plus_P                  | -25.227903  |
| test/reward_per_eps            | -35.2       |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0131      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0505     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.4270576   |
| stats_o/std                    | 0.045767706 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0475      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00625    |
| test/info_shaping_reward_mean  | -0.0899     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -26.030151  |
| test/Q_plus_P                  | -26.030151  |
| test/reward_per_eps            | -38.1       |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0593     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 93         |
| stats_o/mean                   | 0.42727223 |
| stats_o/std                    | 0.04575224 |
| test/episodes                  | 940        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0557    |
| test/info_shaping_reward_mean  | -0.0877    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -26.202785 |
| test/Q_plus_P                  | -26.202785 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37600      |
| train/episodes                 | 3760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0618    |
| train/info_shaping_reward_mean | -0.121     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 150400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.4274292  |
| stats_o/std                    | 0.04571016 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0697    |
| test/info_shaping_reward_mean  | -0.0987    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -26.517708 |
| test/Q_plus_P                  | -26.517708 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0564    |
| train/info_shaping_reward_mean | -0.115     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.42762086  |
| stats_o/std                    | 0.045679618 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0656     |
| test/info_shaping_reward_mean  | -0.0918     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -26.267035  |
| test/Q_plus_P                  | -26.267035  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00375     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0513     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.42780808 |
| stats_o/std                    | 0.04567519 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0685    |
| test/info_shaping_reward_mean  | -0.0944    |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -26.304058 |
| test/Q_plus_P                  | -26.304058 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0565    |
| train/info_shaping_reward_mean | -0.118     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.42795098 |
| stats_o/std                    | 0.04563452 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.122      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00576   |
| test/info_shaping_reward_mean  | -0.0806    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -25.294361 |
| test/Q_plus_P                  | -25.294361 |
| test/reward_per_eps            | -35.1      |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0106     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0454    |
| train/info_shaping_reward_mean | -0.112     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.42813373  |
| stats_o/std                    | 0.045626592 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.165       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00571    |
| test/info_shaping_reward_mean  | -0.0796     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -24.69521   |
| test/Q_plus_P                  | -24.69521   |
| test/reward_per_eps            | -33.4       |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0537     |
| train/info_shaping_reward_mean | -0.11       |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.4283024   |
| stats_o/std                    | 0.045595616 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.115       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00555    |
| test/info_shaping_reward_mean  | -0.0787     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -24.808916  |
| test/Q_plus_P                  | -24.808916  |
| test/reward_per_eps            | -35.4       |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0548     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.42848447  |
| stats_o/std                    | 0.045561716 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.13        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00583    |
| test/info_shaping_reward_mean  | -0.0798     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -25.725254  |
| test/Q_plus_P                  | -25.725254  |
| test/reward_per_eps            | -34.8       |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0545     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 101        |
| stats_o/mean                   | 0.4286388  |
| stats_o/std                    | 0.04551865 |
| test/episodes                  | 1020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0563    |
| test/info_shaping_reward_mean  | -0.0861    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -25.263971 |
| test/Q_plus_P                  | -25.263971 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40800      |
| train/episodes                 | 4080       |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0075     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0467    |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 163200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 102        |
| stats_o/mean                   | 0.42880452 |
| stats_o/std                    | 0.04549106 |
| test/episodes                  | 1030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00682   |
| test/info_shaping_reward_mean  | -0.0797    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -25.16702  |
| test/Q_plus_P                  | -25.16702  |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 41200      |
| train/episodes                 | 4120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0546    |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 164800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.42897204  |
| stats_o/std                    | 0.045472424 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00544    |
| test/info_shaping_reward_mean  | -0.0838     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -25.708223  |
| test/Q_plus_P                  | -25.708223  |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0557     |
| train/info_shaping_reward_mean | -0.108      |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.4291376   |
| stats_o/std                    | 0.045433164 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0675      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00615    |
| test/info_shaping_reward_mean  | -0.0875     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -25.873253  |
| test/Q_plus_P                  | -25.873253  |
| test/reward_per_eps            | -37.3       |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0112      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0491     |
| train/info_shaping_reward_mean | -0.094      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.42928398  |
| stats_o/std                    | 0.045383066 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00557    |
| test/info_shaping_reward_mean  | -0.0817     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -26.874783  |
| test/Q_plus_P                  | -26.874783  |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0486     |
| train/info_shaping_reward_mean | -0.0914     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.42942783  |
| stats_o/std                    | 0.045340464 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.13        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00551    |
| test/info_shaping_reward_mean  | -0.0841     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -25.294502  |
| test/Q_plus_P                  | -25.294502  |
| test/reward_per_eps            | -34.8       |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0563     |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.4295744   |
| stats_o/std                    | 0.045287725 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0664     |
| test/info_shaping_reward_mean  | -0.0892     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -27.199425  |
| test/Q_plus_P                  | -27.199425  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0106      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0506     |
| train/info_shaping_reward_mean | -0.0932     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.42972532  |
| stats_o/std                    | 0.045262735 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00559    |
| test/info_shaping_reward_mean  | -0.0932     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -26.179682  |
| test/Q_plus_P                  | -26.179682  |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0512     |
| train/info_shaping_reward_mean | -0.0947     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.42989215  |
| stats_o/std                    | 0.045261737 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0633     |
| test/info_shaping_reward_mean  | -0.0884     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -27.059877  |
| test/Q_plus_P                  | -27.059877  |
| test/reward_per_eps            | -40         |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0599     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.43005466 |
| stats_o/std                    | 0.04523393 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0642    |
| test/info_shaping_reward_mean  | -0.0921    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -26.709543 |
| test/Q_plus_P                  | -26.709543 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0496    |
| train/info_shaping_reward_mean | -0.0978    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.43020558 |
| stats_o/std                    | 0.04523912 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00571   |
| test/info_shaping_reward_mean  | -0.0772    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -25.244505 |
| test/Q_plus_P                  | -25.244505 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.054     |
| train/info_shaping_reward_mean | -0.109     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.43030894 |
| stats_o/std                    | 0.04532109 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.07       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00462   |
| test/info_shaping_reward_mean  | -0.083     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -26.21365  |
| test/Q_plus_P                  | -26.21365  |
| test/reward_per_eps            | -37.2      |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0559    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.43043545  |
| stats_o/std                    | 0.045357596 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00537    |
| test/info_shaping_reward_mean  | -0.0854     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -26.421064  |
| test/Q_plus_P                  | -26.421064  |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0538     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.43053004  |
| stats_o/std                    | 0.045390707 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0672     |
| test/info_shaping_reward_mean  | -0.0909     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -27.246113  |
| test/Q_plus_P                  | -27.246113  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0576     |
| train/info_shaping_reward_mean | -0.116      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.43061876  |
| stats_o/std                    | 0.045519125 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0516     |
| test/info_shaping_reward_mean  | -0.0814     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -27.01185   |
| test/Q_plus_P                  | -27.01185   |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0106      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0536     |
| train/info_shaping_reward_mean | -0.155      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.43072936  |
| stats_o/std                    | 0.045619685 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0539     |
| test/info_shaping_reward_mean  | -0.0969     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -27.454355  |
| test/Q_plus_P                  | -27.454355  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0492     |
| train/info_shaping_reward_mean | -0.14       |
| train/info_shaping_reward_min  | -0.379      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.43085673  |
| stats_o/std                    | 0.045633774 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00643    |
| test/info_shaping_reward_mean  | -0.0841     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -26.014288  |
| test/Q_plus_P                  | -26.014288  |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0562     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.43088126  |
| stats_o/std                    | 0.045822416 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0546     |
| test/info_shaping_reward_mean  | -0.118      |
| test/info_shaping_reward_min   | -0.422      |
| test/Q                         | -27.52149   |
| test/Q_plus_P                  | -27.52149   |
| test/reward_per_eps            | -40         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0558     |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.383      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.43099296 |
| stats_o/std                    | 0.04587773 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0529    |
| test/info_shaping_reward_mean  | -0.0958    |
| test/info_shaping_reward_min   | -0.347     |
| test/Q                         | -27.280249 |
| test/Q_plus_P                  | -27.280249 |
| test/reward_per_eps            | -40        |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0571    |
| train/info_shaping_reward_mean | -0.13      |
| train/info_shaping_reward_min  | -0.302     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.43108138  |
| stats_o/std                    | 0.045938194 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0537     |
| test/info_shaping_reward_mean  | -0.104      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -27.640825  |
| test/Q_plus_P                  | -27.640825  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.055      |
| train/info_shaping_reward_mean | -0.133      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.43112767  |
| stats_o/std                    | 0.046007354 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0544     |
| test/info_shaping_reward_mean  | -0.0945     |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -27.351622  |
| test/Q_plus_P                  | -27.351622  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0558     |
| train/info_shaping_reward_mean | -0.138      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.43121016  |
| stats_o/std                    | 0.046071902 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0544     |
| test/info_shaping_reward_mean  | -0.108      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -27.921293  |
| test/Q_plus_P                  | -27.921293  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.061      |
| train/info_shaping_reward_mean | -0.136      |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 123        |
| stats_o/mean                   | 0.43132043 |
| stats_o/std                    | 0.04611306 |
| test/episodes                  | 1240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0554    |
| test/info_shaping_reward_mean  | -0.113     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -27.782652 |
| test/Q_plus_P                  | -27.782652 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49600      |
| train/episodes                 | 4960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0619    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 198400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.43143785  |
| stats_o/std                    | 0.046134185 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0543     |
| test/info_shaping_reward_mean  | -0.0949     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -27.730122  |
| test/Q_plus_P                  | -27.730122  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0563     |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.43154776  |
| stats_o/std                    | 0.046169017 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0549     |
| test/info_shaping_reward_mean  | -0.0883     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -27.582226  |
| test/Q_plus_P                  | -27.582226  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0571     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.43165454  |
| stats_o/std                    | 0.046197254 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0547     |
| test/info_shaping_reward_mean  | -0.0934     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -27.76268   |
| test/Q_plus_P                  | -27.76268   |
| test/reward_per_eps            | -40         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0571     |
| train/info_shaping_reward_mean | -0.131      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.43177435  |
| stats_o/std                    | 0.046220716 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.056      |
| test/info_shaping_reward_mean  | -0.0994     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -28.171381  |
| test/Q_plus_P                  | -28.171381  |
| test/reward_per_eps            | -40         |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0562     |
| train/info_shaping_reward_mean | -0.128      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.43192184  |
| stats_o/std                    | 0.046316054 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0565     |
| test/info_shaping_reward_mean  | -0.275      |
| test/info_shaping_reward_min   | -0.372      |
| test/Q                         | -28.962885  |
| test/Q_plus_P                  | -28.962885  |
| test/reward_per_eps            | -40         |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0656     |
| train/info_shaping_reward_mean | -0.152      |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.4321142   |
| stats_o/std                    | 0.046484083 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0537     |
| test/info_shaping_reward_mean  | -0.0979     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -28.19961   |
| test/Q_plus_P                  | -28.19961   |
| test/reward_per_eps            | -40         |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0829     |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.43225822  |
| stats_o/std                    | 0.046499114 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0525      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00451    |
| test/info_shaping_reward_mean  | -0.0966     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -27.342989  |
| test/Q_plus_P                  | -27.342989  |
| test/reward_per_eps            | -37.9       |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0187      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0491     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.432386   |
| stats_o/std                    | 0.04651035 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0543    |
| test/info_shaping_reward_mean  | -0.0873    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -28.228569 |
| test/Q_plus_P                  | -28.228569 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0542    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.43252227  |
| stats_o/std                    | 0.046522383 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00265    |
| test/info_shaping_reward_mean  | -0.0945     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -27.28      |
| test/Q_plus_P                  | -27.28      |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0534     |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.432658    |
| stats_o/std                    | 0.046523254 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0545     |
| test/info_shaping_reward_mean  | -0.0921     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -28.283314  |
| test/Q_plus_P                  | -28.283314  |
| test/reward_per_eps            | -40         |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0547     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.43277434 |
| stats_o/std                    | 0.04652196 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0542    |
| test/info_shaping_reward_mean  | -0.0858    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -28.73991  |
| test/Q_plus_P                  | -28.73991  |
| test/reward_per_eps            | -40        |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0531    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.43288863  |
| stats_o/std                    | 0.046495948 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0544     |
| test/info_shaping_reward_mean  | -0.0895     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -28.618471  |
| test/Q_plus_P                  | -28.618471  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0125      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0483     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.43301058  |
| stats_o/std                    | 0.046471674 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.005       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0459     |
| test/info_shaping_reward_mean  | -0.0905     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -28.421846  |
| test/Q_plus_P                  | -28.421846  |
| test/reward_per_eps            | -39.8       |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0533     |
| train/info_shaping_reward_mean | -0.0976     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.433113    |
| stats_o/std                    | 0.046433598 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0546     |
| test/info_shaping_reward_mean  | -0.0927     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -28.361015  |
| test/Q_plus_P                  | -28.361015  |
| test/reward_per_eps            | -40         |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0535     |
| train/info_shaping_reward_mean | -0.1        |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.43320903 |
| stats_o/std                    | 0.0464042  |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0547    |
| test/info_shaping_reward_mean  | -0.0942    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -28.91061  |
| test/Q_plus_P                  | -28.91061  |
| test/reward_per_eps            | -40        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0538    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.43328384 |
| stats_o/std                    | 0.04637001 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0562    |
| test/info_shaping_reward_mean  | -0.0945    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -28.253008 |
| test/Q_plus_P                  | -28.253008 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0534    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.43332902 |
| stats_o/std                    | 0.04631323 |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.04       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00253   |
| test/info_shaping_reward_mean  | -0.0998    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -26.965174 |
| test/Q_plus_P                  | -26.965174 |
| test/reward_per_eps            | -38.4      |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00625    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0515    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.43338147  |
| stats_o/std                    | 0.046263028 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.055      |
| test/info_shaping_reward_mean  | -0.098      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -28.141558  |
| test/Q_plus_P                  | -28.141558  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0106      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0516     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.43349934  |
| stats_o/std                    | 0.046260074 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0541     |
| test/info_shaping_reward_mean  | -0.106      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -28.09759   |
| test/Q_plus_P                  | -28.09759   |
| test/reward_per_eps            | -40         |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.061      |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.4336685   |
| stats_o/std                    | 0.046288345 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.168       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00607    |
| test/info_shaping_reward_mean  | -0.0864     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -24.424795  |
| test/Q_plus_P                  | -24.424795  |
| test/reward_per_eps            | -33.3       |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0562     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.4338162   |
| stats_o/std                    | 0.046285678 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0529     |
| test/info_shaping_reward_mean  | -0.0943     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -28.85776   |
| test/Q_plus_P                  | -28.85776   |
| test/reward_per_eps            | -40         |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0175      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0498     |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.43393877  |
| stats_o/std                    | 0.046264384 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.063      |
| test/info_shaping_reward_mean  | -0.101      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -28.40333   |
| test/Q_plus_P                  | -28.40333   |
| test/reward_per_eps            | -40         |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.03        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0454     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.43406484 |
| stats_o/std                    | 0.04624851 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.125      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00555   |
| test/info_shaping_reward_mean  | -0.0835    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -24.858196 |
| test/Q_plus_P                  | -24.858196 |
| test/reward_per_eps            | -35        |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00937    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0547    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.4341434   |
| stats_o/std                    | 0.046199698 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00705    |
| test/info_shaping_reward_mean  | -0.0948     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -25.56644   |
| test/Q_plus_P                  | -25.56644   |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0325      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0438     |
| train/info_shaping_reward_mean | -0.0955     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.7       |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.43423524 |
| stats_o/std                    | 0.04614189 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.0934    |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -25.876474 |
| test/Q_plus_P                  | -25.876474 |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0494    |
| train/info_shaping_reward_mean | -0.0973    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.4343101   |
| stats_o/std                    | 0.046073586 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0066     |
| test/info_shaping_reward_mean  | -0.0901     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -23.992197  |
| test/Q_plus_P                  | -23.992197  |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0356      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0406     |
| train/info_shaping_reward_mean | -0.0946     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.6       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.43437245  |
| stats_o/std                    | 0.045999844 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.18        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0056     |
| test/info_shaping_reward_mean  | -0.0788     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -21.725508  |
| test/Q_plus_P                  | -21.725508  |
| test/reward_per_eps            | -32.8       |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0619      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0281     |
| train/info_shaping_reward_mean | -0.096      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.5       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.43444273  |
| stats_o/std                    | 0.045928758 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.07        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00668    |
| test/info_shaping_reward_mean  | -0.0848     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -25.36179   |
| test/Q_plus_P                  | -25.36179   |
| test/reward_per_eps            | -37.2       |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0387      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0441     |
| train/info_shaping_reward_mean | -0.092      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.5       |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.4345237  |
| stats_o/std                    | 0.04586562 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.212      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00544   |
| test/info_shaping_reward_mean  | -0.0796    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -21.976059 |
| test/Q_plus_P                  | -21.976059 |
| test/reward_per_eps            | -31.5      |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0356     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0413    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.43463162 |
| stats_o/std                    | 0.04580382 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.19       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0789    |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -23.227392 |
| test/Q_plus_P                  | -23.227392 |
| test/reward_per_eps            | -32.4      |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 0.6        |
| train/info_is_success_mean     | 0.0556     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0335    |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.8      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.43472803 |
| stats_o/std                    | 0.04574353 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0672    |
| test/info_shaping_reward_mean  | -0.0916    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -24.964663 |
| test/Q_plus_P                  | -24.964663 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0944     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0199    |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.2      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.4348309  |
| stats_o/std                    | 0.04568456 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.195      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00639   |
| test/info_shaping_reward_mean  | -0.0788    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -23.337374 |
| test/Q_plus_P                  | -23.337374 |
| test/reward_per_eps            | -32.2      |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.0781     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0218    |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.9      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.43492794 |
| stats_o/std                    | 0.04562771 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.18       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.083     |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -21.744688 |
| test/Q_plus_P                  | -21.744688 |
| test/reward_per_eps            | -32.8      |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.156      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.0999    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.8      |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.43503442  |
| stats_o/std                    | 0.045574192 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.233       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0779     |
| test/info_shaping_reward_min   | -0.21       |
| test/Q                         | -20.21556   |
| test/Q_plus_P                  | -20.21556   |
| test/reward_per_eps            | -30.7       |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0531      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0304     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.9       |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.43514422  |
| stats_o/std                    | 0.045535993 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.24        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00337    |
| test/info_shaping_reward_mean  | -0.083      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -19.78948   |
| test/Q_plus_P                  | -19.78948   |
| test/reward_per_eps            | -30.4       |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0825      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0314     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.7       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.4352347   |
| stats_o/std                    | 0.045483124 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.27        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00535    |
| test/info_shaping_reward_mean  | -0.0759     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -18.900877  |
| test/Q_plus_P                  | -18.900877  |
| test/reward_per_eps            | -29.2       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.0981      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0215     |
| train/info_shaping_reward_mean | -0.104      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.1       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.4353336   |
| stats_o/std                    | 0.045420844 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.475       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0035     |
| test/info_shaping_reward_mean  | -0.0593     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -14.9332    |
| test/Q_plus_P                  | -14.9332    |
| test/reward_per_eps            | -21         |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.0987      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0192     |
| train/info_shaping_reward_mean | -0.0957     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36         |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.4354305   |
| stats_o/std                    | 0.045362566 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0775      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00543    |
| test/info_shaping_reward_mean  | -0.089      |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -22.052086  |
| test/Q_plus_P                  | -22.052086  |
| test/reward_per_eps            | -36.9       |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.0906      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0251     |
| train/info_shaping_reward_mean | -0.0981     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.4       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.43553028 |
| stats_o/std                    | 0.04530975 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.537      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00438   |
| test/info_shaping_reward_mean  | -0.061     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -14.116733 |
| test/Q_plus_P                  | -14.116733 |
| test/reward_per_eps            | -18.5      |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0781     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0266    |
| train/info_shaping_reward_mean | -0.0978    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.9      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.43561685  |
| stats_o/std                    | 0.045252655 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.422       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0033     |
| test/info_shaping_reward_mean  | -0.0623     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -16.007763  |
| test/Q_plus_P                  | -16.007763  |
| test/reward_per_eps            | -23.1       |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.17        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.013      |
| train/info_shaping_reward_mean | -0.0907     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.2       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.4356948   |
| stats_o/std                    | 0.045187045 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.26        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00343    |
| test/info_shaping_reward_mean  | -0.0715     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -17.90712   |
| test/Q_plus_P                  | -17.90712   |
| test/reward_per_eps            | -29.6       |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.146       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0126     |
| train/info_shaping_reward_mean | -0.0907     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.2       |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.43576252  |
| stats_o/std                    | 0.045123205 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.527       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00422    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -13.71732   |
| test/Q_plus_P                  | -13.71732   |
| test/reward_per_eps            | -18.9       |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.246       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0145     |
| train/info_shaping_reward_mean | -0.085      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.1       |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.4358283   |
| stats_o/std                    | 0.045060273 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.49        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -14.831071  |
| test/Q_plus_P                  | -14.831071  |
| test/reward_per_eps            | -20.4       |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.176       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0144     |
| train/info_shaping_reward_mean | -0.0901     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33         |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.43588588 |
| stats_o/std                    | 0.04499599 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.435      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00668   |
| test/info_shaping_reward_mean  | -0.0681    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -14.033469 |
| test/Q_plus_P                  | -14.033469 |
| test/reward_per_eps            | -22.6      |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.219      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0163    |
| train/info_shaping_reward_mean | -0.0887    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.2      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.43596146  |
| stats_o/std                    | 0.044931773 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.472       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0141     |
| test/info_shaping_reward_mean  | -0.0666     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -13.877886  |
| test/Q_plus_P                  | -13.877886  |
| test/reward_per_eps            | -21.1       |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.176       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0247     |
| train/info_shaping_reward_mean | -0.0868     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33         |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.43603203  |
| stats_o/std                    | 0.044871125 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.407       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00675    |
| test/info_shaping_reward_mean  | -0.0691     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -14.072466  |
| test/Q_plus_P                  | -14.072466  |
| test/reward_per_eps            | -23.7       |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.129       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.021      |
| train/info_shaping_reward_mean | -0.0922     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.9       |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.43610153 |
| stats_o/std                    | 0.04481584 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.323      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00557   |
| test/info_shaping_reward_mean  | -0.069     |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -16.14592  |
| test/Q_plus_P                  | -16.14592  |
| test/reward_per_eps            | -27.1      |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.131      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.028     |
| train/info_shaping_reward_mean | -0.0942    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.43618414 |
| stats_o/std                    | 0.04477295 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.507      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00903   |
| test/info_shaping_reward_mean  | -0.0662    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -12.537154 |
| test/Q_plus_P                  | -12.537154 |
| test/reward_per_eps            | -19.7      |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.121      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0337    |
| train/info_shaping_reward_mean | -0.104     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.2      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.43625954  |
| stats_o/std                    | 0.044721235 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.46        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0111     |
| test/info_shaping_reward_mean  | -0.0663     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -12.908043  |
| test/Q_plus_P                  | -12.908043  |
| test/reward_per_eps            | -21.6       |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.221       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0188     |
| train/info_shaping_reward_mean | -0.0906     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.2       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.4363269   |
| stats_o/std                    | 0.044665307 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.37        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00819    |
| test/info_shaping_reward_mean  | -0.0694     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -12.963062  |
| test/Q_plus_P                  | -12.963062  |
| test/reward_per_eps            | -25.2       |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.214       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0141     |
| train/info_shaping_reward_mean | -0.0894     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.4       |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.43639004  |
| stats_o/std                    | 0.044608682 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.505       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00682    |
| test/info_shaping_reward_mean  | -0.0657     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -11.34209   |
| test/Q_plus_P                  | -11.34209   |
| test/reward_per_eps            | -19.8       |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.307       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0117     |
| train/info_shaping_reward_mean | -0.086      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.7       |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.43645287  |
| stats_o/std                    | 0.044557672 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.623       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0198     |
| test/info_shaping_reward_mean  | -0.0633     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -10.338231  |
| test/Q_plus_P                  | -10.338231  |
| test/reward_per_eps            | -15.1       |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.241       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0181     |
| train/info_shaping_reward_mean | -0.0919     |
| train/info_shaping_reward_min  | -0.278      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.4       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.43651378  |
| stats_o/std                    | 0.044498805 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.172       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00657    |
| test/info_shaping_reward_mean  | -0.0802     |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -16.136415  |
| test/Q_plus_P                  | -16.136415  |
| test/reward_per_eps            | -33.1       |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.209       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0148     |
| train/info_shaping_reward_mean | -0.0893     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.6       |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 177        |
| stats_o/mean                   | 0.4365739  |
| stats_o/std                    | 0.04444575 |
| test/episodes                  | 1780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.593      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00268   |
| test/info_shaping_reward_mean  | -0.0597    |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -9.638108  |
| test/Q_plus_P                  | -9.638108  |
| test/reward_per_eps            | -16.3      |
| test/steps                     | 71200      |
| train/episodes                 | 7120       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.289      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0142    |
| train/info_shaping_reward_mean | -0.0872    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -28.4      |
| train/steps                    | 284800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.43664733 |
| stats_o/std                    | 0.04440178 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0779    |
| test/info_shaping_reward_mean  | -0.154     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -21.167374 |
| test/Q_plus_P                  | -21.167374 |
| test/reward_per_eps            | -40        |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.235      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0151    |
| train/info_shaping_reward_mean | -0.0942    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.6      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.43675613  |
| stats_o/std                    | 0.044383075 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.223       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00499    |
| test/info_shaping_reward_mean  | -0.0758     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -15.270308  |
| test/Q_plus_P                  | -15.270308  |
| test/reward_per_eps            | -31.1       |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0456      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0539     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.43682134  |
| stats_o/std                    | 0.044338863 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.37        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.063      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -13.037417  |
| test/Q_plus_P                  | -13.037417  |
| test/reward_per_eps            | -25.2       |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0544      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0335     |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 181        |
| stats_o/mean                   | 0.4368863  |
| stats_o/std                    | 0.0442941  |
| test/episodes                  | 1820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.24       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00309   |
| test/info_shaping_reward_mean  | -0.0805    |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -15.012786 |
| test/Q_plus_P                  | -15.012786 |
| test/reward_per_eps            | -30.4      |
| test/steps                     | 72800      |
| train/episodes                 | 7280       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.145      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0233    |
| train/info_shaping_reward_mean | -0.096     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 291200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.43694428  |
| stats_o/std                    | 0.044246614 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.1         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00439    |
| test/info_shaping_reward_mean  | -0.0879     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -16.395487  |
| test/Q_plus_P                  | -16.395487  |
| test/reward_per_eps            | -36         |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.223       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00669    |
| train/info_shaping_reward_mean | -0.0889     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.1       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.4369978   |
| stats_o/std                    | 0.044195414 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.448       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0679     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -11.568188  |
| test/Q_plus_P                  | -11.568188  |
| test/reward_per_eps            | -22.1       |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.293       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00993    |
| train/info_shaping_reward_mean | -0.0854     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.3       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.4370521  |
| stats_o/std                    | 0.044144   |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.545      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00254   |
| test/info_shaping_reward_mean  | -0.0688    |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -10.575196 |
| test/Q_plus_P                  | -10.575196 |
| test/reward_per_eps            | -18.2      |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.341      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00656   |
| train/info_shaping_reward_mean | -0.0819    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 185        |
| stats_o/mean                   | 0.43710533 |
| stats_o/std                    | 0.0440944  |
| test/episodes                  | 1860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000615  |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -7.9300537 |
| test/Q_plus_P                  | -7.9300537 |
| test/reward_per_eps            | -13        |
| test/steps                     | 74400      |
| train/episodes                 | 7440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.316      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00538   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.4      |
| train/steps                    | 297600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.43715096 |
| stats_o/std                    | 0.04405153 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.472      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0579    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -10.642482 |
| test/Q_plus_P                  | -10.642482 |
| test/reward_per_eps            | -21.1      |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.339      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00585   |
| train/info_shaping_reward_mean | -0.083     |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.4      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.43719614  |
| stats_o/std                    | 0.044007894 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.427       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0641     |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -11.191093  |
| test/Q_plus_P                  | -11.191093  |
| test/reward_per_eps            | -22.9       |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.361       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0834     |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.6       |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.43723214  |
| stats_o/std                    | 0.043968376 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.497       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0021     |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -9.811547   |
| test/Q_plus_P                  | -9.811547   |
| test/reward_per_eps            | -20.1       |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.408       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0831     |
| train/info_shaping_reward_min  | -0.289      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.7       |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 189        |
| stats_o/mean                   | 0.4372778  |
| stats_o/std                    | 0.04391698 |
| test/episodes                  | 1900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000856  |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -8.411939  |
| test/Q_plus_P                  | -8.411939  |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 76000      |
| train/episodes                 | 7600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.364      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00835   |
| train/info_shaping_reward_mean | -0.0822    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.4      |
| train/steps                    | 304000     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 190       |
| stats_o/mean                   | 0.4373244 |
| stats_o/std                    | 0.0438695 |
| test/episodes                  | 1910      |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.68      |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.0023   |
| test/info_shaping_reward_mean  | -0.0503   |
| test/info_shaping_reward_min   | -0.278    |
| test/Q                         | -7.667262 |
| test/Q_plus_P                  | -7.667262 |
| test/reward_per_eps            | -12.8     |
| test/steps                     | 76400     |
| train/episodes                 | 7640      |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.373     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00367  |
| train/info_shaping_reward_mean | -0.0837   |
| train/info_shaping_reward_min  | -0.266    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -25.1     |
| train/steps                    | 305600    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.4373707   |
| stats_o/std                    | 0.043822292 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000738   |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -7.7380466  |
| test/Q_plus_P                  | -7.7380466  |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.289       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0848     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.4       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.43741867  |
| stats_o/std                    | 0.043773208 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -7.271368   |
| test/Q_plus_P                  | -7.271368   |
| test/reward_per_eps            | -14         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.369       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0803     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.2       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.43746722  |
| stats_o/std                    | 0.043724943 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -6.8740187  |
| test/Q_plus_P                  | -6.8740187  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.316       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0112     |
| train/info_shaping_reward_mean | -0.0874     |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.4       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.43750823  |
| stats_o/std                    | 0.043678477 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -6.5823865  |
| test/Q_plus_P                  | -6.5823865  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.418       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0788     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.3       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.43755147  |
| stats_o/std                    | 0.043630544 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -6.9190626  |
| test/Q_plus_P                  | -6.9190626  |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.376       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0801     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.9       |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.4376389   |
| stats_o/std                    | 0.043621827 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.122       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0029     |
| test/info_shaping_reward_mean  | -0.112      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -14.198272  |
| test/Q_plus_P                  | -14.198272  |
| test/reward_per_eps            | -35.1       |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.166       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0368     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.4       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.4377093   |
| stats_o/std                    | 0.043590084 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.233       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00262    |
| test/info_shaping_reward_mean  | -0.0788     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -10.518337  |
| test/Q_plus_P                  | -10.518337  |
| test/reward_per_eps            | -30.7       |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.182       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0156     |
| train/info_shaping_reward_mean | -0.0949     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.7       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.43776286  |
| stats_o/std                    | 0.043543294 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.618       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -7.149978   |
| test/Q_plus_P                  | -7.149978   |
| test/reward_per_eps            | -15.3       |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.227       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00984    |
| train/info_shaping_reward_mean | -0.0895     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.9       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.437814    |
| stats_o/std                    | 0.043499857 |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.56        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -7.8146715  |
| test/Q_plus_P                  | -7.8146715  |
| test/reward_per_eps            | -17.6       |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.297       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00762    |
| train/info_shaping_reward_mean | -0.0871     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.1       |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.43785846  |
| stats_o/std                    | 0.043458063 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.333       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0724     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -9.778786   |
| test/Q_plus_P                  | -9.778786   |
| test/reward_per_eps            | -26.7       |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.339       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0055     |
| train/info_shaping_reward_mean | -0.0864     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.4       |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.43790844  |
| stats_o/std                    | 0.043410804 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.245       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0036     |
| test/info_shaping_reward_mean  | -0.077      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -10.6323805 |
| test/Q_plus_P                  | -10.6323805 |
| test/reward_per_eps            | -30.2       |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.302       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00587    |
| train/info_shaping_reward_mean | -0.0856     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.9       |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.43796209  |
| stats_o/std                    | 0.043365497 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.547       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -7.4700637  |
| test/Q_plus_P                  | -7.4700637  |
| test/reward_per_eps            | -18.1       |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.268       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0875     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.3       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 203        |
| stats_o/mean                   | 0.4380162  |
| stats_o/std                    | 0.04332288 |
| test/episodes                  | 2040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00424   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -6.285188  |
| test/Q_plus_P                  | -6.285188  |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 81600      |
| train/episodes                 | 8160       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.214      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0148    |
| train/info_shaping_reward_mean | -0.0937    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 326400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.4380666   |
| stats_o/std                    | 0.043280456 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.438       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00389    |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -7.8957024  |
| test/Q_plus_P                  | -7.8957024  |
| test/reward_per_eps            | -22.5       |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.185       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0956     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.6       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.43810952 |
| stats_o/std                    | 0.0432301  |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.677      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00562   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -6.3005466 |
| test/Q_plus_P                  | -6.3005466 |
| test/reward_per_eps            | -12.9      |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.343      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00705   |
| train/info_shaping_reward_mean | -0.0805    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.3      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.43814996  |
| stats_o/std                    | 0.043182198 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00429    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -5.303695   |
| test/Q_plus_P                  | -5.303695   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.393       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00529    |
| train/info_shaping_reward_mean | -0.0793     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.3       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 207         |
| stats_o/mean                   | 0.43818453  |
| stats_o/std                    | 0.043133978 |
| test/episodes                  | 2080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00368    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -6.1325     |
| test/Q_plus_P                  | -6.1325     |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 83200       |
| train/episodes                 | 8320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.411       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0046     |
| train/info_shaping_reward_mean | -0.0761     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.6       |
| train/steps                    | 332800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 208        |
| stats_o/mean                   | 0.43822327 |
| stats_o/std                    | 0.04308634 |
| test/episodes                  | 2090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00455   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -5.971491  |
| test/Q_plus_P                  | -5.971491  |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 83600      |
| train/episodes                 | 8360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.417      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0039    |
| train/info_shaping_reward_mean | -0.0757    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.3      |
| train/steps                    | 334400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.43825698  |
| stats_o/std                    | 0.043039788 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00315    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -5.2544513  |
| test/Q_plus_P                  | -5.2544513  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.456       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0725     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.8       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.4382942   |
| stats_o/std                    | 0.042995933 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.55        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -6.521969   |
| test/Q_plus_P                  | -6.521969   |
| test/reward_per_eps            | -18         |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.4         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0765     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24         |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.43833256  |
| stats_o/std                    | 0.042950872 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00426    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -5.5107307  |
| test/Q_plus_P                  | -5.5107307  |
| test/reward_per_eps            | -13         |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.309       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0817     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.6       |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.43836918  |
| stats_o/std                    | 0.042907182 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00436    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -4.966317   |
| test/Q_plus_P                  | -4.966317   |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.443       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.3       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.43840143  |
| stats_o/std                    | 0.042861555 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00434    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -5.328125   |
| test/Q_plus_P                  | -5.328125   |
| test/reward_per_eps            | -12         |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.443       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0724     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.3       |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.4384327   |
| stats_o/std                    | 0.042821188 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.46        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00392    |
| test/info_shaping_reward_mean  | -0.0639     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -6.2060595  |
| test/Q_plus_P                  | -6.2060595  |
| test/reward_per_eps            | -21.6       |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.448       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00963    |
| train/info_shaping_reward_mean | -0.073      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.1       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.43846563  |
| stats_o/std                    | 0.042779345 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00334    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -5.108361   |
| test/Q_plus_P                  | -5.108361   |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.373       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00487    |
| train/info_shaping_reward_mean | -0.0798     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.1       |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.43848953 |
| stats_o/std                    | 0.04273439 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.672      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -5.2801113 |
| test/Q_plus_P                  | -5.2801113 |
| test/reward_per_eps            | -13.1      |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.529      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00311   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.9      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.43852222  |
| stats_o/std                    | 0.042694468 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00377    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -4.6689515  |
| test/Q_plus_P                  | -4.6689515  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.379       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0778     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.8       |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.43855992  |
| stats_o/std                    | 0.042646673 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000901   |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -4.809877   |
| test/Q_plus_P                  | -4.809877   |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.42        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.2       |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.43859544  |
| stats_o/std                    | 0.042604193 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00251    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -4.14396    |
| test/Q_plus_P                  | -4.14396    |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.444       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0728     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.2       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.43862954  |
| stats_o/std                    | 0.042564083 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -4.470152   |
| test/Q_plus_P                  | -4.470152   |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.391       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00497    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.4       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.43864992 |
| stats_o/std                    | 0.04252975 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -4.2940345 |
| test/Q_plus_P                  | -4.2940345 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.516      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00279   |
| train/info_shaping_reward_mean | -0.0746    |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.43867812  |
| stats_o/std                    | 0.042493057 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -4.426848   |
| test/Q_plus_P                  | -4.426848   |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.449       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00537    |
| train/info_shaping_reward_mean | -0.0765     |
| train/info_shaping_reward_min  | -0.284      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22         |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.4387107   |
| stats_o/std                    | 0.042453695 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -4.0609727  |
| test/Q_plus_P                  | -4.0609727  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.407       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00539    |
| train/info_shaping_reward_mean | -0.0783     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.7       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.4387388   |
| stats_o/std                    | 0.042411808 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -4.069719   |
| test/Q_plus_P                  | -4.069719   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.471       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0719     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.4387902  |
| stats_o/std                    | 0.04238324 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0688    |
| test/info_shaping_reward_mean  | -0.13      |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -9.831819  |
| test/Q_plus_P                  | -9.831819  |
| test/reward_per_eps            | -40        |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.384      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0148    |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.6      |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.43885803  |
| stats_o/std                    | 0.042358067 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.49        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00207    |
| test/info_shaping_reward_mean  | -0.0587     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -5.5846643  |
| test/Q_plus_P                  | -5.5846643  |
| test/reward_per_eps            | -20.4       |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.126       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0322     |
| train/info_shaping_reward_mean | -0.0982     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35         |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.43889692  |
| stats_o/std                    | 0.042313233 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00356    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -4.240288   |
| test/Q_plus_P                  | -4.240288   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.313       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00933    |
| train/info_shaping_reward_mean | -0.0809     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.5       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.438931    |
| stats_o/std                    | 0.042266633 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -4.0559473  |
| test/Q_plus_P                  | -4.0559473  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.445       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.2       |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.43896404  |
| stats_o/std                    | 0.042221785 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00244    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -3.9270084  |
| test/Q_plus_P                  | -3.9270084  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.392       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.3       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.438996    |
| stats_o/std                    | 0.042177707 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00052    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -4.3207493  |
| test/Q_plus_P                  | -4.3207493  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.422       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00476    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.1       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.43903053 |
| stats_o/std                    | 0.0421339  |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -4.170415  |
| test/Q_plus_P                  | -4.170415  |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.379      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00411   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.8      |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.43906257  |
| stats_o/std                    | 0.042093854 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -4.0938163  |
| test/Q_plus_P                  | -4.0938163  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.408       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0748     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.7       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.4390942   |
| stats_o/std                    | 0.042050254 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.61        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -4.516952   |
| test/Q_plus_P                  | -4.516952   |
| test/reward_per_eps            | -15.6       |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.439       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0722     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.4       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 234         |
| stats_o/mean                   | 0.43911913  |
| stats_o/std                    | 0.042008188 |
| test/episodes                  | 2350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000957   |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -3.9054787  |
| test/Q_plus_P                  | -3.9054787  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 94000       |
| train/episodes                 | 9400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.463       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.5       |
| train/steps                    | 376000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.43915096  |
| stats_o/std                    | 0.041966584 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00318    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -4.583422   |
| test/Q_plus_P                  | -4.583422   |
| test/reward_per_eps            | -13         |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.407       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00453    |
| train/info_shaping_reward_mean | -0.0757     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.7       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.43917987  |
| stats_o/std                    | 0.041927736 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -3.69341    |
| test/Q_plus_P                  | -3.69341    |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.469       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.073      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.43921018 |
| stats_o/std                    | 0.04188486 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -3.6131122 |
| test/Q_plus_P                  | -3.6131122 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.445      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0717    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.43923804  |
| stats_o/std                    | 0.041844066 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00035    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -3.6779914  |
| test/Q_plus_P                  | -3.6779914  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.457       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.43926552  |
| stats_o/std                    | 0.041805565 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -3.8472145  |
| test/Q_plus_P                  | -3.8472145  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.494       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 240        |
| stats_o/mean                   | 0.43929434 |
| stats_o/std                    | 0.04176548 |
| test/episodes                  | 2410       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -3.6532211 |
| test/Q_plus_P                  | -3.6532211 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 96400      |
| train/episodes                 | 9640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.436      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00645   |
| train/info_shaping_reward_mean | -0.0723    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.6      |
| train/steps                    | 385600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.43931618 |
| stats_o/std                    | 0.04173149 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0361    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -3.3262336 |
| test/Q_plus_P                  | -3.3262336 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00269   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.273     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.43934014  |
| stats_o/std                    | 0.041691583 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000831   |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -3.6673934  |
| test/Q_plus_P                  | -3.6673934  |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.4393671  |
| stats_o/std                    | 0.04165666 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000314  |
| test/info_shaping_reward_mean  | -0.0397    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -3.4124804 |
| test/Q_plus_P                  | -3.4124804 |
| test/reward_per_eps            | -10        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.419      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0027    |
| train/info_shaping_reward_mean | -0.0775    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.2      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.43939176  |
| stats_o/std                    | 0.041623328 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -3.3763518  |
| test/Q_plus_P                  | -3.3763518  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.483       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0712     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.4394165   |
| stats_o/std                    | 0.041584313 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -3.2833135  |
| test/Q_plus_P                  | -3.2833135  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.4394361   |
| stats_o/std                    | 0.041548118 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000891   |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -3.1435666  |
| test/Q_plus_P                  | -3.1435666  |
| test/reward_per_eps            | -10         |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.43945572 |
| stats_o/std                    | 0.04150823 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00118   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -3.2067194 |
| test/Q_plus_P                  | -3.2067194 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.555      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00272   |
| train/info_shaping_reward_mean | -0.0635    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.43947855  |
| stats_o/std                    | 0.041473985 |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000841   |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -3.3676789  |
| test/Q_plus_P                  | -3.3676789  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.426       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0761     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23         |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.43949965 |
| stats_o/std                    | 0.04144086 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -3.224821  |
| test/Q_plus_P                  | -3.224821  |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.47       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00372   |
| train/info_shaping_reward_mean | -0.0782    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.2      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.43952116  |
| stats_o/std                    | 0.041403133 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -3.3491611  |
| test/Q_plus_P                  | -3.3491611  |
| test/reward_per_eps            | -12         |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.43954477  |
| stats_o/std                    | 0.041363884 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -3.870354   |
| test/Q_plus_P                  | -3.870354   |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.0678     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.43956175 |
| stats_o/std                    | 0.04132907 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -3.2220423 |
| test/Q_plus_P                  | -3.2220423 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.559      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.6      |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.43958235 |
| stats_o/std                    | 0.04129343 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000418  |
| test/info_shaping_reward_mean  | -0.0407    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -2.876624  |
| test/Q_plus_P                  | -2.876624  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.505      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00268   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.8      |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.4396056   |
| stats_o/std                    | 0.041260254 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -3.078922   |
| test/Q_plus_P                  | -3.078922   |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.462       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0739     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.5       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 255        |
| stats_o/mean                   | 0.43962446 |
| stats_o/std                    | 0.0412233  |
| test/episodes                  | 2560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000982  |
| test/info_shaping_reward_mean  | -0.0409    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -2.8729277 |
| test/Q_plus_P                  | -2.8729277 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 102400     |
| train/episodes                 | 10240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.554      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.0649    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 409600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.43964717  |
| stats_o/std                    | 0.041187286 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -2.899057   |
| test/Q_plus_P                  | -2.899057   |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.43966684  |
| stats_o/std                    | 0.041150402 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000983   |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -2.7993472  |
| test/Q_plus_P                  | -2.7993472  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.43968543  |
| stats_o/std                    | 0.041115884 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000605   |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -2.9834623  |
| test/Q_plus_P                  | -2.9834623  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.4397012   |
| stats_o/std                    | 0.041084345 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -3.0387976  |
| test/Q_plus_P                  | -3.0387976  |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0686     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 260        |
| stats_o/mean                   | 0.43972266 |
| stats_o/std                    | 0.04104924 |
| test/episodes                  | 2610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000537  |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -3.1544144 |
| test/Q_plus_P                  | -3.1544144 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 104400     |
| train/episodes                 | 10440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.454      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0724    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 417600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.43974558  |
| stats_o/std                    | 0.041017544 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00085    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -2.7341142  |
| test/Q_plus_P                  | -2.7341142  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.468       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00365    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.3       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.4397684  |
| stats_o/std                    | 0.04098437 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000729  |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -2.9433398 |
| test/Q_plus_P                  | -2.9433398 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.486      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00261   |
| train/info_shaping_reward_mean | -0.0688    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.43978485  |
| stats_o/std                    | 0.040951494 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -2.8232114  |
| test/Q_plus_P                  | -2.8232114  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.513       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0723     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.43980467  |
| stats_o/std                    | 0.040922455 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -2.6696613  |
| test/Q_plus_P                  | -2.6696613  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.447       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0749     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.1       |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.4398222   |
| stats_o/std                    | 0.040891018 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -2.7499032  |
| test/Q_plus_P                  | -2.7499032  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.43983886  |
| stats_o/std                    | 0.040858205 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000853   |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -2.803374   |
| test/Q_plus_P                  | -2.803374   |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.43985513  |
| stats_o/std                    | 0.040824693 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -2.715427   |
| test/Q_plus_P                  | -2.715427   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.43987468  |
| stats_o/std                    | 0.040795546 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -2.705484   |
| test/Q_plus_P                  | -2.705484   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.469       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0719     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.43989006  |
| stats_o/std                    | 0.040764984 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -2.7602668  |
| test/Q_plus_P                  | -2.7602668  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 270        |
| stats_o/mean                   | 0.43990934 |
| stats_o/std                    | 0.04073522 |
| test/episodes                  | 2710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0043    |
| test/info_shaping_reward_mean  | -0.0428    |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -2.6132834 |
| test/Q_plus_P                  | -2.6132834 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 108400     |
| train/episodes                 | 10840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.466      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00391   |
| train/info_shaping_reward_mean | -0.0742    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.4      |
| train/steps                    | 433600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.43992946 |
| stats_o/std                    | 0.04070152 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00324   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -2.8523219 |
| test/Q_plus_P                  | -2.8523219 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.486      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0695    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.4399521  |
| stats_o/std                    | 0.04066961 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00348   |
| test/info_shaping_reward_mean  | -0.0448    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -3.126039  |
| test/Q_plus_P                  | -3.126039  |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.481      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0728    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.8      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.43997118 |
| stats_o/std                    | 0.04063854 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.662      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00233   |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -3.0643835 |
| test/Q_plus_P                  | -3.0643835 |
| test/reward_per_eps            | -13.5      |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.539      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.4      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.43998793  |
| stats_o/std                    | 0.040607743 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00213    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -2.5446787  |
| test/Q_plus_P                  | -2.5446787  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0696     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.440007    |
| stats_o/std                    | 0.040578287 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -2.656566   |
| test/Q_plus_P                  | -2.656566   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.516       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0708     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.44002524 |
| stats_o/std                    | 0.04054877 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00326   |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -2.790367  |
| test/Q_plus_P                  | -2.790367  |
| test/reward_per_eps            | -11        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.498      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00551   |
| train/info_shaping_reward_mean | -0.0727    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.44004416  |
| stats_o/std                    | 0.040520646 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00185    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -2.4701395  |
| test/Q_plus_P                  | -2.4701395  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0705     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.44005945  |
| stats_o/std                    | 0.040489558 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00437    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -2.703742   |
| test/Q_plus_P                  | -2.703742   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.505       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00466    |
| train/info_shaping_reward_mean | -0.0715     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.44007894  |
| stats_o/std                    | 0.040461965 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -2.564664   |
| test/Q_plus_P                  | -2.564664   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.501       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.072      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.44009006 |
| stats_o/std                    | 0.04042918 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00489   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -2.379633  |
| test/Q_plus_P                  | -2.379633  |
| test/reward_per_eps            | -9         |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.57       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.44010696  |
| stats_o/std                    | 0.040399052 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -2.6167867  |
| test/Q_plus_P                  | -2.6167867  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.44012704  |
| stats_o/std                    | 0.040370658 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -2.7197168  |
| test/Q_plus_P                  | -2.7197168  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.44014803 |
| stats_o/std                    | 0.04033762 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0152    |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -2.5080674 |
| test/Q_plus_P                  | -2.5080674 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00328   |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.4401673   |
| stats_o/std                    | 0.040309384 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.557       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00435    |
| test/info_shaping_reward_mean  | -0.0596     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -3.4497879  |
| test/Q_plus_P                  | -3.4497879  |
| test/reward_per_eps            | -17.7       |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.461       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.44018945  |
| stats_o/std                    | 0.040283244 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0598     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -2.8838558  |
| test/Q_plus_P                  | -2.8838558  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.421       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00466    |
| train/info_shaping_reward_mean | -0.0792     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.1       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.44020966 |
| stats_o/std                    | 0.04025926 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -2.7550886 |
| test/Q_plus_P                  | -2.7550886 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.396      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00377   |
| train/info_shaping_reward_mean | -0.0837    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.2      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.44022998 |
| stats_o/std                    | 0.0402313  |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.623      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00255   |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -3.1780996 |
| test/Q_plus_P                  | -3.1780996 |
| test/reward_per_eps            | -15.1      |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.452      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0758    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.44024432  |
| stats_o/std                    | 0.040204603 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00323    |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -3.3663862  |
| test/Q_plus_P                  | -3.3663862  |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.459       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.44025767  |
| stats_o/std                    | 0.040185463 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -2.7513447  |
| test/Q_plus_P                  | -2.7513447  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.478       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.0799     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.44027758 |
| stats_o/std                    | 0.04015839 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -2.6073208 |
| test/Q_plus_P                  | -2.6073208 |
| test/reward_per_eps            | -11        |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.471      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.075     |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.44029272  |
| stats_o/std                    | 0.040135924 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00297    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -2.8846684  |
| test/Q_plus_P                  | -2.8846684  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.457       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0753     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.44030645 |
| stats_o/std                    | 0.04011019 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00439   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -2.7502377 |
| test/Q_plus_P                  | -2.7502377 |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.438      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0771    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.5      |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.44032145 |
| stats_o/std                    | 0.04008432 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -2.5286067 |
| test/Q_plus_P                  | -2.5286067 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.503      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00318   |
| train/info_shaping_reward_mean | -0.0707    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.44033375  |
| stats_o/std                    | 0.040057316 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00448    |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -2.5699112  |
| test/Q_plus_P                  | -2.5699112  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.44035038 |
| stats_o/std                    | 0.04002926 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00181   |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -2.7140794 |
| test/Q_plus_P                  | -2.7140794 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0694    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.44036683  |
| stats_o/std                    | 0.040005963 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00217    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -2.3864937  |
| test/Q_plus_P                  | -2.3864937  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.459       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0779     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.44038185  |
| stats_o/std                    | 0.039987244 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00299    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -2.4608254  |
| test/Q_plus_P                  | -2.4608254  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.481       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0794     |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.44039515  |
| stats_o/std                    | 0.039960366 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -2.3213556  |
| test/Q_plus_P                  | -2.3213556  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.552       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.44040862  |
| stats_o/std                    | 0.039936334 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000975   |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -2.409718   |
| test/Q_plus_P                  | -2.409718   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.44043174  |
| stats_o/std                    | 0.039916143 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00282    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -2.4618692  |
| test/Q_plus_P                  | -2.4618692  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.429       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00736    |
| train/info_shaping_reward_mean | -0.0781     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.44044816  |
| stats_o/std                    | 0.039893344 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00291    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -2.136635   |
| test/Q_plus_P                  | -2.136635   |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.508       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.7       |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.44046223  |
| stats_o/std                    | 0.039867926 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -2.3318186  |
| test/Q_plus_P                  | -2.3318186  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.538       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0676     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.44047436  |
| stats_o/std                    | 0.039846014 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00209    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -2.1959865  |
| test/Q_plus_P                  | -2.1959865  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.44048607  |
| stats_o/std                    | 0.039818708 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00195    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -2.3862183  |
| test/Q_plus_P                  | -2.3862183  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.44049498 |
| stats_o/std                    | 0.03979061 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0392    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -2.127738  |
| test/Q_plus_P                  | -2.127738  |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0609    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.44050968  |
| stats_o/std                    | 0.039763555 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -2.1785212  |
| test/Q_plus_P                  | -2.1785212  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.4405246   |
| stats_o/std                    | 0.039737154 |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.637       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00412    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -2.3273733  |
| test/Q_plus_P                  | -2.3273733  |
| test/reward_per_eps            | -14.5       |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0687     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.44053912 |
| stats_o/std                    | 0.03971316 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -2.1044652 |
| test/Q_plus_P                  | -2.1044652 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.483      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00293   |
| train/info_shaping_reward_mean | -0.0739    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.44055435 |
| stats_o/std                    | 0.03968765 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00094   |
| test/info_shaping_reward_mean  | -0.0419    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -1.8760337 |
| test/Q_plus_P                  | -1.8760337 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.491      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00286   |
| train/info_shaping_reward_mean | -0.0699    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.4      |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.44057015  |
| stats_o/std                    | 0.039666474 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.0267003  |
| test/Q_plus_P                  | -2.0267003  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.531       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.44058594 |
| stats_o/std                    | 0.03963777 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -1.9107364 |
| test/Q_plus_P                  | -1.9107364 |
| test/reward_per_eps            | -9         |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.545      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.068     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.4405999   |
| stats_o/std                    | 0.039611544 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -2.0798795  |
| test/Q_plus_P                  | -2.0798795  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0684     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.4406151   |
| stats_o/std                    | 0.039587006 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -2.1791377  |
| test/Q_plus_P                  | -2.1791377  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.488       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0721     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.5       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.44062853 |
| stats_o/std                    | 0.03955921 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00225   |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -2.3535686 |
| test/Q_plus_P                  | -2.3535686 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0035    |
| train/info_shaping_reward_mean | -0.0681    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.4406425  |
| stats_o/std                    | 0.0395304  |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.73       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00528   |
| test/info_shaping_reward_mean  | -0.044     |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -1.9706906 |
| test/Q_plus_P                  | -1.9706906 |
| test/reward_per_eps            | -10.8      |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00315   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.44065928  |
| stats_o/std                    | 0.039501455 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00364    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.9073913  |
| test/Q_plus_P                  | -1.9073913  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00423    |
| train/info_shaping_reward_mean | -0.0698     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.44067204 |
| stats_o/std                    | 0.03947791 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00573   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.898214  |
| test/Q_plus_P                  | -1.898214  |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.0677    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.4406853   |
| stats_o/std                    | 0.039451815 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00559    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.8288845  |
| test/Q_plus_P                  | -1.8288845  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.44069985 |
| stats_o/std                    | 0.03942864 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00406   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -2.22264   |
| test/Q_plus_P                  | -2.22264   |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.479      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00412   |
| train/info_shaping_reward_mean | -0.0735    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.9      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.44071004  |
| stats_o/std                    | 0.039402705 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00475    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.9151131  |
| test/Q_plus_P                  | -1.9151131  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.553       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 321        |
| stats_o/mean                   | 0.4407204  |
| stats_o/std                    | 0.03937891 |
| test/episodes                  | 3220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000927  |
| test/info_shaping_reward_mean  | -0.0425    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -2.3087313 |
| test/Q_plus_P                  | -2.3087313 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 128800     |
| train/episodes                 | 12880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00263   |
| train/info_shaping_reward_mean | -0.0629    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 515200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.44072843 |
| stats_o/std                    | 0.03935329 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00183   |
| test/info_shaping_reward_mean  | -0.0434    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -2.4963791 |
| test/Q_plus_P                  | -2.4963791 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.568      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.44074026  |
| stats_o/std                    | 0.039327215 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00341    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.8690703  |
| test/Q_plus_P                  | -1.8690703  |
| test/reward_per_eps            | -10         |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.44074962  |
| stats_o/std                    | 0.039304707 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -2.0176067  |
| test/Q_plus_P                  | -2.0176067  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.536       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00423    |
| train/info_shaping_reward_mean | -0.0697     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.4407611   |
| stats_o/std                    | 0.039278593 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00449    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.7812972  |
| test/Q_plus_P                  | -1.7812972  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.4407715   |
| stats_o/std                    | 0.039253872 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00399    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.8114591  |
| test/Q_plus_P                  | -1.8114591  |
| test/reward_per_eps            | -9          |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.44078436  |
| stats_o/std                    | 0.039229844 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.6499739  |
| test/Q_plus_P                  | -1.6499739  |
| test/reward_per_eps            | -8          |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.44079748 |
| stats_o/std                    | 0.03920425 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -1.8582932 |
| test/Q_plus_P                  | -1.8582932 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.44080958  |
| stats_o/std                    | 0.039180543 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00409    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.8007834  |
| test/Q_plus_P                  | -1.8007834  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.44081962 |
| stats_o/std                    | 0.03915378 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -1.8148798 |
| test/Q_plus_P                  | -1.8148798 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.596      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00238   |
| train/info_shaping_reward_mean | -0.0589    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.44082806  |
| stats_o/std                    | 0.039132208 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000683   |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.9554927  |
| test/Q_plus_P                  | -1.9554927  |
| test/reward_per_eps            | -10         |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.44083512  |
| stats_o/std                    | 0.039110314 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.6578772  |
| test/Q_plus_P                  | -1.6578772  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.44084713  |
| stats_o/std                    | 0.039085057 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.5954726  |
| test/Q_plus_P                  | -1.5954726  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00212    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.44085586 |
| stats_o/std                    | 0.03906117 |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0396    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -1.8213025 |
| test/Q_plus_P                  | -1.8213025 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.567      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00243   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.3      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.44086477  |
| stats_o/std                    | 0.039039426 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000859   |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.6586139  |
| test/Q_plus_P                  | -1.6586139  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.557       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.44087622  |
| stats_o/std                    | 0.039017078 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000469   |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.6939427  |
| test/Q_plus_P                  | -1.6939427  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.44088566  |
| stats_o/std                    | 0.038994852 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -1.6579741  |
| test/Q_plus_P                  | -1.6579741  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.44089338 |
| stats_o/std                    | 0.03897464 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00165   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.6777582 |
| test/Q_plus_P                  | -1.6777582 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00293   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.4408995   |
| stats_o/std                    | 0.038954448 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000848   |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.5908666  |
| test/Q_plus_P                  | -1.5908666  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.4409074   |
| stats_o/std                    | 0.038933698 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.5975842  |
| test/Q_plus_P                  | -1.5975842  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.44091615 |
| stats_o/std                    | 0.03891164 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000186  |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.6986798 |
| test/Q_plus_P                  | -1.6986798 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.615      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00298   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.44092083 |
| stats_o/std                    | 0.03889225 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00219   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -1.5633878 |
| test/Q_plus_P                  | -1.5633878 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00263   |
| train/info_shaping_reward_mean | -0.0665    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.44092658  |
| stats_o/std                    | 0.038871344 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.6901512  |
| test/Q_plus_P                  | -1.6901512  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.4409405   |
| stats_o/std                    | 0.038848016 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00242    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -1.5788214  |
| test/Q_plus_P                  | -1.5788214  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.548       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.44094947  |
| stats_o/std                    | 0.038827382 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.6350186  |
| test/Q_plus_P                  | -1.6350186  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 346        |
| stats_o/mean                   | 0.44095877 |
| stats_o/std                    | 0.03880623 |
| test/episodes                  | 3470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00116   |
| test/info_shaping_reward_mean  | -0.0421    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -1.6064923 |
| test/Q_plus_P                  | -1.6064923 |
| test/reward_per_eps            | -9         |
| test/steps                     | 138800     |
| train/episodes                 | 13880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00287   |
| train/info_shaping_reward_mean | -0.067     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 555200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.44096506  |
| stats_o/std                    | 0.038787242 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.5895317  |
| test/Q_plus_P                  | -1.5895317  |
| test/reward_per_eps            | -9          |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.44097415  |
| stats_o/std                    | 0.038763106 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00171    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -1.6405913  |
| test/Q_plus_P                  | -1.6405913  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.44098127  |
| stats_o/std                    | 0.038741827 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00258    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.6810781  |
| test/Q_plus_P                  | -1.6810781  |
| test/reward_per_eps            | -10         |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.44098946 |
| stats_o/std                    | 0.03872267 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0403    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.5254887 |
| test/Q_plus_P                  | -1.5254887 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00315   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.44099912  |
| stats_o/std                    | 0.038702488 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000967   |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.5637462  |
| test/Q_plus_P                  | -1.5637462  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.441009    |
| stats_o/std                    | 0.038680736 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -1.5832499  |
| test/Q_plus_P                  | -1.5832499  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.4410132   |
| stats_o/std                    | 0.038656794 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.5735053  |
| test/Q_plus_P                  | -1.5735053  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.4410225  |
| stats_o/std                    | 0.0386375  |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00159   |
| test/info_shaping_reward_mean  | -0.0514    |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -1.8813237 |
| test/Q_plus_P                  | -1.8813237 |
| test/reward_per_eps            | -12        |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.4410307   |
| stats_o/std                    | 0.038617548 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000959   |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.6343057  |
| test/Q_plus_P                  | -1.6343057  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.44103816 |
| stats_o/std                    | 0.03859557 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00216   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -1.4931531 |
| test/Q_plus_P                  | -1.4931531 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00302   |
| train/info_shaping_reward_mean | -0.0616    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.4410453   |
| stats_o/std                    | 0.038574215 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.6550223  |
| test/Q_plus_P                  | -1.6550223  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.4410546   |
| stats_o/std                    | 0.038551733 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00029    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.3564506  |
| test/Q_plus_P                  | -1.3564506  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.441062    |
| stats_o/std                    | 0.038530882 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.5732974  |
| test/Q_plus_P                  | -1.5732974  |
| test/reward_per_eps            | -10         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.44107166  |
| stats_o/std                    | 0.038509343 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -1.5089358  |
| test/Q_plus_P                  | -1.5089358  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.441077    |
| stats_o/std                    | 0.038487244 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.3261921  |
| test/Q_plus_P                  | -1.3261921  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.44108394  |
| stats_o/std                    | 0.038467564 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00258    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -1.445035   |
| test/Q_plus_P                  | -1.445035   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.4411101   |
| stats_o/std                    | 0.038457368 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0662     |
| test/info_shaping_reward_mean  | -0.107      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -5.095572   |
| test/Q_plus_P                  | -5.095572   |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.454       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0168     |
| train/info_shaping_reward_mean | -0.0809     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.8       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.4411256   |
| stats_o/std                    | 0.038441528 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.6772226  |
| test/Q_plus_P                  | -1.6772226  |
| test/reward_per_eps            | -11         |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.399       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00464    |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.1       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.4411348  |
| stats_o/std                    | 0.03841977 |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.67       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000745  |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.9490724 |
| test/Q_plus_P                  | -1.9490724 |
| test/reward_per_eps            | -13.2      |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.504      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00262   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.9      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.44114408  |
| stats_o/std                    | 0.038398493 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.7299597  |
| test/Q_plus_P                  | -1.7299597  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 367        |
| stats_o/mean                   | 0.4411502  |
| stats_o/std                    | 0.03837696 |
| test/episodes                  | 3680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00257   |
| test/info_shaping_reward_mean  | -0.0361    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -1.5294814 |
| test/Q_plus_P                  | -1.5294814 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 147200     |
| train/episodes                 | 14720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00309   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 588800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.44116068  |
| stats_o/std                    | 0.038354617 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.5378727  |
| test/Q_plus_P                  | -1.5378727  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.44117093 |
| stats_o/std                    | 0.03833287 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00367   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -1.5305203 |
| test/Q_plus_P                  | -1.5305203 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.513      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00347   |
| train/info_shaping_reward_mean | -0.0708    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.5      |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.4411787   |
| stats_o/std                    | 0.038311716 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.5891483  |
| test/Q_plus_P                  | -1.5891483  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00411    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.4411881   |
| stats_o/std                    | 0.038290266 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.622206   |
| test/Q_plus_P                  | -1.622206   |
| test/reward_per_eps            | -10         |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 372        |
| stats_o/mean                   | 0.4411944  |
| stats_o/std                    | 0.03827088 |
| test/episodes                  | 3730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00072   |
| test/info_shaping_reward_mean  | -0.0387    |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -1.6489213 |
| test/Q_plus_P                  | -1.6489213 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 149200     |
| train/episodes                 | 14920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.581      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.8      |
| train/steps                    | 596800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.4412013  |
| stats_o/std                    | 0.03824876 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00127   |
| test/info_shaping_reward_mean  | -0.037     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -1.3988458 |
| test/Q_plus_P                  | -1.3988458 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00266   |
| train/info_shaping_reward_mean | -0.0611    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.4412084   |
| stats_o/std                    | 0.038226314 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.326925   |
| test/Q_plus_P                  | -1.326925   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.44121456  |
| stats_o/std                    | 0.038206495 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00332    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.6513739  |
| test/Q_plus_P                  | -1.6513739  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.44122264  |
| stats_o/std                    | 0.038184848 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000404   |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.5292511  |
| test/Q_plus_P                  | -1.5292511  |
| test/reward_per_eps            | -9          |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.44122827  |
| stats_o/std                    | 0.038164053 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.445184   |
| test/Q_plus_P                  | -1.445184   |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.44123378 |
| stats_o/std                    | 0.03814662 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0409    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.5569721 |
| test/Q_plus_P                  | -1.5569721 |
| test/reward_per_eps            | -9         |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00306   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.44124138 |
| stats_o/std                    | 0.03812788 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0389    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -1.4404445 |
| test/Q_plus_P                  | -1.4404445 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.586      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.44124982  |
| stats_o/std                    | 0.038110692 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000545   |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.3061202  |
| test/Q_plus_P                  | -1.3061202  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.4412556   |
| stats_o/std                    | 0.038090944 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -1.3300902  |
| test/Q_plus_P                  | -1.3300902  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 382         |
| stats_o/mean                   | 0.44126388  |
| stats_o/std                    | 0.038073976 |
| test/episodes                  | 3830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.2013912  |
| test/Q_plus_P                  | -1.2013912  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 153200      |
| train/episodes                 | 15320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 612800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.44127056  |
| stats_o/std                    | 0.038052846 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.3794158  |
| test/Q_plus_P                  | -1.3794158  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.44127822  |
| stats_o/std                    | 0.038032647 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0324     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.1882977  |
| test/Q_plus_P                  | -1.1882977  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00217    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.4412841  |
| stats_o/std                    | 0.03801712 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000494  |
| test/info_shaping_reward_mean  | -0.0359    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.3804038 |
| test/Q_plus_P                  | -1.3804038 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00201   |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.44129145  |
| stats_o/std                    | 0.037997548 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000473   |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -1.5235088  |
| test/Q_plus_P                  | -1.5235088  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.441298    |
| stats_o/std                    | 0.037978567 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000671   |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.2181809  |
| test/Q_plus_P                  | -1.2181809  |
| test/reward_per_eps            | -8          |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00216    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.4413058   |
| stats_o/std                    | 0.037959274 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.2106094  |
| test/Q_plus_P                  | -1.2106094  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 389        |
| stats_o/mean                   | 0.4413116  |
| stats_o/std                    | 0.03794165 |
| test/episodes                  | 3900       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0384    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.3192668 |
| test/Q_plus_P                  | -1.3192668 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 156000     |
| train/episodes                 | 15600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.608      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.7      |
| train/steps                    | 624000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.44131732  |
| stats_o/std                    | 0.037923895 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -1.3549864  |
| test/Q_plus_P                  | -1.3549864  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.44132355  |
| stats_o/std                    | 0.037903856 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.565298   |
| test/Q_plus_P                  | -1.565298   |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.441329    |
| stats_o/std                    | 0.037884407 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.4638324  |
| test/Q_plus_P                  | -1.4638324  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.44133738 |
| stats_o/std                    | 0.03786569 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00194   |
| test/info_shaping_reward_mean  | -0.0408    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -1.4369631 |
| test/Q_plus_P                  | -1.4369631 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00262   |
| train/info_shaping_reward_mean | -0.0632    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.44134247  |
| stats_o/std                    | 0.037846062 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000888   |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.6117921  |
| test/Q_plus_P                  | -1.6117921  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.4413469   |
| stats_o/std                    | 0.037828628 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000334   |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.3898616  |
| test/Q_plus_P                  | -1.3898616  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00199    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.44135323  |
| stats_o/std                    | 0.037806984 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.3872219  |
| test/Q_plus_P                  | -1.3872219  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.44136265  |
| stats_o/std                    | 0.037789073 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00242    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -1.3180722  |
| test/Q_plus_P                  | -1.3180722  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00409    |
| train/info_shaping_reward_mean | -0.0693     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.4413701   |
| stats_o/std                    | 0.037770633 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000361   |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.1514703  |
| test/Q_plus_P                  | -1.1514703  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.44137588 |
| stats_o/std                    | 0.03775214 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00218   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.1759841 |
| test/Q_plus_P                  | -1.1759841 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.597      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.44138277 |
| stats_o/std                    | 0.03773581 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000591  |
| test/info_shaping_reward_mean  | -0.0395    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.2352393 |
| test/Q_plus_P                  | -1.2352393 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.604      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00243   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.44138932  |
| stats_o/std                    | 0.037720334 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -1.1407516  |
| test/Q_plus_P                  | -1.1407516  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.44139585 |
| stats_o/std                    | 0.03770343 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0473    |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -1.3815408 |
| test/Q_plus_P                  | -1.3815408 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00261   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.44140112  |
| stats_o/std                    | 0.037686568 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.3946633  |
| test/Q_plus_P                  | -1.3946633  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.44140744  |
| stats_o/std                    | 0.037668888 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000646   |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -1.3208213  |
| test/Q_plus_P                  | -1.3208213  |
| test/reward_per_eps            | -9          |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0681     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.44141078  |
| stats_o/std                    | 0.037651483 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.1421171  |
| test/Q_plus_P                  | -1.1421171  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00226    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.4414166   |
| stats_o/std                    | 0.037631076 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000772   |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.1875569  |
| test/Q_plus_P                  | -1.1875569  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.44143665  |
| stats_o/std                    | 0.037624907 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0179     |
| test/info_shaping_reward_mean  | -0.13       |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -5.449445   |
| test/Q_plus_P                  | -5.449445   |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.465       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0172     |
| train/info_shaping_reward_mean | -0.083      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.4       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.44146028  |
| stats_o/std                    | 0.037614617 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.7787987  |
| test/Q_plus_P                  | -1.7787987  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.276       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00661    |
| train/info_shaping_reward_mean | -0.0884     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.9       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 409        |
| stats_o/mean                   | 0.44146714 |
| stats_o/std                    | 0.03759748 |
| test/episodes                  | 4100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0397    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -1.0949991 |
| test/Q_plus_P                  | -1.0949991 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 164000     |
| train/episodes                 | 16400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.501      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00358   |
| train/info_shaping_reward_mean | -0.0691    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20        |
| train/steps                    | 656000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.4414709   |
| stats_o/std                    | 0.037580203 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00249    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.2348026  |
| test/Q_plus_P                  | -1.2348026  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.44147655  |
| stats_o/std                    | 0.037562903 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.2979206  |
| test/Q_plus_P                  | -1.2979206  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 412        |
| stats_o/mean                   | 0.44148317 |
| stats_o/std                    | 0.03754436 |
| test/episodes                  | 4130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000429  |
| test/info_shaping_reward_mean  | -0.0375    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -1.3772985 |
| test/Q_plus_P                  | -1.3772985 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 165200     |
| train/episodes                 | 16520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00253   |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 660800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.44148946  |
| stats_o/std                    | 0.037525266 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.3702949  |
| test/Q_plus_P                  | -1.3702949  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.582       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 414        |
| stats_o/mean                   | 0.44149342 |
| stats_o/std                    | 0.03750968 |
| test/episodes                  | 4150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00069   |
| test/info_shaping_reward_mean  | -0.0335    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -1.0892936 |
| test/Q_plus_P                  | -1.0892936 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 166000     |
| train/episodes                 | 16600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 664000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.44149897  |
| stats_o/std                    | 0.037489373 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.2573184  |
| test/Q_plus_P                  | -1.2573184  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.4415042  |
| stats_o/std                    | 0.03747299 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0375    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -1.1884532 |
| test/Q_plus_P                  | -1.1884532 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00274   |
| train/info_shaping_reward_mean | -0.0598    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.44150898 |
| stats_o/std                    | 0.037457   |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.3221151 |
| test/Q_plus_P                  | -1.3221151 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00268   |
| train/info_shaping_reward_mean | -0.0614    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.44151226  |
| stats_o/std                    | 0.037439328 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00171    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.2967874  |
| test/Q_plus_P                  | -1.2967874  |
| test/reward_per_eps            | -9          |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.44152573 |
| stats_o/std                    | 0.03742394 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0373    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -1.137547  |
| test/Q_plus_P                  | -1.137547  |
| test/reward_per_eps            | -8         |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.496      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0159    |
| train/info_shaping_reward_mean | -0.0704    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.2      |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 420        |
| stats_o/mean                   | 0.44153354 |
| stats_o/std                    | 0.03740591 |
| test/episodes                  | 4210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000735  |
| test/info_shaping_reward_mean  | -0.0384    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -1.2351325 |
| test/Q_plus_P                  | -1.2351325 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 168400     |
| train/episodes                 | 16840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00283   |
| train/info_shaping_reward_mean | -0.0591    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 673600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.44153693 |
| stats_o/std                    | 0.03738868 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0377    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -1.2070384 |
| test/Q_plus_P                  | -1.2070384 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00276   |
| train/info_shaping_reward_mean | -0.0605    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.44154128  |
| stats_o/std                    | 0.037373397 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000462   |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0684743  |
| test/Q_plus_P                  | -1.0684743  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 423        |
| stats_o/mean                   | 0.4415463  |
| stats_o/std                    | 0.03735513 |
| test/episodes                  | 4240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00123   |
| test/info_shaping_reward_mean  | -0.0365    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -1.1576065 |
| test/Q_plus_P                  | -1.1576065 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 169600     |
| train/episodes                 | 16960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00255   |
| train/info_shaping_reward_mean | -0.0627    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 678400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.4415486  |
| stats_o/std                    | 0.03733932 |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000517  |
| test/info_shaping_reward_mean  | -0.0379    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -1.1370512 |
| test/Q_plus_P                  | -1.1370512 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00283   |
| train/info_shaping_reward_mean | -0.0617    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.4415524   |
| stats_o/std                    | 0.037381805 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.01        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00835    |
| test/info_shaping_reward_mean  | -0.143      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -4.9822     |
| test/Q_plus_P                  | -4.9822     |
| test/reward_per_eps            | -39.6       |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.386       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0443     |
| train/info_shaping_reward_mean | -0.134      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.6       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.44156134  |
| stats_o/std                    | 0.037372082 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000792   |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.462363   |
| test/Q_plus_P                  | -1.462363   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.384       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0166     |
| train/info_shaping_reward_mean | -0.0877     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.6       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.44156644  |
| stats_o/std                    | 0.037355542 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00146    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.3801742  |
| test/Q_plus_P                  | -1.3801742  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 428        |
| stats_o/mean                   | 0.4415736  |
| stats_o/std                    | 0.03733852 |
| test/episodes                  | 4290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000958  |
| test/info_shaping_reward_mean  | -0.0376    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.214012  |
| test/Q_plus_P                  | -1.214012  |
| test/reward_per_eps            | -8         |
| test/steps                     | 171600     |
| train/episodes                 | 17160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00343   |
| train/info_shaping_reward_mean | -0.0669    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 686400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.44157898  |
| stats_o/std                    | 0.037321806 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000618   |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.4559714  |
| test/Q_plus_P                  | -1.4559714  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00242    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.44158387  |
| stats_o/std                    | 0.037306953 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000619   |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.3037683  |
| test/Q_plus_P                  | -1.3037683  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.4415896   |
| stats_o/std                    | 0.037289534 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.2251561  |
| test/Q_plus_P                  | -1.2251561  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.44159472  |
| stats_o/std                    | 0.037273165 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000751   |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0805771  |
| test/Q_plus_P                  | -1.0805771  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.44159892  |
| stats_o/std                    | 0.037255574 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000473   |
| test/info_shaping_reward_mean  | -0.0339     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.0233676  |
| test/Q_plus_P                  | -1.0233676  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.44160458 |
| stats_o/std                    | 0.03723801 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.0393    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.0525204 |
| test/Q_plus_P                  | -1.0525204 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.0556    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.4416106   |
| stats_o/std                    | 0.037222654 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00058    |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.0715619  |
| test/Q_plus_P                  | -1.0715619  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00255    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.44161534  |
| stats_o/std                    | 0.037206907 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.1022071  |
| test/Q_plus_P                  | -1.1022071  |
| test/reward_per_eps            | -8          |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.44162062  |
| stats_o/std                    | 0.037192028 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000759   |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -1.0062075  |
| test/Q_plus_P                  | -1.0062075  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00253    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 438        |
| stats_o/mean                   | 0.4416248  |
| stats_o/std                    | 0.03717516 |
| test/episodes                  | 4390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.037     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.1025035 |
| test/Q_plus_P                  | -1.1025035 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 175600     |
| train/episodes                 | 17560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 702400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.44162974  |
| stats_o/std                    | 0.037158463 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000851   |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.0221015  |
| test/Q_plus_P                  | -1.0221015  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.4416339   |
| stats_o/std                    | 0.037142955 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000919   |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0268967  |
| test/Q_plus_P                  | -1.0268967  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.44163895  |
| stats_o/std                    | 0.037127126 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000995   |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -1.1504288  |
| test/Q_plus_P                  | -1.1504288  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00219    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.44164407  |
| stats_o/std                    | 0.037112303 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -1.1546073  |
| test/Q_plus_P                  | -1.1546073  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.4416474   |
| stats_o/std                    | 0.037097964 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000631   |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -1.0406815  |
| test/Q_plus_P                  | -1.0406815  |
| test/reward_per_eps            | -8          |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.44165015  |
| stats_o/std                    | 0.037081532 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000594   |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -1.1096605  |
| test/Q_plus_P                  | -1.1096605  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.44165698 |
| stats_o/std                    | 0.03706935 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -1.1576934 |
| test/Q_plus_P                  | -1.1576934 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.556      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00676   |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.8      |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.44166124  |
| stats_o/std                    | 0.037051998 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0331     |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -0.95500535 |
| test/Q_plus_P                  | -0.95500535 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00193    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.4416637   |
| stats_o/std                    | 0.037038077 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.99500656 |
| test/Q_plus_P                  | -0.99500656 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.44166696  |
| stats_o/std                    | 0.037023608 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.8962493  |
| test/Q_plus_P                  | -0.8962493  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0672     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.44167146 |
| stats_o/std                    | 0.0370082  |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.103     |
| test/info_shaping_reward_mean  | -0.382     |
| test/info_shaping_reward_min   | -0.493     |
| test/Q                         | -8.66696   |
| test/Q_plus_P                  | -8.66696   |
| test/reward_per_eps            | -40        |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00286   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.44169173  |
| stats_o/std                    | 0.037068937 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.275       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00219    |
| test/info_shaping_reward_mean  | -0.0883     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -3.4997015  |
| test/Q_plus_P                  | -3.4997015  |
| test/reward_per_eps            | -29         |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.149       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0565     |
| train/info_shaping_reward_mean | -0.166      |
| train/info_shaping_reward_min  | -0.298      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34         |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.44169882  |
| stats_o/std                    | 0.037058268 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.7601029  |
| test/Q_plus_P                  | -1.7601029  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.349       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0839     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26         |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.4417032   |
| stats_o/std                    | 0.037043873 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.7159597  |
| test/Q_plus_P                  | -1.7159597  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.53        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00216    |
| train/info_shaping_reward_mean | -0.0653     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.8       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.44170806  |
| stats_o/std                    | 0.037026625 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.4539108  |
| test/Q_plus_P                  | -1.4539108  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.44171152  |
| stats_o/std                    | 0.037008215 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.6919974  |
| test/Q_plus_P                  | -1.6919974  |
| test/reward_per_eps            | -12         |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.44171774  |
| stats_o/std                    | 0.036990095 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.9087157  |
| test/Q_plus_P                  | -1.9087157  |
| test/reward_per_eps            | -14         |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.44172177  |
| stats_o/std                    | 0.036974598 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.4804     |
| test/Q_plus_P                  | -1.4804     |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.44172466  |
| stats_o/std                    | 0.036961973 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.21       |
| test/Q                         | -1.5046473  |
| test/Q_plus_P                  | -1.5046473  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.551       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.4417285  |
| stats_o/std                    | 0.0369472  |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00288   |
| test/info_shaping_reward_mean  | -0.0437    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.5993266 |
| test/Q_plus_P                  | -1.5993266 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.585      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0597    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.6      |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.44173595 |
| stats_o/std                    | 0.03693137 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -1.6113417 |
| test/Q_plus_P                  | -1.6113417 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.562      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0647    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.44174227  |
| stats_o/std                    | 0.036915015 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.3362356  |
| test/Q_plus_P                  | -1.3362356  |
| test/reward_per_eps            | -9          |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.4417478  |
| stats_o/std                    | 0.03689783 |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00222   |
| test/info_shaping_reward_mean  | -0.0371    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.2218422 |
| test/Q_plus_P                  | -1.2218422 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.606      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00318   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.44175485 |
| stats_o/std                    | 0.03687827 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0375    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -1.3976365 |
| test/Q_plus_P                  | -1.3976365 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.4417596   |
| stats_o/std                    | 0.036860876 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000458   |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.2270192  |
| test/Q_plus_P                  | -1.2270192  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.44176635  |
| stats_o/std                    | 0.036846742 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.2006129  |
| test/Q_plus_P                  | -1.2006129  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00402    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.44177273  |
| stats_o/std                    | 0.036832288 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0322     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.99595714 |
| test/Q_plus_P                  | -0.99595714 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.44177744  |
| stats_o/std                    | 0.036817454 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.212739   |
| test/Q_plus_P                  | -1.212739   |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.44178084  |
| stats_o/std                    | 0.036804818 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0348     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.085719   |
| test/Q_plus_P                  | -1.085719   |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 468        |
| stats_o/mean                   | 0.44178382 |
| stats_o/std                    | 0.03679136 |
| test/episodes                  | 4690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00227   |
| test/info_shaping_reward_mean  | -0.037     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -1.0299354 |
| test/Q_plus_P                  | -1.0299354 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 187600     |
| train/episodes                 | 18760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.652      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0605    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 750400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.44178846  |
| stats_o/std                    | 0.03677817  |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000961   |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.94268394 |
| test/Q_plus_P                  | -0.94268394 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.4417914   |
| stats_o/std                    | 0.036766842 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9398393  |
| test/Q_plus_P                  | -0.9398393  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.44179454  |
| stats_o/std                    | 0.036751345 |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000824   |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.9922905  |
| test/Q_plus_P                  | -0.9922905  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.4418001   |
| stats_o/std                    | 0.036736935 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00213    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.97587323 |
| test/Q_plus_P                  | -0.97587323 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.44180226  |
| stats_o/std                    | 0.036727197 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00317    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0604359  |
| test/Q_plus_P                  | -1.0604359  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.4418061  |
| stats_o/std                    | 0.03671565 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0409    |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -1.1168896 |
| test/Q_plus_P                  | -1.1168896 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0028    |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.44180956  |
| stats_o/std                    | 0.036700208 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.0817685  |
| test/Q_plus_P                  | -1.0817685  |
| test/reward_per_eps            | -8          |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.44181272  |
| stats_o/std                    | 0.036682963 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000308   |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.926481   |
| test/Q_plus_P                  | -0.926481   |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00229    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.4418145   |
| stats_o/std                    | 0.036670137 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.9101651  |
| test/Q_plus_P                  | -0.9101651  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00213    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.4418178   |
| stats_o/std                    | 0.036656465 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000639   |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -1.0391705  |
| test/Q_plus_P                  | -1.0391705  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.44182312  |
| stats_o/std                    | 0.036643077 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0317     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.86269325 |
| test/Q_plus_P                  | -0.86269325 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.4418398  |
| stats_o/std                    | 0.03664088 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0737    |
| test/info_shaping_reward_mean  | -0.171     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -6.011307  |
| test/Q_plus_P                  | -6.011307  |
| test/reward_per_eps            | -40        |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.507      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0158    |
| train/info_shaping_reward_mean | -0.0807    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.7      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.44186354 |
| stats_o/std                    | 0.03663699 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.593      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0498    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -1.9233164 |
| test/Q_plus_P                  | -1.9233164 |
| test/reward_per_eps            | -16.3      |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.271      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0103    |
| train/info_shaping_reward_mean | -0.0901    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.2      |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.44186893  |
| stats_o/std                    | 0.036622286 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.2583205  |
| test/Q_plus_P                  | -1.2583205  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.533       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.44187215  |
| stats_o/std                    | 0.036609717 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.4303465  |
| test/Q_plus_P                  | -1.4303465  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.522       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.4418788   |
| stats_o/std                    | 0.036594894 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000871   |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.4617451  |
| test/Q_plus_P                  | -1.4617451  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.44188538  |
| stats_o/std                    | 0.036580276 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.277848   |
| test/Q_plus_P                  | -1.277848   |
| test/reward_per_eps            | -9          |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 486        |
| stats_o/mean                   | 0.44188955 |
| stats_o/std                    | 0.03656402 |
| test/episodes                  | 4870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00239   |
| test/info_shaping_reward_mean  | -0.0365    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.241645  |
| test/Q_plus_P                  | -1.241645  |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 194800     |
| train/episodes                 | 19480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00271   |
| train/info_shaping_reward_mean | -0.0541    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 779200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.4418925  |
| stats_o/std                    | 0.03654714 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00191   |
| test/info_shaping_reward_mean  | -0.0346    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.1202807 |
| test/Q_plus_P                  | -1.1202807 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.64       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00247   |
| train/info_shaping_reward_mean | -0.0553    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.44189474 |
| stats_o/std                    | 0.03653547 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -1.2929035 |
| test/Q_plus_P                  | -1.2929035 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.588      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0663    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.5      |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.44189835  |
| stats_o/std                    | 0.036519002 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.1554629  |
| test/Q_plus_P                  | -1.1554629  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 490        |
| stats_o/mean                   | 0.44190326 |
| stats_o/std                    | 0.03650637 |
| test/episodes                  | 4910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0394    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -1.1222045 |
| test/Q_plus_P                  | -1.1222045 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 196400     |
| train/episodes                 | 19640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0027    |
| train/info_shaping_reward_mean | -0.0603    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 785600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.44190323  |
| stats_o/std                    | 0.036495328 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000307   |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0790877  |
| test/Q_plus_P                  | -1.0790877  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.44190732  |
| stats_o/std                    | 0.036480285 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.9373719  |
| test/Q_plus_P                  | -0.9373719  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.44190994  |
| stats_o/std                    | 0.036467995 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.2889682  |
| test/Q_plus_P                  | -1.2889682  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.4419135   |
| stats_o/std                    | 0.036454324 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000641   |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.1270537  |
| test/Q_plus_P                  | -1.1270537  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.44191754  |
| stats_o/std                    | 0.036439825 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000859   |
| test/info_shaping_reward_mean  | -0.035      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.0497621  |
| test/Q_plus_P                  | -1.0497621  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.44192052 |
| stats_o/std                    | 0.03642907 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00145   |
| test/info_shaping_reward_mean  | -0.0374    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -0.9103555 |
| test/Q_plus_P                  | -0.9103555 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.44192478  |
| stats_o/std                    | 0.036415737 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000466   |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.94510126 |
| test/Q_plus_P                  | -0.94510126 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.44192907  |
| stats_o/std                    | 0.036402047 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.8767671  |
| test/Q_plus_P                  | -0.8767671  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.44193017  |
| stats_o/std                    | 0.036387485 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00311    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -0.97075266 |
| test/Q_plus_P                  | -0.97075266 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.44193327 |
| stats_o/std                    | 0.03637322 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0334    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.9538576 |
| test/Q_plus_P                  | -0.9538576 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.595      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00223   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.44193593  |
| stats_o/std                    | 0.036359537 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.9560193  |
| test/Q_plus_P                  | -0.9560193  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00199    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.4419408  |
| stats_o/std                    | 0.03634692 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000499  |
| test/info_shaping_reward_mean  | -0.0307    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -0.9019125 |
| test/Q_plus_P                  | -0.9019125 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00291   |
| train/info_shaping_reward_mean | -0.063     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.4419447   |
| stats_o/std                    | 0.03633138  |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000582   |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.94618386 |
| test/Q_plus_P                  | -0.94618386 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.44194916  |
| stats_o/std                    | 0.036318153 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.0134767  |
| test/Q_plus_P                  | -1.0134767  |
| test/reward_per_eps            | -8          |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.44195306  |
| stats_o/std                    | 0.03630409  |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.89160734 |
| test/Q_plus_P                  | -0.89160734 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.44195414  |
| stats_o/std                    | 0.036291447 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000584   |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.8847053  |
| test/Q_plus_P                  | -0.8847053  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.44195494  |
| stats_o/std                    | 0.036277194 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.9365525  |
| test/Q_plus_P                  | -0.9365525  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.44195727  |
| stats_o/std                    | 0.036262024 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.9471006  |
| test/Q_plus_P                  | -0.9471006  |
| test/reward_per_eps            | -8          |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.44196013  |
| stats_o/std                    | 0.036250226 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000266   |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -1.0589275  |
| test/Q_plus_P                  | -1.0589275  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00215    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.44196245  |
| stats_o/std                    | 0.03623658  |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.91147447 |
| test/Q_plus_P                  | -0.91147447 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.44196662  |
| stats_o/std                    | 0.036222633 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0342     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.88322634 |
| test/Q_plus_P                  | -0.88322634 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.44197023 |
| stats_o/std                    | 0.0362097  |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000446  |
| test/info_shaping_reward_mean  | -0.0366    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -0.9979713 |
| test/Q_plus_P                  | -0.9979713 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00291   |
| train/info_shaping_reward_mean | -0.0594    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.44197342  |
| stats_o/std                    | 0.036196657 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000697   |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.9198952  |
| test/Q_plus_P                  | -0.9198952  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.595       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.44197598  |
| stats_o/std                    | 0.036183555 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0347     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.015353   |
| test/Q_plus_P                  | -1.015353   |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.44197986  |
| stats_o/std                    | 0.036170293 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00095    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.2763153  |
| test/Q_plus_P                  | -1.2763153  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00212    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 516        |
| stats_o/mean                   | 0.44198373 |
| stats_o/std                    | 0.03615753 |
| test/episodes                  | 5170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000867  |
| test/info_shaping_reward_mean  | -0.0365    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -0.961891  |
| test/Q_plus_P                  | -0.961891  |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 206800     |
| train/episodes                 | 20680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.549      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00236   |
| train/info_shaping_reward_mean | -0.0662    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18        |
| train/steps                    | 827200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.44198653  |
| stats_o/std                    | 0.036143757 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.8771458  |
| test/Q_plus_P                  | -0.8771458  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00226    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.44198838  |
| stats_o/std                    | 0.036130957 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0348     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.9189467  |
| test/Q_plus_P                  | -0.9189467  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.4419929   |
| stats_o/std                    | 0.03611752  |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.95844024 |
| test/Q_plus_P                  | -0.95844024 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.44199646  |
| stats_o/std                    | 0.03610621  |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0332     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.94424284 |
| test/Q_plus_P                  | -0.94424284 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.44199812 |
| stats_o/std                    | 0.03609495 |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0342    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.9402388 |
| test/Q_plus_P                  | -0.9402388 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00244   |
| train/info_shaping_reward_mean | -0.0552    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.44199982  |
| stats_o/std                    | 0.0360832   |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00355    |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.91731286 |
| test/Q_plus_P                  | -0.91731286 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.44200405  |
| stats_o/std                    | 0.03607209  |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00361    |
| test/info_shaping_reward_mean  | -0.0316     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.86664253 |
| test/Q_plus_P                  | -0.86664253 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.44200668  |
| stats_o/std                    | 0.036058713 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00209    |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.8679941  |
| test/Q_plus_P                  | -0.8679941  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.44200972  |
| stats_o/std                    | 0.036043726 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.98161936 |
| test/Q_plus_P                  | -0.98161936 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.44201353  |
| stats_o/std                    | 0.036030024 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.9039879  |
| test/Q_plus_P                  | -0.9039879  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.442017    |
| stats_o/std                    | 0.0360155   |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.81176466 |
| test/Q_plus_P                  | -0.81176466 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.44202134  |
| stats_o/std                    | 0.036003932 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -0.83351046 |
| test/Q_plus_P                  | -0.83351046 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.4420255   |
| stats_o/std                    | 0.035988938 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000819   |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.8522089  |
| test/Q_plus_P                  | -0.8522089  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.44202912  |
| stats_o/std                    | 0.035976578 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00234    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.9437481  |
| test/Q_plus_P                  | -0.9437481  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.4420354   |
| stats_o/std                    | 0.035991345 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.0606006  |
| test/Q_plus_P                  | -1.0606006  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.533       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0108     |
| train/info_shaping_reward_mean | -0.0912     |
| train/info_shaping_reward_min  | -0.279      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.44203874  |
| stats_o/std                    | 0.035979286 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.90575737 |
| test/Q_plus_P                  | -0.90575737 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.44205198  |
| stats_o/std                    | 0.035975073 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00457    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.2797873  |
| test/Q_plus_P                  | -1.2797873  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.355       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00535    |
| train/info_shaping_reward_mean | -0.0904     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.8       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.4420577  |
| stats_o/std                    | 0.03596384 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00682   |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.2556684 |
| test/Q_plus_P                  | -1.2556684 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.526      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00435   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.4420607  |
| stats_o/std                    | 0.03595064 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.1147951 |
| test/Q_plus_P                  | -1.1147951 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.574      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.44206467  |
| stats_o/std                    | 0.035939675 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -1.0914053  |
| test/Q_plus_P                  | -1.0914053  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.527       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.44206774 |
| stats_o/std                    | 0.0359263  |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.0394    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -1.1602207 |
| test/Q_plus_P                  | -1.1602207 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00362   |
| train/info_shaping_reward_mean | -0.0624    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.4420706   |
| stats_o/std                    | 0.035913136 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.1304988  |
| test/Q_plus_P                  | -1.1304988  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.4420737  |
| stats_o/std                    | 0.03590132 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000944  |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.2820003 |
| test/Q_plus_P                  | -1.2820003 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00294   |
| train/info_shaping_reward_mean | -0.0652    |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.442078    |
| stats_o/std                    | 0.035886887 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000842   |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.1165053  |
| test/Q_plus_P                  | -1.1165053  |
| test/reward_per_eps            | -8          |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.4420824   |
| stats_o/std                    | 0.035874393 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.1350105  |
| test/Q_plus_P                  | -1.1350105  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.44208583  |
| stats_o/std                    | 0.035860732 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.0925709  |
| test/Q_plus_P                  | -1.0925709  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.44209078 |
| stats_o/std                    | 0.03584755 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0402    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -1.0678333 |
| test/Q_plus_P                  | -1.0678333 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.003     |
| train/info_shaping_reward_mean | -0.0615    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.44209352  |
| stats_o/std                    | 0.035835654 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0128     |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.0302453  |
| test/Q_plus_P                  | -1.0302453  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.44209695  |
| stats_o/std                    | 0.035823632 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00263    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.0881058  |
| test/Q_plus_P                  | -1.0881058  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.4420999   |
| stats_o/std                    | 0.035810787 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00435    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0844188  |
| test/Q_plus_P                  | -1.0844188  |
| test/reward_per_eps            | -8          |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.44210157  |
| stats_o/std                    | 0.035799522 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.948956   |
| test/Q_plus_P                  | -0.948956   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.44210663  |
| stats_o/std                    | 0.035786815 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00835    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.2416741  |
| test/Q_plus_P                  | -1.2416741  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.44211105  |
| stats_o/std                    | 0.03577386  |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0161     |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.94736254 |
| test/Q_plus_P                  | -0.94736254 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.44211534  |
| stats_o/std                    | 0.035760075 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00741    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.956712   |
| test/Q_plus_P                  | -0.956712   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.4421188   |
| stats_o/std                    | 0.035746787 |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000156   |
| test/info_shaping_reward_mean  | -0.0315     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.91851676 |
| test/Q_plus_P                  | -0.91851676 |
| test/reward_per_eps            | -7          |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.442122    |
| stats_o/std                    | 0.035735726 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00371    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.93450975 |
| test/Q_plus_P                  | -0.93450975 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.44212627  |
| stats_o/std                    | 0.035721306 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.98799676 |
| test/Q_plus_P                  | -0.98799676 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.44212925  |
| stats_o/std                    | 0.035709668 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.1192397  |
| test/Q_plus_P                  | -1.1192397  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.44213262  |
| stats_o/std                    | 0.035695206 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.9510514  |
| test/Q_plus_P                  | -0.9510514  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.4421351   |
| stats_o/std                    | 0.035682827 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.8665222  |
| test/Q_plus_P                  | -0.8665222  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.4421394   |
| stats_o/std                    | 0.03567102  |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.85767317 |
| test/Q_plus_P                  | -0.85767317 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.44214416  |
| stats_o/std                    | 0.035659853 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00334    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.973357   |
| test/Q_plus_P                  | -0.973357   |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.44214857  |
| stats_o/std                    | 0.035646915 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000693   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.9434337  |
| test/Q_plus_P                  | -0.9434337  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.44215122  |
| stats_o/std                    | 0.035635035 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.82884467 |
| test/Q_plus_P                  | -0.82884467 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.4421536  |
| stats_o/std                    | 0.03562146 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000913  |
| test/info_shaping_reward_mean  | -0.0334    |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -0.7793057 |
| test/Q_plus_P                  | -0.7793057 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00315   |
| train/info_shaping_reward_mean | -0.0575    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.4421581   |
| stats_o/std                    | 0.03560893  |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0342     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -0.80202776 |
| test/Q_plus_P                  | -0.80202776 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.4421595   |
| stats_o/std                    | 0.035597634 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.932558   |
| test/Q_plus_P                  | -0.932558   |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 564        |
| stats_o/mean                   | 0.4421623  |
| stats_o/std                    | 0.03558438 |
| test/episodes                  | 5650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00223   |
| test/info_shaping_reward_mean  | -0.0363    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -1.0062526 |
| test/Q_plus_P                  | -1.0062526 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 226000     |
| train/episodes                 | 22600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00272   |
| train/info_shaping_reward_mean | -0.0526    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 904000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.4421655   |
| stats_o/std                    | 0.035572052 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.8554515  |
| test/Q_plus_P                  | -0.8554515  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.4421688   |
| stats_o/std                    | 0.035561338 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000957   |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.8311319  |
| test/Q_plus_P                  | -0.8311319  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.44217107  |
| stats_o/std                    | 0.035548422 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000641   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.94398314 |
| test/Q_plus_P                  | -0.94398314 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0024     |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.44217384  |
| stats_o/std                    | 0.035536226 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00031    |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.89098567 |
| test/Q_plus_P                  | -0.89098567 |
| test/reward_per_eps            | -7          |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.4421755   |
| stats_o/std                    | 0.0355241   |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000995   |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.96401215 |
| test/Q_plus_P                  | -0.96401215 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 570        |
| stats_o/mean                   | 0.44217744 |
| stats_o/std                    | 0.03551133 |
| test/episodes                  | 5710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0845    |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -5.9351025 |
| test/Q_plus_P                  | -5.9351025 |
| test/reward_per_eps            | -40        |
| test/steps                     | 228400     |
| train/episodes                 | 22840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00295   |
| train/info_shaping_reward_mean | -0.0563    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 913600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.4422041   |
| stats_o/std                    | 0.035518166 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.2365552  |
| test/Q_plus_P                  | -1.2365552  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.247       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0306     |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.1       |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.4422075  |
| stats_o/std                    | 0.03550562 |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0406    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -1.129313  |
| test/Q_plus_P                  | -1.129313  |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.517      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.4422123   |
| stats_o/std                    | 0.035492454 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000686   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0835404  |
| test/Q_plus_P                  | -1.0835404  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 574        |
| stats_o/mean                   | 0.4422158  |
| stats_o/std                    | 0.03548031 |
| test/episodes                  | 5750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -1.0987034 |
| test/Q_plus_P                  | -1.0987034 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 230000     |
| train/episodes                 | 23000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.0575    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 920000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.44221905  |
| stats_o/std                    | 0.035469864 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00042    |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.9530917  |
| test/Q_plus_P                  | -0.9530917  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00253    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.44222245 |
| stats_o/std                    | 0.03546047 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00291   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.1078496 |
| test/Q_plus_P                  | -1.1078496 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00418   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.44222513  |
| stats_o/std                    | 0.035448633 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.0206598  |
| test/Q_plus_P                  | -1.0206598  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.44222733 |
| stats_o/std                    | 0.03543709 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000799  |
| test/info_shaping_reward_mean  | -0.0356    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.0156233 |
| test/Q_plus_P                  | -1.0156233 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.4422299   |
| stats_o/std                    | 0.035427053 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.0775968  |
| test/Q_plus_P                  | -1.0775968  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.44223097 |
| stats_o/std                    | 0.03541597 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000996  |
| test/info_shaping_reward_mean  | -0.0372    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -0.9964957 |
| test/Q_plus_P                  | -0.9964957 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00286   |
| train/info_shaping_reward_mean | -0.0559    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.44223246  |
| stats_o/std                    | 0.035403863 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.92061806 |
| test/Q_plus_P                  | -0.92061806 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00203    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.44223347  |
| stats_o/std                    | 0.035393078 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.91629404 |
| test/Q_plus_P                  | -0.91629404 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.44223627  |
| stats_o/std                    | 0.035380784 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -0.8059581  |
| test/Q_plus_P                  | -0.8059581  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.44223905  |
| stats_o/std                    | 0.035369117 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0343     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.9488932  |
| test/Q_plus_P                  | -0.9488932  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 585        |
| stats_o/mean                   | 0.44223967 |
| stats_o/std                    | 0.03535952 |
| test/episodes                  | 5860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00063   |
| test/info_shaping_reward_mean  | -0.0367    |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -1.0387499 |
| test/Q_plus_P                  | -1.0387499 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 234400     |
| train/episodes                 | 23440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0561    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 937600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.44224155  |
| stats_o/std                    | 0.035350014 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000909   |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.84233063 |
| test/Q_plus_P                  | -0.84233063 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.44224468  |
| stats_o/std                    | 0.035340805 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.93670464 |
| test/Q_plus_P                  | -0.93670464 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.44224632  |
| stats_o/std                    | 0.035333097 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00099    |
| test/info_shaping_reward_mean  | -0.0334     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.8409347  |
| test/Q_plus_P                  | -0.8409347  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.4422506   |
| stats_o/std                    | 0.035322633 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.96594423 |
| test/Q_plus_P                  | -0.96594423 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.44225502  |
| stats_o/std                    | 0.035312258 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00319    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.82204145 |
| test/Q_plus_P                  | -0.82204145 |
| test/reward_per_eps            | -7          |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.44225702  |
| stats_o/std                    | 0.035303365 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0347     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.9133663  |
| test/Q_plus_P                  | -0.9133663  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.44226     |
| stats_o/std                    | 0.035292897 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000711   |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.2903011  |
| test/Q_plus_P                  | -1.2903011  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.44226286  |
| stats_o/std                    | 0.035280738 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00206    |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.92251885 |
| test/Q_plus_P                  | -0.92251885 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.44226468  |
| stats_o/std                    | 0.035270955 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.0006062  |
| test/Q_plus_P                  | -1.0006062  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.44226786  |
| stats_o/std                    | 0.035258338 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.3273636  |
| test/Q_plus_P                  | -1.3273636  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.4422692   |
| stats_o/std                    | 0.035247073 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0899825  |
| test/Q_plus_P                  | -1.0899825  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.44227186  |
| stats_o/std                    | 0.035237152 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000694   |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.0700421  |
| test/Q_plus_P                  | -1.0700421  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.44227603  |
| stats_o/std                    | 0.035225783 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000833   |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -1.0868154  |
| test/Q_plus_P                  | -1.0868154  |
| test/reward_per_eps            | -8          |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.44227707  |
| stats_o/std                    | 0.035215218 |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00469    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.9510759  |
| test/Q_plus_P                  | -0.9510759  |
| test/reward_per_eps            | -8          |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
