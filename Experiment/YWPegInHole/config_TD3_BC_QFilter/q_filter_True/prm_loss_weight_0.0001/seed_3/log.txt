Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC_QFilter/q_filter_True/prm_loss_weight_0.0001/seed_3
-----------------------------------------------
| epoch                          | 0          |
| stats_o/mean                   | 0.44562423 |
| stats_o/std                    | 0.04981302 |
| test/episodes                  | 10         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.1       |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -1.2976685 |
| test/Q_plus_P                  | -1.2976685 |
| test/reward_per_eps            | -40        |
| test/steps                     | 400        |
| train/episodes                 | 40         |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0925    |
| train/info_shaping_reward_mean | -0.203     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 1600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.44499925  |
| stats_o/std                    | 0.049181532 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0557     |
| test/info_shaping_reward_mean  | -0.133      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.601113   |
| test/Q_plus_P                  | -1.601113   |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0812     |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.44462562  |
| stats_o/std                    | 0.048745837 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0581     |
| test/info_shaping_reward_mean  | -0.125      |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -2.0173035  |
| test/Q_plus_P                  | -2.0173035  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.076      |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.44509563 |
| stats_o/std                    | 0.04723993 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.076     |
| test/info_shaping_reward_mean  | -0.141     |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -2.3451657 |
| test/Q_plus_P                  | -2.3451657 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0238     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0564    |
| train/info_shaping_reward_mean | -0.144     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.44457042 |
| stats_o/std                    | 0.04547089 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00277   |
| test/info_shaping_reward_mean  | -0.0983    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -2.6499298 |
| test/Q_plus_P                  | -2.6499298 |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0582    |
| train/info_shaping_reward_mean | -0.12      |
| train/info_shaping_reward_min  | -0.265     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.44505048  |
| stats_o/std                    | 0.043860387 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.117       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00239    |
| test/info_shaping_reward_mean  | -0.0945     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.9676385  |
| test/Q_plus_P                  | -2.9676385  |
| test/reward_per_eps            | -35.3       |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0181      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0484     |
| train/info_shaping_reward_mean | -0.11       |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.44583097  |
| stats_o/std                    | 0.043088287 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0875      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00407    |
| test/info_shaping_reward_mean  | -0.106      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -3.4272354  |
| test/Q_plus_P                  | -3.4272354  |
| test/reward_per_eps            | -36.5       |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00562     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0519     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.44632533  |
| stats_o/std                    | 0.042093147 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.182       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0832     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -3.5155191  |
| test/Q_plus_P                  | -3.5155191  |
| test/reward_per_eps            | -32.7       |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0194      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0556     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.44665363  |
| stats_o/std                    | 0.041315753 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.193       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00299    |
| test/info_shaping_reward_mean  | -0.107      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -3.91101    |
| test/Q_plus_P                  | -3.91101    |
| test/reward_per_eps            | -32.3       |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0595     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.44657466 |
| stats_o/std                    | 0.04045699 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0925     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.101     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -4.505248  |
| test/Q_plus_P                  | -4.505248  |
| test/reward_per_eps            | -36.3      |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0319     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.043     |
| train/info_shaping_reward_mean | -0.102     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.44654974  |
| stats_o/std                    | 0.039834607 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0859     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -4.9578056  |
| test/Q_plus_P                  | -4.9578056  |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0566     |
| train/info_shaping_reward_mean | -0.104      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.44644442  |
| stats_o/std                    | 0.039256815 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.172       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0804     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -4.8220563  |
| test/Q_plus_P                  | -4.8220563  |
| test/reward_per_eps            | -33.1       |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.01        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0513     |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.44640073  |
| stats_o/std                    | 0.038734958 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.065       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00547    |
| test/info_shaping_reward_mean  | -0.0849     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -5.711082   |
| test/Q_plus_P                  | -5.711082   |
| test/reward_per_eps            | -37.4       |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0415     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.44658992  |
| stats_o/std                    | 0.038382623 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00314    |
| test/info_shaping_reward_mean  | -0.0868     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -6.068449   |
| test/Q_plus_P                  | -6.068449   |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0212      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0438     |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.44669476 |
| stats_o/std                    | 0.0381206  |
| test/episodes                  | 150        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.105      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00562   |
| test/info_shaping_reward_mean  | -0.0836    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -6.1425886 |
| test/Q_plus_P                  | -6.1425886 |
| test/reward_per_eps            | -35.8      |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.0325     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0419    |
| train/info_shaping_reward_mean | -0.116     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.7      |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.44697025 |
| stats_o/std                    | 0.03788963 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0699    |
| test/info_shaping_reward_mean  | -0.0898    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -7.1670403 |
| test/Q_plus_P                  | -7.1670403 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0532    |
| train/info_shaping_reward_mean | -0.114     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.446952    |
| stats_o/std                    | 0.037580885 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.122       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0126     |
| test/info_shaping_reward_mean  | -0.0859     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -6.6301403  |
| test/Q_plus_P                  | -6.6301403  |
| test/reward_per_eps            | -35.1       |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0419      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0406     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.44705462  |
| stats_o/std                    | 0.037407156 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.185       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.011      |
| test/info_shaping_reward_mean  | -0.0824     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -6.6740966  |
| test/Q_plus_P                  | -6.6740966  |
| test/reward_per_eps            | -32.6       |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.015       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0508     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.44692764  |
| stats_o/std                    | 0.037189554 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.115       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00307    |
| test/info_shaping_reward_mean  | -0.0828     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -7.4053164  |
| test/Q_plus_P                  | -7.4053164  |
| test/reward_per_eps            | -35.4       |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0312      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0425     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.44688198 |
| stats_o/std                    | 0.03695744 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.198      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00777   |
| test/info_shaping_reward_mean  | -0.0763    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -7.0174513 |
| test/Q_plus_P                  | -7.0174513 |
| test/reward_per_eps            | -32.1      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.5        |
| train/info_is_success_mean     | 0.0456     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0444    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.2      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.44698575  |
| stats_o/std                    | 0.036735438 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0682     |
| test/info_shaping_reward_mean  | -0.0911     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -8.798138   |
| test/Q_plus_P                  | -8.798138   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0506     |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.4469365   |
| stats_o/std                    | 0.036482606 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0681     |
| test/info_shaping_reward_mean  | -0.108      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -9.230882   |
| test/Q_plus_P                  | -9.230882   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0312      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0426     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.8       |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.44690678  |
| stats_o/std                    | 0.036217082 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.185       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00529    |
| test/info_shaping_reward_mean  | -0.076      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -7.524358   |
| test/Q_plus_P                  | -7.524358   |
| test/reward_per_eps            | -32.6       |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0831      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0254     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.7       |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.44696417 |
| stats_o/std                    | 0.03603746 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.175      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00462   |
| test/info_shaping_reward_mean  | -0.0819    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -8.354705  |
| test/Q_plus_P                  | -8.354705  |
| test/reward_per_eps            | -33        |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0212     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.054     |
| train/info_shaping_reward_mean | -0.113     |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.1      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.44701803 |
| stats_o/std                    | 0.03582949 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.055      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00806   |
| test/info_shaping_reward_mean  | -0.0866    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -9.839708  |
| test/Q_plus_P                  | -9.839708  |
| test/reward_per_eps            | -37.8      |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.0706     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0268    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.2      |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.44700086  |
| stats_o/std                    | 0.035580304 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.195       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.094      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -8.056369   |
| test/Q_plus_P                  | -8.056369   |
| test/reward_per_eps            | -32.2       |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0725      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0278     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.1       |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.44700897 |
| stats_o/std                    | 0.0354049  |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.403      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.0722    |
| test/info_shaping_reward_min   | -0.215     |
| test/Q                         | -6.4472914 |
| test/Q_plus_P                  | -6.4472914 |
| test/reward_per_eps            | -23.9      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0431     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0326    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.3      |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.4469926   |
| stats_o/std                    | 0.035205003 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.312       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0755     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -7.7645397  |
| test/Q_plus_P                  | -7.7645397  |
| test/reward_per_eps            | -27.5       |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0444      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0373     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.2       |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.44700518  |
| stats_o/std                    | 0.034997556 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.36        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00379    |
| test/info_shaping_reward_mean  | -0.0621     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -7.1311364  |
| test/Q_plus_P                  | -7.1311364  |
| test/reward_per_eps            | -25.6       |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0413      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0352     |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.4       |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.44703743  |
| stats_o/std                    | 0.034861926 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.24        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00215    |
| test/info_shaping_reward_mean  | -0.0679     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -8.592838   |
| test/Q_plus_P                  | -8.592838   |
| test/reward_per_eps            | -30.4       |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0725      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0248     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.1       |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.44705328  |
| stats_o/std                    | 0.034725044 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.255       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00555    |
| test/info_shaping_reward_mean  | -0.0777     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -7.6012406  |
| test/Q_plus_P                  | -7.6012406  |
| test/reward_per_eps            | -29.8       |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0619      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0289     |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.5       |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.44707736  |
| stats_o/std                    | 0.034583863 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.235       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00419    |
| test/info_shaping_reward_mean  | -0.0728     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -8.161957   |
| test/Q_plus_P                  | -8.161957   |
| test/reward_per_eps            | -30.6       |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.0844      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0154     |
| train/info_shaping_reward_mean | -0.0983     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.6       |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 32         |
| stats_o/mean                   | 0.4470997  |
| stats_o/std                    | 0.03444089 |
| test/episodes                  | 330        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.28       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00188   |
| test/info_shaping_reward_mean  | -0.0697    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -7.7878566 |
| test/Q_plus_P                  | -7.7878566 |
| test/reward_per_eps            | -28.8      |
| test/steps                     | 13200      |
| train/episodes                 | 1320       |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0581     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0387    |
| train/info_shaping_reward_mean | -0.101     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.7      |
| train/steps                    | 52800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.4470644  |
| stats_o/std                    | 0.03427618 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.255      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00142   |
| test/info_shaping_reward_mean  | -0.0729    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -8.844625  |
| test/Q_plus_P                  | -8.844625  |
| test/reward_per_eps            | -29.8      |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.108      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0218    |
| train/info_shaping_reward_mean | -0.0919    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.7      |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.4470574   |
| stats_o/std                    | 0.034162182 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.385       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00379    |
| test/info_shaping_reward_mean  | -0.0682     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -6.9492283  |
| test/Q_plus_P                  | -6.9492283  |
| test/reward_per_eps            | -24.6       |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.133       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0118     |
| train/info_shaping_reward_mean | -0.094      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34.7       |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.44704643 |
| stats_o/std                    | 0.03400522 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.56       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -5.0461802 |
| test/Q_plus_P                  | -5.0461802 |
| test/reward_per_eps            | -17.6      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.157      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00921   |
| train/info_shaping_reward_mean | -0.0881    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -33.7      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.44710967 |
| stats_o/std                    | 0.03386993 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.417      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0635    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -7.156145  |
| test/Q_plus_P                  | -7.156145  |
| test/reward_per_eps            | -23.3      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.13       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0148    |
| train/info_shaping_reward_mean | -0.094     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.8      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.4471002   |
| stats_o/std                    | 0.033764437 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.455       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -7.012798   |
| test/Q_plus_P                  | -7.012798   |
| test/reward_per_eps            | -21.8       |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.188       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00621    |
| train/info_shaping_reward_mean | -0.087      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.5       |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.44706556  |
| stats_o/std                    | 0.033698846 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.485       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00305    |
| test/info_shaping_reward_mean  | -0.0594     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -6.752859   |
| test/Q_plus_P                  | -6.752859   |
| test/reward_per_eps            | -20.6       |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.208       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0874     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.7       |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 39         |
| stats_o/mean                   | 0.44711164 |
| stats_o/std                    | 0.03358582 |
| test/episodes                  | 400        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.525      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -5.9829254 |
| test/Q_plus_P                  | -5.9829254 |
| test/reward_per_eps            | -19        |
| test/steps                     | 16000      |
| train/episodes                 | 1600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.21       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00742   |
| train/info_shaping_reward_mean | -0.0879    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.6      |
| train/steps                    | 64000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.44712856  |
| stats_o/std                    | 0.033476602 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -5.875213   |
| test/Q_plus_P                  | -5.875213   |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.212       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0845     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.5       |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 41         |
| stats_o/mean                   | 0.4471394  |
| stats_o/std                    | 0.03334794 |
| test/episodes                  | 420        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00153   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.205     |
| test/Q                         | -4.8650827 |
| test/Q_plus_P                  | -4.8650827 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 16800      |
| train/episodes                 | 1680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.209      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00378   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.6      |
| train/steps                    | 67200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 42         |
| stats_o/mean                   | 0.4471537  |
| stats_o/std                    | 0.03324434 |
| test/episodes                  | 430        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00302   |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -4.6064596 |
| test/Q_plus_P                  | -4.6064596 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 17200      |
| train/episodes                 | 1720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.356      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00356   |
| train/info_shaping_reward_mean | -0.0763    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 68800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.44716224 |
| stats_o/std                    | 0.03315371 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.367      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0652    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -7.356554  |
| test/Q_plus_P                  | -7.356554  |
| test/reward_per_eps            | -25.3      |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.226      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00774   |
| train/info_shaping_reward_mean | -0.085     |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -30.9      |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.44718614  |
| stats_o/std                    | 0.033044454 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -4.228666   |
| test/Q_plus_P                  | -4.228666   |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.287       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0792     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.5       |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.44717088  |
| stats_o/std                    | 0.032956213 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -3.8359709  |
| test/Q_plus_P                  | -3.8359709  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.378       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0725     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.9       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.44716415 |
| stats_o/std                    | 0.03287667 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000718  |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -3.962335  |
| test/Q_plus_P                  | -3.962335  |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.331      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.077     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.8      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.44716963  |
| stats_o/std                    | 0.032788362 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.565       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -4.9738607  |
| test/Q_plus_P                  | -4.9738607  |
| test/reward_per_eps            | -17.4       |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.376       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0753     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25         |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.44720712  |
| stats_o/std                    | 0.032718193 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.39        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0671     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -6.3026648  |
| test/Q_plus_P                  | -6.3026648  |
| test/reward_per_eps            | -24.4       |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.266       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.4       |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.44722906  |
| stats_o/std                    | 0.032649588 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.63        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000941   |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -3.7465928  |
| test/Q_plus_P                  | -3.7465928  |
| test/reward_per_eps            | -14.8       |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.294       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0772     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -28.2       |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.4472864  |
| stats_o/std                    | 0.03261527 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.517      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -5.2173576 |
| test/Q_plus_P                  | -5.2173576 |
| test/reward_per_eps            | -19.3      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.223      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0171    |
| train/info_shaping_reward_mean | -0.0848    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.1      |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.4473319   |
| stats_o/std                    | 0.032559745 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.482       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -4.6718526  |
| test/Q_plus_P                  | -4.6718526  |
| test/reward_per_eps            | -20.7       |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.333       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0746     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.7       |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.4473782  |
| stats_o/std                    | 0.03250034 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.42       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00236   |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -5.28221   |
| test/Q_plus_P                  | -5.28221   |
| test/reward_per_eps            | -23.2      |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.264      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00397   |
| train/info_shaping_reward_mean | -0.0831    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.4      |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 53         |
| stats_o/mean                   | 0.4474661  |
| stats_o/std                    | 0.03244521 |
| test/episodes                  | 540        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -3.552159  |
| test/Q_plus_P                  | -3.552159  |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 21600      |
| train/episodes                 | 2160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.263      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0821    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -29.5      |
| train/steps                    | 86400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.44750082  |
| stats_o/std                    | 0.032395244 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000472   |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -3.5162005  |
| test/Q_plus_P                  | -3.5162005  |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.374       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0757     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.1       |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.44752836 |
| stats_o/std                    | 0.03234837 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.001     |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -3.335132  |
| test/Q_plus_P                  | -3.335132  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.371      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.1      |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.4475473   |
| stats_o/std                    | 0.032284167 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.545       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -4.043989   |
| test/Q_plus_P                  | -4.043989   |
| test/reward_per_eps            | -18.2       |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.389       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.4       |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 57         |
| stats_o/mean                   | 0.4475639  |
| stats_o/std                    | 0.0322272  |
| test/episodes                  | 580        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000423  |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -3.1946228 |
| test/Q_plus_P                  | -3.1946228 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 23200      |
| train/episodes                 | 2320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.316      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00745   |
| train/info_shaping_reward_mean | -0.0781    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -27.4      |
| train/steps                    | 92800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.44756243  |
| stats_o/std                    | 0.032173768 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.55        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00285    |
| test/info_shaping_reward_mean  | -0.0618     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -3.8263352  |
| test/Q_plus_P                  | -3.8263352  |
| test/reward_per_eps            | -18         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.41        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.6       |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 59         |
| stats_o/mean                   | 0.44757298 |
| stats_o/std                    | 0.03211658 |
| test/episodes                  | 600        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.693      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00378   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -3.003009  |
| test/Q_plus_P                  | -3.003009  |
| test/reward_per_eps            | -12.3      |
| test/steps                     | 24000      |
| train/episodes                 | 2400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.332      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0762    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.7      |
| train/steps                    | 96000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.44757974 |
| stats_o/std                    | 0.03206284 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.682      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00289   |
| test/info_shaping_reward_mean  | -0.0421    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -3.1497226 |
| test/Q_plus_P                  | -3.1497226 |
| test/reward_per_eps            | -12.7      |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.393      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0034    |
| train/info_shaping_reward_mean | -0.0729    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.3      |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.44758913  |
| stats_o/std                    | 0.031998035 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -3.0405328  |
| test/Q_plus_P                  | -3.0405328  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.377       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00439    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.9       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.4475825  |
| stats_o/std                    | 0.03195807 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00319   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -3.0485861 |
| test/Q_plus_P                  | -3.0485861 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.424      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00312   |
| train/info_shaping_reward_mean | -0.0737    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.1      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.4475863   |
| stats_o/std                    | 0.031907786 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -3.003154   |
| test/Q_plus_P                  | -3.003154   |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.359       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0771     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.6       |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.447574    |
| stats_o/std                    | 0.031861678 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -3.267516   |
| test/Q_plus_P                  | -3.267516   |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.428       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0697     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.44755003  |
| stats_o/std                    | 0.031810842 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.458       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -3.9124126  |
| test/Q_plus_P                  | -3.9124126  |
| test/reward_per_eps            | -21.7       |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.467       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.3       |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.44754806  |
| stats_o/std                    | 0.031773705 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -2.8968241  |
| test/Q_plus_P                  | -2.8968241  |
| test/reward_per_eps            | -12         |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.329       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00729    |
| train/info_shaping_reward_mean | -0.0777     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.9       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.4475316  |
| stats_o/std                    | 0.03174818 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00134   |
| test/info_shaping_reward_mean  | -0.0393    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -2.750966  |
| test/Q_plus_P                  | -2.750966  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.411      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00359   |
| train/info_shaping_reward_mean | -0.0734    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.6      |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.4475071   |
| stats_o/std                    | 0.031705648 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000969   |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -2.8951952  |
| test/Q_plus_P                  | -2.8951952  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.4474796   |
| stats_o/std                    | 0.031684253 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0396     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.6896334  |
| test/Q_plus_P                  | -2.6896334  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.483       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.44743037 |
| stats_o/std                    | 0.03166693 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00106   |
| test/info_shaping_reward_mean  | -0.0366    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -2.4409065 |
| test/Q_plus_P                  | -2.4409065 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.4475321   |
| stats_o/std                    | 0.031717665 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0017     |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -2.9227831  |
| test/Q_plus_P                  | -2.9227831  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.261       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0261     |
| train/info_shaping_reward_mean | -0.099      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.6       |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 72         |
| stats_o/mean                   | 0.44752622 |
| stats_o/std                    | 0.03167853 |
| test/episodes                  | 730        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -2.8260763 |
| test/Q_plus_P                  | -2.8260763 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 29200      |
| train/episodes                 | 2920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.444      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00324   |
| train/info_shaping_reward_mean | -0.0698    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -22.2      |
| train/steps                    | 116800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 73         |
| stats_o/mean                   | 0.44750968 |
| stats_o/std                    | 0.03164069 |
| test/episodes                  | 740        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -3.0920575 |
| test/Q_plus_P                  | -3.0920575 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 29600      |
| train/episodes                 | 2960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.497      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00324   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.1      |
| train/steps                    | 118400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.4474899   |
| stats_o/std                    | 0.031614576 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000796   |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -2.5990815  |
| test/Q_plus_P                  | -2.5990815  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.474       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00207    |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.44746175  |
| stats_o/std                    | 0.031594444 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000417   |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -2.89078    |
| test/Q_plus_P                  | -2.89078    |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.423       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0718     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.1       |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.44744256  |
| stats_o/std                    | 0.031547625 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -3.2228642  |
| test/Q_plus_P                  | -3.2228642  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.487       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.5       |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.44741583  |
| stats_o/std                    | 0.031531867 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -2.8540504  |
| test/Q_plus_P                  | -2.8540504  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.459       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0704     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.44741964  |
| stats_o/std                    | 0.031511396 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -2.738608   |
| test/Q_plus_P                  | -2.738608   |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.427       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.073      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.44741234 |
| stats_o/std                    | 0.03147947 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0389    |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -2.543144  |
| test/Q_plus_P                  | -2.543144  |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.482      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00264   |
| train/info_shaping_reward_mean | -0.0671    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.4474182   |
| stats_o/std                    | 0.031461302 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0367     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -2.4063416  |
| test/Q_plus_P                  | -2.4063416  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.444       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.2       |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.44742104 |
| stats_o/std                    | 0.0314286  |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -2.6876707 |
| test/Q_plus_P                  | -2.6876707 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.414      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00265   |
| train/info_shaping_reward_mean | -0.0718    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.4      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.4474195   |
| stats_o/std                    | 0.031396016 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -2.9539938  |
| test/Q_plus_P                  | -2.9539938  |
| test/reward_per_eps            | -14         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.412       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0697     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.5       |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.44740906  |
| stats_o/std                    | 0.031371616 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -2.570393   |
| test/Q_plus_P                  | -2.570393   |
| test/reward_per_eps            | -12         |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.504       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0671     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.4473933   |
| stats_o/std                    | 0.031351488 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -2.6853814  |
| test/Q_plus_P                  | -2.6853814  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.467       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0692     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.3       |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.4473838   |
| stats_o/std                    | 0.031328786 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -2.3952408  |
| test/Q_plus_P                  | -2.3952408  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.437       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.5       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.44736278  |
| stats_o/std                    | 0.031302024 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00209    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -2.3786917  |
| test/Q_plus_P                  | -2.3786917  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.512       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.44735238 |
| stats_o/std                    | 0.03128381 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000604  |
| test/info_shaping_reward_mean  | -0.0381    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -2.3699431 |
| test/Q_plus_P                  | -2.3699431 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.485      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00262   |
| train/info_shaping_reward_mean | -0.0672    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.6      |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.4473293   |
| stats_o/std                    | 0.031268056 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -2.4433732  |
| test/Q_plus_P                  | -2.4433732  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.472       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.44730914  |
| stats_o/std                    | 0.031246854 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.102       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00294    |
| test/info_shaping_reward_mean  | -0.0789     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -4.4572716  |
| test/Q_plus_P                  | -4.4572716  |
| test/reward_per_eps            | -35.9       |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.506       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00905    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.4472997   |
| stats_o/std                    | 0.031221734 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -2.5206966  |
| test/Q_plus_P                  | -2.5206966  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.489       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.4472868   |
| stats_o/std                    | 0.031187857 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000759   |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -2.2517402  |
| test/Q_plus_P                  | -2.2517402  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.44727457  |
| stats_o/std                    | 0.031160334 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -2.353148   |
| test/Q_plus_P                  | -2.353148   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.44728574  |
| stats_o/std                    | 0.031158546 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -2.5199134  |
| test/Q_plus_P                  | -2.5199134  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.348       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00866    |
| train/info_shaping_reward_mean | -0.0767     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.1       |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.4472777   |
| stats_o/std                    | 0.031149378 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.5701716  |
| test/Q_plus_P                  | -2.5701716  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.504       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.44725838  |
| stats_o/std                    | 0.031122766 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -2.1695597  |
| test/Q_plus_P                  | -2.1695597  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.526       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.4472456   |
| stats_o/std                    | 0.031116223 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -2.2718263  |
| test/Q_plus_P                  | -2.2718263  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.441       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0732     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.4       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.44724926  |
| stats_o/std                    | 0.031095117 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000927   |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -2.233899   |
| test/Q_plus_P                  | -2.233899   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.457       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.44724604  |
| stats_o/std                    | 0.031056235 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000781   |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -2.2617874  |
| test/Q_plus_P                  | -2.2617874  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.529       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.44723225 |
| stats_o/std                    | 0.03104985 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0011    |
| test/info_shaping_reward_mean  | -0.0377    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -2.0661147 |
| test/Q_plus_P                  | -2.0661147 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00198   |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.4472246   |
| stats_o/std                    | 0.031037102 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000827   |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -2.0052185  |
| test/Q_plus_P                  | -2.0052185  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.501       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.44721338  |
| stats_o/std                    | 0.031011736 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000973   |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -2.0387425  |
| test/Q_plus_P                  | -2.0387425  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.499       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.4472047   |
| stats_o/std                    | 0.031000515 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.9519708  |
| test/Q_plus_P                  | -1.9519708  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.527       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.44718802  |
| stats_o/std                    | 0.030978965 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.9749081  |
| test/Q_plus_P                  | -1.9749081  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.44717383 |
| stats_o/std                    | 0.03095751 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000445  |
| test/info_shaping_reward_mean  | -0.0379    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.987702  |
| test/Q_plus_P                  | -1.987702  |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00185   |
| train/info_shaping_reward_mean | -0.0565    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.44716743  |
| stats_o/std                    | 0.030941227 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.9252926  |
| test/Q_plus_P                  | -1.9252926  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.478       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0085     |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.44716987  |
| stats_o/std                    | 0.030928897 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000552   |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.2543154  |
| test/Q_plus_P                  | -2.2543154  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.523       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00197    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.44714844  |
| stats_o/std                    | 0.030916892 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -2.0017672  |
| test/Q_plus_P                  | -2.0017672  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.4471332   |
| stats_o/std                    | 0.030894898 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000775   |
| test/info_shaping_reward_mean  | -0.034      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.989533   |
| test/Q_plus_P                  | -1.989533   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00226    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.44712314  |
| stats_o/std                    | 0.030877167 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000614   |
| test/info_shaping_reward_mean  | -0.0343     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.8930347  |
| test/Q_plus_P                  | -1.8930347  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.548       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00222    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.44711342  |
| stats_o/std                    | 0.030861055 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -2.0713227  |
| test/Q_plus_P                  | -2.0713227  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.545       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.4470941   |
| stats_o/std                    | 0.030854011 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000726   |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.7523721  |
| test/Q_plus_P                  | -1.7523721  |
| test/reward_per_eps            | -9          |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.522       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.44708276  |
| stats_o/std                    | 0.030830339 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.8197049  |
| test/Q_plus_P                  | -1.8197049  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.4470806   |
| stats_o/std                    | 0.030833172 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0685     |
| test/info_shaping_reward_mean  | -0.118      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -7.1073303  |
| test/Q_plus_P                  | -7.1073303  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.417       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00908    |
| train/info_shaping_reward_mean | -0.0755     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.3       |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.4470364   |
| stats_o/std                    | 0.030948995 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.07        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.0802     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -6.0491633  |
| test/Q_plus_P                  | -6.0491633  |
| test/reward_per_eps            | -37.2       |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.117       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0274     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -35.3       |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.44704297  |
| stats_o/std                    | 0.030940099 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.573       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -3.0916991  |
| test/Q_plus_P                  | -3.0916991  |
| test/reward_per_eps            | -17.1       |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.406       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0038     |
| train/info_shaping_reward_mean | -0.0751     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.8       |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.44704223  |
| stats_o/std                    | 0.030913932 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000616   |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -2.2837865  |
| test/Q_plus_P                  | -2.2837865  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.458       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0683     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.44702625 |
| stats_o/std                    | 0.03089613 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00176   |
| test/info_shaping_reward_mean  | -0.037     |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.9981968 |
| test/Q_plus_P                  | -1.9981968 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.544      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00277   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 118        |
| stats_o/mean                   | 0.4470137  |
| stats_o/std                    | 0.03087481 |
| test/episodes                  | 1190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -2.2760262 |
| test/Q_plus_P                  | -2.2760262 |
| test/reward_per_eps            | -12        |
| test/steps                     | 47600      |
| train/episodes                 | 4760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.482      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00311   |
| train/info_shaping_reward_mean | -0.0682    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -20.7      |
| train/steps                    | 190400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 119        |
| stats_o/mean                   | 0.4470197  |
| stats_o/std                    | 0.03085669 |
| test/episodes                  | 1200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00217   |
| test/info_shaping_reward_mean  | -0.0405    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -2.0761697 |
| test/Q_plus_P                  | -2.0761697 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 48000      |
| train/episodes                 | 4800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.452      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0706    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.9      |
| train/steps                    | 192000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.44702458 |
| stats_o/std                    | 0.03084036 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0406    |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -1.9810411 |
| test/Q_plus_P                  | -1.9810411 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.472      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00339   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.1      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.4470167   |
| stats_o/std                    | 0.030833883 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00093    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -2.2981167  |
| test/Q_plus_P                  | -2.2981167  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.458       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0717     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.447013    |
| stats_o/std                    | 0.030826082 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00232    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -2.334811   |
| test/Q_plus_P                  | -2.334811   |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.458       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0739     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.4470056   |
| stats_o/std                    | 0.030819388 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.9815161  |
| test/Q_plus_P                  | -1.9815161  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.452       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00227    |
| train/info_shaping_reward_mean | -0.0699     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.9       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.44700328  |
| stats_o/std                    | 0.030813545 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -2.3721523  |
| test/Q_plus_P                  | -2.3721523  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.471       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0713     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.44699475 |
| stats_o/std                    | 0.0308052  |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -2.1505833 |
| test/Q_plus_P                  | -2.1505833 |
| test/reward_per_eps            | -11        |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.518      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00258   |
| train/info_shaping_reward_mean | -0.0659    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.3      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.4469805   |
| stats_o/std                    | 0.030800112 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000371   |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -2.0117314  |
| test/Q_plus_P                  | -2.0117314  |
| test/reward_per_eps            | -10         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.4469745   |
| stats_o/std                    | 0.030779421 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000869   |
| test/info_shaping_reward_mean  | -0.0378     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -2.0112474  |
| test/Q_plus_P                  | -2.0112474  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.4469657   |
| stats_o/std                    | 0.030766724 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000862   |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.279128   |
| test/Q_plus_P                  | -2.279128   |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.502       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.0675     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.9       |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.44695732 |
| stats_o/std                    | 0.0307595  |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00058   |
| test/info_shaping_reward_mean  | -0.0436    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -2.211916  |
| test/Q_plus_P                  | -2.211916  |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.531      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00277   |
| train/info_shaping_reward_mean | -0.0641    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.8      |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.44694027  |
| stats_o/std                    | 0.030754475 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000708   |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -2.0358164  |
| test/Q_plus_P                  | -2.0358164  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00221    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.4469367   |
| stats_o/std                    | 0.030738173 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -2.0749004  |
| test/Q_plus_P                  | -2.0749004  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.515       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0652     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.44693938  |
| stats_o/std                    | 0.030725779 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000741   |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.8351712  |
| test/Q_plus_P                  | -1.8351712  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.434       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.6       |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.4469372   |
| stats_o/std                    | 0.030716842 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00181    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.9817953  |
| test/Q_plus_P                  | -1.9817953  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.49        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.44691774 |
| stats_o/std                    | 0.03070501 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -2.37219   |
| test/Q_plus_P                  | -2.37219   |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00291   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.4469082  |
| stats_o/std                    | 0.03069642 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000982  |
| test/info_shaping_reward_mean  | -0.0393    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -2.0958703 |
| test/Q_plus_P                  | -2.0958703 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00213   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.44689342  |
| stats_o/std                    | 0.030679554 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000653   |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.9855962  |
| test/Q_plus_P                  | -1.9855962  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.563       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.44689044  |
| stats_o/std                    | 0.030668482 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -2.4002736  |
| test/Q_plus_P                  | -2.4002736  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.473       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.1       |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.44687653  |
| stats_o/std                    | 0.030659067 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000903   |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -2.1509178  |
| test/Q_plus_P                  | -2.1509178  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.4468679   |
| stats_o/std                    | 0.030642167 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000988   |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.0903454  |
| test/Q_plus_P                  | -2.0903454  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00242    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.44686     |
| stats_o/std                    | 0.030631797 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -1.9785761  |
| test/Q_plus_P                  | -1.9785761  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.496       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.068      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.44684827 |
| stats_o/std                    | 0.03061797 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000646  |
| test/info_shaping_reward_mean  | -0.0388    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -2.1688304 |
| test/Q_plus_P                  | -2.1688304 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.563      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.5      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.4468287   |
| stats_o/std                    | 0.030608455 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.9772967  |
| test/Q_plus_P                  | -1.9772967  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.534       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.6       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.4468172   |
| stats_o/std                    | 0.030594831 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.1922424  |
| test/Q_plus_P                  | -2.1922424  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.4468037   |
| stats_o/std                    | 0.030581743 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.9487748  |
| test/Q_plus_P                  | -1.9487748  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.505       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.44678947  |
| stats_o/std                    | 0.030560797 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.9788818  |
| test/Q_plus_P                  | -1.9788818  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.4467833   |
| stats_o/std                    | 0.030552901 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000721   |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.093347   |
| test/Q_plus_P                  | -2.093347   |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.513       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.44676647  |
| stats_o/std                    | 0.030543117 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000457   |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -2.2029858  |
| test/Q_plus_P                  | -2.2029858  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.44674745  |
| stats_o/std                    | 0.030529201 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -2.033132   |
| test/Q_plus_P                  | -2.033132   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.573       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.44681963  |
| stats_o/std                    | 0.030576162 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000735   |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.9797618  |
| test/Q_plus_P                  | -1.9797618  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.151       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0412     |
| train/info_shaping_reward_mean | -0.115      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -34         |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.44680634  |
| stats_o/std                    | 0.030562924 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.9142369  |
| test/Q_plus_P                  | -1.9142369  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.516       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.44679844  |
| stats_o/std                    | 0.030546768 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.537       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -3.2747138  |
| test/Q_plus_P                  | -3.2747138  |
| test/reward_per_eps            | -18.5       |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.44678023  |
| stats_o/std                    | 0.030539518 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.7971524  |
| test/Q_plus_P                  | -1.7971524  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.543       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.44678223  |
| stats_o/std                    | 0.030530399 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.9677621  |
| test/Q_plus_P                  | -1.9677621  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.469       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.44677174  |
| stats_o/std                    | 0.030523026 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000598   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.7761182  |
| test/Q_plus_P                  | -1.7761182  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.543       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.44675854  |
| stats_o/std                    | 0.030511701 |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00217    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.9135405  |
| test/Q_plus_P                  | -1.9135405  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.44674587  |
| stats_o/std                    | 0.030494062 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.6388366  |
| test/Q_plus_P                  | -1.6388366  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.44673467  |
| stats_o/std                    | 0.030480236 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000588   |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.760993   |
| test/Q_plus_P                  | -1.760993   |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.446728    |
| stats_o/std                    | 0.030477874 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.962714   |
| test/Q_plus_P                  | -1.962714   |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.44672787  |
| stats_o/std                    | 0.030469766 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.8387016  |
| test/Q_plus_P                  | -1.8387016  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.516       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0679     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.4467194   |
| stats_o/std                    | 0.030467972 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.7543664  |
| test/Q_plus_P                  | -1.7543664  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0696     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.44670954  |
| stats_o/std                    | 0.030460915 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000529   |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.8366175  |
| test/Q_plus_P                  | -1.8366175  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.44670513  |
| stats_o/std                    | 0.030457249 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.6974121  |
| test/Q_plus_P                  | -1.6974121  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.44669938  |
| stats_o/std                    | 0.030449668 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.9262005  |
| test/Q_plus_P                  | -1.9262005  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.537       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.4466926   |
| stats_o/std                    | 0.030441845 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.9185404  |
| test/Q_plus_P                  | -1.9185404  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.485       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0707     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.4466896   |
| stats_o/std                    | 0.030429577 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.8542408  |
| test/Q_plus_P                  | -1.8542408  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.512       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00253    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 166        |
| stats_o/mean                   | 0.44667777 |
| stats_o/std                    | 0.0304251  |
| test/episodes                  | 1670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000951  |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -2.083398  |
| test/Q_plus_P                  | -2.083398  |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 66800      |
| train/episodes                 | 6680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.573      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00246   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 267200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.44666874  |
| stats_o/std                    | 0.030429684 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.9104674  |
| test/Q_plus_P                  | -1.9104674  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.4466517   |
| stats_o/std                    | 0.030419817 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.739538   |
| test/Q_plus_P                  | -1.739538   |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.4466387   |
| stats_o/std                    | 0.030423725 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000559   |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.8116157  |
| test/Q_plus_P                  | -1.8116157  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.561       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0674     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.44663444  |
| stats_o/std                    | 0.030408613 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.877826   |
| test/Q_plus_P                  | -1.877826   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.538       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.44662237  |
| stats_o/std                    | 0.030404694 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.8808674  |
| test/Q_plus_P                  | -1.8808674  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.532       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0675     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.7       |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.4466124   |
| stats_o/std                    | 0.030396735 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000717   |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.6979965  |
| test/Q_plus_P                  | -1.6979965  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.542       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.44660208  |
| stats_o/std                    | 0.030395938 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.7070917  |
| test/Q_plus_P                  | -1.7070917  |
| test/reward_per_eps            | -9          |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00229    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.44658792 |
| stats_o/std                    | 0.030392   |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0388    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.7628711 |
| test/Q_plus_P                  | -1.7628711 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.552      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0644    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.9      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.44657758 |
| stats_o/std                    | 0.03038326 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00122   |
| test/info_shaping_reward_mean  | -0.0355    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.511287  |
| test/Q_plus_P                  | -1.511287  |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.583      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00282   |
| train/info_shaping_reward_mean | -0.0629    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.7      |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.4465678   |
| stats_o/std                    | 0.030364072 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.5865136  |
| test/Q_plus_P                  | -1.5865136  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.4465491   |
| stats_o/std                    | 0.030367717 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000608   |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.5279143  |
| test/Q_plus_P                  | -1.5279143  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.446537    |
| stats_o/std                    | 0.030360393 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.6327473  |
| test/Q_plus_P                  | -1.6327473  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.4465395   |
| stats_o/std                    | 0.030353896 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00172    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.7911639  |
| test/Q_plus_P                  | -1.7911639  |
| test/reward_per_eps            | -11         |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.495       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0711     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.4465333  |
| stats_o/std                    | 0.03033893 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.688      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.9149811 |
| test/Q_plus_P                  | -1.9149811 |
| test/reward_per_eps            | -12.5      |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.524      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00367   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.4465243   |
| stats_o/std                    | 0.030326547 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000715   |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.6847228  |
| test/Q_plus_P                  | -1.6847228  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.44651464  |
| stats_o/std                    | 0.030314833 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.6234206  |
| test/Q_plus_P                  | -1.6234206  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.55        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.4465084   |
| stats_o/std                    | 0.030304573 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.7906262  |
| test/Q_plus_P                  | -1.7906262  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.565       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.446501    |
| stats_o/std                    | 0.030294368 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.4283037  |
| test/Q_plus_P                  | -1.4283037  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.575       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.44649485  |
| stats_o/std                    | 0.030278668 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000406   |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.7384439  |
| test/Q_plus_P                  | -1.7384439  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.44648728  |
| stats_o/std                    | 0.030267758 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.8477657  |
| test/Q_plus_P                  | -1.8477657  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.4464819  |
| stats_o/std                    | 0.03025841 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.037     |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -1.6050202 |
| test/Q_plus_P                  | -1.6050202 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0666    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.44647527  |
| stats_o/std                    | 0.030245349 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.6126442  |
| test/Q_plus_P                  | -1.6126442  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.44645795  |
| stats_o/std                    | 0.030240143 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.5740896  |
| test/Q_plus_P                  | -1.5740896  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.4464495   |
| stats_o/std                    | 0.030231252 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.7622873  |
| test/Q_plus_P                  | -1.7622873  |
| test/reward_per_eps            | -11         |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.4464418   |
| stats_o/std                    | 0.030224234 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000615   |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.4446311  |
| test/Q_plus_P                  | -1.4446311  |
| test/reward_per_eps            | -9          |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.58        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 192        |
| stats_o/mean                   | 0.446439   |
| stats_o/std                    | 0.03020797 |
| test/episodes                  | 1930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00112   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -1.5446668 |
| test/Q_plus_P                  | -1.5446668 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 77200      |
| train/episodes                 | 7720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 308800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.4464316   |
| stats_o/std                    | 0.030197484 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000811   |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.5175602  |
| test/Q_plus_P                  | -1.5175602  |
| test/reward_per_eps            | -9          |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.44644102  |
| stats_o/std                    | 0.030207453 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00166    |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.4435604  |
| test/Q_plus_P                  | -1.4435604  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.461       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0767     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.44642797  |
| stats_o/std                    | 0.030205049 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0378     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.5582675  |
| test/Q_plus_P                  | -1.5582675  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.4464159   |
| stats_o/std                    | 0.030196078 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0378     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.5196184  |
| test/Q_plus_P                  | -1.5196184  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.44640377 |
| stats_o/std                    | 0.03018991 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000432  |
| test/info_shaping_reward_mean  | -0.0387    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -1.5205934 |
| test/Q_plus_P                  | -1.5205934 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 198        |
| stats_o/mean                   | 0.44639394 |
| stats_o/std                    | 0.03018332 |
| test/episodes                  | 1990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00107   |
| test/info_shaping_reward_mean  | -0.0371    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -1.5277772 |
| test/Q_plus_P                  | -1.5277772 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 79600      |
| train/episodes                 | 7960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.546      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00254   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.2      |
| train/steps                    | 318400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.4463859   |
| stats_o/std                    | 0.030176809 |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.5252734  |
| test/Q_plus_P                  | -1.5252734  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.44638106  |
| stats_o/std                    | 0.030172253 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000976   |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -1.411752   |
| test/Q_plus_P                  | -1.411752   |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00242    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.44636595  |
| stats_o/std                    | 0.030176714 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.6584331  |
| test/Q_plus_P                  | -1.6584331  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0685     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.44635335  |
| stats_o/std                    | 0.030175982 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000821   |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.4486482  |
| test/Q_plus_P                  | -1.4486482  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.4463431   |
| stats_o/std                    | 0.030166673 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.5284735  |
| test/Q_plus_P                  | -1.5284735  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.44633588  |
| stats_o/std                    | 0.030157082 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000859   |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.7677987  |
| test/Q_plus_P                  | -1.7677987  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.542       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.4463228   |
| stats_o/std                    | 0.030156663 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.358158   |
| test/Q_plus_P                  | -1.358158   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.44631228  |
| stats_o/std                    | 0.030148188 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000643   |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.6968809  |
| test/Q_plus_P                  | -1.6968809  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 207         |
| stats_o/mean                   | 0.4463046   |
| stats_o/std                    | 0.030141281 |
| test/episodes                  | 2080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000839   |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.4823728  |
| test/Q_plus_P                  | -1.4823728  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 83200       |
| train/episodes                 | 8320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 332800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.4462953   |
| stats_o/std                    | 0.030129008 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00076    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -2.002207   |
| test/Q_plus_P                  | -2.002207   |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0023     |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.4463056   |
| stats_o/std                    | 0.030137314 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.588       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000984   |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -2.4576912  |
| test/Q_plus_P                  | -2.4576912  |
| test/reward_per_eps            | -16.5       |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.405       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0141     |
| train/info_shaping_reward_mean | -0.0773     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.8       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.44629672  |
| stats_o/std                    | 0.030131705 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.9049481  |
| test/Q_plus_P                  | -1.9049481  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.505       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.4462887  |
| stats_o/std                    | 0.03012015 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00237   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.8299261 |
| test/Q_plus_P                  | -1.8299261 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.509      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00273   |
| train/info_shaping_reward_mean | -0.0654    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.6      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 212        |
| stats_o/mean                   | 0.4462801  |
| stats_o/std                    | 0.030108   |
| test/episodes                  | 2130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00114   |
| test/info_shaping_reward_mean  | -0.0419    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -1.8266025 |
| test/Q_plus_P                  | -1.8266025 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 85200      |
| train/episodes                 | 8520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.589      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0582    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 340800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.44626847  |
| stats_o/std                    | 0.030107489 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000907   |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.9458051  |
| test/Q_plus_P                  | -1.9458051  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.558       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.7       |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.44625416  |
| stats_o/std                    | 0.030098578 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000905   |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.7905399  |
| test/Q_plus_P                  | -1.7905399  |
| test/reward_per_eps            | -11         |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.595       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.44624886  |
| stats_o/std                    | 0.030087551 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000803   |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.7580421  |
| test/Q_plus_P                  | -1.7580421  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.44624114  |
| stats_o/std                    | 0.030076431 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000832   |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.6504     |
| test/Q_plus_P                  | -1.6504     |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.44623318  |
| stats_o/std                    | 0.030073097 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.5398817  |
| test/Q_plus_P                  | -1.5398817  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.44622245  |
| stats_o/std                    | 0.030060783 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.3733346  |
| test/Q_plus_P                  | -1.3733346  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.4462144  |
| stats_o/std                    | 0.03005547 |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0007    |
| test/info_shaping_reward_mean  | -0.0359    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.4088343 |
| test/Q_plus_P                  | -1.4088343 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.592      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00297   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.27      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.4462018   |
| stats_o/std                    | 0.030059494 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000947   |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.3121285  |
| test/Q_plus_P                  | -1.3121285  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.44619453  |
| stats_o/std                    | 0.030055193 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.5318433  |
| test/Q_plus_P                  | -1.5318433  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.44618845  |
| stats_o/std                    | 0.030048968 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.3999596  |
| test/Q_plus_P                  | -1.3999596  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.446178    |
| stats_o/std                    | 0.030044505 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.3999236  |
| test/Q_plus_P                  | -1.3999236  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.44616726 |
| stats_o/std                    | 0.03004457 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000538  |
| test/info_shaping_reward_mean  | -0.0346    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -1.271146  |
| test/Q_plus_P                  | -1.271146  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00232   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.44615754  |
| stats_o/std                    | 0.030042022 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000609   |
| test/info_shaping_reward_mean  | -0.0331     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.363462   |
| test/Q_plus_P                  | -1.363462   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 226        |
| stats_o/mean                   | 0.44615078 |
| stats_o/std                    | 0.03003721 |
| test/episodes                  | 2270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000964  |
| test/info_shaping_reward_mean  | -0.0308    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -1.2281781 |
| test/Q_plus_P                  | -1.2281781 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 90800      |
| train/episodes                 | 9080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.557      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00295   |
| train/info_shaping_reward_mean | -0.0657    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 363200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.4461385   |
| stats_o/std                    | 0.030037614 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -1.4081373  |
| test/Q_plus_P                  | -1.4081373  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.44613516  |
| stats_o/std                    | 0.030030608 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.457245   |
| test/Q_plus_P                  | -1.457245   |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.44612744  |
| stats_o/std                    | 0.030024404 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0343     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.2156833  |
| test/Q_plus_P                  | -1.2156833  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.4461138  |
| stats_o/std                    | 0.03002332 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000878  |
| test/info_shaping_reward_mean  | -0.034     |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -1.1548319 |
| test/Q_plus_P                  | -1.1548319 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.622      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00241   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.44610357  |
| stats_o/std                    | 0.030016312 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.4094756  |
| test/Q_plus_P                  | -1.4094756  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.4460945   |
| stats_o/std                    | 0.030011082 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000817   |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.5038677  |
| test/Q_plus_P                  | -1.5038677  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00224    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.44608688  |
| stats_o/std                    | 0.030004695 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -1.2477882  |
| test/Q_plus_P                  | -1.2477882  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.4460783  |
| stats_o/std                    | 0.02999825 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00069   |
| test/info_shaping_reward_mean  | -0.0332    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -1.2536281 |
| test/Q_plus_P                  | -1.2536281 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.614      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.4      |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.44606724  |
| stats_o/std                    | 0.029994795 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000753   |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.2508987  |
| test/Q_plus_P                  | -1.2508987  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.4460616   |
| stats_o/std                    | 0.029986434 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000629   |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.2458692  |
| test/Q_plus_P                  | -1.2458692  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 237        |
| stats_o/mean                   | 0.44605252 |
| stats_o/std                    | 0.02998791 |
| test/episodes                  | 2380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00323   |
| test/info_shaping_reward_mean  | -0.041     |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.2923032 |
| test/Q_plus_P                  | -1.2923032 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 95200      |
| train/episodes                 | 9520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.578      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00252   |
| train/info_shaping_reward_mean | -0.0661    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 380800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 238        |
| stats_o/mean                   | 0.44604504 |
| stats_o/std                    | 0.02998375 |
| test/episodes                  | 2390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00133   |
| test/info_shaping_reward_mean  | -0.0362    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -1.2141516 |
| test/Q_plus_P                  | -1.2141516 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 95600      |
| train/episodes                 | 9560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.601      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0031    |
| train/info_shaping_reward_mean | -0.0617    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16        |
| train/steps                    | 382400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.44603625  |
| stats_o/std                    | 0.029981218 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000573   |
| test/info_shaping_reward_mean  | -0.0396     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.5096794  |
| test/Q_plus_P                  | -1.5096794  |
| test/reward_per_eps            | -10         |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.44603202  |
| stats_o/std                    | 0.029973514 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000436   |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.1335809  |
| test/Q_plus_P                  | -1.1335809  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.563       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.44602337 |
| stats_o/std                    | 0.02996589 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00141   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.4958444 |
| test/Q_plus_P                  | -1.4958444 |
| test/reward_per_eps            | -10        |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00235   |
| train/info_shaping_reward_mean | -0.0594    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.44601616  |
| stats_o/std                    | 0.029962137 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000509   |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.3615947  |
| test/Q_plus_P                  | -1.3615947  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.44600675  |
| stats_o/std                    | 0.029958026 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0347     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.3150402  |
| test/Q_plus_P                  | -1.3150402  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.44599736  |
| stats_o/std                    | 0.029953545 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000844   |
| test/info_shaping_reward_mean  | -0.0323     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0807257  |
| test/Q_plus_P                  | -1.0807257  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.445986    |
| stats_o/std                    | 0.029949227 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.2863487  |
| test/Q_plus_P                  | -1.2863487  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00255    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.44597673  |
| stats_o/std                    | 0.029950775 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000938   |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.156587   |
| test/Q_plus_P                  | -1.156587   |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0023     |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.44596958  |
| stats_o/std                    | 0.029946709 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.2251248  |
| test/Q_plus_P                  | -1.2251248  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00196    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.44596198  |
| stats_o/std                    | 0.029933045 |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000753   |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.23797    |
| test/Q_plus_P                  | -1.23797    |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.44595298  |
| stats_o/std                    | 0.029921075 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000651   |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.1969277  |
| test/Q_plus_P                  | -1.1969277  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00218    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.4459469   |
| stats_o/std                    | 0.029910892 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000904   |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -1.3515635  |
| test/Q_plus_P                  | -1.3515635  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.4459364  |
| stats_o/std                    | 0.02990582 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000855  |
| test/info_shaping_reward_mean  | -0.0361    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -1.1703732 |
| test/Q_plus_P                  | -1.1703732 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00244   |
| train/info_shaping_reward_mean | -0.0565    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.44592822  |
| stats_o/std                    | 0.029901529 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0322     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.1110505  |
| test/Q_plus_P                  | -1.1110505  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.445922    |
| stats_o/std                    | 0.029894933 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00074    |
| test/info_shaping_reward_mean  | -0.0268     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.8249045  |
| test/Q_plus_P                  | -0.8249045  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.44591746  |
| stats_o/std                    | 0.029885562 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000604   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.328038   |
| test/Q_plus_P                  | -1.328038   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.4459088   |
| stats_o/std                    | 0.029884746 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000884   |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.1025455  |
| test/Q_plus_P                  | -1.1025455  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.44590345  |
| stats_o/std                    | 0.029873513 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.031      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.97255087 |
| test/Q_plus_P                  | -0.97255087 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.4458976   |
| stats_o/std                    | 0.029862648 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.1449646  |
| test/Q_plus_P                  | -1.1449646  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.44588754 |
| stats_o/std                    | 0.02985996 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000938  |
| test/info_shaping_reward_mean  | -0.0335    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -1.127424  |
| test/Q_plus_P                  | -1.127424  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00293   |
| train/info_shaping_reward_mean | -0.0629    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.44587445  |
| stats_o/std                    | 0.029857732 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.9783102  |
| test/Q_plus_P                  | -0.9783102  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.44586587  |
| stats_o/std                    | 0.029852943 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000451   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.0962653  |
| test/Q_plus_P                  | -1.0962653  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.4458567   |
| stats_o/std                    | 0.029848965 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000865   |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.2733557  |
| test/Q_plus_P                  | -1.2733557  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.4458511   |
| stats_o/std                    | 0.029844826 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000589   |
| test/info_shaping_reward_mean  | -0.0307     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.88107497 |
| test/Q_plus_P                  | -0.88107497 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.44584247  |
| stats_o/std                    | 0.029841945 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000872   |
| test/info_shaping_reward_mean  | -0.0324     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.9595163  |
| test/Q_plus_P                  | -0.9595163  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 264        |
| stats_o/mean                   | 0.44583777 |
| stats_o/std                    | 0.02983056 |
| test/episodes                  | 2650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000834  |
| test/info_shaping_reward_mean  | -0.038     |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -1.2522371 |
| test/Q_plus_P                  | -1.2522371 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 106000     |
| train/episodes                 | 10600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00319   |
| train/info_shaping_reward_mean | -0.0576    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 424000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.44582918  |
| stats_o/std                    | 0.029820243 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.9656149  |
| test/Q_plus_P                  | -0.9656149  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.44581914  |
| stats_o/std                    | 0.029814987 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000522   |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.1937879  |
| test/Q_plus_P                  | -1.1937879  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.44581154  |
| stats_o/std                    | 0.029806174 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00225    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.0767035  |
| test/Q_plus_P                  | -1.0767035  |
| test/reward_per_eps            | -8          |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.44580543  |
| stats_o/std                    | 0.029799156 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000717   |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.1159523  |
| test/Q_plus_P                  | -1.1159523  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.44579875  |
| stats_o/std                    | 0.029794872 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.1772748  |
| test/Q_plus_P                  | -1.1772748  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.4457945   |
| stats_o/std                    | 0.029786026 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.1378982  |
| test/Q_plus_P                  | -1.1378982  |
| test/reward_per_eps            | -9          |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.44578612  |
| stats_o/std                    | 0.029779658 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000291   |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.97630864 |
| test/Q_plus_P                  | -0.97630864 |
| test/reward_per_eps            | -8          |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.44578397  |
| stats_o/std                    | 0.029765794 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.4128709  |
| test/Q_plus_P                  | -1.4128709  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.44577658  |
| stats_o/std                    | 0.029756248 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0009     |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.1087581  |
| test/Q_plus_P                  | -1.1087581  |
| test/reward_per_eps            | -9          |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.44577095  |
| stats_o/std                    | 0.029750006 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.3318235  |
| test/Q_plus_P                  | -1.3318235  |
| test/reward_per_eps            | -10         |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.44576085 |
| stats_o/std                    | 0.02974332 |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00163   |
| test/info_shaping_reward_mean  | -0.0393    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.3541259 |
| test/Q_plus_P                  | -1.3541259 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00218   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.44575462  |
| stats_o/std                    | 0.029738002 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000659   |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.2665702  |
| test/Q_plus_P                  | -1.2665702  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00203    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.44575068  |
| stats_o/std                    | 0.029733473 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.1983235  |
| test/Q_plus_P                  | -1.1983235  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.592       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.4457462  |
| stats_o/std                    | 0.02972172 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0383    |
| test/info_shaping_reward_min   | -0.219     |
| test/Q                         | -1.184944  |
| test/Q_plus_P                  | -1.184944  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.617      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00295   |
| train/info_shaping_reward_mean | -0.0584    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.44574025  |
| stats_o/std                    | 0.029714031 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.1131091  |
| test/Q_plus_P                  | -1.1131091  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.44573545  |
| stats_o/std                    | 0.029708445 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000695   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.1672356  |
| test/Q_plus_P                  | -1.1672356  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.44573033 |
| stats_o/std                    | 0.02969794 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.041     |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -1.2820764 |
| test/Q_plus_P                  | -1.2820764 |
| test/reward_per_eps            | -9         |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00282   |
| train/info_shaping_reward_mean | -0.0565    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.4457265   |
| stats_o/std                    | 0.029688725 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.3485136  |
| test/Q_plus_P                  | -1.3485136  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.44572088 |
| stats_o/std                    | 0.0296862  |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00119   |
| test/info_shaping_reward_mean  | -0.0396    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -1.4387822 |
| test/Q_plus_P                  | -1.4387822 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00265   |
| train/info_shaping_reward_mean | -0.0609    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.44571662  |
| stats_o/std                    | 0.029682053 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00213    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.4273099  |
| test/Q_plus_P                  | -1.4273099  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.44570982  |
| stats_o/std                    | 0.029678224 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00058    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.2492115  |
| test/Q_plus_P                  | -1.2492115  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.44570315 |
| stats_o/std                    | 0.02967476 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00269   |
| test/info_shaping_reward_mean  | -0.0383    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.1164314 |
| test/Q_plus_P                  | -1.1164314 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.602      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00365   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.4457575   |
| stats_o/std                    | 0.029722428 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.6647235  |
| test/Q_plus_P                  | -1.6647235  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.232       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0354     |
| train/info_shaping_reward_mean | -0.116      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -30.7       |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 288        |
| stats_o/mean                   | 0.44575715 |
| stats_o/std                    | 0.02971237 |
| test/episodes                  | 2890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00101   |
| test/info_shaping_reward_mean  | -0.0371    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -1.245204  |
| test/Q_plus_P                  | -1.245204  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 115600     |
| train/episodes                 | 11560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.548      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00316   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.1      |
| train/steps                    | 462400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.44575092  |
| stats_o/std                    | 0.029707918 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00288    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.4393382  |
| test/Q_plus_P                  | -1.4393382  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.4457451   |
| stats_o/std                    | 0.029701991 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.3183266  |
| test/Q_plus_P                  | -1.3183266  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.559       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.6       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.44573864 |
| stats_o/std                    | 0.02969458 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00201   |
| test/info_shaping_reward_mean  | -0.0387    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -1.33571   |
| test/Q_plus_P                  | -1.33571   |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00279   |
| train/info_shaping_reward_mean | -0.0609    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.44573662  |
| stats_o/std                    | 0.029688964 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -1.759934   |
| test/Q_plus_P                  | -1.759934   |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.44573006  |
| stats_o/std                    | 0.029681966 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000665   |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.3499458  |
| test/Q_plus_P                  | -1.3499458  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.44572374  |
| stats_o/std                    | 0.029675229 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.1453627  |
| test/Q_plus_P                  | -1.1453627  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.44571826 |
| stats_o/std                    | 0.0296719  |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000984  |
| test/info_shaping_reward_mean  | -0.0367    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -1.2092284 |
| test/Q_plus_P                  | -1.2092284 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00271   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.445712    |
| stats_o/std                    | 0.029667385 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0450904  |
| test/Q_plus_P                  | -1.0450904  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00214    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.44570866  |
| stats_o/std                    | 0.029658133 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.18623    |
| test/Q_plus_P                  | -1.18623    |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.44570306  |
| stats_o/std                    | 0.02965263  |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000795   |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.99631065 |
| test/Q_plus_P                  | -0.99631065 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.44569552  |
| stats_o/std                    | 0.029648453 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00078    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.2999381  |
| test/Q_plus_P                  | -1.2999381  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.44568965  |
| stats_o/std                    | 0.029647449 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.125155   |
| test/Q_plus_P                  | -1.125155   |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.4456837   |
| stats_o/std                    | 0.029641138 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00271    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.173806   |
| test/Q_plus_P                  | -1.173806   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.44567737  |
| stats_o/std                    | 0.029638516 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0472765  |
| test/Q_plus_P                  | -1.0472765  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.44567072  |
| stats_o/std                    | 0.029628552 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.4460618  |
| test/Q_plus_P                  | -1.4460618  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0024     |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.44566402  |
| stats_o/std                    | 0.029621994 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.0261995  |
| test/Q_plus_P                  | -1.0261995  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.44566047 |
| stats_o/std                    | 0.02961789 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00577   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.4478164 |
| test/Q_plus_P                  | -1.4478164 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.44565606  |
| stats_o/std                    | 0.029610107 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.2062335  |
| test/Q_plus_P                  | -1.2062335  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.4456545   |
| stats_o/std                    | 0.029608779 |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.2492486  |
| test/Q_plus_P                  | -1.2492486  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.4456507   |
| stats_o/std                    | 0.029601568 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00401    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.2058938  |
| test/Q_plus_P                  | -1.2058938  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.44564593  |
| stats_o/std                    | 0.029595835 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.9110052  |
| test/Q_plus_P                  | -0.9110052  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.44564426  |
| stats_o/std                    | 0.029587276 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00482    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.9028141  |
| test/Q_plus_P                  | -0.9028141  |
| test/reward_per_eps            | -7          |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.44564044  |
| stats_o/std                    | 0.029578753 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0333     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.99006987 |
| test/Q_plus_P                  | -0.99006987 |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.44563496  |
| stats_o/std                    | 0.029574566 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.0463749  |
| test/Q_plus_P                  | -1.0463749  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 313        |
| stats_o/mean                   | 0.4456334  |
| stats_o/std                    | 0.02957338 |
| test/episodes                  | 3140       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00229   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -1.5078063 |
| test/Q_plus_P                  | -1.5078063 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 125600     |
| train/episodes                 | 12560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00319   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 502400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.44562975  |
| stats_o/std                    | 0.029563366 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.2191057  |
| test/Q_plus_P                  | -1.2191057  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.44562134  |
| stats_o/std                    | 0.029562483 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00213    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.2198719  |
| test/Q_plus_P                  | -1.2198719  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.44561613  |
| stats_o/std                    | 0.029556178 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000926   |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.1073605  |
| test/Q_plus_P                  | -1.1073605  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.44561175  |
| stats_o/std                    | 0.029546723 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.3339818  |
| test/Q_plus_P                  | -1.3339818  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.4456016  |
| stats_o/std                    | 0.02954545 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000906  |
| test/info_shaping_reward_mean  | -0.0405    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -1.2407342 |
| test/Q_plus_P                  | -1.2407342 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00281   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.44559324 |
| stats_o/std                    | 0.02954564 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -1.5014222 |
| test/Q_plus_P                  | -1.5014222 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0026    |
| train/info_shaping_reward_mean | -0.061     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 320        |
| stats_o/mean                   | 0.44558516 |
| stats_o/std                    | 0.0295474  |
| test/episodes                  | 3210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0391    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -1.2966342 |
| test/Q_plus_P                  | -1.2966342 |
| test/reward_per_eps            | -9         |
| test/steps                     | 128400     |
| train/episodes                 | 12840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0592    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 513600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.4455781   |
| stats_o/std                    | 0.029544571 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.9886237  |
| test/Q_plus_P                  | -0.9886237  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.44557026  |
| stats_o/std                    | 0.029541783 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000483   |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.2083905  |
| test/Q_plus_P                  | -1.2083905  |
| test/reward_per_eps            | -9          |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.44556484  |
| stats_o/std                    | 0.029541105 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -1.0620608  |
| test/Q_plus_P                  | -1.0620608  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.44555578  |
| stats_o/std                    | 0.029540071 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0514632  |
| test/Q_plus_P                  | -1.0514632  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.44554925  |
| stats_o/std                    | 0.029540151 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0308     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.9032222  |
| test/Q_plus_P                  | -0.9032222  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.44554412  |
| stats_o/std                    | 0.029533073 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.1367698  |
| test/Q_plus_P                  | -1.1367698  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.4455388   |
| stats_o/std                    | 0.029526254 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.1168642  |
| test/Q_plus_P                  | -1.1168642  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.44553438  |
| stats_o/std                    | 0.029519133 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.1948133  |
| test/Q_plus_P                  | -1.1948133  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.44552565  |
| stats_o/std                    | 0.029521553 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.0805534  |
| test/Q_plus_P                  | -1.0805534  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.44552028  |
| stats_o/std                    | 0.029514194 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.9818847  |
| test/Q_plus_P                  | -0.9818847  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.44551504  |
| stats_o/std                    | 0.029508607 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -1.1839496  |
| test/Q_plus_P                  | -1.1839496  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00203    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.44550887  |
| stats_o/std                    | 0.029505374 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0031     |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.2321954  |
| test/Q_plus_P                  | -1.2321954  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.44550455  |
| stats_o/std                    | 0.029504666 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.9341587  |
| test/Q_plus_P                  | -0.9341587  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.44550052  |
| stats_o/std                    | 0.029499153 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.982748   |
| test/Q_plus_P                  | -0.982748   |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.44549692 |
| stats_o/std                    | 0.0294926  |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000505  |
| test/info_shaping_reward_mean  | -0.0338    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -1.0480328 |
| test/Q_plus_P                  | -1.0480328 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.627      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00302   |
| train/info_shaping_reward_mean | -0.0578    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.44548997  |
| stats_o/std                    | 0.029490734 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.0028249  |
| test/Q_plus_P                  | -1.0028249  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00242    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.4454836   |
| stats_o/std                    | 0.029489921 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.98520887 |
| test/Q_plus_P                  | -0.98520887 |
| test/reward_per_eps            | -8          |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.4454762   |
| stats_o/std                    | 0.029484717 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00221    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.88563895 |
| test/Q_plus_P                  | -0.88563895 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.4454724   |
| stats_o/std                    | 0.029479811 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000617   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.0879045  |
| test/Q_plus_P                  | -1.0879045  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.44546604  |
| stats_o/std                    | 0.029472142 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -1.0694481  |
| test/Q_plus_P                  | -1.0694481  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.44546258  |
| stats_o/std                    | 0.02946684  |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.98279357 |
| test/Q_plus_P                  | -0.98279357 |
| test/reward_per_eps            | -8          |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.44545957  |
| stats_o/std                    | 0.029463932 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -1.144135   |
| test/Q_plus_P                  | -1.144135   |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.44545433  |
| stats_o/std                    | 0.029458204 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.5672244  |
| test/Q_plus_P                  | -1.5672244  |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.44544744  |
| stats_o/std                    | 0.029458625 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.99209696 |
| test/Q_plus_P                  | -0.99209696 |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.44544038  |
| stats_o/std                    | 0.029456386 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000675   |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.9878247  |
| test/Q_plus_P                  | -0.9878247  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.44543743  |
| stats_o/std                    | 0.029452585 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000642   |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.0171646  |
| test/Q_plus_P                  | -1.0171646  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.4454331   |
| stats_o/std                    | 0.029451827 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000803   |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.95086694 |
| test/Q_plus_P                  | -0.95086694 |
| test/reward_per_eps            | -8          |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.44542763  |
| stats_o/std                    | 0.029450385 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00181    |
| test/info_shaping_reward_mean  | -0.0342     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.97372514 |
| test/Q_plus_P                  | -0.97372514 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.44542322  |
| stats_o/std                    | 0.029447248 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.8966062  |
| test/Q_plus_P                  | -0.8966062  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.44541752  |
| stats_o/std                    | 0.029447494 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.0105252  |
| test/Q_plus_P                  | -1.0105252  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.4454143   |
| stats_o/std                    | 0.029440515 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000686   |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.94228643 |
| test/Q_plus_P                  | -0.94228643 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.44541016  |
| stats_o/std                    | 0.029433222 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0027     |
| test/info_shaping_reward_mean  | -0.0322     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.9142673  |
| test/Q_plus_P                  | -0.9142673  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.44540218  |
| stats_o/std                    | 0.029430525 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000872   |
| test/info_shaping_reward_mean  | -0.029      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.8079597  |
| test/Q_plus_P                  | -0.8079597  |
| test/reward_per_eps            | -7          |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.4453992   |
| stats_o/std                    | 0.029423997 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000412   |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.2025276  |
| test/Q_plus_P                  | -1.2025276  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.44539502  |
| stats_o/std                    | 0.029418128 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000838   |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.0480721  |
| test/Q_plus_P                  | -1.0480721  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 356        |
| stats_o/mean                   | 0.44539157 |
| stats_o/std                    | 0.02940885 |
| test/episodes                  | 3570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000924  |
| test/info_shaping_reward_mean  | -0.0382    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -0.8786106 |
| test/Q_plus_P                  | -0.8786106 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 142800     |
| train/episodes                 | 14280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 571200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.4453844   |
| stats_o/std                    | 0.029408544 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.85875165 |
| test/Q_plus_P                  | -0.85875165 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.4453765   |
| stats_o/std                    | 0.029407717 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000713   |
| test/info_shaping_reward_mean  | -0.0321     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.8146652  |
| test/Q_plus_P                  | -0.8146652  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 359        |
| stats_o/mean                   | 0.44537076 |
| stats_o/std                    | 0.0294064  |
| test/episodes                  | 3600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000578  |
| test/info_shaping_reward_mean  | -0.0361    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -1.0518588 |
| test/Q_plus_P                  | -1.0518588 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 144000     |
| train/episodes                 | 14400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00267   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 576000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.44536555 |
| stats_o/std                    | 0.02940919 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000423  |
| test/info_shaping_reward_mean  | -0.0338    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.0152061 |
| test/Q_plus_P                  | -1.0152061 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.613      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00246   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.5      |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.44536045  |
| stats_o/std                    | 0.029402865 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.9655949  |
| test/Q_plus_P                  | -0.9655949  |
| test/reward_per_eps            | -8          |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00255    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.44535208  |
| stats_o/std                    | 0.029398851 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.9721066  |
| test/Q_plus_P                  | -0.9721066  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.4453486   |
| stats_o/std                    | 0.029397292 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.0581863  |
| test/Q_plus_P                  | -1.0581863  |
| test/reward_per_eps            | -9          |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.4453448   |
| stats_o/std                    | 0.029395735 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00205    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.9722925  |
| test/Q_plus_P                  | -0.9722925  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.4453405   |
| stats_o/std                    | 0.029393442 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000936   |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.98644996 |
| test/Q_plus_P                  | -0.98644996 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.44533703  |
| stats_o/std                    | 0.029389871 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.035      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.95198905 |
| test/Q_plus_P                  | -0.95198905 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.44533345  |
| stats_o/std                    | 0.029381925 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00238    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.0367101  |
| test/Q_plus_P                  | -1.0367101  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.44532833 |
| stats_o/std                    | 0.02937428 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0368    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -1.0163472 |
| test/Q_plus_P                  | -1.0163472 |
| test/reward_per_eps            | -8         |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.687      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0024    |
| train/info_shaping_reward_mean | -0.0519    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.44532308  |
| stats_o/std                    | 0.029374013 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.9242432  |
| test/Q_plus_P                  | -0.9242432  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.4453152   |
| stats_o/std                    | 0.029372355 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0324     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.8208847  |
| test/Q_plus_P                  | -0.8208847  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00224    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.44531265  |
| stats_o/std                    | 0.029366925 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0027     |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.8513857  |
| test/Q_plus_P                  | -0.8513857  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.4453094   |
| stats_o/std                    | 0.029357875 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.9044757  |
| test/Q_plus_P                  | -0.9044757  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.445305    |
| stats_o/std                    | 0.029356232 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.75357467 |
| test/Q_plus_P                  | -0.75357467 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.44530237  |
| stats_o/std                    | 0.029351823 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0378     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.82376426 |
| test/Q_plus_P                  | -0.82376426 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.44529703  |
| stats_o/std                    | 0.029349068 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000585   |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.9892507  |
| test/Q_plus_P                  | -0.9892507  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.44529322  |
| stats_o/std                    | 0.029350922 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.9756214  |
| test/Q_plus_P                  | -0.9756214  |
| test/reward_per_eps            | -8          |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.4452907   |
| stats_o/std                    | 0.029349    |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.87534994 |
| test/Q_plus_P                  | -0.87534994 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.4452856   |
| stats_o/std                    | 0.029344521 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.0945011  |
| test/Q_plus_P                  | -1.0945011  |
| test/reward_per_eps            | -9          |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.44528064  |
| stats_o/std                    | 0.029343149 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000838   |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.0561419  |
| test/Q_plus_P                  | -1.0561419  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.44527933  |
| stats_o/std                    | 0.029333336 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00279    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.8941944  |
| test/Q_plus_P                  | -0.8941944  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.44527796  |
| stats_o/std                    | 0.029329488 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.97699285 |
| test/Q_plus_P                  | -0.97699285 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 382         |
| stats_o/mean                   | 0.44527677  |
| stats_o/std                    | 0.029325448 |
| test/episodes                  | 3830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000944   |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.9428541  |
| test/Q_plus_P                  | -0.9428541  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 153200      |
| train/episodes                 | 15320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 612800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.44527265  |
| stats_o/std                    | 0.029319445 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000232   |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.8244313  |
| test/Q_plus_P                  | -0.8244313  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.44526646  |
| stats_o/std                    | 0.029312812 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.0477669  |
| test/Q_plus_P                  | -1.0477669  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.4452628   |
| stats_o/std                    | 0.029305683 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000781   |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9147165  |
| test/Q_plus_P                  | -0.9147165  |
| test/reward_per_eps            | -8          |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.44526127  |
| stats_o/std                    | 0.029300228 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.0030557  |
| test/Q_plus_P                  | -1.0030557  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.44525602  |
| stats_o/std                    | 0.029297449 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000905   |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.9831462  |
| test/Q_plus_P                  | -0.9831462  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.44525102 |
| stats_o/std                    | 0.02929026 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00322   |
| test/info_shaping_reward_mean  | -0.0394    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -0.9014315 |
| test/Q_plus_P                  | -0.9014315 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00245   |
| train/info_shaping_reward_mean | -0.0546    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.44524488  |
| stats_o/std                    | 0.029285042 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.79089254 |
| test/Q_plus_P                  | -0.79089254 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.44524086 |
| stats_o/std                    | 0.02927941 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000372  |
| test/info_shaping_reward_mean  | -0.043     |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -1.092401  |
| test/Q_plus_P                  | -1.092401  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00199   |
| train/info_shaping_reward_mean | -0.0569    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.44523612  |
| stats_o/std                    | 0.029276842 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0024     |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.121365   |
| test/Q_plus_P                  | -1.121365   |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.4452342   |
| stats_o/std                    | 0.029272964 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000859   |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0291831  |
| test/Q_plus_P                  | -1.0291831  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.44522884  |
| stats_o/std                    | 0.029271038 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0338     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.8828329  |
| test/Q_plus_P                  | -0.8828329  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.445225    |
| stats_o/std                    | 0.029267171 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000559   |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.0171983  |
| test/Q_plus_P                  | -1.0171983  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.44522133  |
| stats_o/std                    | 0.029266402 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000648   |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.1026092  |
| test/Q_plus_P                  | -1.1026092  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.44521928  |
| stats_o/std                    | 0.029261163 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000883   |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -0.89466804 |
| test/Q_plus_P                  | -0.89466804 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.44521418  |
| stats_o/std                    | 0.029261336 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.9307697  |
| test/Q_plus_P                  | -0.9307697  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.445213    |
| stats_o/std                    | 0.029255612 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.92262155 |
| test/Q_plus_P                  | -0.92262155 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.44521022 |
| stats_o/std                    | 0.02924945 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00146   |
| test/info_shaping_reward_mean  | -0.0363    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -0.809196  |
| test/Q_plus_P                  | -0.809196  |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0026    |
| train/info_shaping_reward_mean | -0.0535    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.44520673  |
| stats_o/std                    | 0.029250711 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.034      |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -0.71666396 |
| test/Q_plus_P                  | -0.71666396 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.4452007   |
| stats_o/std                    | 0.029246708 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -0.874484   |
| test/Q_plus_P                  | -0.874484   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.44519523  |
| stats_o/std                    | 0.029244417 |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000899   |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.99654806 |
| test/Q_plus_P                  | -0.99654806 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.44519174  |
| stats_o/std                    | 0.029239856 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.0739948  |
| test/Q_plus_P                  | -1.0739948  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.4451863   |
| stats_o/std                    | 0.029234426 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.0703331  |
| test/Q_plus_P                  | -1.0703331  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0024     |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.44518432  |
| stats_o/std                    | 0.029227028 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.83301353 |
| test/Q_plus_P                  | -0.83301353 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0023     |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.44518015  |
| stats_o/std                    | 0.029223772 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000978   |
| test/info_shaping_reward_mean  | -0.0327     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.7994221  |
| test/Q_plus_P                  | -0.7994221  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.44517672  |
| stats_o/std                    | 0.02922591  |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.96069807 |
| test/Q_plus_P                  | -0.96069807 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.44517103  |
| stats_o/std                    | 0.029221319 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000962   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.9751973  |
| test/Q_plus_P                  | -0.9751973  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.4451673   |
| stats_o/std                    | 0.029218083 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0339     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.8397026  |
| test/Q_plus_P                  | -0.8397026  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.44516233  |
| stats_o/std                    | 0.029216766 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0035     |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.9400914  |
| test/Q_plus_P                  | -0.9400914  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0023     |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.44516015  |
| stats_o/std                    | 0.029210975 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.035      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.8906289  |
| test/Q_plus_P                  | -0.8906289  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.4451557   |
| stats_o/std                    | 0.029204244 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.0227793  |
| test/Q_plus_P                  | -1.0227793  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.44515026  |
| stats_o/std                    | 0.029200375 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000965   |
| test/info_shaping_reward_mean  | -0.0323     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.7835516  |
| test/Q_plus_P                  | -0.7835516  |
| test/reward_per_eps            | -7          |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00227    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.44514608  |
| stats_o/std                    | 0.029194562 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00327    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.81626266 |
| test/Q_plus_P                  | -0.81626266 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.44514278  |
| stats_o/std                    | 0.029191682 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9295782  |
| test/Q_plus_P                  | -0.9295782  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.44513726  |
| stats_o/std                    | 0.029186243 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.9800148  |
| test/Q_plus_P                  | -0.9800148  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.4451331   |
| stats_o/std                    | 0.029181378 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.998798   |
| test/Q_plus_P                  | -0.998798   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 418        |
| stats_o/mean                   | 0.44513285 |
| stats_o/std                    | 0.02917342 |
| test/episodes                  | 4190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000796  |
| test/info_shaping_reward_mean  | -0.0342    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -0.8059256 |
| test/Q_plus_P                  | -0.8059256 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 167600     |
| train/episodes                 | 16760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00289   |
| train/info_shaping_reward_mean | -0.0584    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 670400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.44513035  |
| stats_o/std                    | 0.029172739 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8768238  |
| test/Q_plus_P                  | -0.8768238  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.44512674  |
| stats_o/std                    | 0.029167347 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000479   |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.0858475  |
| test/Q_plus_P                  | -1.0858475  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.4451252   |
| stats_o/std                    | 0.029162424 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0376     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.8843413  |
| test/Q_plus_P                  | -0.8843413  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.4451201  |
| stats_o/std                    | 0.02915731 |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -0.9051563 |
| test/Q_plus_P                  | -0.9051563 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.69       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00255   |
| train/info_shaping_reward_mean | -0.0534    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.4451187   |
| stats_o/std                    | 0.029151559 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00061    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.0310134  |
| test/Q_plus_P                  | -1.0310134  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.44511738  |
| stats_o/std                    | 0.029145202 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000594   |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.931708   |
| test/Q_plus_P                  | -0.931708   |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.44511423  |
| stats_o/std                    | 0.02914258  |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00276    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.86142653 |
| test/Q_plus_P                  | -0.86142653 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.44511017  |
| stats_o/std                    | 0.029143387 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.8467094  |
| test/Q_plus_P                  | -0.8467094  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.44510734  |
| stats_o/std                    | 0.029140742 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.9546989  |
| test/Q_plus_P                  | -0.9546989  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.44510117  |
| stats_o/std                    | 0.029137544 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00269    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.86545926 |
| test/Q_plus_P                  | -0.86545926 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.44509986  |
| stats_o/std                    | 0.02913536  |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00067    |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.97282034 |
| test/Q_plus_P                  | -0.97282034 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.44509566 |
| stats_o/std                    | 0.02913356 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000769  |
| test/info_shaping_reward_mean  | -0.0385    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -0.829523  |
| test/Q_plus_P                  | -0.829523  |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00366   |
| train/info_shaping_reward_mean | -0.0573    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.44509217  |
| stats_o/std                    | 0.029130107 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.9021904  |
| test/Q_plus_P                  | -0.9021904  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.44508794 |
| stats_o/std                    | 0.02912367 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00157   |
| test/info_shaping_reward_mean  | -0.0345    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -0.7944027 |
| test/Q_plus_P                  | -0.7944027 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00239   |
| train/info_shaping_reward_mean | -0.0547    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.44508564  |
| stats_o/std                    | 0.029117698 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00281    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -0.90270746 |
| test/Q_plus_P                  | -0.90270746 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.44508246  |
| stats_o/std                    | 0.029114546 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.80347925 |
| test/Q_plus_P                  | -0.80347925 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.6         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.44507816  |
| stats_o/std                    | 0.029113675 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000755   |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.0500885  |
| test/Q_plus_P                  | -1.0500885  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.4450746   |
| stats_o/std                    | 0.029112743 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0063902  |
| test/Q_plus_P                  | -1.0063902  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.44507012  |
| stats_o/std                    | 0.029107088 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0330495  |
| test/Q_plus_P                  | -1.0330495  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.44506702  |
| stats_o/std                    | 0.029104428 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.8668877  |
| test/Q_plus_P                  | -0.8668877  |
| test/reward_per_eps            | -8          |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.44506487  |
| stats_o/std                    | 0.029097578 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.2518873  |
| test/Q_plus_P                  | -1.2518873  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.4450617   |
| stats_o/std                    | 0.029094962 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.8018565  |
| test/Q_plus_P                  | -0.8018565  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.44505808  |
| stats_o/std                    | 0.029090554 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00236    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.9358159  |
| test/Q_plus_P                  | -0.9358159  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.44505525  |
| stats_o/std                    | 0.029085929 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.8489123  |
| test/Q_plus_P                  | -0.8489123  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.44505444  |
| stats_o/std                    | 0.029080382 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000666   |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.9241073  |
| test/Q_plus_P                  | -0.9241073  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.44505265  |
| stats_o/std                    | 0.029077232 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00066    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.896379   |
| test/Q_plus_P                  | -0.896379   |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00253    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.4450519   |
| stats_o/std                    | 0.029073203 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000708   |
| test/info_shaping_reward_mean  | -0.0339     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.8885448  |
| test/Q_plus_P                  | -0.8885448  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.4450495   |
| stats_o/std                    | 0.029068798 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.83472    |
| test/Q_plus_P                  | -0.83472    |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.44504395 |
| stats_o/std                    | 0.02907248 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -0.8283305 |
| test/Q_plus_P                  | -0.8283305 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.598      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0664    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.44504333  |
| stats_o/std                    | 0.029067984 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000974   |
| test/info_shaping_reward_mean  | -0.0345     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.8043583  |
| test/Q_plus_P                  | -0.8043583  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 449        |
| stats_o/mean                   | 0.44504035 |
| stats_o/std                    | 0.02906531 |
| test/episodes                  | 4500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00227   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.9449018 |
| test/Q_plus_P                  | -0.9449018 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 180000     |
| train/episodes                 | 18000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0029    |
| train/info_shaping_reward_mean | -0.0578    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.9      |
| train/steps                    | 720000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.44503725  |
| stats_o/std                    | 0.029061997 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.6904089  |
| test/Q_plus_P                  | -0.6904089  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.4450358   |
| stats_o/std                    | 0.029056095 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00049    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8040416  |
| test/Q_plus_P                  | -0.8040416  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.4450344   |
| stats_o/std                    | 0.029054813 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000963   |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.72494453 |
| test/Q_plus_P                  | -0.72494453 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.4450332   |
| stats_o/std                    | 0.029048659 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000668   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.9043977  |
| test/Q_plus_P                  | -0.9043977  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.4450308   |
| stats_o/std                    | 0.029043725 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0331     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.777072   |
| test/Q_plus_P                  | -0.777072   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.4450313   |
| stats_o/std                    | 0.029037975 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.83417404 |
| test/Q_plus_P                  | -0.83417404 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.445029    |
| stats_o/std                    | 0.029033238 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.8668806  |
| test/Q_plus_P                  | -0.8668806  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.44502726  |
| stats_o/std                    | 0.029028183 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00233    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.0003247  |
| test/Q_plus_P                  | -1.0003247  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.4450241   |
| stats_o/std                    | 0.029026574 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0336     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.6495769  |
| test/Q_plus_P                  | -0.6495769  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.44502285  |
| stats_o/std                    | 0.029019661 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.84975606 |
| test/Q_plus_P                  | -0.84975606 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.44501945  |
| stats_o/std                    | 0.029016001 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.655       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.1646776  |
| test/Q_plus_P                  | -1.1646776  |
| test/reward_per_eps            | -13.8       |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 461        |
| stats_o/mean                   | 0.44501817 |
| stats_o/std                    | 0.0290132  |
| test/episodes                  | 4620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00637   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -0.8683022 |
| test/Q_plus_P                  | -0.8683022 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 184800     |
| train/episodes                 | 18480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00327   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 739200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.4450167   |
| stats_o/std                    | 0.029007694 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00328    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.76338255 |
| test/Q_plus_P                  | -0.76338255 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.44501412  |
| stats_o/std                    | 0.029003605 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00415    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.90620786 |
| test/Q_plus_P                  | -0.90620786 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.4450101  |
| stats_o/std                    | 0.029001   |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.8939449 |
| test/Q_plus_P                  | -0.8939449 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.66       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0588    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.44500652  |
| stats_o/std                    | 0.029000254 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00445    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.8436023  |
| test/Q_plus_P                  | -0.8436023  |
| test/reward_per_eps            | -8          |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00454    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.44500354 |
| stats_o/std                    | 0.02899727 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00708   |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -0.7072851 |
| test/Q_plus_P                  | -0.7072851 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.44500038  |
| stats_o/std                    | 0.028996719 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.81486243 |
| test/Q_plus_P                  | -0.81486243 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00362    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.44499815  |
| stats_o/std                    | 0.028994726 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.9127356  |
| test/Q_plus_P                  | -0.9127356  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.44499636  |
| stats_o/std                    | 0.028992662 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00545    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.8198871  |
| test/Q_plus_P                  | -0.8198871  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00415    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.4449955  |
| stats_o/std                    | 0.0289858  |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0419    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -0.9680844 |
| test/Q_plus_P                  | -0.9680844 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.665      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0552    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.444994    |
| stats_o/std                    | 0.02898071  |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.89926064 |
| test/Q_plus_P                  | -0.89926064 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.44499087  |
| stats_o/std                    | 0.028978603 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000693   |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.8221543  |
| test/Q_plus_P                  | -0.8221543  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.44499     |
| stats_o/std                    | 0.028973578 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0029     |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.87557167 |
| test/Q_plus_P                  | -0.87557167 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.44498828  |
| stats_o/std                    | 0.028968953 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.0250845  |
| test/Q_plus_P                  | -1.0250845  |
| test/reward_per_eps            | -9          |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.44498363  |
| stats_o/std                    | 0.028968476 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00595    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0712813  |
| test/Q_plus_P                  | -1.0712813  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.44498262  |
| stats_o/std                    | 0.028965281 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.93733686 |
| test/Q_plus_P                  | -0.93733686 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.44497883  |
| stats_o/std                    | 0.028964097 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.9345507  |
| test/Q_plus_P                  | -0.9345507  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.44497576  |
| stats_o/std                    | 0.028958535 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000904   |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.9005031  |
| test/Q_plus_P                  | -0.9005031  |
| test/reward_per_eps            | -8          |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.44497147  |
| stats_o/std                    | 0.02895805  |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000848   |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.82854736 |
| test/Q_plus_P                  | -0.82854736 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.44496974  |
| stats_o/std                    | 0.028954191 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.93138033 |
| test/Q_plus_P                  | -0.93138033 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00216    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.44496837  |
| stats_o/std                    | 0.028951088 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.9546011  |
| test/Q_plus_P                  | -0.9546011  |
| test/reward_per_eps            | -8          |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.44496453  |
| stats_o/std                    | 0.028949255 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.8963568  |
| test/Q_plus_P                  | -0.8963568  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.4449618   |
| stats_o/std                    | 0.028944938 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000796   |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.9132906  |
| test/Q_plus_P                  | -0.9132906  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.4449586   |
| stats_o/std                    | 0.028940598 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0371     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.8479301  |
| test/Q_plus_P                  | -0.8479301  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.44495568  |
| stats_o/std                    | 0.028939076 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.86773765 |
| test/Q_plus_P                  | -0.86773765 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.44495377  |
| stats_o/std                    | 0.028936103 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00683    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.8787261  |
| test/Q_plus_P                  | -0.8787261  |
| test/reward_per_eps            | -8          |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.4449502   |
| stats_o/std                    | 0.028934518 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9472778  |
| test/Q_plus_P                  | -0.9472778  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.4449483   |
| stats_o/std                    | 0.028932193 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00341    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.89249754 |
| test/Q_plus_P                  | -0.89249754 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.4449422   |
| stats_o/std                    | 0.028933572 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.01999    |
| test/Q_plus_P                  | -1.01999    |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.44493958  |
| stats_o/std                    | 0.028930763 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.85951704 |
| test/Q_plus_P                  | -0.85951704 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.44493732  |
| stats_o/std                    | 0.028930167 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.7942865  |
| test/Q_plus_P                  | -0.7942865  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.4449352   |
| stats_o/std                    | 0.028926231 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.946989   |
| test/Q_plus_P                  | -0.946989   |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.444932    |
| stats_o/std                    | 0.028921971 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.0436822  |
| test/Q_plus_P                  | -1.0436822  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.44493067  |
| stats_o/std                    | 0.028921502 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.6866073  |
| test/Q_plus_P                  | -0.6866073  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.44492805  |
| stats_o/std                    | 0.028918676 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00233    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7186409  |
| test/Q_plus_P                  | -0.7186409  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.44492432  |
| stats_o/std                    | 0.02891475  |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0023     |
| test/info_shaping_reward_mean  | -0.0341     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -0.70003104 |
| test/Q_plus_P                  | -0.70003104 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.44492126  |
| stats_o/std                    | 0.028911235 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.73828423 |
| test/Q_plus_P                  | -0.73828423 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.44492042  |
| stats_o/std                    | 0.02890849  |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00596    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.74787134 |
| test/Q_plus_P                  | -0.74787134 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.44491705  |
| stats_o/std                    | 0.028906697 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00303    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7982646  |
| test/Q_plus_P                  | -0.7982646  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.44491586  |
| stats_o/std                    | 0.028905183 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00465    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.8048361  |
| test/Q_plus_P                  | -0.8048361  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.4449135   |
| stats_o/std                    | 0.02890004  |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.72980857 |
| test/Q_plus_P                  | -0.72980857 |
| test/reward_per_eps            | -7          |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.44491112  |
| stats_o/std                    | 0.02889651  |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000923   |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -0.87409234 |
| test/Q_plus_P                  | -0.87409234 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.4449099   |
| stats_o/std                    | 0.028892865 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00311    |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.885365   |
| test/Q_plus_P                  | -0.885365   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.44490793  |
| stats_o/std                    | 0.028889267 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00354    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.94603276 |
| test/Q_plus_P                  | -0.94603276 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.44490638  |
| stats_o/std                    | 0.028882893 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000852   |
| test/info_shaping_reward_mean  | -0.0314     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.76078284 |
| test/Q_plus_P                  | -0.76078284 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.44490588  |
| stats_o/std                    | 0.028878937 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.9966528  |
| test/Q_plus_P                  | -0.9966528  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.44490287  |
| stats_o/std                    | 0.028878786 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000393   |
| test/info_shaping_reward_mean  | -0.0326     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.8382699  |
| test/Q_plus_P                  | -0.8382699  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.4449012   |
| stats_o/std                    | 0.028879547 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.033      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.7058179  |
| test/Q_plus_P                  | -0.7058179  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.44489956 |
| stats_o/std                    | 0.0288762  |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0389    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.9976311 |
| test/Q_plus_P                  | -0.9976311 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0589    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.44489694 |
| stats_o/std                    | 0.02887565 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000536  |
| test/info_shaping_reward_mean  | -0.0358    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -0.8172877 |
| test/Q_plus_P                  | -0.8172877 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00326   |
| train/info_shaping_reward_mean | -0.0603    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.44489583 |
| stats_o/std                    | 0.02887193 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000689  |
| test/info_shaping_reward_mean  | -0.0384    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -0.900561  |
| test/Q_plus_P                  | -0.900561  |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00256   |
| train/info_shaping_reward_mean | -0.0559    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.44489273 |
| stats_o/std                    | 0.02887032 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00231   |
| test/info_shaping_reward_mean  | -0.0385    |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -0.7899704 |
| test/Q_plus_P                  | -0.7899704 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00239   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.4448899   |
| stats_o/std                    | 0.028868461 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000729   |
| test/info_shaping_reward_mean  | -0.0361     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.9031577  |
| test/Q_plus_P                  | -0.9031577  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.4448857   |
| stats_o/std                    | 0.028868312 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000957   |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.8474074  |
| test/Q_plus_P                  | -0.8474074  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.444883    |
| stats_o/std                    | 0.028864203 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.655       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.2182777  |
| test/Q_plus_P                  | -1.2182777  |
| test/reward_per_eps            | -13.8       |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.44488204  |
| stats_o/std                    | 0.028859233 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0325     |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -0.69647956 |
| test/Q_plus_P                  | -0.69647956 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.44488013  |
| stats_o/std                    | 0.028854163 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.9012528  |
| test/Q_plus_P                  | -0.9012528  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.4448787   |
| stats_o/std                    | 0.028853083 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.77374864 |
| test/Q_plus_P                  | -0.77374864 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.44487643  |
| stats_o/std                    | 0.028850893 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.98133796 |
| test/Q_plus_P                  | -0.98133796 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.44487473  |
| stats_o/std                    | 0.028851269 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000909   |
| test/info_shaping_reward_mean  | -0.0349     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.79744554 |
| test/Q_plus_P                  | -0.79744554 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0024     |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 521        |
| stats_o/mean                   | 0.44487247 |
| stats_o/std                    | 0.0288477  |
| test/episodes                  | 5220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00154   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -1.0598123 |
| test/Q_plus_P                  | -1.0598123 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 208800     |
| train/episodes                 | 20880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00264   |
| train/info_shaping_reward_mean | -0.0542    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 835200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 522        |
| stats_o/mean                   | 0.44487    |
| stats_o/std                    | 0.02884314 |
| test/episodes                  | 5230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00113   |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.0962166 |
| test/Q_plus_P                  | -1.0962166 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 209200     |
| train/episodes                 | 20920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.703      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0523    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 836800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.44486776  |
| stats_o/std                    | 0.028842008 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.95480216 |
| test/Q_plus_P                  | -0.95480216 |
| test/reward_per_eps            | -8          |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 524        |
| stats_o/mean                   | 0.4448638  |
| stats_o/std                    | 0.02884108 |
| test/episodes                  | 5250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -1.0201862 |
| test/Q_plus_P                  | -1.0201862 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 210000     |
| train/episodes                 | 21000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00282   |
| train/info_shaping_reward_mean | -0.0592    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 840000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.44486353  |
| stats_o/std                    | 0.028838841 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.9369853  |
| test/Q_plus_P                  | -0.9369853  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.44486067  |
| stats_o/std                    | 0.028837768 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.1635457  |
| test/Q_plus_P                  | -1.1635457  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.4448596   |
| stats_o/std                    | 0.028836789 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00229    |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.0063505  |
| test/Q_plus_P                  | -1.0063505  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.44485787  |
| stats_o/std                    | 0.028834047 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000258   |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.1718013  |
| test/Q_plus_P                  | -1.1718013  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.4448553   |
| stats_o/std                    | 0.028831229 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.7669906  |
| test/Q_plus_P                  | -0.7669906  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.44485274  |
| stats_o/std                    | 0.028831178 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000686   |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.9436749  |
| test/Q_plus_P                  | -0.9436749  |
| test/reward_per_eps            | -8          |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.44485298  |
| stats_o/std                    | 0.028825969 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.9209786  |
| test/Q_plus_P                  | -0.9209786  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.44485077  |
| stats_o/std                    | 0.028824551 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.9576552  |
| test/Q_plus_P                  | -0.9576552  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.44484952  |
| stats_o/std                    | 0.028825382 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000901   |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.97830385 |
| test/Q_plus_P                  | -0.97830385 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.4448476   |
| stats_o/std                    | 0.028822392 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.87020105 |
| test/Q_plus_P                  | -0.87020105 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.44484618  |
| stats_o/std                    | 0.028818166 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00192    |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.8133819  |
| test/Q_plus_P                  | -0.8133819  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 536        |
| stats_o/mean                   | 0.4448448  |
| stats_o/std                    | 0.02881636 |
| test/episodes                  | 5370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00162   |
| test/info_shaping_reward_mean  | -0.0404    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -0.9585045 |
| test/Q_plus_P                  | -0.9585045 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 214800     |
| train/episodes                 | 21480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.634      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0574    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 859200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.44484243  |
| stats_o/std                    | 0.028816747 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00252    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.88811606 |
| test/Q_plus_P                  | -0.88811606 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.44484234  |
| stats_o/std                    | 0.028818687 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00435    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.98783624 |
| test/Q_plus_P                  | -0.98783624 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 539        |
| stats_o/mean                   | 0.44484067 |
| stats_o/std                    | 0.02881548 |
| test/episodes                  | 5400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00195   |
| test/info_shaping_reward_mean  | -0.0392    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.9876046 |
| test/Q_plus_P                  | -0.9876046 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 216000     |
| train/episodes                 | 21600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00327   |
| train/info_shaping_reward_mean | -0.0603    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 864000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.44483855  |
| stats_o/std                    | 0.028812528 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.86055136 |
| test/Q_plus_P                  | -0.86055136 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 541        |
| stats_o/mean                   | 0.4448388  |
| stats_o/std                    | 0.02880804 |
| test/episodes                  | 5420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -0.9298023 |
| test/Q_plus_P                  | -0.9298023 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 216800     |
| train/episodes                 | 21680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 867200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.44483626  |
| stats_o/std                    | 0.028805442 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.87500554 |
| test/Q_plus_P                  | -0.87500554 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.44483495 |
| stats_o/std                    | 0.02880355 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -1.1866841 |
| test/Q_plus_P                  | -1.1866841 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00306   |
| train/info_shaping_reward_mean | -0.0574    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.44483277  |
| stats_o/std                    | 0.028801754 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00308    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.8242354  |
| test/Q_plus_P                  | -0.8242354  |
| test/reward_per_eps            | -7          |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 545        |
| stats_o/mean                   | 0.4448315  |
| stats_o/std                    | 0.02880047 |
| test/episodes                  | 5460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0414    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.0200256 |
| test/Q_plus_P                  | -1.0200256 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 218400     |
| train/episodes                 | 21840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 873600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.44482982  |
| stats_o/std                    | 0.028798888 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.8335471  |
| test/Q_plus_P                  | -0.8335471  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.44482672  |
| stats_o/std                    | 0.028798208 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00369    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.9294194  |
| test/Q_plus_P                  | -0.9294194  |
| test/reward_per_eps            | -8          |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.44482636  |
| stats_o/std                    | 0.028794667 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00209    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.9515932  |
| test/Q_plus_P                  | -0.9515932  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.44482386  |
| stats_o/std                    | 0.028792948 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000267   |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.84858    |
| test/Q_plus_P                  | -0.84858    |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.44482157  |
| stats_o/std                    | 0.028791042 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00244    |
| test/info_shaping_reward_mean  | -0.0348     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.89902365 |
| test/Q_plus_P                  | -0.89902365 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00433    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 551         |
| stats_o/mean                   | 0.44482037  |
| stats_o/std                    | 0.02878765  |
| test/episodes                  | 5520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.90224594 |
| test/Q_plus_P                  | -0.90224594 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 220800      |
| train/episodes                 | 22080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 883200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.4448187   |
| stats_o/std                    | 0.028783731 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0026     |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.89595145 |
| test/Q_plus_P                  | -0.89595145 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.4448183   |
| stats_o/std                    | 0.028781138 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00436    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.109933   |
| test/Q_plus_P                  | -1.109933   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.44481647 |
| stats_o/std                    | 0.02877911 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000932  |
| test/info_shaping_reward_mean  | -0.0377    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -0.8068397 |
| test/Q_plus_P                  | -0.8068397 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.624      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00295   |
| train/info_shaping_reward_mean | -0.0614    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.444813    |
| stats_o/std                    | 0.028779069 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.9905925  |
| test/Q_plus_P                  | -0.9905925  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.44481167  |
| stats_o/std                    | 0.028775074 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00181    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.87512636 |
| test/Q_plus_P                  | -0.87512636 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.44480968  |
| stats_o/std                    | 0.028773734 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.78406024 |
| test/Q_plus_P                  | -0.78406024 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.44480875  |
| stats_o/std                    | 0.028769465 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000477   |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.8909931  |
| test/Q_plus_P                  | -0.8909931  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.44480804  |
| stats_o/std                    | 0.02876653  |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00215    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.70519614 |
| test/Q_plus_P                  | -0.70519614 |
| test/reward_per_eps            | -7          |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.44480643  |
| stats_o/std                    | 0.028763244 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000745   |
| test/info_shaping_reward_mean  | -0.0343     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.7789024  |
| test/Q_plus_P                  | -0.7789024  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.44480407 |
| stats_o/std                    | 0.02876064 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00205   |
| test/info_shaping_reward_mean  | -0.0406    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -0.8592674 |
| test/Q_plus_P                  | -0.8592674 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00274   |
| train/info_shaping_reward_mean | -0.0574    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.4448022   |
| stats_o/std                    | 0.028758481 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.97777754 |
| test/Q_plus_P                  | -0.97777754 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.44480368  |
| stats_o/std                    | 0.028755924 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000806   |
| test/info_shaping_reward_mean  | -0.0353     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.8624958  |
| test/Q_plus_P                  | -0.8624958  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.44480273  |
| stats_o/std                    | 0.028753517 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00243    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.0043993  |
| test/Q_plus_P                  | -1.0043993  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.44479904  |
| stats_o/std                    | 0.028753743 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.9677292  |
| test/Q_plus_P                  | -0.9677292  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.44479647  |
| stats_o/std                    | 0.028751755 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.8420594  |
| test/Q_plus_P                  | -0.8420594  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 567        |
| stats_o/mean                   | 0.44479433 |
| stats_o/std                    | 0.02874896 |
| test/episodes                  | 5680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00461   |
| test/info_shaping_reward_mean  | -0.0381    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.0157905 |
| test/Q_plus_P                  | -1.0157905 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 227200     |
| train/episodes                 | 22720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00323   |
| train/info_shaping_reward_mean | -0.0557    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 908800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.44479206  |
| stats_o/std                    | 0.028747268 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.85889834 |
| test/Q_plus_P                  | -0.85889834 |
| test/reward_per_eps            | -7          |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.444791    |
| stats_o/std                    | 0.028745951 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.94187105 |
| test/Q_plus_P                  | -0.94187105 |
| test/reward_per_eps            | -8          |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.4447898   |
| stats_o/std                    | 0.028741354 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000903   |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.95491165 |
| test/Q_plus_P                  | -0.95491165 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.4447877   |
| stats_o/std                    | 0.028737405 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.9249867  |
| test/Q_plus_P                  | -0.9249867  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 572        |
| stats_o/mean                   | 0.44478667 |
| stats_o/std                    | 0.0287313  |
| test/episodes                  | 5730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00185   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -0.9379989 |
| test/Q_plus_P                  | -0.9379989 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 229200     |
| train/episodes                 | 22920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.668      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00279   |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.228     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 916800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.4447863   |
| stats_o/std                    | 0.028724154 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.82521    |
| test/Q_plus_P                  | -0.82521    |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.4447856   |
| stats_o/std                    | 0.028724082 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00367    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.9137827  |
| test/Q_plus_P                  | -0.9137827  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.61        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.44478592  |
| stats_o/std                    | 0.028719937 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00429    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.8836148  |
| test/Q_plus_P                  | -0.8836148  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.44478333 |
| stats_o/std                    | 0.02871793 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00569   |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -0.8797362 |
| test/Q_plus_P                  | -0.8797362 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0603    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.44478396  |
| stats_o/std                    | 0.028714122 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00251    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.2126439  |
| test/Q_plus_P                  | -1.2126439  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00513    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.44478354  |
| stats_o/std                    | 0.028712397 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00288    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.7804827  |
| test/Q_plus_P                  | -0.7804827  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.44478145  |
| stats_o/std                    | 0.028708776 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.0379324  |
| test/Q_plus_P                  | -1.0379324  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.44478026  |
| stats_o/std                    | 0.028702253 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00459    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.7774233  |
| test/Q_plus_P                  | -0.7774233  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00394    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.4447777   |
| stats_o/std                    | 0.028702425 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00323    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.8317001  |
| test/Q_plus_P                  | -0.8317001  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0041     |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.44477668  |
| stats_o/std                    | 0.028700015 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000843   |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.82346797 |
| test/Q_plus_P                  | -0.82346797 |
| test/reward_per_eps            | -7          |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.44477502  |
| stats_o/std                    | 0.028696777 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0362     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.845308   |
| test/Q_plus_P                  | -0.845308   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.44477442  |
| stats_o/std                    | 0.028689126 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.9494029  |
| test/Q_plus_P                  | -0.9494029  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.44477275  |
| stats_o/std                    | 0.028687209 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000866   |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.8328928  |
| test/Q_plus_P                  | -0.8328928  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.44476977  |
| stats_o/std                    | 0.028683051 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.9417694  |
| test/Q_plus_P                  | -0.9417694  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.44476935  |
| stats_o/std                    | 0.028678944 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00285    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.1011962  |
| test/Q_plus_P                  | -1.1011962  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.4447677   |
| stats_o/std                    | 0.028676221 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.9440754  |
| test/Q_plus_P                  | -0.9440754  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.44476452  |
| stats_o/std                    | 0.028674582 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000647   |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.78009534 |
| test/Q_plus_P                  | -0.78009534 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 590        |
| stats_o/mean                   | 0.44476524 |
| stats_o/std                    | 0.02867361 |
| test/episodes                  | 5910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00109   |
| test/info_shaping_reward_mean  | -0.0395    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -0.8302907 |
| test/Q_plus_P                  | -0.8302907 |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 236400     |
| train/episodes                 | 23640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00403   |
| train/info_shaping_reward_mean | -0.0579    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 945600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.44476447  |
| stats_o/std                    | 0.028669683 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000964   |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.7475096  |
| test/Q_plus_P                  | -0.7475096  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.4447635   |
| stats_o/std                    | 0.028665604 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.8290517  |
| test/Q_plus_P                  | -0.8290517  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.4447614   |
| stats_o/std                    | 0.028662955 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0344     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.7720253  |
| test/Q_plus_P                  | -0.7720253  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.44475988  |
| stats_o/std                    | 0.028659696 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.85501176 |
| test/Q_plus_P                  | -0.85501176 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.44475904  |
| stats_o/std                    | 0.028656194 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0375     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.75558615 |
| test/Q_plus_P                  | -0.75558615 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.44475946  |
| stats_o/std                    | 0.028653583 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00146    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.8398432  |
| test/Q_plus_P                  | -0.8398432  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.44475725  |
| stats_o/std                    | 0.028653773 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.8873326  |
| test/Q_plus_P                  | -0.8873326  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.44475558  |
| stats_o/std                    | 0.028652435 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00452    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.0356768  |
| test/Q_plus_P                  | -1.0356768  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.4447535  |
| stats_o/std                    | 0.02865091 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0031    |
| test/info_shaping_reward_mean  | -0.0376    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -0.7934787 |
| test/Q_plus_P                  | -0.7934787 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.637      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00389   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
