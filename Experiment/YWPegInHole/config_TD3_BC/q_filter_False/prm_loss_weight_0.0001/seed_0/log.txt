Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC/q_filter_False/prm_loss_weight_0.0001/seed_0
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.44477853  |
| stats_o/std                    | 0.030474566 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.295       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000977   |
| test/info_shaping_reward_mean  | -0.0786     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.2848418  |
| test/Q_plus_P                  | -1.2848418  |
| test/reward_per_eps            | -28.2       |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0593     |
| train/info_shaping_reward_mean | -0.116      |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.44478643  |
| stats_o/std                    | 0.030184686 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.14        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.084      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.6136088  |
| test/Q_plus_P                  | -1.6136088  |
| test/reward_per_eps            | -34.4       |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0609     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.44516563  |
| stats_o/std                    | 0.030633317 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0775      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00618    |
| test/info_shaping_reward_mean  | -0.0848     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.8766665  |
| test/Q_plus_P                  | -1.8766665  |
| test/reward_per_eps            | -36.9       |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0194      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0446     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.44536284 |
| stats_o/std                    | 0.0308162  |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.138      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0793    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -2.1125343 |
| test/Q_plus_P                  | -2.1125343 |
| test/reward_per_eps            | -34.5      |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0338     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0464    |
| train/info_shaping_reward_mean | -0.105     |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.6      |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.44528708  |
| stats_o/std                    | 0.031037584 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.273       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0735     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -2.2217627  |
| test/Q_plus_P                  | -2.2217627  |
| test/reward_per_eps            | -29.1       |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.00688     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0507     |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.44542262  |
| stats_o/std                    | 0.031353593 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.44        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0588     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -2.066289   |
| test/Q_plus_P                  | -2.066289   |
| test/reward_per_eps            | -22.4       |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0319      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0417     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.7       |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.4459548   |
| stats_o/std                    | 0.031788103 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.297       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000682   |
| test/info_shaping_reward_mean  | -0.0774     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -2.7848055  |
| test/Q_plus_P                  | -2.7848055  |
| test/reward_per_eps            | -28.1       |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00625     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0534     |
| train/info_shaping_reward_mean | -0.123      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.4458886   |
| stats_o/std                    | 0.031780798 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.297       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0698     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -3.0846918  |
| test/Q_plus_P                  | -3.0846918  |
| test/reward_per_eps            | -28.1       |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0212      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0477     |
| train/info_shaping_reward_mean | -0.112      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.44617262  |
| stats_o/std                    | 0.031997196 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.265       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000652   |
| test/info_shaping_reward_mean  | -0.0736     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -3.3366385  |
| test/Q_plus_P                  | -3.3366385  |
| test/reward_per_eps            | -29.4       |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.005       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0497     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.8       |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.44645715  |
| stats_o/std                    | 0.032169845 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.37        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000806   |
| test/info_shaping_reward_mean  | -0.0649     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -3.446599   |
| test/Q_plus_P                  | -3.446599   |
| test/reward_per_eps            | -25.2       |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00688     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0542     |
| train/info_shaping_reward_mean | -0.118      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.44694152  |
| stats_o/std                    | 0.032470174 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.177       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000962   |
| test/info_shaping_reward_mean  | -0.0811     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -4.246794   |
| test/Q_plus_P                  | -4.246794   |
| test/reward_per_eps            | -32.9       |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00875     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0531     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.285      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.6       |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.44719625  |
| stats_o/std                    | 0.032417666 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.412       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0651     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -3.397537   |
| test/Q_plus_P                  | -3.397537   |
| test/reward_per_eps            | -23.5       |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0144      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0547     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 12         |
| stats_o/mean                   | 0.4472746  |
| stats_o/std                    | 0.03242392 |
| test/episodes                  | 130        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.312      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0769    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -4.286916  |
| test/Q_plus_P                  | -4.286916  |
| test/reward_per_eps            | -27.5      |
| test/steps                     | 5200       |
| train/episodes                 | 520        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0138     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0485    |
| train/info_shaping_reward_mean | -0.117     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 20800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.4474258   |
| stats_o/std                    | 0.032387596 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.422       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0596     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -4.005172   |
| test/Q_plus_P                  | -4.005172   |
| test/reward_per_eps            | -23.1       |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.00688     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0472     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.4475067   |
| stats_o/std                    | 0.032423113 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.27        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0729     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -4.8445067  |
| test/Q_plus_P                  | -4.8445067  |
| test/reward_per_eps            | -29.2       |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0187      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0473     |
| train/info_shaping_reward_mean | -0.117      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 15         |
| stats_o/mean                   | 0.44767442 |
| stats_o/std                    | 0.03251602 |
| test/episodes                  | 160        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.245      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0785    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -5.348398  |
| test/Q_plus_P                  | -5.348398  |
| test/reward_per_eps            | -30.2      |
| test/steps                     | 6400       |
| train/episodes                 | 640        |
| train/info_is_success_max      | 0.4        |
| train/info_is_success_mean     | 0.0475     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0381    |
| train/info_shaping_reward_mean | -0.119     |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.1      |
| train/steps                    | 25600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.44771066  |
| stats_o/std                    | 0.032580547 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.378       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000809   |
| test/info_shaping_reward_mean  | -0.0718     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -4.7970977  |
| test/Q_plus_P                  | -4.7970977  |
| test/reward_per_eps            | -24.9       |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0175      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0417     |
| train/info_shaping_reward_mean | -0.122      |
| train/info_shaping_reward_min  | -0.278      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.44762424  |
| stats_o/std                    | 0.032444235 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.292       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0739     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -5.2415514  |
| test/Q_plus_P                  | -5.2415514  |
| test/reward_per_eps            | -28.3       |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0912      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.02       |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.4       |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.44758168  |
| stats_o/std                    | 0.032298367 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.352       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0716     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -5.226057   |
| test/Q_plus_P                  | -5.226057   |
| test/reward_per_eps            | -25.9       |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0.8         |
| train/info_is_success_mean     | 0.0738      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0169     |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.4474876  |
| stats_o/std                    | 0.03209653 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.347      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000791  |
| test/info_shaping_reward_mean  | -0.0754    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -5.3769226 |
| test/Q_plus_P                  | -5.3769226 |
| test/reward_per_eps            | -26.1      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0719     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0279    |
| train/info_shaping_reward_mean | -0.0992    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.1      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.44751045 |
| stats_o/std                    | 0.0319633  |
| test/episodes                  | 210        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.417      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0675    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -5.102744  |
| test/Q_plus_P                  | -5.102744  |
| test/reward_per_eps            | -23.3      |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0.8        |
| train/info_is_success_mean     | 0.065      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0204    |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -37.4      |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 21         |
| stats_o/mean                   | 0.44749418 |
| stats_o/std                    | 0.03185365 |
| test/episodes                  | 220        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.367      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0683    |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -5.6026325 |
| test/Q_plus_P                  | -5.6026325 |
| test/reward_per_eps            | -25.3      |
| test/steps                     | 8800       |
| train/episodes                 | 880        |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.135      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0132    |
| train/info_shaping_reward_mean | -0.1       |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.6      |
| train/steps                    | 35200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.447453   |
| stats_o/std                    | 0.03171883 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.49       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00186   |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -4.3952727 |
| test/Q_plus_P                  | -4.3952727 |
| test/reward_per_eps            | -20.4      |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.115      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0151    |
| train/info_shaping_reward_mean | -0.0997    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -35.4      |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.44734284 |
| stats_o/std                    | 0.03162333 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.485      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0663    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -4.769435  |
| test/Q_plus_P                  | -4.769435  |
| test/reward_per_eps            | -20.6      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.141      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00924   |
| train/info_shaping_reward_mean | -0.0938    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.4      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 24         |
| stats_o/mean                   | 0.44731808 |
| stats_o/std                    | 0.03145812 |
| test/episodes                  | 250        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.445      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00357   |
| test/info_shaping_reward_mean  | -0.0677    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -4.930625  |
| test/Q_plus_P                  | -4.930625  |
| test/reward_per_eps            | -22.2      |
| test/steps                     | 10000      |
| train/episodes                 | 1000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.216      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00717   |
| train/info_shaping_reward_mean | -0.089     |
| train/info_shaping_reward_min  | -0.231     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.4      |
| train/steps                    | 40000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.44724968  |
| stats_o/std                    | 0.031389195 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.535       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.0633     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -4.540387   |
| test/Q_plus_P                  | -4.540387   |
| test/reward_per_eps            | -18.6       |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.198       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00762    |
| train/info_shaping_reward_mean | -0.0943     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.1       |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.4472166  |
| stats_o/std                    | 0.03127541 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000815  |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -4.5101757 |
| test/Q_plus_P                  | -4.5101757 |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.218      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0096    |
| train/info_shaping_reward_mean | -0.0915    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -31.3      |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.44718158  |
| stats_o/std                    | 0.031187864 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.468       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00186    |
| test/info_shaping_reward_mean  | -0.0649     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -5.0755534  |
| test/Q_plus_P                  | -5.0755534  |
| test/reward_per_eps            | -21.3       |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.197       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.01       |
| train/info_shaping_reward_mean | -0.0927     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.1       |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.447145    |
| stats_o/std                    | 0.031098092 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.51        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0637     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -5.204171   |
| test/Q_plus_P                  | -5.204171   |
| test/reward_per_eps            | -19.6       |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.251       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00414    |
| train/info_shaping_reward_mean | -0.0869     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.9       |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.44709706  |
| stats_o/std                    | 0.031019032 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.48        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000898   |
| test/info_shaping_reward_mean  | -0.0645     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -5.420838   |
| test/Q_plus_P                  | -5.420838   |
| test/reward_per_eps            | -20.8       |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.384       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0776     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.6       |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.4470509  |
| stats_o/std                    | 0.03093596 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.53       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00118   |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -4.798132  |
| test/Q_plus_P                  | -4.798132  |
| test/reward_per_eps            | -18.8      |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.346      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0029    |
| train/info_shaping_reward_mean | -0.078     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -26.2      |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.44695985  |
| stats_o/std                    | 0.030862452 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.552       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -4.822343   |
| test/Q_plus_P                  | -4.822343   |
| test/reward_per_eps            | -17.9       |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.319       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0795     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.2       |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.4468987   |
| stats_o/std                    | 0.030820936 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.557       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -4.9035096  |
| test/Q_plus_P                  | -4.9035096  |
| test/reward_per_eps            | -17.7       |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.403       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0747     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.9       |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.44685674  |
| stats_o/std                    | 0.030722158 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.542       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00304    |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -4.892207   |
| test/Q_plus_P                  | -4.892207   |
| test/reward_per_eps            | -18.3       |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.362       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00484    |
| train/info_shaping_reward_mean | -0.0786     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -25.5       |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.44680467  |
| stats_o/std                    | 0.030643284 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.585       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000632   |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -4.5531416  |
| test/Q_plus_P                  | -4.5531416  |
| test/reward_per_eps            | -16.6       |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.483       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.44671372  |
| stats_o/std                    | 0.030600382 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.54        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000612   |
| test/info_shaping_reward_mean  | -0.058      |
| test/info_shaping_reward_min   | -0.214      |
| test/Q                         | -5.0519876  |
| test/Q_plus_P                  | -5.0519876  |
| test/reward_per_eps            | -18.4       |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.448       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0731     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.1       |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.44664875 |
| stats_o/std                    | 0.03052899 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.562      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00136   |
| test/info_shaping_reward_mean  | -0.0551    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -4.7474127 |
| test/Q_plus_P                  | -4.7474127 |
| test/reward_per_eps            | -17.5      |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.392      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0719    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -24.3      |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.44661793  |
| stats_o/std                    | 0.030467749 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.547       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0588     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -4.8385925  |
| test/Q_plus_P                  | -4.8385925  |
| test/reward_per_eps            | -18.1       |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.402       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0742     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.9       |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.44660446  |
| stats_o/std                    | 0.030402096 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.588       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -4.4320235  |
| test/Q_plus_P                  | -4.4320235  |
| test/reward_per_eps            | -16.5       |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.35        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0768     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.44657993  |
| stats_o/std                    | 0.030332685 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.625       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -4.2228703  |
| test/Q_plus_P                  | -4.2228703  |
| test/reward_per_eps            | -15         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.431       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0702     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.8       |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.44651392  |
| stats_o/std                    | 0.030276371 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.6         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000176   |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.207      |
| test/Q                         | -4.329717   |
| test/Q_plus_P                  | -4.329717   |
| test/reward_per_eps            | -16         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.479       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0669     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.8       |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.4464712   |
| stats_o/std                    | 0.030218115 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.552       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000801   |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -4.855534   |
| test/Q_plus_P                  | -4.855534   |
| test/reward_per_eps            | -17.9       |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.506       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0672     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.8       |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.44642714  |
| stats_o/std                    | 0.030151127 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.6         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -4.2800756  |
| test/Q_plus_P                  | -4.2800756  |
| test/reward_per_eps            | -16         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.464       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.4       |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.4463726   |
| stats_o/std                    | 0.030098686 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.598       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000574   |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -4.1028953  |
| test/Q_plus_P                  | -4.1028953  |
| test/reward_per_eps            | -16.1       |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.507       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.7       |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.4463266   |
| stats_o/std                    | 0.030056804 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.608       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -4.123209   |
| test/Q_plus_P                  | -4.123209   |
| test/reward_per_eps            | -15.7       |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00214    |
| train/info_shaping_reward_mean | -0.0663     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.44627956  |
| stats_o/std                    | 0.030026287 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -3.6866016  |
| test/Q_plus_P                  | -3.6866016  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.495       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0643     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.2       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.4462442  |
| stats_o/std                    | 0.03000133 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.585      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000971  |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -4.384243  |
| test/Q_plus_P                  | -4.384243  |
| test/reward_per_eps            | -16.6      |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.455      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00267   |
| train/info_shaping_reward_mean | -0.0701    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -21.8      |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.44620052  |
| stats_o/std                    | 0.029974332 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00194    |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -4.134303   |
| test/Q_plus_P                  | -4.134303   |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.489       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.4       |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.446115    |
| stats_o/std                    | 0.029981932 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000528   |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -3.799458   |
| test/Q_plus_P                  | -3.799458   |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.5         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0661     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20         |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.4460717   |
| stats_o/std                    | 0.029945256 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.555       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0575     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -4.517642   |
| test/Q_plus_P                  | -4.517642   |
| test/reward_per_eps            | -17.8       |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.511       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.44602373 |
| stats_o/std                    | 0.02991833 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.603      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00246   |
| test/info_shaping_reward_mean  | -0.0518    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -4.078364  |
| test/Q_plus_P                  | -4.078364  |
| test/reward_per_eps            | -15.9      |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.525      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0026    |
| train/info_shaping_reward_mean | -0.0616    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.4460059   |
| stats_o/std                    | 0.029873816 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000736   |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -3.394283   |
| test/Q_plus_P                  | -3.394283   |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.497       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.44596112  |
| stats_o/std                    | 0.029835539 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.605       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000942   |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -3.6232662  |
| test/Q_plus_P                  | -3.6232662  |
| test/reward_per_eps            | -15.8       |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.514       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.4       |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.44593883  |
| stats_o/std                    | 0.029798709 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000955   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -3.63763    |
| test/Q_plus_P                  | -3.63763    |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.483       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0643     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.7       |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.44588923  |
| stats_o/std                    | 0.029789692 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.635       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000922   |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -3.6828146  |
| test/Q_plus_P                  | -3.6828146  |
| test/reward_per_eps            | -14.6       |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.512       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.5       |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.44581696  |
| stats_o/std                    | 0.029786283 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.652       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -3.3998847  |
| test/Q_plus_P                  | -3.3998847  |
| test/reward_per_eps            | -13.9       |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.586       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.002      |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.44577238  |
| stats_o/std                    | 0.029759957 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -3.357507   |
| test/Q_plus_P                  | -3.357507   |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.44572833  |
| stats_o/std                    | 0.029728482 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000668   |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -3.376335   |
| test/Q_plus_P                  | -3.376335   |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.556       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.44569182  |
| stats_o/std                    | 0.029723227 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.63        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -3.4461908  |
| test/Q_plus_P                  | -3.4461908  |
| test/reward_per_eps            | -14.8       |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.445644    |
| stats_o/std                    | 0.029725285 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000643   |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -3.3124497  |
| test/Q_plus_P                  | -3.3124497  |
| test/reward_per_eps            | -14         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.4456091   |
| stats_o/std                    | 0.029707758 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000688   |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -2.593449   |
| test/Q_plus_P                  | -2.593449   |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.528       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.44557285  |
| stats_o/std                    | 0.029696068 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000741   |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -3.4444594  |
| test/Q_plus_P                  | -3.4444594  |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.544       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.44552946 |
| stats_o/std                    | 0.0296735  |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000715  |
| test/info_shaping_reward_mean  | -0.0467    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -3.1005943 |
| test/Q_plus_P                  | -3.1005943 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.576      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00237   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.9      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.44549152  |
| stats_o/std                    | 0.029638404 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000639   |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -3.3140764  |
| test/Q_plus_P                  | -3.3140764  |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.44544378 |
| stats_o/std                    | 0.02962868 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000948  |
| test/info_shaping_reward_mean  | -0.0456    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -2.91224   |
| test/Q_plus_P                  | -2.91224   |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.569      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00195   |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.2      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 65         |
| stats_o/mean                   | 0.44540504 |
| stats_o/std                    | 0.02960389 |
| test/episodes                  | 660        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00077   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -3.1314292 |
| test/Q_plus_P                  | -3.1314292 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 26400      |
| train/episodes                 | 2640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.599      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00205   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.1      |
| train/steps                    | 105600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.44535097  |
| stats_o/std                    | 0.029587084 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.623       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000363   |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -3.1745007  |
| test/Q_plus_P                  | -3.1745007  |
| test/reward_per_eps            | -15.1       |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.62        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.44532466  |
| stats_o/std                    | 0.029557751 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000747   |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.9265249  |
| test/Q_plus_P                  | -2.9265249  |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.44529775  |
| stats_o/std                    | 0.029537857 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000405   |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -2.8643136  |
| test/Q_plus_P                  | -2.8643136  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.44527188  |
| stats_o/std                    | 0.029527634 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00037    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.56437    |
| test/Q_plus_P                  | -2.56437    |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.538       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.4452338  |
| stats_o/std                    | 0.02953785 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000765  |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -2.7702374 |
| test/Q_plus_P                  | -2.7702374 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.542      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00218   |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.3      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.44520465  |
| stats_o/std                    | 0.029515276 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000704   |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -2.6011157  |
| test/Q_plus_P                  | -2.6011157  |
| test/reward_per_eps            | -13         |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.592       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.44518682  |
| stats_o/std                    | 0.029491797 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000326   |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -2.5524256  |
| test/Q_plus_P                  | -2.5524256  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.566       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.44516313  |
| stats_o/std                    | 0.029466426 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000586   |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.6752353  |
| test/Q_plus_P                  | -2.6752353  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.44511592  |
| stats_o/std                    | 0.029463718 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000262   |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.9135208  |
| test/Q_plus_P                  | -1.9135208  |
| test/reward_per_eps            | -10         |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00193    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.44509363  |
| stats_o/std                    | 0.029437909 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -2.76377    |
| test/Q_plus_P                  | -2.76377    |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00197    |
| train/info_shaping_reward_mean | -0.0505     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.44506192  |
| stats_o/std                    | 0.029409187 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -2.5908213  |
| test/Q_plus_P                  | -2.5908213  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.002      |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.44502643  |
| stats_o/std                    | 0.029394785 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000549   |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -2.5109694  |
| test/Q_plus_P                  | -2.5109694  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00204    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.44498527  |
| stats_o/std                    | 0.029368676 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000418   |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -2.2551603  |
| test/Q_plus_P                  | -2.2551603  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00172    |
| train/info_shaping_reward_mean | -0.0509     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.44495872  |
| stats_o/std                    | 0.029336581 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000573   |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -2.4193418  |
| test/Q_plus_P                  | -2.4193418  |
| test/reward_per_eps            | -12         |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.4449285   |
| stats_o/std                    | 0.029314125 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -2.1358876  |
| test/Q_plus_P                  | -2.1358876  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00203    |
| train/info_shaping_reward_mean | -0.0508     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.4448813   |
| stats_o/std                    | 0.029302524 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.9130024  |
| test/Q_plus_P                  | -1.9130024  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.44485065  |
| stats_o/std                    | 0.029294766 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -2.151743   |
| test/Q_plus_P                  | -2.151743   |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.4448184   |
| stats_o/std                    | 0.029298006 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000376   |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -2.2807882  |
| test/Q_plus_P                  | -2.2807882  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00181    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.44478762  |
| stats_o/std                    | 0.029279506 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -2.377281   |
| test/Q_plus_P                  | -2.377281   |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00181    |
| train/info_shaping_reward_mean | -0.0491     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.4447595   |
| stats_o/std                    | 0.029255241 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000579   |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -2.4746501  |
| test/Q_plus_P                  | -2.4746501  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00226    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.44473803  |
| stats_o/std                    | 0.029243404 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -2.0602555  |
| test/Q_plus_P                  | -2.0602555  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00222    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.44472075 |
| stats_o/std                    | 0.02922926 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00086   |
| test/info_shaping_reward_mean  | -0.0398    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -2.1576855 |
| test/Q_plus_P                  | -2.1576855 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.626      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00187   |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15        |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.4446958   |
| stats_o/std                    | 0.029205883 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000372   |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -2.0525596  |
| test/Q_plus_P                  | -2.0525596  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00206    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.44467965 |
| stats_o/std                    | 0.02917879 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -2.0780127 |
| test/Q_plus_P                  | -2.0780127 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0019    |
| train/info_shaping_reward_mean | -0.0532    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.4446508   |
| stats_o/std                    | 0.029170876 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000809   |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -2.1529734  |
| test/Q_plus_P                  | -2.1529734  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.0515     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.4446297  |
| stats_o/std                    | 0.02915968 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00065   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -2.460882  |
| test/Q_plus_P                  | -2.460882  |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00179   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.44461027  |
| stats_o/std                    | 0.029152928 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -2.3258169  |
| test/Q_plus_P                  | -2.3258169  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.572       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.44457838  |
| stats_o/std                    | 0.029144851 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000552   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -2.19537    |
| test/Q_plus_P                  | -2.19537    |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00202    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.44455656  |
| stats_o/std                    | 0.029135227 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -2.2839744  |
| test/Q_plus_P                  | -2.2839744  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.567       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.4445382   |
| stats_o/std                    | 0.029120022 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00082    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -2.3521063  |
| test/Q_plus_P                  | -2.3521063  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0023     |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.4445149   |
| stats_o/std                    | 0.029103348 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000572   |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.7891779  |
| test/Q_plus_P                  | -1.7891779  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00194    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.44450128  |
| stats_o/std                    | 0.029077783 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.9582349  |
| test/Q_plus_P                  | -1.9582349  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00182    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.44447276  |
| stats_o/std                    | 0.029078811 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00185    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -2.0190578  |
| test/Q_plus_P                  | -2.0190578  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.44445515  |
| stats_o/std                    | 0.029065581 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000508   |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -2.1252863  |
| test/Q_plus_P                  | -2.1252863  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.444448    |
| stats_o/std                    | 0.029054618 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000905   |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -2.0405903  |
| test/Q_plus_P                  | -2.0405903  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.4444294   |
| stats_o/std                    | 0.029051801 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.9669341  |
| test/Q_plus_P                  | -1.9669341  |
| test/reward_per_eps            | -11         |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0019     |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.44440892  |
| stats_o/std                    | 0.029049465 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000829   |
| test/info_shaping_reward_mean  | -0.036      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.6762081  |
| test/Q_plus_P                  | -1.6762081  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00184    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 103        |
| stats_o/mean                   | 0.44439128 |
| stats_o/std                    | 0.02903112 |
| test/episodes                  | 1040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000793  |
| test/info_shaping_reward_mean  | -0.0392    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.9010867 |
| test/Q_plus_P                  | -1.9010867 |
| test/reward_per_eps            | -11        |
| test/steps                     | 41600      |
| train/episodes                 | 4160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.63       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00256   |
| train/info_shaping_reward_mean | -0.0543    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 166400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 104        |
| stats_o/mean                   | 0.44437578 |
| stats_o/std                    | 0.02901653 |
| test/episodes                  | 1050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000748  |
| test/info_shaping_reward_mean  | -0.0384    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.8061593 |
| test/Q_plus_P                  | -1.8061593 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 42000      |
| train/episodes                 | 4200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00261   |
| train/info_shaping_reward_mean | -0.0532    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 168000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.44436577  |
| stats_o/std                    | 0.029005751 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.588       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -2.282787   |
| test/Q_plus_P                  | -2.282787   |
| test/reward_per_eps            | -16.5       |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00206    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.44435075  |
| stats_o/std                    | 0.028992072 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000419   |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -2.0464272  |
| test/Q_plus_P                  | -2.0464272  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00213    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.4443407   |
| stats_o/std                    | 0.028983004 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000723   |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.1636236  |
| test/Q_plus_P                  | -2.1636236  |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.599       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.44433045  |
| stats_o/std                    | 0.028962346 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000921   |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.9532338  |
| test/Q_plus_P                  | -1.9532338  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.44431987  |
| stats_o/std                    | 0.028953603 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000677   |
| test/info_shaping_reward_mean  | -0.0335     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.6073455  |
| test/Q_plus_P                  | -1.6073455  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.44430402  |
| stats_o/std                    | 0.028934104 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000328   |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.9478856  |
| test/Q_plus_P                  | -1.9478856  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00197    |
| train/info_shaping_reward_mean | -0.0499     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.44427702  |
| stats_o/std                    | 0.028932804 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000929   |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -2.1164398  |
| test/Q_plus_P                  | -2.1164398  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00202    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.4442619   |
| stats_o/std                    | 0.028922165 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -2.3667357  |
| test/Q_plus_P                  | -2.3667357  |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00229    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.44424486  |
| stats_o/std                    | 0.028907575 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000252   |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -2.4650142  |
| test/Q_plus_P                  | -2.4650142  |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.44422713  |
| stats_o/std                    | 0.028904773 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.1110938  |
| test/Q_plus_P                  | -2.1110938  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.4442155   |
| stats_o/std                    | 0.028893152 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000972   |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.5958736  |
| test/Q_plus_P                  | -1.5958736  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00207    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.4442      |
| stats_o/std                    | 0.028886834 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000936   |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -2.0654676  |
| test/Q_plus_P                  | -2.0654676  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.44418886  |
| stats_o/std                    | 0.028874384 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000632   |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.2056649  |
| test/Q_plus_P                  | -2.2056649  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.44417727  |
| stats_o/std                    | 0.028873878 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00151    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -2.1498322  |
| test/Q_plus_P                  | -2.1498322  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.44415942  |
| stats_o/std                    | 0.028861418 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000873   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.6549138  |
| test/Q_plus_P                  | -1.6549138  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.44414318  |
| stats_o/std                    | 0.028850377 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000916   |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.783608   |
| test/Q_plus_P                  | -1.783608   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.44413435  |
| stats_o/std                    | 0.028835846 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.665       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000673   |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -2.2513256  |
| test/Q_plus_P                  | -2.2513256  |
| test/reward_per_eps            | -13.4       |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.44411635  |
| stats_o/std                    | 0.028829368 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000636   |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.6160228  |
| test/Q_plus_P                  | -1.6160228  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.4441121   |
| stats_o/std                    | 0.028809635 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -2.244035   |
| test/Q_plus_P                  | -2.244035   |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.44410375  |
| stats_o/std                    | 0.028792894 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00032    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -2.355388   |
| test/Q_plus_P                  | -2.355388   |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.4440882  |
| stats_o/std                    | 0.02877938 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000391  |
| test/info_shaping_reward_mean  | -0.042     |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -2.0919533 |
| test/Q_plus_P                  | -2.0919533 |
| test/reward_per_eps            | -12        |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00228   |
| train/info_shaping_reward_mean | -0.0528    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 126        |
| stats_o/mean                   | 0.44408187 |
| stats_o/std                    | 0.02876647 |
| test/episodes                  | 1270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000856  |
| test/info_shaping_reward_mean  | -0.0392    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.6733152 |
| test/Q_plus_P                  | -1.6733152 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 50800      |
| train/episodes                 | 5080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00192   |
| train/info_shaping_reward_mean | -0.0512    |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 203200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.44407097  |
| stats_o/std                    | 0.028758353 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000542   |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -1.7012442  |
| test/Q_plus_P                  | -1.7012442  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00211    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.4440594   |
| stats_o/std                    | 0.028754234 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000795   |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.7587714  |
| test/Q_plus_P                  | -1.7587714  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.44405153  |
| stats_o/std                    | 0.028736109 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000311   |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.601342   |
| test/Q_plus_P                  | -1.601342   |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.44404984  |
| stats_o/std                    | 0.028729558 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0365     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.3156064  |
| test/Q_plus_P                  | -1.3156064  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.4440438   |
| stats_o/std                    | 0.028710065 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.9264367  |
| test/Q_plus_P                  | -1.9264367  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.44403413  |
| stats_o/std                    | 0.028693631 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -2.1207936  |
| test/Q_plus_P                  | -2.1207936  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.44401982  |
| stats_o/std                    | 0.028686719 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.7448486  |
| test/Q_plus_P                  | -1.7448486  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 134        |
| stats_o/mean                   | 0.44401363 |
| stats_o/std                    | 0.02866778 |
| test/episodes                  | 1350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0416    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -1.8255067 |
| test/Q_plus_P                  | -1.8255067 |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 54000      |
| train/episodes                 | 5400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00209   |
| train/info_shaping_reward_mean | -0.0516    |
| train/info_shaping_reward_min  | -0.23      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 216000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.44400632  |
| stats_o/std                    | 0.028662847 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.8579514  |
| test/Q_plus_P                  | -1.8579514  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00212    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.44400024  |
| stats_o/std                    | 0.028645536 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.8141567  |
| test/Q_plus_P                  | -1.8141567  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.4439926   |
| stats_o/std                    | 0.028630065 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000691   |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -1.6104916  |
| test/Q_plus_P                  | -1.6104916  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00248    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.44398603  |
| stats_o/std                    | 0.028619455 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000759   |
| test/info_shaping_reward_mean  | -0.0337     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.3313462  |
| test/Q_plus_P                  | -1.3313462  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.4439675   |
| stats_o/std                    | 0.028612098 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.6144354  |
| test/Q_plus_P                  | -1.6144354  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0025     |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.44395944  |
| stats_o/std                    | 0.028598063 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.6005818  |
| test/Q_plus_P                  | -1.6005818  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00253    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.44396237 |
| stats_o/std                    | 0.02858012 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000733  |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -1.6810503 |
| test/Q_plus_P                  | -1.6810503 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.594      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00277   |
| train/info_shaping_reward_mean | -0.056     |
| train/info_shaping_reward_min  | -0.236     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.2      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.44394746  |
| stats_o/std                    | 0.028568788 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -2.018476   |
| test/Q_plus_P                  | -2.018476   |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.44393834  |
| stats_o/std                    | 0.028557943 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00166    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.6478088  |
| test/Q_plus_P                  | -1.6478088  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.44393304  |
| stats_o/std                    | 0.028547755 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00205    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.6530162  |
| test/Q_plus_P                  | -1.6530162  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.44392404  |
| stats_o/std                    | 0.028540745 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000925   |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.771393   |
| test/Q_plus_P                  | -1.771393   |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.44391444  |
| stats_o/std                    | 0.028528258 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000665   |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.4093643  |
| test/Q_plus_P                  | -1.4093643  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00206    |
| train/info_shaping_reward_mean | -0.0509     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.4439073   |
| stats_o/std                    | 0.028519576 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -1.6561952  |
| test/Q_plus_P                  | -1.6561952  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.44389462 |
| stats_o/std                    | 0.02851568 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0443    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.7827973 |
| test/Q_plus_P                  | -1.7827973 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00202   |
| train/info_shaping_reward_mean | -0.0542    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.44388208  |
| stats_o/std                    | 0.028505236 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.0193858  |
| test/Q_plus_P                  | -2.0193858  |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.4438782   |
| stats_o/std                    | 0.028488053 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000919   |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.5320129  |
| test/Q_plus_P                  | -1.5320129  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.4438649   |
| stats_o/std                    | 0.028474577 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.5019177  |
| test/Q_plus_P                  | -1.5019177  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.44385746  |
| stats_o/std                    | 0.028465917 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.9894251  |
| test/Q_plus_P                  | -1.9894251  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0499     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.44385207  |
| stats_o/std                    | 0.028457021 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.9155828  |
| test/Q_plus_P                  | -1.9155828  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00215    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.44384256  |
| stats_o/std                    | 0.028449317 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00092    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.6417111  |
| test/Q_plus_P                  | -1.6417111  |
| test/reward_per_eps            | -11         |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.44383442  |
| stats_o/std                    | 0.028445244 |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000489   |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.6547558  |
| test/Q_plus_P                  | -1.6547558  |
| test/reward_per_eps            | -11         |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.44382703  |
| stats_o/std                    | 0.028432414 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.6464283  |
| test/Q_plus_P                  | -1.6464283  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.4438206   |
| stats_o/std                    | 0.028425938 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.574057   |
| test/Q_plus_P                  | -1.574057   |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.44381213  |
| stats_o/std                    | 0.028419197 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000736   |
| test/info_shaping_reward_mean  | -0.0337     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.0709418  |
| test/Q_plus_P                  | -1.0709418  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.4438031   |
| stats_o/std                    | 0.028403657 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000878   |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.218603   |
| test/Q_plus_P                  | -1.218603   |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0503     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.4438016   |
| stats_o/std                    | 0.028389744 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000867   |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -1.0999461  |
| test/Q_plus_P                  | -1.0999461  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00224    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.44379643  |
| stats_o/std                    | 0.028376011 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000883   |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.4893317  |
| test/Q_plus_P                  | -1.4893317  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.4437842   |
| stats_o/std                    | 0.028373891 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000633   |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.2283694  |
| test/Q_plus_P                  | -1.2283694  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00214    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.44378212  |
| stats_o/std                    | 0.028364299 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000964   |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.4390484  |
| test/Q_plus_P                  | -1.4390484  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.4437791   |
| stats_o/std                    | 0.028363874 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.647       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00182    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.9078189  |
| test/Q_plus_P                  | -1.9078189  |
| test/reward_per_eps            | -14.1       |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.44377944  |
| stats_o/std                    | 0.028363103 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00055    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.7374142  |
| test/Q_plus_P                  | -1.7374142  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00212    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.44377697  |
| stats_o/std                    | 0.028350784 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000431   |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.7954035  |
| test/Q_plus_P                  | -1.7954035  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.44376457 |
| stats_o/std                    | 0.02834282 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00117   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -1.6455952 |
| test/Q_plus_P                  | -1.6455952 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00219   |
| train/info_shaping_reward_mean | -0.0534    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.44375244  |
| stats_o/std                    | 0.028334232 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000848   |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.6434673  |
| test/Q_plus_P                  | -1.6434673  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.44374534  |
| stats_o/std                    | 0.028326152 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000566   |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.3194706  |
| test/Q_plus_P                  | -1.3194706  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00207    |
| train/info_shaping_reward_mean | -0.0496     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.44373238 |
| stats_o/std                    | 0.02832085 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00148   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -1.8260645 |
| test/Q_plus_P                  | -1.8260645 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.657      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00238   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.44372305  |
| stats_o/std                    | 0.028307231 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000507   |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.198      |
| test/Q                         | -1.5038878  |
| test/Q_plus_P                  | -1.5038878  |
| test/reward_per_eps            | -10         |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0505     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.44371226  |
| stats_o/std                    | 0.028303899 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000628   |
| test/info_shaping_reward_mean  | -0.0355     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.1405779  |
| test/Q_plus_P                  | -1.1405779  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.4437079  |
| stats_o/std                    | 0.02829142 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000864  |
| test/info_shaping_reward_mean  | -0.0381    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -1.3051907 |
| test/Q_plus_P                  | -1.3051907 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00253   |
| train/info_shaping_reward_mean | -0.0546    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.44370303  |
| stats_o/std                    | 0.028277034 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000756   |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.5570146  |
| test/Q_plus_P                  | -1.5570146  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00202    |
| train/info_shaping_reward_mean | -0.0506     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.44369784  |
| stats_o/std                    | 0.028267726 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.5023488  |
| test/Q_plus_P                  | -1.5023488  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.4436979   |
| stats_o/std                    | 0.028257953 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.3777509  |
| test/Q_plus_P                  | -1.3777509  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.44369498  |
| stats_o/std                    | 0.028245883 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.6208589  |
| test/Q_plus_P                  | -1.6208589  |
| test/reward_per_eps            | -11         |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.4436932  |
| stats_o/std                    | 0.02823439 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.723      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000938  |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -1.5769484 |
| test/Q_plus_P                  | -1.5769484 |
| test/reward_per_eps            | -11.1      |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00213   |
| train/info_shaping_reward_mean | -0.0526    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.44369432  |
| stats_o/std                    | 0.028222471 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.3823171  |
| test/Q_plus_P                  | -1.3823171  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.4436901  |
| stats_o/std                    | 0.02821424 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0396    |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -1.3171322 |
| test/Q_plus_P                  | -1.3171322 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.0576    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.44368497  |
| stats_o/std                    | 0.028204093 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.3598     |
| test/Q_plus_P                  | -1.3598     |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.44367635  |
| stats_o/std                    | 0.028201027 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.1459905  |
| test/Q_plus_P                  | -1.1459905  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.4436679   |
| stats_o/std                    | 0.028190592 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.568542   |
| test/Q_plus_P                  | -1.568542   |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.4436632   |
| stats_o/std                    | 0.028178772 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.6487069  |
| test/Q_plus_P                  | -1.6487069  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.44366077  |
| stats_o/std                    | 0.028170357 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -1.4047737  |
| test/Q_plus_P                  | -1.4047737  |
| test/reward_per_eps            | -10         |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.4436572   |
| stats_o/std                    | 0.028157137 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.2004645  |
| test/Q_plus_P                  | -1.2004645  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.44365087  |
| stats_o/std                    | 0.028149523 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.3270634  |
| test/Q_plus_P                  | -1.3270634  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.4436401   |
| stats_o/std                    | 0.028140461 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00181    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.1994423  |
| test/Q_plus_P                  | -1.1994423  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0502     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.44363508  |
| stats_o/std                    | 0.028131478 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.4858289  |
| test/Q_plus_P                  | -1.4858289  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0507     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.4436295   |
| stats_o/std                    | 0.028126547 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.1788783  |
| test/Q_plus_P                  | -1.1788783  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.4436264   |
| stats_o/std                    | 0.028117523 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.2793093  |
| test/Q_plus_P                  | -1.2793093  |
| test/reward_per_eps            | -9          |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.44362387  |
| stats_o/std                    | 0.028109761 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.9104668  |
| test/Q_plus_P                  | -1.9104668  |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.44361782  |
| stats_o/std                    | 0.028099513 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.4436704  |
| test/Q_plus_P                  | -1.4436704  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.44361392 |
| stats_o/std                    | 0.02808691 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.713      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00197   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -1.6454401 |
| test/Q_plus_P                  | -1.6454401 |
| test/reward_per_eps            | -11.5      |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00271   |
| train/info_shaping_reward_mean | -0.0528    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.44360772  |
| stats_o/std                    | 0.028076664 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000785   |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.3109028  |
| test/Q_plus_P                  | -1.3109028  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.44359493  |
| stats_o/std                    | 0.028074458 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00343    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.5466781  |
| test/Q_plus_P                  | -1.5466781  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.44358966  |
| stats_o/std                    | 0.028070562 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00377    |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.3585662  |
| test/Q_plus_P                  | -1.3585662  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.44358614  |
| stats_o/std                    | 0.028067047 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000782   |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.4981389  |
| test/Q_plus_P                  | -1.4981389  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.44358054  |
| stats_o/std                    | 0.028061477 |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000314   |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.2938417  |
| test/Q_plus_P                  | -1.2938417  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00206    |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.44357336  |
| stats_o/std                    | 0.028053448 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -1.1454513  |
| test/Q_plus_P                  | -1.1454513  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.4435741   |
| stats_o/std                    | 0.028042449 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.5077118  |
| test/Q_plus_P                  | -1.5077118  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00242    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.44356918  |
| stats_o/std                    | 0.028039152 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.4300566  |
| test/Q_plus_P                  | -1.4300566  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.443567    |
| stats_o/std                    | 0.028035127 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.3179775  |
| test/Q_plus_P                  | -1.3179775  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.4435613   |
| stats_o/std                    | 0.028022876 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.3112519  |
| test/Q_plus_P                  | -1.3112519  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0498     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.44355917  |
| stats_o/std                    | 0.028012061 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000604   |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -1.3257539  |
| test/Q_plus_P                  | -1.3257539  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.44355568  |
| stats_o/std                    | 0.028001504 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.2527972  |
| test/Q_plus_P                  | -1.2527972  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 207         |
| stats_o/mean                   | 0.4435556   |
| stats_o/std                    | 0.027988985 |
| test/episodes                  | 2080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000838   |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.4009839  |
| test/Q_plus_P                  | -1.4009839  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 83200       |
| train/episodes                 | 8320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 332800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.44355118  |
| stats_o/std                    | 0.027984424 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00126    |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.1945846  |
| test/Q_plus_P                  | -1.1945846  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.44354558  |
| stats_o/std                    | 0.027981764 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.57779    |
| test/Q_plus_P                  | -1.57779    |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.44354165 |
| stats_o/std                    | 0.02797729 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00115   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -1.6375403 |
| test/Q_plus_P                  | -1.6375403 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.642      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00259   |
| train/info_shaping_reward_mean | -0.0557    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.4435393   |
| stats_o/std                    | 0.027967492 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00154    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.5387945  |
| test/Q_plus_P                  | -1.5387945  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.44353175  |
| stats_o/std                    | 0.027963752 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000727   |
| test/info_shaping_reward_mean  | -0.0366     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.0278107  |
| test/Q_plus_P                  | -1.0278107  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.443529    |
| stats_o/std                    | 0.027957452 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000944   |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.0764488  |
| test/Q_plus_P                  | -1.0764488  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.44352534  |
| stats_o/std                    | 0.027954029 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000364   |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.3151844  |
| test/Q_plus_P                  | -1.3151844  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.44352248 |
| stats_o/std                    | 0.0279383  |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00262   |
| test/info_shaping_reward_mean  | -0.0375    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -1.020132  |
| test/Q_plus_P                  | -1.020132  |
| test/reward_per_eps            | -8         |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.731      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00257   |
| train/info_shaping_reward_mean | -0.0465    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -10.8      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.44351828  |
| stats_o/std                    | 0.027927002 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00261    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.0646476  |
| test/Q_plus_P                  | -1.0646476  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.4435124  |
| stats_o/std                    | 0.02791973 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.0374    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.0805573 |
| test/Q_plus_P                  | -1.0805573 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0027    |
| train/info_shaping_reward_mean | -0.0552    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.4435105   |
| stats_o/std                    | 0.027910536 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.1451629  |
| test/Q_plus_P                  | -1.1451629  |
| test/reward_per_eps            | -9          |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00245    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.44350946  |
| stats_o/std                    | 0.027903674 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0385     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.1675019  |
| test/Q_plus_P                  | -1.1675019  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.44350508 |
| stats_o/std                    | 0.02789397 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00135   |
| test/info_shaping_reward_mean  | -0.0439    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.3147966 |
| test/Q_plus_P                  | -1.3147966 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.687      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00253   |
| train/info_shaping_reward_mean | -0.0519    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 221        |
| stats_o/mean                   | 0.44350627 |
| stats_o/std                    | 0.02788272 |
| test/episodes                  | 2220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.7        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000857  |
| test/info_shaping_reward_mean  | -0.0423    |
| test/info_shaping_reward_min   | -0.223     |
| test/Q                         | -1.5199268 |
| test/Q_plus_P                  | -1.5199268 |
| test/reward_per_eps            | -12        |
| test/steps                     | 88800      |
| train/episodes                 | 8880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.677      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00294   |
| train/info_shaping_reward_mean | -0.0507    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 355200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.4435021   |
| stats_o/std                    | 0.027883893 |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.5363936  |
| test/Q_plus_P                  | -1.5363936  |
| test/reward_per_eps            | -12         |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 223        |
| stats_o/mean                   | 0.44350114 |
| stats_o/std                    | 0.02787483 |
| test/episodes                  | 2240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00177   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -1.5284626 |
| test/Q_plus_P                  | -1.5284626 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 89600      |
| train/episodes                 | 8960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00294   |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 358400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.44349992 |
| stats_o/std                    | 0.02786572 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.72       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000578  |
| test/info_shaping_reward_mean  | -0.0427    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -1.371813  |
| test/Q_plus_P                  | -1.371813  |
| test/reward_per_eps            | -11.2      |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.687      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00293   |
| train/info_shaping_reward_mean | -0.0532    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.44349763  |
| stats_o/std                    | 0.027857034 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00182    |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.154792   |
| test/Q_plus_P                  | -1.154792   |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.4434937   |
| stats_o/std                    | 0.027849825 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0008     |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.297935   |
| test/Q_plus_P                  | -1.297935   |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.4434891   |
| stats_o/std                    | 0.027846493 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.5785135  |
| test/Q_plus_P                  | -1.5785135  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.44348788 |
| stats_o/std                    | 0.02783872 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000898  |
| test/info_shaping_reward_mean  | -0.0373    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -1.1809524 |
| test/Q_plus_P                  | -1.1809524 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00286   |
| train/info_shaping_reward_mean | -0.0545    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.44348586  |
| stats_o/std                    | 0.027829079 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.1740636  |
| test/Q_plus_P                  | -1.1740636  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.44348428  |
| stats_o/std                    | 0.027822407 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00261    |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.1703424  |
| test/Q_plus_P                  | -1.1703424  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00216    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.4434794   |
| stats_o/std                    | 0.027820865 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.2181249  |
| test/Q_plus_P                  | -1.2181249  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.44347534  |
| stats_o/std                    | 0.027812032 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000331   |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.3275467  |
| test/Q_plus_P                  | -1.3275467  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.44347283  |
| stats_o/std                    | 0.027811585 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00163    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.4607126  |
| test/Q_plus_P                  | -1.4607126  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 234         |
| stats_o/mean                   | 0.4434698   |
| stats_o/std                    | 0.027807012 |
| test/episodes                  | 2350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.2318004  |
| test/Q_plus_P                  | -1.2318004  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 94000       |
| train/episodes                 | 9400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 376000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.4434646   |
| stats_o/std                    | 0.027802179 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000945   |
| test/info_shaping_reward_mean  | -0.0382     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.9764671  |
| test/Q_plus_P                  | -0.9764671  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.44346467 |
| stats_o/std                    | 0.02779981 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00266   |
| test/info_shaping_reward_mean  | -0.0433    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -1.196933  |
| test/Q_plus_P                  | -1.196933  |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.618      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00212   |
| train/info_shaping_reward_mean | -0.0581    |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.44346365  |
| stats_o/std                    | 0.027790515 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00202    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -1.319469   |
| test/Q_plus_P                  | -1.319469   |
| test/reward_per_eps            | -10         |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.44346228  |
| stats_o/std                    | 0.027783403 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00329    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.3803203  |
| test/Q_plus_P                  | -1.3803203  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.44346014  |
| stats_o/std                    | 0.027771687 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.1386774  |
| test/Q_plus_P                  | -1.1386774  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00207    |
| train/info_shaping_reward_mean | -0.0523     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.44345978  |
| stats_o/std                    | 0.027763618 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.2815601  |
| test/Q_plus_P                  | -1.2815601  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.44345772  |
| stats_o/std                    | 0.027756942 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.1559751  |
| test/Q_plus_P                  | -1.1559751  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.44345507  |
| stats_o/std                    | 0.027746163 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0352     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.92586106 |
| test/Q_plus_P                  | -0.92586106 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00207    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.4434525  |
| stats_o/std                    | 0.02774225 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000804  |
| test/info_shaping_reward_mean  | -0.0421    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.2666233 |
| test/Q_plus_P                  | -1.2666233 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00321   |
| train/info_shaping_reward_mean | -0.0551    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.44344974 |
| stats_o/std                    | 0.0277396  |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00161   |
| test/info_shaping_reward_mean  | -0.0386    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -1.0825857 |
| test/Q_plus_P                  | -1.0825857 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.609      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00281   |
| train/info_shaping_reward_mean | -0.0582    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.44344723 |
| stats_o/std                    | 0.02772563 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.2440298 |
| test/Q_plus_P                  | -1.2440298 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.688      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.44344553  |
| stats_o/std                    | 0.027717024 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.2696261  |
| test/Q_plus_P                  | -1.2696261  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.44344428  |
| stats_o/std                    | 0.027710244 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000464   |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.1233633  |
| test/Q_plus_P                  | -1.1233633  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.712       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.44344148  |
| stats_o/std                    | 0.027701318 |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000974   |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.061216   |
| test/Q_plus_P                  | -1.061216   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.4434376  |
| stats_o/std                    | 0.02769846 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00131   |
| test/info_shaping_reward_mean  | -0.0417    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.1854014 |
| test/Q_plus_P                  | -1.1854014 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00305   |
| train/info_shaping_reward_mean | -0.0582    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 250        |
| stats_o/mean                   | 0.44343355 |
| stats_o/std                    | 0.0276976  |
| test/episodes                  | 2510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000781  |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -1.210503  |
| test/Q_plus_P                  | -1.210503  |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 100400     |
| train/episodes                 | 10040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00273   |
| train/info_shaping_reward_mean | -0.0553    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 401600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.44343245  |
| stats_o/std                    | 0.027691046 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000486   |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.3201194  |
| test/Q_plus_P                  | -1.3201194  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00314    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.44343027  |
| stats_o/std                    | 0.027682759 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00468    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -1.3719198  |
| test/Q_plus_P                  | -1.3719198  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0504     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.44342923  |
| stats_o/std                    | 0.027673043 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000691   |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.614393   |
| test/Q_plus_P                  | -1.614393   |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.4434284   |
| stats_o/std                    | 0.027663631 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000332   |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.4880478  |
| test/Q_plus_P                  | -1.4880478  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.4434292   |
| stats_o/std                    | 0.027657747 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.0408     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.3594291  |
| test/Q_plus_P                  | -1.3594291  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.44342592  |
| stats_o/std                    | 0.027653446 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.413627   |
| test/Q_plus_P                  | -1.413627   |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.44342366  |
| stats_o/std                    | 0.027651517 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000599   |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.95459193 |
| test/Q_plus_P                  | -0.95459193 |
| test/reward_per_eps            | -8          |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.44342303  |
| stats_o/std                    | 0.027649129 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00341    |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.0863566  |
| test/Q_plus_P                  | -1.0863566  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.44342574  |
| stats_o/std                    | 0.027642278 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.3083444  |
| test/Q_plus_P                  | -1.3083444  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.44342244  |
| stats_o/std                    | 0.027634762 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0009     |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.1544672  |
| test/Q_plus_P                  | -1.1544672  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.44341877  |
| stats_o/std                    | 0.027636081 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000833   |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.5326838  |
| test/Q_plus_P                  | -1.5326838  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.44341493  |
| stats_o/std                    | 0.027634872 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.4956683  |
| test/Q_plus_P                  | -1.4956683  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00255    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.44341147  |
| stats_o/std                    | 0.027631065 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.1369439  |
| test/Q_plus_P                  | -1.1369439  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.44340774  |
| stats_o/std                    | 0.027623659 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000867   |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.5797813  |
| test/Q_plus_P                  | -1.5797813  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.4434092   |
| stats_o/std                    | 0.027613243 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.2089915  |
| test/Q_plus_P                  | -1.2089915  |
| test/reward_per_eps            | -9          |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.4434065   |
| stats_o/std                    | 0.027607886 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.0388     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.1734567  |
| test/Q_plus_P                  | -1.1734567  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.44340178  |
| stats_o/std                    | 0.027604626 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.1845441  |
| test/Q_plus_P                  | -1.1845441  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0489     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.4433978   |
| stats_o/std                    | 0.027601192 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -1.175317   |
| test/Q_plus_P                  | -1.175317   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 269        |
| stats_o/mean                   | 0.44339743 |
| stats_o/std                    | 0.02759605 |
| test/episodes                  | 2700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00256   |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.1497008 |
| test/Q_plus_P                  | -1.1497008 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 108000     |
| train/episodes                 | 10800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.695      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00215   |
| train/info_shaping_reward_mean | -0.0512    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 432000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.44339514  |
| stats_o/std                    | 0.02758799  |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00112    |
| test/info_shaping_reward_mean  | -0.0359     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.82810366 |
| test/Q_plus_P                  | -0.82810366 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 271        |
| stats_o/mean                   | 0.44339213 |
| stats_o/std                    | 0.02758375 |
| test/episodes                  | 2720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.0425    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -1.3033309 |
| test/Q_plus_P                  | -1.3033309 |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 108800     |
| train/episodes                 | 10880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.686      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00266   |
| train/info_shaping_reward_mean | -0.0525    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 435200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.44339204  |
| stats_o/std                    | 0.027574107 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000835   |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -1.0730807  |
| test/Q_plus_P                  | -1.0730807  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.44338942  |
| stats_o/std                    | 0.027573839 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00267    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0612019  |
| test/Q_plus_P                  | -1.0612019  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.44338897  |
| stats_o/std                    | 0.027566245 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.2180164  |
| test/Q_plus_P                  | -1.2180164  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0501     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.443385    |
| stats_o/std                    | 0.027560705 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0032     |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.105718   |
| test/Q_plus_P                  | -1.105718   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.44338313 |
| stats_o/std                    | 0.02755553 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -1.3109854 |
| test/Q_plus_P                  | -1.3109854 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00463   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.44338354  |
| stats_o/std                    | 0.027554357 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.2441502  |
| test/Q_plus_P                  | -1.2441502  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00438    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.44337985  |
| stats_o/std                    | 0.027549433 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -1.2042335  |
| test/Q_plus_P                  | -1.2042335  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.44337395  |
| stats_o/std                    | 0.027549453 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.1934974  |
| test/Q_plus_P                  | -1.1934974  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.4433762   |
| stats_o/std                    | 0.027543671 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.6885628  |
| test/Q_plus_P                  | -1.6885628  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.44337597  |
| stats_o/std                    | 0.027536474 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.1100368  |
| test/Q_plus_P                  | -1.1100368  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00338    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.44337666 |
| stats_o/std                    | 0.02753368 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000839  |
| test/info_shaping_reward_mean  | -0.0383    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.0733126 |
| test/Q_plus_P                  | -1.0733126 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00296   |
| train/info_shaping_reward_mean | -0.0559    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.44337532  |
| stats_o/std                    | 0.027530426 |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000862   |
| test/info_shaping_reward_mean  | -0.038      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.0331621  |
| test/Q_plus_P                  | -1.0331621  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.44337305 |
| stats_o/std                    | 0.02752168 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00171   |
| test/info_shaping_reward_mean  | -0.0377    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -1.1195166 |
| test/Q_plus_P                  | -1.1195166 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.686      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0526    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.44337153  |
| stats_o/std                    | 0.027515762 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.0926144  |
| test/Q_plus_P                  | -1.0926144  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00227    |
| train/info_shaping_reward_mean | -0.0516     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.4433676   |
| stats_o/std                    | 0.027508749 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0358     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.9084697  |
| test/Q_plus_P                  | -0.9084697  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.44336674  |
| stats_o/std                    | 0.027504675 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000458   |
| test/info_shaping_reward_mean  | -0.0329     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.94275093 |
| test/Q_plus_P                  | -0.94275093 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.4433662   |
| stats_o/std                    | 0.027498497 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00079    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.1174498  |
| test/Q_plus_P                  | -1.1174498  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.44336382  |
| stats_o/std                    | 0.027496053 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.2477187  |
| test/Q_plus_P                  | -1.2477187  |
| test/reward_per_eps            | -10         |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.4433675   |
| stats_o/std                    | 0.027488528 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.4765861  |
| test/Q_plus_P                  | -1.4765861  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.44336602  |
| stats_o/std                    | 0.027483908 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.519503   |
| test/Q_plus_P                  | -1.519503   |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00365    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.44336644  |
| stats_o/std                    | 0.027478293 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.3604906  |
| test/Q_plus_P                  | -1.3604906  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.44336322  |
| stats_o/std                    | 0.027472215 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0389     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0229105  |
| test/Q_plus_P                  | -1.0229105  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.4433613   |
| stats_o/std                    | 0.027464079 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00515    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.203      |
| test/Q                         | -1.2314304  |
| test/Q_plus_P                  | -1.2314304  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0514     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.4433582   |
| stats_o/std                    | 0.027457746 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0351     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.914762   |
| test/Q_plus_P                  | -0.914762   |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.4433581   |
| stats_o/std                    | 0.027450195 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000453   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.0442758  |
| test/Q_plus_P                  | -1.0442758  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.4433554   |
| stats_o/std                    | 0.027449124 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.159491   |
| test/Q_plus_P                  | -1.159491   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.44335547  |
| stats_o/std                    | 0.027439041 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.1213582  |
| test/Q_plus_P                  | -1.1213582  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.4433558   |
| stats_o/std                    | 0.027431712 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.9739635  |
| test/Q_plus_P                  | -0.9739635  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.4433538   |
| stats_o/std                    | 0.027426124 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0346     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.8319583  |
| test/Q_plus_P                  | -0.8319583  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.44335112  |
| stats_o/std                    | 0.027422966 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.0383     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.0768205  |
| test/Q_plus_P                  | -1.0768205  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 302        |
| stats_o/mean                   | 0.44334957 |
| stats_o/std                    | 0.0274165  |
| test/episodes                  | 3030       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000629  |
| test/info_shaping_reward_mean  | -0.0378    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.0362343 |
| test/Q_plus_P                  | -1.0362343 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 121200     |
| train/episodes                 | 12120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0029    |
| train/info_shaping_reward_mean | -0.0552    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 484800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.44335136  |
| stats_o/std                    | 0.027412223 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000346   |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.357319   |
| test/Q_plus_P                  | -1.357319   |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0026     |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.4433508   |
| stats_o/std                    | 0.027406266 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.2415936  |
| test/Q_plus_P                  | -1.2415936  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.44334885 |
| stats_o/std                    | 0.02739968 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00152   |
| test/info_shaping_reward_mean  | -0.039     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -1.1121686 |
| test/Q_plus_P                  | -1.1121686 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.4433483   |
| stats_o/std                    | 0.027390884 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00117    |
| test/info_shaping_reward_mean  | -0.0363     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.93771094 |
| test/Q_plus_P                  | -0.93771094 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.44334558  |
| stats_o/std                    | 0.027389407 |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.24341    |
| test/Q_plus_P                  | -1.24341    |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 308        |
| stats_o/mean                   | 0.44334224 |
| stats_o/std                    | 0.02738972 |
| test/episodes                  | 3090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00137   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -1.2866852 |
| test/Q_plus_P                  | -1.2866852 |
| test/reward_per_eps            | -11        |
| test/steps                     | 123600     |
| train/episodes                 | 12360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00273   |
| train/info_shaping_reward_mean | -0.061     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 494400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.44334278  |
| stats_o/std                    | 0.027387278 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000647   |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.3318907  |
| test/Q_plus_P                  | -1.3318907  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.44334075  |
| stats_o/std                    | 0.027384764 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00165    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.3269069  |
| test/Q_plus_P                  | -1.3269069  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.44334093  |
| stats_o/std                    | 0.027376896 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000592   |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.1965573  |
| test/Q_plus_P                  | -1.1965573  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0027     |
| train/info_shaping_reward_mean | -0.0505     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.44334248  |
| stats_o/std                    | 0.027372187 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00212    |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.3910363  |
| test/Q_plus_P                  | -1.3910363  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.44333732  |
| stats_o/std                    | 0.027368495 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.2416824  |
| test/Q_plus_P                  | -1.2416824  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.44333836  |
| stats_o/std                    | 0.027366536 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0372     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.921647   |
| test/Q_plus_P                  | -0.921647   |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 315        |
| stats_o/mean                   | 0.44333708 |
| stats_o/std                    | 0.02736047 |
| test/episodes                  | 3160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00199   |
| test/info_shaping_reward_mean  | -0.0403    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.1153423 |
| test/Q_plus_P                  | -1.1153423 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 126400     |
| train/episodes                 | 12640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.693      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00243   |
| train/info_shaping_reward_mean | -0.0513    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 505600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.44333604  |
| stats_o/std                    | 0.027353792 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.1275644  |
| test/Q_plus_P                  | -1.1275644  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.44333434  |
| stats_o/std                    | 0.027346047 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.97287595 |
| test/Q_plus_P                  | -0.97287595 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.716       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00229    |
| train/info_shaping_reward_mean | -0.0487     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.44333494 |
| stats_o/std                    | 0.02733999 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00119   |
| test/info_shaping_reward_mean  | -0.0375    |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -1.0237362 |
| test/Q_plus_P                  | -1.0237362 |
| test/reward_per_eps            | -8         |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00276   |
| train/info_shaping_reward_mean | -0.0536    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.44333485  |
| stats_o/std                    | 0.027333954 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000613   |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.014586   |
| test/Q_plus_P                  | -1.014586   |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.44333458  |
| stats_o/std                    | 0.027331354 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0997946  |
| test/Q_plus_P                  | -1.0997946  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00458    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.4433328   |
| stats_o/std                    | 0.027329637 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00106    |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.5758117  |
| test/Q_plus_P                  | -1.5758117  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.44333482  |
| stats_o/std                    | 0.027322995 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00234    |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -1.0472591  |
| test/Q_plus_P                  | -1.0472591  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.44333485  |
| stats_o/std                    | 0.027319955 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.2561371  |
| test/Q_plus_P                  | -1.2561371  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.44333252  |
| stats_o/std                    | 0.027320014 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00358    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.2656848  |
| test/Q_plus_P                  | -1.2656848  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.44332936  |
| stats_o/std                    | 0.027317261 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000682   |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.2030331  |
| test/Q_plus_P                  | -1.2030331  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.44332716  |
| stats_o/std                    | 0.027314939 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00289    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.9521223  |
| test/Q_plus_P                  | -0.9521223  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0024     |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.44332728  |
| stats_o/std                    | 0.027308144 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.7276773  |
| test/Q_plus_P                  | -1.7276773  |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.44332346  |
| stats_o/std                    | 0.027308626 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.1941177  |
| test/Q_plus_P                  | -1.1941177  |
| test/reward_per_eps            | -9          |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.4433252   |
| stats_o/std                    | 0.027303489 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00647    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.9601978  |
| test/Q_plus_P                  | -0.9601978  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00448    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 330        |
| stats_o/mean                   | 0.4433256  |
| stats_o/std                    | 0.02730029 |
| test/episodes                  | 3310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0414    |
| test/info_shaping_reward_min   | -0.213     |
| test/Q                         | -1.1259526 |
| test/Q_plus_P                  | -1.1259526 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 132400     |
| train/episodes                 | 13240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0557    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 529600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.44332287  |
| stats_o/std                    | 0.027296348 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00307    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.2254913  |
| test/Q_plus_P                  | -1.2254913  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.44332254  |
| stats_o/std                    | 0.027289225 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00317    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.3691906  |
| test/Q_plus_P                  | -1.3691906  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0523     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.4433242  |
| stats_o/std                    | 0.02728662 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000751  |
| test/info_shaping_reward_mean  | -0.0445    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -1.1648537 |
| test/Q_plus_P                  | -1.1648537 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.0568    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.44332525  |
| stats_o/std                    | 0.027280306 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.1988723  |
| test/Q_plus_P                  | -1.1988723  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 335         |
| stats_o/mean                   | 0.44332612  |
| stats_o/std                    | 0.027273634 |
| test/episodes                  | 3360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.3085699  |
| test/Q_plus_P                  | -1.3085699  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 134400      |
| train/episodes                 | 13440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 537600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.44332412  |
| stats_o/std                    | 0.027268961 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -1.0860047  |
| test/Q_plus_P                  | -1.0860047  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00403    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.4433246  |
| stats_o/std                    | 0.02726359 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00184   |
| test/info_shaping_reward_mean  | -0.0453    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.2559794 |
| test/Q_plus_P                  | -1.2559794 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00325   |
| train/info_shaping_reward_mean | -0.0549    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.44332674 |
| stats_o/std                    | 0.02725598 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0051    |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -1.1989511 |
| test/Q_plus_P                  | -1.1989511 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.723      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00365   |
| train/info_shaping_reward_mean | -0.0511    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.1      |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.44332495  |
| stats_o/std                    | 0.027249932 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00287    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.046315   |
| test/Q_plus_P                  | -1.046315   |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00391    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.44332418  |
| stats_o/std                    | 0.027244588 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00257    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.5640734  |
| test/Q_plus_P                  | -1.5640734  |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.44332588 |
| stats_o/std                    | 0.02723687 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00167   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.383431  |
| test/Q_plus_P                  | -1.383431  |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.686      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0532    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.4433258   |
| stats_o/std                    | 0.027235756 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00311    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.4629958  |
| test/Q_plus_P                  | -1.4629958  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.44332612  |
| stats_o/std                    | 0.027230302 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.3148843  |
| test/Q_plus_P                  | -1.3148843  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 344        |
| stats_o/mean                   | 0.44332442 |
| stats_o/std                    | 0.02722656 |
| test/episodes                  | 3450       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0013    |
| test/info_shaping_reward_mean  | -0.0416    |
| test/info_shaping_reward_min   | -0.226     |
| test/Q                         | -1.3409027 |
| test/Q_plus_P                  | -1.3409027 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 138000     |
| train/episodes                 | 13800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.676      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00286   |
| train/info_shaping_reward_mean | -0.053     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 552000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 345        |
| stats_o/mean                   | 0.44332564 |
| stats_o/std                    | 0.02721984 |
| test/episodes                  | 3460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00351   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -1.1808366 |
| test/Q_plus_P                  | -1.1808366 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 138400     |
| train/episodes                 | 13840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00288   |
| train/info_shaping_reward_mean | -0.0538    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 553600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.44332325  |
| stats_o/std                    | 0.027215386 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000816   |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -1.2719518  |
| test/Q_plus_P                  | -1.2719518  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.44332168 |
| stats_o/std                    | 0.02721137 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0438    |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -1.3374289 |
| test/Q_plus_P                  | -1.3374289 |
| test/reward_per_eps            | -10        |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.685      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0549    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.4433199   |
| stats_o/std                    | 0.027210237 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00185    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.1800822  |
| test/Q_plus_P                  | -1.1800822  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.44331858  |
| stats_o/std                    | 0.027204698 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.9472558  |
| test/Q_plus_P                  | -0.9472558  |
| test/reward_per_eps            | -8          |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.4433173   |
| stats_o/std                    | 0.027199758 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00257    |
| test/info_shaping_reward_mean  | -0.0387     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.1773422  |
| test/Q_plus_P                  | -1.1773422  |
| test/reward_per_eps            | -9          |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.44331646  |
| stats_o/std                    | 0.027191872 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.2550805  |
| test/Q_plus_P                  | -1.2550805  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 352        |
| stats_o/mean                   | 0.443316   |
| stats_o/std                    | 0.02718901 |
| test/episodes                  | 3530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000962  |
| test/info_shaping_reward_mean  | -0.0392    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.0174325 |
| test/Q_plus_P                  | -1.0174325 |
| test/reward_per_eps            | -8         |
| test/steps                     | 141200     |
| train/episodes                 | 14120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.267     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 564800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.4433173   |
| stats_o/std                    | 0.027179778 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00501    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.4506431  |
| test/Q_plus_P                  | -1.4506431  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.44331828  |
| stats_o/std                    | 0.0271794   |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000883   |
| test/info_shaping_reward_mean  | -0.0394     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.99119294 |
| test/Q_plus_P                  | -0.99119294 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.4433186   |
| stats_o/std                    | 0.027174823 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00333    |
| test/info_shaping_reward_mean  | -0.0396     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.0441775  |
| test/Q_plus_P                  | -1.0441775  |
| test/reward_per_eps            | -8          |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.4433173   |
| stats_o/std                    | 0.027170315 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00416    |
| test/info_shaping_reward_mean  | -0.0396     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.948816   |
| test/Q_plus_P                  | -0.948816   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0498     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.44331333  |
| stats_o/std                    | 0.027170533 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0339     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.76646376 |
| test/Q_plus_P                  | -0.76646376 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.44331148  |
| stats_o/std                    | 0.027164204 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0348     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0913684  |
| test/Q_plus_P                  | -1.0913684  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0525     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.44330952  |
| stats_o/std                    | 0.027158631 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00127    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.2305701  |
| test/Q_plus_P                  | -1.2305701  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.4433078   |
| stats_o/std                    | 0.027155126 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.037      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.0329584  |
| test/Q_plus_P                  | -1.0329584  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.44330665  |
| stats_o/std                    | 0.027149983 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000854   |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.5571225  |
| test/Q_plus_P                  | -1.5571225  |
| test/reward_per_eps            | -12         |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00409    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.44330516  |
| stats_o/std                    | 0.027147984 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00796    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.349254   |
| test/Q_plus_P                  | -1.349254   |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.44330573  |
| stats_o/std                    | 0.027143473 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00378    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.2161512  |
| test/Q_plus_P                  | -1.2161512  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 364        |
| stats_o/mean                   | 0.44330332 |
| stats_o/std                    | 0.0271417  |
| test/episodes                  | 3650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -1.1955091 |
| test/Q_plus_P                  | -1.1955091 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 146000     |
| train/episodes                 | 14600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.671      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.0566    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 584000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 365        |
| stats_o/mean                   | 0.44330266 |
| stats_o/std                    | 0.02713661 |
| test/episodes                  | 3660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00453   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -1.1920224 |
| test/Q_plus_P                  | -1.1920224 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 146400     |
| train/episodes                 | 14640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.688      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 585600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.44330236 |
| stats_o/std                    | 0.02713126 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -1.1927785 |
| test/Q_plus_P                  | -1.1927785 |
| test/reward_per_eps            | -9         |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0548    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.44330224  |
| stats_o/std                    | 0.027126247 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00266    |
| test/info_shaping_reward_mean  | -0.0381     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.95336336 |
| test/Q_plus_P                  | -0.95336336 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00365    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.44330278  |
| stats_o/std                    | 0.027123345 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0061     |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.194405   |
| test/Q_plus_P                  | -1.194405   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.44330177  |
| stats_o/std                    | 0.027121967 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00752    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.1345772  |
| test/Q_plus_P                  | -1.1345772  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.4433024   |
| stats_o/std                    | 0.027114647 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00405    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.293227   |
| test/Q_plus_P                  | -1.293227   |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.44330123  |
| stats_o/std                    | 0.027111828 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.88692874 |
| test/Q_plus_P                  | -0.88692874 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00232    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.44330207  |
| stats_o/std                    | 0.027107397 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.2468163  |
| test/Q_plus_P                  | -1.2468163  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.4433013   |
| stats_o/std                    | 0.027101884 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0378     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0903064  |
| test/Q_plus_P                  | -1.0903064  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.44329992 |
| stats_o/std                    | 0.02709748 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00248   |
| test/info_shaping_reward_mean  | -0.0355    |
| test/info_shaping_reward_min   | -0.216     |
| test/Q                         | -0.959283  |
| test/Q_plus_P                  | -0.959283  |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.698      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00264   |
| train/info_shaping_reward_mean | -0.0524    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.44329807  |
| stats_o/std                    | 0.027093938 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.1942279  |
| test/Q_plus_P                  | -1.1942279  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00311    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.44330022  |
| stats_o/std                    | 0.027088339 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.0073284  |
| test/Q_plus_P                  | -1.0073284  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.44330153  |
| stats_o/std                    | 0.027087146 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000879   |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.2178028  |
| test/Q_plus_P                  | -1.2178028  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.44330266 |
| stats_o/std                    | 0.02708344 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.695      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000975  |
| test/info_shaping_reward_mean  | -0.0464    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.6083232 |
| test/Q_plus_P                  | -1.6083232 |
| test/reward_per_eps            | -12.2      |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00305   |
| train/info_shaping_reward_mean | -0.0565    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.44330063  |
| stats_o/std                    | 0.027083263 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.3525913  |
| test/Q_plus_P                  | -1.3525913  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.44329977  |
| stats_o/std                    | 0.027080951 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.5631481  |
| test/Q_plus_P                  | -1.5631481  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.44329834  |
| stats_o/std                    | 0.027081028 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00206    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.3621706  |
| test/Q_plus_P                  | -1.3621706  |
| test/reward_per_eps            | -11         |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 382         |
| stats_o/mean                   | 0.44329834  |
| stats_o/std                    | 0.027076276 |
| test/episodes                  | 3830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.0632893  |
| test/Q_plus_P                  | -1.0632893  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 153200      |
| train/episodes                 | 15320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 612800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.4432973   |
| stats_o/std                    | 0.027075687 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.0886359  |
| test/Q_plus_P                  | -1.0886359  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00398    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 384        |
| stats_o/mean                   | 0.4432954  |
| stats_o/std                    | 0.02707463 |
| test/episodes                  | 3850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00209   |
| test/info_shaping_reward_mean  | -0.0403    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.0627918 |
| test/Q_plus_P                  | -1.0627918 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 154000     |
| train/episodes                 | 15400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 616000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.44329336  |
| stats_o/std                    | 0.027073236 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000697   |
| test/info_shaping_reward_mean  | -0.0377     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.93815875 |
| test/Q_plus_P                  | -0.93815875 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.44329432  |
| stats_o/std                    | 0.027067095 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00743    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.9604234  |
| test/Q_plus_P                  | -0.9604234  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00308    |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.44329402  |
| stats_o/std                    | 0.027064463 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00252    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.1207988  |
| test/Q_plus_P                  | -1.1207988  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.44329306  |
| stats_o/std                    | 0.027058689 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00161    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.3672814  |
| test/Q_plus_P                  | -1.3672814  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.4432927   |
| stats_o/std                    | 0.027055308 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.2076546  |
| test/Q_plus_P                  | -1.2076546  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.717       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.4432907   |
| stats_o/std                    | 0.027049677 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0335277  |
| test/Q_plus_P                  | -1.0335277  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.724       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.1       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 391         |
| stats_o/mean                   | 0.4432894   |
| stats_o/std                    | 0.027047182 |
| test/episodes                  | 3920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.8421408  |
| test/Q_plus_P                  | -0.8421408  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 156800      |
| train/episodes                 | 15680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 627200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.44328627  |
| stats_o/std                    | 0.02704511  |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000881   |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.95480347 |
| test/Q_plus_P                  | -0.95480347 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.44328737  |
| stats_o/std                    | 0.027038828 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000555   |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.0008326  |
| test/Q_plus_P                  | -1.0008326  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.4432858   |
| stats_o/std                    | 0.027034903 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.98841363 |
| test/Q_plus_P                  | -0.98841363 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.44328693  |
| stats_o/std                    | 0.027030408 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00374    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.98306346 |
| test/Q_plus_P                  | -0.98306346 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0529     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.4432859   |
| stats_o/std                    | 0.027031101 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.87001944 |
| test/Q_plus_P                  | -0.87001944 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.44328454  |
| stats_o/std                    | 0.027025877 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0168996  |
| test/Q_plus_P                  | -1.0168996  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0511     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.44328466 |
| stats_o/std                    | 0.02702305 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00258   |
| test/info_shaping_reward_mean  | -0.0381    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.921458  |
| test/Q_plus_P                  | -0.921458  |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.709      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00332   |
| train/info_shaping_reward_mean | -0.0521    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.7      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.44328222 |
| stats_o/std                    | 0.02701828 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00428   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -0.9947525 |
| test/Q_plus_P                  | -0.9947525 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00334   |
| train/info_shaping_reward_mean | -0.054     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.443282   |
| stats_o/std                    | 0.02701217 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000648  |
| test/info_shaping_reward_mean  | -0.0407    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -0.9259411 |
| test/Q_plus_P                  | -0.9259411 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.674      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00331   |
| train/info_shaping_reward_mean | -0.0525    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.4432805   |
| stats_o/std                    | 0.027011337 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.2178537  |
| test/Q_plus_P                  | -1.2178537  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.44328013  |
| stats_o/std                    | 0.027012175 |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -1.67742    |
| test/Q_plus_P                  | -1.67742    |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.601       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.4432766   |
| stats_o/std                    | 0.027011478 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00337    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.3283627  |
| test/Q_plus_P                  | -1.3283627  |
| test/reward_per_eps            | -11         |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.44327602  |
| stats_o/std                    | 0.027008848 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0059     |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.1244766  |
| test/Q_plus_P                  | -1.1244766  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.4432747   |
| stats_o/std                    | 0.027006716 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.218      |
| test/Q                         | -1.2580036  |
| test/Q_plus_P                  | -1.2580036  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00402    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.44327402  |
| stats_o/std                    | 0.027003722 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000593   |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.211      |
| test/Q                         | -1.1544235  |
| test/Q_plus_P                  | -1.1544235  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00336    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.44327143  |
| stats_o/std                    | 0.027000504 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000848   |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.9610584  |
| test/Q_plus_P                  | -0.9610584  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.4432701   |
| stats_o/std                    | 0.026996411 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000336   |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.1332777  |
| test/Q_plus_P                  | -1.1332777  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00202    |
| train/info_shaping_reward_mean | -0.0519     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.44327083  |
| stats_o/std                    | 0.026991919 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00225    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.0487199  |
| test/Q_plus_P                  | -1.0487199  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.44326925  |
| stats_o/std                    | 0.026987476 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.3447409  |
| test/Q_plus_P                  | -1.3447409  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.44327083  |
| stats_o/std                    | 0.026981702 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00348    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.9758639  |
| test/Q_plus_P                  | -0.9758639  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.44326898  |
| stats_o/std                    | 0.026981702 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.021159   |
| test/Q_plus_P                  | -1.021159   |
| test/reward_per_eps            | -8          |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.44326827  |
| stats_o/std                    | 0.026980916 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00387    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.110273   |
| test/Q_plus_P                  | -1.110273   |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.4432684   |
| stats_o/std                    | 0.026974246 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00329    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.1066711  |
| test/Q_plus_P                  | -1.1066711  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0527     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.44326842 |
| stats_o/std                    | 0.02697295 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000717  |
| test/info_shaping_reward_mean  | -0.0415    |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -1.0222583 |
| test/Q_plus_P                  | -1.0222583 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00442   |
| train/info_shaping_reward_mean | -0.0588    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.44326648  |
| stats_o/std                    | 0.026969641 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.33317    |
| test/Q_plus_P                  | -1.33317    |
| test/reward_per_eps            | -10         |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 417        |
| stats_o/mean                   | 0.4432659  |
| stats_o/std                    | 0.0269667  |
| test/episodes                  | 4180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000396  |
| test/info_shaping_reward_mean  | -0.0428    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.0979457 |
| test/Q_plus_P                  | -1.0979457 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 167200     |
| train/episodes                 | 16720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00342   |
| train/info_shaping_reward_mean | -0.0564    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 668800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.44326642  |
| stats_o/std                    | 0.026963228 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.1064184  |
| test/Q_plus_P                  | -1.1064184  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.44326615  |
| stats_o/std                    | 0.026959786 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00077    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.95208174 |
| test/Q_plus_P                  | -0.95208174 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.443266    |
| stats_o/std                    | 0.026957123 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00103    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.3639716  |
| test/Q_plus_P                  | -1.3639716  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.44326448 |
| stats_o/std                    | 0.02695263 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00463   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.9050211 |
| test/Q_plus_P                  | -0.9050211 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00285   |
| train/info_shaping_reward_mean | -0.0516    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 422        |
| stats_o/mean                   | 0.4432669  |
| stats_o/std                    | 0.02694871 |
| test/episodes                  | 4230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00234   |
| test/info_shaping_reward_mean  | -0.0421    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.1008825 |
| test/Q_plus_P                  | -1.1008825 |
| test/reward_per_eps            | -9         |
| test/steps                     | 169200     |
| train/episodes                 | 16920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 676800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.443267    |
| stats_o/std                    | 0.026948603 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.9088863  |
| test/Q_plus_P                  | -0.9088863  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.4432664   |
| stats_o/std                    | 0.026947087 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -1.111502   |
| test/Q_plus_P                  | -1.111502   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.44326806  |
| stats_o/std                    | 0.026941009 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -0.94482005 |
| test/Q_plus_P                  | -0.94482005 |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.44326785  |
| stats_o/std                    | 0.026938004 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00348    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.0927643  |
| test/Q_plus_P                  | -1.0927643  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0539     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.44326708  |
| stats_o/std                    | 0.026935553 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0031     |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.96752924 |
| test/Q_plus_P                  | -0.96752924 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.4432689   |
| stats_o/std                    | 0.026934877 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00464    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.2611147  |
| test/Q_plus_P                  | -1.2611147  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.44326696  |
| stats_o/std                    | 0.026936807 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00308    |
| test/info_shaping_reward_mean  | -0.0423     |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -1.3694857  |
| test/Q_plus_P                  | -1.3694857  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.4432657   |
| stats_o/std                    | 0.026933981 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00059    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.1361964  |
| test/Q_plus_P                  | -1.1361964  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00322    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.443267    |
| stats_o/std                    | 0.026931383 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0374     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.1839776  |
| test/Q_plus_P                  | -1.1839776  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.4432671   |
| stats_o/std                    | 0.026926361 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000954   |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.1582521  |
| test/Q_plus_P                  | -1.1582521  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.4432676   |
| stats_o/std                    | 0.026926942 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000916   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.8861568  |
| test/Q_plus_P                  | -0.8861568  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.44326892  |
| stats_o/std                    | 0.026921706 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00403    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.3622537  |
| test/Q_plus_P                  | -1.3622537  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.44326985  |
| stats_o/std                    | 0.026917068 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0354     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.86721975 |
| test/Q_plus_P                  | -0.86721975 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00352    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.44326818 |
| stats_o/std                    | 0.02691448 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000275  |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -0.9109626 |
| test/Q_plus_P                  | -0.9109626 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00334   |
| train/info_shaping_reward_mean | -0.0545    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.44326913  |
| stats_o/std                    | 0.026913794 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.2149403  |
| test/Q_plus_P                  | -1.2149403  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.44326973  |
| stats_o/std                    | 0.026910728 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000707   |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.9562527  |
| test/Q_plus_P                  | -0.9562527  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.44327223  |
| stats_o/std                    | 0.026907392 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00362    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.1793973  |
| test/Q_plus_P                  | -1.1793973  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.44327185  |
| stats_o/std                    | 0.026903585 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.0585487  |
| test/Q_plus_P                  | -1.0585487  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.44327414  |
| stats_o/std                    | 0.026900848 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00398    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.94472426 |
| test/Q_plus_P                  | -0.94472426 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0055     |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.44327223 |
| stats_o/std                    | 0.02690322 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000942  |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -1.1414156 |
| test/Q_plus_P                  | -1.1414156 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.0581    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.44327232  |
| stats_o/std                    | 0.026901336 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00172    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.9941122  |
| test/Q_plus_P                  | -0.9941122  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.44327185  |
| stats_o/std                    | 0.026901169 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00207    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.95810556 |
| test/Q_plus_P                  | -0.95810556 |
| test/reward_per_eps            | -8          |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 445        |
| stats_o/mean                   | 0.44327202 |
| stats_o/std                    | 0.02690094 |
| test/episodes                  | 4460       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00221   |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.0407051 |
| test/Q_plus_P                  | -1.0407051 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 178400     |
| train/episodes                 | 17840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0589    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 713600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.44327077  |
| stats_o/std                    | 0.026896574 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.8650335  |
| test/Q_plus_P                  | -0.8650335  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.44327056 |
| stats_o/std                    | 0.02689448 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0054    |
| test/info_shaping_reward_mean  | -0.045     |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.8444557 |
| test/Q_plus_P                  | -0.8444557 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00352   |
| train/info_shaping_reward_mean | -0.0572    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.44326964 |
| stats_o/std                    | 0.02689048 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00193   |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -0.8795691 |
| test/Q_plus_P                  | -0.8795691 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.699      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00346   |
| train/info_shaping_reward_mean | -0.0529    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.44327006  |
| stats_o/std                    | 0.026886731 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000935   |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.9831308  |
| test/Q_plus_P                  | -0.9831308  |
| test/reward_per_eps            | -8          |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.44326997  |
| stats_o/std                    | 0.026882224 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00293    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.1256771  |
| test/Q_plus_P                  | -1.1256771  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.44327006  |
| stats_o/std                    | 0.026878463 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0022     |
| test/info_shaping_reward_mean  | -0.0357     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.85193956 |
| test/Q_plus_P                  | -0.85193956 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00561    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.44327235 |
| stats_o/std                    | 0.02687496 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00129   |
| test/info_shaping_reward_mean  | -0.0413    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -0.7719851 |
| test/Q_plus_P                  | -0.7719851 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.687      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00393   |
| train/info_shaping_reward_mean | -0.0547    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.44327044 |
| stats_o/std                    | 0.02687493 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00307   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.245     |
| test/Q                         | -1.0162506 |
| test/Q_plus_P                  | -1.0162506 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.692      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00349   |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.44327095  |
| stats_o/std                    | 0.026873857 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.9614389  |
| test/Q_plus_P                  | -0.9614389  |
| test/reward_per_eps            | -8          |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.44327244  |
| stats_o/std                    | 0.026868505 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00537    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.1241744  |
| test/Q_plus_P                  | -1.1241744  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00548    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.44327173  |
| stats_o/std                    | 0.026867537 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00302    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.9912467  |
| test/Q_plus_P                  | -0.9912467  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00447    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.4432709   |
| stats_o/std                    | 0.026863823 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00179    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.0312974  |
| test/Q_plus_P                  | -1.0312974  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0521     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.44327232  |
| stats_o/std                    | 0.026856713 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0084     |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.9644316  |
| test/Q_plus_P                  | -0.9644316  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00409    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 459        |
| stats_o/mean                   | 0.44327223 |
| stats_o/std                    | 0.02685217 |
| test/episodes                  | 4600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00155   |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -1.0233245 |
| test/Q_plus_P                  | -1.0233245 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 184000     |
| train/episodes                 | 18400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00345   |
| train/info_shaping_reward_mean | -0.057     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 736000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.44327402  |
| stats_o/std                    | 0.026850836 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.1175611  |
| test/Q_plus_P                  | -1.1175611  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.44327274  |
| stats_o/std                    | 0.02684843  |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00154    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.82754844 |
| test/Q_plus_P                  | -0.82754844 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.44327295  |
| stats_o/std                    | 0.026847474 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.90764016 |
| test/Q_plus_P                  | -0.90764016 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.44327268  |
| stats_o/std                    | 0.026845394 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00371    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.8784732  |
| test/Q_plus_P                  | -0.8784732  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.711       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0524     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.44327402  |
| stats_o/std                    | 0.026843995 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00288    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.0812622  |
| test/Q_plus_P                  | -1.0812622  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00345    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.44327435  |
| stats_o/std                    | 0.026842339 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00445    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.1854795  |
| test/Q_plus_P                  | -1.1854795  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.4432738   |
| stats_o/std                    | 0.026837824 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.1468986  |
| test/Q_plus_P                  | -1.1468986  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00201    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.4432749   |
| stats_o/std                    | 0.026832512 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.88504463 |
| test/Q_plus_P                  | -0.88504463 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.44327593  |
| stats_o/std                    | 0.026828924 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00752    |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.1254487  |
| test/Q_plus_P                  | -1.1254487  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00285    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.44327447  |
| stats_o/std                    | 0.026826078 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.009      |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.96688604 |
| test/Q_plus_P                  | -0.96688604 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.44327274 |
| stats_o/std                    | 0.02682229 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00173   |
| test/info_shaping_reward_mean  | -0.0387    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -0.7758669 |
| test/Q_plus_P                  | -0.7758669 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.676      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00327   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.44327223  |
| stats_o/std                    | 0.026816266 |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00296    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.0535305  |
| test/Q_plus_P                  | -1.0535305  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.4432735   |
| stats_o/std                    | 0.026809772 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00457    |
| test/info_shaping_reward_mean  | -0.0418     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.86763537 |
| test/Q_plus_P                  | -0.86763537 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.44327375  |
| stats_o/std                    | 0.026807606 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00171    |
| test/info_shaping_reward_mean  | -0.0403     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.75068057 |
| test/Q_plus_P                  | -0.75068057 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.44327542  |
| stats_o/std                    | 0.026803525 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00302    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.353254   |
| test/Q_plus_P                  | -1.353254   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00359    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.4432763   |
| stats_o/std                    | 0.026798325 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00248    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.93323416 |
| test/Q_plus_P                  | -0.93323416 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00528    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.4432776   |
| stats_o/std                    | 0.026793383 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.0392     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.81630707 |
| test/Q_plus_P                  | -0.81630707 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.44327614  |
| stats_o/std                    | 0.026791489 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00364    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.93820953 |
| test/Q_plus_P                  | -0.93820953 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.44327268 |
| stats_o/std                    | 0.02679079 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00972   |
| test/info_shaping_reward_mean  | -0.0471    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -0.9598137 |
| test/Q_plus_P                  | -0.9598137 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0577    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.44327292  |
| stats_o/std                    | 0.026785826 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00576    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.105455   |
| test/Q_plus_P                  | -1.105455   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00376    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.44327274 |
| stats_o/std                    | 0.0267832  |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00284   |
| test/info_shaping_reward_mean  | -0.0422    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -1.0099376 |
| test/Q_plus_P                  | -1.0099376 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.711      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00299   |
| train/info_shaping_reward_mean | -0.0519    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.6      |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.4432708   |
| stats_o/std                    | 0.026781565 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000868   |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.9765423  |
| test/Q_plus_P                  | -0.9765423  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.4432695   |
| stats_o/std                    | 0.026778368 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00411    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -1.0336407  |
| test/Q_plus_P                  | -1.0336407  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.44326672  |
| stats_o/std                    | 0.026777    |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00397    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.94313246 |
| test/Q_plus_P                  | -0.94313246 |
| test/reward_per_eps            | -8          |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.4432666   |
| stats_o/std                    | 0.02677343  |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.94774055 |
| test/Q_plus_P                  | -0.94774055 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.4432671   |
| stats_o/std                    | 0.026770078 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000532   |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0012901  |
| test/Q_plus_P                  | -1.0012901  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00512    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.44326743  |
| stats_o/std                    | 0.026768608 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.655       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.5048544  |
| test/Q_plus_P                  | -1.5048544  |
| test/reward_per_eps            | -13.8       |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.4432664   |
| stats_o/std                    | 0.026764264 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.2720491  |
| test/Q_plus_P                  | -1.2720491  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.44326508  |
| stats_o/std                    | 0.026764646 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000722   |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.3857762  |
| test/Q_plus_P                  | -1.3857762  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.44326565  |
| stats_o/std                    | 0.026762092 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00469    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.1893585  |
| test/Q_plus_P                  | -1.1893585  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.44326448  |
| stats_o/std                    | 0.026762491 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.0726638  |
| test/Q_plus_P                  | -1.0726638  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.44326532  |
| stats_o/std                    | 0.026756676 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000931   |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.1318312  |
| test/Q_plus_P                  | -1.1318312  |
| test/reward_per_eps            | -9          |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.44326377  |
| stats_o/std                    | 0.026755467 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.0377182  |
| test/Q_plus_P                  | -1.0377182  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.4432626  |
| stats_o/std                    | 0.02675485 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0023    |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.0488569 |
| test/Q_plus_P                  | -1.0488569 |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00235   |
| train/info_shaping_reward_mean | -0.0562    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.44326487  |
| stats_o/std                    | 0.026752954 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00283    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.074635   |
| test/Q_plus_P                  | -1.074635   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00576    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.44326636  |
| stats_o/std                    | 0.026750637 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00443    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0063072  |
| test/Q_plus_P                  | -1.0063072  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00489    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 496        |
| stats_o/mean                   | 0.44326612 |
| stats_o/std                    | 0.02674775 |
| test/episodes                  | 4970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00389   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -1.0657452 |
| test/Q_plus_P                  | -1.0657452 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 198800     |
| train/episodes                 | 19880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.688      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00286   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 795200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.4432657  |
| stats_o/std                    | 0.02674613 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.797      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -0.996826  |
| test/Q_plus_P                  | -0.996826  |
| test/reward_per_eps            | -8.1       |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.44326404  |
| stats_o/std                    | 0.026745683 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00792    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9050585  |
| test/Q_plus_P                  | -0.9050585  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00253    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.44326222  |
| stats_o/std                    | 0.026742816 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00167    |
| test/info_shaping_reward_mean  | -0.042      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.94731295 |
| test/Q_plus_P                  | -0.94731295 |
| test/reward_per_eps            | -8          |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.44326392  |
| stats_o/std                    | 0.026736869 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00359    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -0.8976918  |
| test/Q_plus_P                  | -0.8976918  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.44326434  |
| stats_o/std                    | 0.026735203 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.88146937 |
| test/Q_plus_P                  | -0.88146937 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.44326556  |
| stats_o/std                    | 0.026731053 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00379    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.0744236  |
| test/Q_plus_P                  | -1.0744236  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00312    |
| train/info_shaping_reward_mean | -0.0509     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.44326603  |
| stats_o/std                    | 0.026729712 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00531    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.0300884  |
| test/Q_plus_P                  | -1.0300884  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0043     |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.44326687  |
| stats_o/std                    | 0.026727246 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0045     |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0115705  |
| test/Q_plus_P                  | -1.0115705  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00338    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.44326612  |
| stats_o/std                    | 0.026724515 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00187    |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.91087013 |
| test/Q_plus_P                  | -0.91087013 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00466    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.443267    |
| stats_o/std                    | 0.026722617 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.040013   |
| test/Q_plus_P                  | -1.040013   |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.4432675   |
| stats_o/std                    | 0.026721343 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0049     |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8400937  |
| test/Q_plus_P                  | -0.8400937  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.443267    |
| stats_o/std                    | 0.026721083 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00355    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.9907245  |
| test/Q_plus_P                  | -0.9907245  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.44326627  |
| stats_o/std                    | 0.026717983 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00412    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.006765   |
| test/Q_plus_P                  | -1.006765   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 510        |
| stats_o/mean                   | 0.44326505 |
| stats_o/std                    | 0.02671372 |
| test/episodes                  | 5110       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00208   |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -0.9061478 |
| test/Q_plus_P                  | -0.9061478 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 204400     |
| train/episodes                 | 20440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00447   |
| train/info_shaping_reward_mean | -0.0572    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 817600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.4432633   |
| stats_o/std                    | 0.026711233 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00557    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.1123805  |
| test/Q_plus_P                  | -1.1123805  |
| test/reward_per_eps            | -9          |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.44326285 |
| stats_o/std                    | 0.02670879 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00359   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -1.0504093 |
| test/Q_plus_P                  | -1.0504093 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00398   |
| train/info_shaping_reward_mean | -0.058     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.44326177  |
| stats_o/std                    | 0.026706323 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0186     |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.83133966 |
| test/Q_plus_P                  | -0.83133966 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.4432626   |
| stats_o/std                    | 0.026703201 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00584    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -1.1844492  |
| test/Q_plus_P                  | -1.1844492  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.4432628   |
| stats_o/std                    | 0.026701787 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00548    |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.3980954  |
| test/Q_plus_P                  | -1.3980954  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00413    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.4432653   |
| stats_o/std                    | 0.026699563 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0189     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0826609  |
| test/Q_plus_P                  | -1.0826609  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.4432663  |
| stats_o/std                    | 0.02669762 |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00694   |
| test/info_shaping_reward_mean  | -0.051     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.2539934 |
| test/Q_plus_P                  | -1.2539934 |
| test/reward_per_eps            | -10        |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00369   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.4432672   |
| stats_o/std                    | 0.026694736 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00283    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.1291078  |
| test/Q_plus_P                  | -1.1291078  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.44326785  |
| stats_o/std                    | 0.026691027 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00273    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.144372   |
| test/Q_plus_P                  | -1.144372   |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00601    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 520        |
| stats_o/mean                   | 0.44326654 |
| stats_o/std                    | 0.02668731 |
| test/episodes                  | 5210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -1.4672678 |
| test/Q_plus_P                  | -1.4672678 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 208400     |
| train/episodes                 | 20840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.671      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0025    |
| train/info_shaping_reward_mean | -0.0542    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 833600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.44326594  |
| stats_o/std                    | 0.026684431 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.9748079  |
| test/Q_plus_P                  | -0.9748079  |
| test/reward_per_eps            | -8          |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.4432675   |
| stats_o/std                    | 0.026679583 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00366    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.9616996  |
| test/Q_plus_P                  | -0.9616996  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.44326654  |
| stats_o/std                    | 0.026679404 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000745   |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.1561768  |
| test/Q_plus_P                  | -1.1561768  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.44326723  |
| stats_o/std                    | 0.026678666 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00876    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.2107807  |
| test/Q_plus_P                  | -1.2107807  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.44326842  |
| stats_o/std                    | 0.026675768 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0093     |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.2983599  |
| test/Q_plus_P                  | -1.2983599  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.44326928  |
| stats_o/std                    | 0.026672281 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0041     |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.94930685 |
| test/Q_plus_P                  | -0.94930685 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.44326982  |
| stats_o/std                    | 0.026667766 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00185    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.0995728  |
| test/Q_plus_P                  | -1.0995728  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00545    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.44326854  |
| stats_o/std                    | 0.026664136 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00818    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.90982056 |
| test/Q_plus_P                  | -0.90982056 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00442    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.44326863  |
| stats_o/std                    | 0.026661463 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.85405713 |
| test/Q_plus_P                  | -0.85405713 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.44326803  |
| stats_o/std                    | 0.026661789 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00376    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.80429083 |
| test/Q_plus_P                  | -0.80429083 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.44326875  |
| stats_o/std                    | 0.026657218 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0115     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.0475622  |
| test/Q_plus_P                  | -1.0475622  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.44326976  |
| stats_o/std                    | 0.026657052 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.4030702  |
| test/Q_plus_P                  | -1.4030702  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00573    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.4432706   |
| stats_o/std                    | 0.026655069 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.2998885  |
| test/Q_plus_P                  | -1.2998885  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.44327083  |
| stats_o/std                    | 0.026652629 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00955    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.1720834  |
| test/Q_plus_P                  | -1.1720834  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00447    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 535        |
| stats_o/mean                   | 0.44327286 |
| stats_o/std                    | 0.02664748 |
| test/episodes                  | 5360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00192   |
| test/info_shaping_reward_mean  | -0.0477    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -1.004419  |
| test/Q_plus_P                  | -1.004419  |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 214400     |
| train/episodes                 | 21440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.693      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0533    |
| train/info_shaping_reward_min  | -0.233     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 857600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.44327226  |
| stats_o/std                    | 0.026644798 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.90148973 |
| test/Q_plus_P                  | -0.90148973 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.055      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.44327247  |
| stats_o/std                    | 0.026641391 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.1451302  |
| test/Q_plus_P                  | -1.1451302  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.4432727   |
| stats_o/std                    | 0.026639936 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00516    |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.3400764  |
| test/Q_plus_P                  | -1.3400764  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.44327375  |
| stats_o/std                    | 0.026637116 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.89543897 |
| test/Q_plus_P                  | -0.89543897 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00543    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.4432735   |
| stats_o/std                    | 0.026634693 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00647    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.99681336 |
| test/Q_plus_P                  | -0.99681336 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.44327244  |
| stats_o/std                    | 0.026633708 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.8264836  |
| test/Q_plus_P                  | -0.8264836  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.4432733   |
| stats_o/std                    | 0.026630277 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00481    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.8135945  |
| test/Q_plus_P                  | -0.8135945  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.44327116  |
| stats_o/std                    | 0.026630113 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.93860793 |
| test/Q_plus_P                  | -0.93860793 |
| test/reward_per_eps            | -8          |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.44327307  |
| stats_o/std                    | 0.026627831 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00692    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.99003845 |
| test/Q_plus_P                  | -0.99003845 |
| test/reward_per_eps            | -8          |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.44327387  |
| stats_o/std                    | 0.026623799 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00354    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -0.96726817 |
| test/Q_plus_P                  | -0.96726817 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0526     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.44327363  |
| stats_o/std                    | 0.026622066 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.8720392  |
| test/Q_plus_P                  | -0.8720392  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00457    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.44327447  |
| stats_o/std                    | 0.026617013 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00353    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.90709007 |
| test/Q_plus_P                  | -0.90709007 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0532     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.44327566 |
| stats_o/std                    | 0.02661405 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00443   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -1.0483735 |
| test/Q_plus_P                  | -1.0483735 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.711      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0037    |
| train/info_shaping_reward_mean | -0.0525    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.6      |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.44327697  |
| stats_o/std                    | 0.026611032 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.3271289  |
| test/Q_plus_P                  | -1.3271289  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.44327652 |
| stats_o/std                    | 0.0266072  |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.705      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00368   |
| test/info_shaping_reward_mean  | -0.0532    |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -1.6205552 |
| test/Q_plus_P                  | -1.6205552 |
| test/reward_per_eps            | -11.8      |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.708      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0031    |
| train/info_shaping_reward_mean | -0.0519    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.7      |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.44327557 |
| stats_o/std                    | 0.0266081  |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00168   |
| test/info_shaping_reward_mean  | -0.0432    |
| test/info_shaping_reward_min   | -0.239     |
| test/Q                         | -1.0701797 |
| test/Q_plus_P                  | -1.0701797 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00311   |
| train/info_shaping_reward_mean | -0.0561    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.44327334  |
| stats_o/std                    | 0.026606224 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00707    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.1846777  |
| test/Q_plus_P                  | -1.1846777  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.44327247  |
| stats_o/std                    | 0.026602989 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0143     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.2810991  |
| test/Q_plus_P                  | -1.2810991  |
| test/reward_per_eps            | -10         |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.44327238  |
| stats_o/std                    | 0.026602231 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00503    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.0840341  |
| test/Q_plus_P                  | -1.0840341  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.44327083  |
| stats_o/std                    | 0.026601737 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00319    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.0120711  |
| test/Q_plus_P                  | -1.0120711  |
| test/reward_per_eps            | -8          |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.44327125 |
| stats_o/std                    | 0.02659837 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.792      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0114    |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -1.0047725 |
| test/Q_plus_P                  | -1.0047725 |
| test/reward_per_eps            | -8.3       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.698      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00392   |
| train/info_shaping_reward_mean | -0.0534    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.4432708   |
| stats_o/std                    | 0.026598014 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00472    |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -1.0565597  |
| test/Q_plus_P                  | -1.0565597  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.44327092  |
| stats_o/std                    | 0.026598139 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00581    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8853421  |
| test/Q_plus_P                  | -0.8853421  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00343    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.44326946  |
| stats_o/std                    | 0.026596924 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0027     |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.8252936  |
| test/Q_plus_P                  | -0.8252936  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 560        |
| stats_o/mean                   | 0.44326797 |
| stats_o/std                    | 0.0265953  |
| test/episodes                  | 5610       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00278   |
| test/info_shaping_reward_mean  | -0.0459    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -1.0800838 |
| test/Q_plus_P                  | -1.0800838 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 224400     |
| train/episodes                 | 22440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00395   |
| train/info_shaping_reward_mean | -0.0565    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 897600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.44326806  |
| stats_o/std                    | 0.026592866 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00383    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.3272556  |
| test/Q_plus_P                  | -1.3272556  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.44326532  |
| stats_o/std                    | 0.026591456 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0137     |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.90780276 |
| test/Q_plus_P                  | -0.90780276 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.44326606  |
| stats_o/std                    | 0.026587246 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00674    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.94063455 |
| test/Q_plus_P                  | -0.94063455 |
| test/reward_per_eps            | -8          |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00426    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.44326448  |
| stats_o/std                    | 0.026587239 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00964    |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9702285  |
| test/Q_plus_P                  | -0.9702285  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.44326332  |
| stats_o/std                    | 0.026585117 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000648   |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.1427449  |
| test/Q_plus_P                  | -1.1427449  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.44326258  |
| stats_o/std                    | 0.026583724 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00429    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -1.0209718  |
| test/Q_plus_P                  | -1.0209718  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0528     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.443261    |
| stats_o/std                    | 0.026579447 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00221    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.95878965 |
| test/Q_plus_P                  | -0.95878965 |
| test/reward_per_eps            | -8          |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.051      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.44325998  |
| stats_o/std                    | 0.026578283 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.945443   |
| test/Q_plus_P                  | -0.945443   |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.4432585   |
| stats_o/std                    | 0.026580496 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00352    |
| test/info_shaping_reward_mean  | -0.0379     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.88685083 |
| test/Q_plus_P                  | -0.88685083 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.4432578   |
| stats_o/std                    | 0.026574695 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00247    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.0467638  |
| test/Q_plus_P                  | -1.0467638  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.717       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0494     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.44325843  |
| stats_o/std                    | 0.026572043 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00426    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.035792   |
| test/Q_plus_P                  | -1.035792   |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00432    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.44325933  |
| stats_o/std                    | 0.026568307 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000412   |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.0924063  |
| test/Q_plus_P                  | -1.0924063  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.4432596  |
| stats_o/std                    | 0.02656693 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00444   |
| test/info_shaping_reward_mean  | -0.0463    |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -1.1481897 |
| test/Q_plus_P                  | -1.1481897 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00348   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.44325843  |
| stats_o/std                    | 0.026566567 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.9944704  |
| test/Q_plus_P                  | -0.9944704  |
| test/reward_per_eps            | -8          |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0033     |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 575        |
| stats_o/mean                   | 0.44325987 |
| stats_o/std                    | 0.02656518 |
| test/episodes                  | 5760       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00349   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -1.1751963 |
| test/Q_plus_P                  | -1.1751963 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 230400     |
| train/episodes                 | 23040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.621      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0571    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 921600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.44325897  |
| stats_o/std                    | 0.026563788 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.110101   |
| test/Q_plus_P                  | -1.110101   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.44325793  |
| stats_o/std                    | 0.026559813 |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0320817  |
| test/Q_plus_P                  | -1.0320817  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.4432559   |
| stats_o/std                    | 0.026558636 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.0130646  |
| test/Q_plus_P                  | -1.0130646  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.44325647  |
| stats_o/std                    | 0.026557386 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00478    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.8265392  |
| test/Q_plus_P                  | -0.8265392  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.4432572   |
| stats_o/std                    | 0.026556315 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00208    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9537515  |
| test/Q_plus_P                  | -0.9537515  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.4432582   |
| stats_o/std                    | 0.026555762 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00383    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.1932908  |
| test/Q_plus_P                  | -1.1932908  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.4432584   |
| stats_o/std                    | 0.026554229 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00218    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.1406494  |
| test/Q_plus_P                  | -1.1406494  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.4432591   |
| stats_o/std                    | 0.026552707 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0057     |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.1859641  |
| test/Q_plus_P                  | -1.1859641  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.44326118  |
| stats_o/std                    | 0.026548924 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00398    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.0386424  |
| test/Q_plus_P                  | -1.0386424  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.4432594   |
| stats_o/std                    | 0.026547024 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00365    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0846744  |
| test/Q_plus_P                  | -1.0846744  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.44326007  |
| stats_o/std                    | 0.026546719 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00735    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.0375694  |
| test/Q_plus_P                  | -1.0375694  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00477    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.4432614   |
| stats_o/std                    | 0.026545048 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0145     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0015639  |
| test/Q_plus_P                  | -1.0015639  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00555    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.44326234  |
| stats_o/std                    | 0.026542181 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00346    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7880609  |
| test/Q_plus_P                  | -0.7880609  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.4432622   |
| stats_o/std                    | 0.026539989 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00929    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.8410186  |
| test/Q_plus_P                  | -0.8410186  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00425    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.44326445  |
| stats_o/std                    | 0.026537573 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00508    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.0199313  |
| test/Q_plus_P                  | -1.0199313  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00409    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.4432641   |
| stats_o/std                    | 0.02653564  |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00243    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.93573767 |
| test/Q_plus_P                  | -0.93573767 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00429    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.44326317  |
| stats_o/std                    | 0.026533777 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00356    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.9432738  |
| test/Q_plus_P                  | -0.9432738  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.44326225  |
| stats_o/std                    | 0.026533058 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0145     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.80940247 |
| test/Q_plus_P                  | -0.80940247 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.4432629   |
| stats_o/std                    | 0.026531378 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00387    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.9356911  |
| test/Q_plus_P                  | -0.9356911  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.4432628   |
| stats_o/std                    | 0.026528776 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00583    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -1.1437843  |
| test/Q_plus_P                  | -1.1437843  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.44326138  |
| stats_o/std                    | 0.026529646 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00276    |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.941859   |
| test/Q_plus_P                  | -0.941859   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.44326043  |
| stats_o/std                    | 0.026528856 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00802    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.9020582  |
| test/Q_plus_P                  | -0.9020582  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00452    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.4432607   |
| stats_o/std                    | 0.026524702 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00758    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.8627303  |
| test/Q_plus_P                  | -0.8627303  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00409    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.44326043  |
| stats_o/std                    | 0.026524035 |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00267    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.2353406  |
| test/Q_plus_P                  | -1.2353406  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00493    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
