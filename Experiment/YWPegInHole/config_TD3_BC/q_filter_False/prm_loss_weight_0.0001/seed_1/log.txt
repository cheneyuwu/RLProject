Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC/q_filter_False/prm_loss_weight_0.0001/seed_1
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.44340876  |
| stats_o/std                    | 0.036848944 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00174    |
| test/info_shaping_reward_mean  | -0.0864     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.2513013  |
| test/Q_plus_P                  | -1.2513013  |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0652     |
| train/info_shaping_reward_mean | -0.147      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.44425276  |
| stats_o/std                    | 0.035758838 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0691     |
| test/info_shaping_reward_mean  | -0.0862     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.5419264  |
| test/Q_plus_P                  | -1.5419264  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0125      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0518     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.44466138 |
| stats_o/std                    | 0.03511738 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.168      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0015    |
| test/info_shaping_reward_mean  | -0.0809    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -1.7135253 |
| test/Q_plus_P                  | -1.7135253 |
| test/reward_per_eps            | -33.3      |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0181     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0484    |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.3      |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.4456574  |
| stats_o/std                    | 0.03474721 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.105      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00053   |
| test/info_shaping_reward_mean  | -0.0818    |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -2.076393  |
| test/Q_plus_P                  | -2.076393  |
| test/reward_per_eps            | -35.8      |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.015      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0515    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.44592845 |
| stats_o/std                    | 0.0343002  |
| test/episodes                  | 50         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.225      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000983  |
| test/info_shaping_reward_mean  | -0.0748    |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -2.2456913 |
| test/Q_plus_P                  | -2.2456913 |
| test/reward_per_eps            | -31        |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0244     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0494    |
| train/info_shaping_reward_mean | -0.107     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39        |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.44615522 |
| stats_o/std                    | 0.03413303 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.0625     |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00138   |
| test/info_shaping_reward_mean  | -0.0857    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -2.894347  |
| test/Q_plus_P                  | -2.894347  |
| test/reward_per_eps            | -37.5      |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.00937    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0494    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.44662535  |
| stats_o/std                    | 0.034100905 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.177       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0785     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -2.9913929  |
| test/Q_plus_P                  | -2.9913929  |
| test/reward_per_eps            | -32.9       |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0175      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0523     |
| train/info_shaping_reward_mean | -0.116      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.44689605  |
| stats_o/std                    | 0.033822604 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.32        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.0693     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -2.8722217  |
| test/Q_plus_P                  | -2.8722217  |
| test/reward_per_eps            | -27.2       |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00688     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0527     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.44727457  |
| stats_o/std                    | 0.033775765 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.318       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0742     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -3.0100715  |
| test/Q_plus_P                  | -3.0100715  |
| test/reward_per_eps            | -27.3       |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0508     |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.4475707   |
| stats_o/std                    | 0.033664484 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.242       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00367    |
| test/info_shaping_reward_mean  | -0.0801     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -3.6378958  |
| test/Q_plus_P                  | -3.6378958  |
| test/reward_per_eps            | -30.3       |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0425      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0313     |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.3       |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.44763562  |
| stats_o/std                    | 0.033401575 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.253       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0727     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -3.7820141  |
| test/Q_plus_P                  | -3.7820141  |
| test/reward_per_eps            | -29.9       |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.02        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0468     |
| train/info_shaping_reward_mean | -0.107      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.44776738 |
| stats_o/std                    | 0.03317259 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.188      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00359   |
| test/info_shaping_reward_mean  | -0.0778    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -4.431935  |
| test/Q_plus_P                  | -4.431935  |
| test/reward_per_eps            | -32.5      |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.01       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0554    |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.6      |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.44781718  |
| stats_o/std                    | 0.033083137 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.045       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00335    |
| test/info_shaping_reward_mean  | -0.0865     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -5.5257435  |
| test/Q_plus_P                  | -5.5257435  |
| test/reward_per_eps            | -38.2       |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.0175      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0476     |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.44779626  |
| stats_o/std                    | 0.032912534 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.2         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00276    |
| test/info_shaping_reward_mean  | -0.0819     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -5.0093346  |
| test/Q_plus_P                  | -5.0093346  |
| test/reward_per_eps            | -32         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00375     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0541     |
| train/info_shaping_reward_mean | -0.108      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.4479833  |
| stats_o/std                    | 0.03286643 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.16       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00709   |
| test/info_shaping_reward_mean  | -0.0802    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -4.9970417 |
| test/Q_plus_P                  | -4.9970417 |
| test/reward_per_eps            | -33.6      |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0312     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0472    |
| train/info_shaping_reward_mean | -0.108     |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -38.8      |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.44793427  |
| stats_o/std                    | 0.032826014 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0625      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.0853     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -6.6665874  |
| test/Q_plus_P                  | -6.6665874  |
| test/reward_per_eps            | -37.5       |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.055       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.046      |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.8       |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.44794452 |
| stats_o/std                    | 0.03270221 |
| test/episodes                  | 170        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.13       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00667   |
| test/info_shaping_reward_mean  | -0.083     |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -6.4733887 |
| test/Q_plus_P                  | -6.4733887 |
| test/reward_per_eps            | -34.8      |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.00813    |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0525    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.7      |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.44791055  |
| stats_o/std                    | 0.032603014 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.122       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00324    |
| test/info_shaping_reward_mean  | -0.0827     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -6.7908063  |
| test/Q_plus_P                  | -6.7908063  |
| test/reward_per_eps            | -35.1       |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0112      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0478     |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.5       |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.44793257  |
| stats_o/std                    | 0.032412942 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.263       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00246    |
| test/info_shaping_reward_mean  | -0.0773     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -6.045037   |
| test/Q_plus_P                  | -6.045037   |
| test/reward_per_eps            | -29.5       |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0.2         |
| train/info_is_success_mean     | 0.02        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.047      |
| train/info_shaping_reward_mean | -0.103      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.2       |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.44793406 |
| stats_o/std                    | 0.03230846 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.14       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00279   |
| test/info_shaping_reward_mean  | -0.0831    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -7.287994  |
| test/Q_plus_P                  | -7.287994  |
| test/reward_per_eps            | -34.4      |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0.2        |
| train/info_is_success_mean     | 0.0131     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0493    |
| train/info_shaping_reward_mean | -0.111     |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.5      |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.44798768  |
| stats_o/std                    | 0.032261185 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.125       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00576    |
| test/info_shaping_reward_mean  | -0.0807     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -7.5627565  |
| test/Q_plus_P                  | -7.5627565  |
| test/reward_per_eps            | -35         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0586     |
| train/info_shaping_reward_mean | -0.114      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.44802248  |
| stats_o/std                    | 0.032194488 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.12        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00295    |
| test/info_shaping_reward_mean  | -0.0838     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -7.900564   |
| test/Q_plus_P                  | -7.900564   |
| test/reward_per_eps            | -35.2       |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0.4         |
| train/info_is_success_mean     | 0.0263      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0419     |
| train/info_shaping_reward_mean | -0.113      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.44809318  |
| stats_o/std                    | 0.032151625 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.193       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00253    |
| test/info_shaping_reward_mean  | -0.0754     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -7.6532288  |
| test/Q_plus_P                  | -7.6532288  |
| test/reward_per_eps            | -32.3       |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0225      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0496     |
| train/info_shaping_reward_mean | -0.119      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.1       |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.44802272 |
| stats_o/std                    | 0.03203187 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.107      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00267   |
| test/info_shaping_reward_mean  | -0.0836    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -8.920396  |
| test/Q_plus_P                  | -8.920396  |
| test/reward_per_eps            | -35.7      |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0.3        |
| train/info_is_success_mean     | 0.005      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0532    |
| train/info_shaping_reward_mean | -0.11      |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.8      |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.44798803  |
| stats_o/std                    | 0.031880546 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.245       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00273    |
| test/info_shaping_reward_mean  | -0.0701     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -7.5631566  |
| test/Q_plus_P                  | -7.5631566  |
| test/reward_per_eps            | -30.2       |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.0344      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0319     |
| train/info_shaping_reward_mean | -0.106      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.6       |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.44790104  |
| stats_o/std                    | 0.031702366 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.362       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00387    |
| test/info_shaping_reward_mean  | -0.072      |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -6.0822864  |
| test/Q_plus_P                  | -6.0822864  |
| test/reward_per_eps            | -25.5       |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0.6         |
| train/info_is_success_mean     | 0.0619      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0296     |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -37.5       |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.44784132 |
| stats_o/std                    | 0.03154291 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.427      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00283   |
| test/info_shaping_reward_mean  | -0.069     |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -5.925236  |
| test/Q_plus_P                  | -5.925236  |
| test/reward_per_eps            | -22.9      |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0.7        |
| train/info_is_success_mean     | 0.0969     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0243    |
| train/info_shaping_reward_mean | -0.096     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -36.1      |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.4477923   |
| stats_o/std                    | 0.031403746 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.343       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00444    |
| test/info_shaping_reward_mean  | -0.074      |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -6.481903   |
| test/Q_plus_P                  | -6.481903   |
| test/reward_per_eps            | -26.3       |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.0888      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0205     |
| train/info_shaping_reward_mean | -0.0985     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -36.5       |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.44775572 |
| stats_o/std                    | 0.03126194 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.48       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00338   |
| test/info_shaping_reward_mean  | -0.0631    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -5.37076   |
| test/Q_plus_P                  | -5.37076   |
| test/reward_per_eps            | -20.8      |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0.9        |
| train/info_is_success_mean     | 0.144      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0167    |
| train/info_shaping_reward_mean | -0.0937    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -34.2      |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 29          |
| stats_o/mean                   | 0.4477383   |
| stats_o/std                    | 0.031163702 |
| test/episodes                  | 300         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.527       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00228    |
| test/info_shaping_reward_mean  | -0.0619     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -5.2663307  |
| test/Q_plus_P                  | -5.2663307  |
| test/reward_per_eps            | -18.9       |
| test/steps                     | 12000       |
| train/episodes                 | 1200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.154       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00843    |
| train/info_shaping_reward_mean | -0.0962     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.9       |
| train/steps                    | 48000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.4477601   |
| stats_o/std                    | 0.031028882 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.453       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00639    |
| test/info_shaping_reward_mean  | -0.0685     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -5.8426294  |
| test/Q_plus_P                  | -5.8426294  |
| test/reward_per_eps            | -21.9       |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.159       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0116     |
| train/info_shaping_reward_mean | -0.0904     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -33.6       |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.44789276  |
| stats_o/std                    | 0.031004345 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.12        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00421    |
| test/info_shaping_reward_mean  | -0.0855     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -7.9464197  |
| test/Q_plus_P                  | -7.9464197  |
| test/reward_per_eps            | -35.2       |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0.3         |
| train/info_is_success_mean     | 0.0488      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0511     |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38         |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.44802567  |
| stats_o/std                    | 0.030958967 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.19        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00329    |
| test/info_shaping_reward_mean  | -0.0825     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -7.7506557  |
| test/Q_plus_P                  | -7.7506557  |
| test/reward_per_eps            | -32.4       |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0.5         |
| train/info_is_success_mean     | 0.0275      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0482     |
| train/info_shaping_reward_mean | -0.102      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -38.9       |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.4480302   |
| stats_o/std                    | 0.030893648 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.562       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00247    |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -5.182971   |
| test/Q_plus_P                  | -5.182971   |
| test/reward_per_eps            | -17.5       |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.209       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00693    |
| train/info_shaping_reward_mean | -0.0857     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -31.6       |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.44798508  |
| stats_o/std                    | 0.030823544 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.542       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00367    |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -5.344683   |
| test/Q_plus_P                  | -5.344683   |
| test/reward_per_eps            | -18.3       |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.35        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00487    |
| train/info_shaping_reward_mean | -0.076      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.44794235 |
| stats_o/std                    | 0.03072331 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.565      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0049    |
| test/info_shaping_reward_mean  | -0.0578    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -5.083091  |
| test/Q_plus_P                  | -5.083091  |
| test/reward_per_eps            | -17.4      |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.356      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00361   |
| train/info_shaping_reward_mean | -0.076     |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -25.8      |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.44793615  |
| stats_o/std                    | 0.030640006 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0018     |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -4.93872    |
| test/Q_plus_P                  | -4.93872    |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.264       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00687    |
| train/info_shaping_reward_mean | -0.0831     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.4       |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.4479142   |
| stats_o/std                    | 0.030603448 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.58        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00155    |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -4.937892   |
| test/Q_plus_P                  | -4.937892   |
| test/reward_per_eps            | -16.8       |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.328       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00524    |
| train/info_shaping_reward_mean | -0.0799     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -26.9       |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.4479033   |
| stats_o/std                    | 0.030525282 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.585       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00233    |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -4.8145933  |
| test/Q_plus_P                  | -4.8145933  |
| test/reward_per_eps            | -16.6       |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.31        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00621    |
| train/info_shaping_reward_mean | -0.0801     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -27.6       |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.4478784   |
| stats_o/std                    | 0.030456373 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00221    |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -4.8897896  |
| test/Q_plus_P                  | -4.8897896  |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.383       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0726     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.7       |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.4478489   |
| stats_o/std                    | 0.030380813 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.568       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -5.1225753  |
| test/Q_plus_P                  | -5.1225753  |
| test/reward_per_eps            | -17.3       |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.424       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0701     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.44781038  |
| stats_o/std                    | 0.030317327 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.593       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00206    |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -4.697874   |
| test/Q_plus_P                  | -4.697874   |
| test/reward_per_eps            | -16.3       |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.426       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00323    |
| train/info_shaping_reward_mean | -0.0682     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.44775864  |
| stats_o/std                    | 0.030281665 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.565       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -4.9049487  |
| test/Q_plus_P                  | -4.9049487  |
| test/reward_per_eps            | -17.4       |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.417       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0706     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.3       |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.44769093  |
| stats_o/std                    | 0.030259175 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.552       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000602   |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -5.20643    |
| test/Q_plus_P                  | -5.20643    |
| test/reward_per_eps            | -17.9       |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.419       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00254    |
| train/info_shaping_reward_mean | -0.0695     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.2       |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.44763163  |
| stats_o/std                    | 0.030224549 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.62        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -4.3903294  |
| test/Q_plus_P                  | -4.3903294  |
| test/reward_per_eps            | -15.2       |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.476       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.9       |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.44758037  |
| stats_o/std                    | 0.030201571 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.615       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -4.5620317  |
| test/Q_plus_P                  | -4.5620317  |
| test/reward_per_eps            | -15.4       |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.461       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0691     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.6       |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.44753194  |
| stats_o/std                    | 0.030151658 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.59        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00113    |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.2        |
| test/Q                         | -4.529238   |
| test/Q_plus_P                  | -4.529238   |
| test/reward_per_eps            | -16.4       |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.457       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0668     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.7       |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.44748417  |
| stats_o/std                    | 0.030127509 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.613       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00223    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -4.277568   |
| test/Q_plus_P                  | -4.277568   |
| test/reward_per_eps            | -15.5       |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.484       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.067      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.6       |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.44742528  |
| stats_o/std                    | 0.030118907 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000474   |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -3.4822533  |
| test/Q_plus_P                  | -3.4822533  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.492       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.3       |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.4473605  |
| stats_o/std                    | 0.03014875 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.618      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.0497    |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -4.11528   |
| test/Q_plus_P                  | -4.11528   |
| test/reward_per_eps            | -15.3      |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.412      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00423   |
| train/info_shaping_reward_mean | -0.0749    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -23.5      |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.44728324  |
| stats_o/std                    | 0.030165598 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.588       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000856   |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -4.4566345  |
| test/Q_plus_P                  | -4.4566345  |
| test/reward_per_eps            | -16.5       |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.47        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0708     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.2       |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.44723427  |
| stats_o/std                    | 0.030137261 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.575       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.0569     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -4.809123   |
| test/Q_plus_P                  | -4.809123   |
| test/reward_per_eps            | -17         |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.508       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.7       |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.4471437   |
| stats_o/std                    | 0.030168721 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.578       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -4.127537   |
| test/Q_plus_P                  | -4.127537   |
| test/reward_per_eps            | -16.9       |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.447085    |
| stats_o/std                    | 0.030162496 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.618       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00095    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -4.060887   |
| test/Q_plus_P                  | -4.060887   |
| test/reward_per_eps            | -15.3       |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.517       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00187    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.3       |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.44702062  |
| stats_o/std                    | 0.030160068 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.62        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000987   |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -3.5758297  |
| test/Q_plus_P                  | -3.5758297  |
| test/reward_per_eps            | -15.2       |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.546       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.2       |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.44698814  |
| stats_o/std                    | 0.030154765 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000739   |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -3.4888263  |
| test/Q_plus_P                  | -3.4888263  |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.427       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0737     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -22.9       |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.44693598 |
| stats_o/std                    | 0.03012331 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.665      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000939  |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -3.2690673 |
| test/Q_plus_P                  | -3.2690673 |
| test/reward_per_eps            | -13.4      |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.523      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00229   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.1      |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.44687915  |
| stats_o/std                    | 0.030121118 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.62        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00055    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -3.6466715  |
| test/Q_plus_P                  | -3.6466715  |
| test/reward_per_eps            | -15.2       |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.521       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00218    |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.1       |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.4468253   |
| stats_o/std                    | 0.030106358 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.632       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000447   |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -3.5510476  |
| test/Q_plus_P                  | -3.5510476  |
| test/reward_per_eps            | -14.7       |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00189    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.4467664   |
| stats_o/std                    | 0.030116541 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.605       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -3.6800373  |
| test/Q_plus_P                  | -3.6800373  |
| test/reward_per_eps            | -15.8       |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.54        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.44672093  |
| stats_o/std                    | 0.030105883 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.637       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -3.394661   |
| test/Q_plus_P                  | -3.394661   |
| test/reward_per_eps            | -14.5       |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.44664076  |
| stats_o/std                    | 0.030130567 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -2.8939707  |
| test/Q_plus_P                  | -2.8939707  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00214    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 62         |
| stats_o/mean                   | 0.44660798 |
| stats_o/std                    | 0.03010258 |
| test/episodes                  | 630        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.642      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000937  |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -3.1593702 |
| test/Q_plus_P                  | -3.1593702 |
| test/reward_per_eps            | -14.3      |
| test/steps                     | 25200      |
| train/episodes                 | 2520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00256   |
| train/info_shaping_reward_mean | -0.0605    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 100800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 63         |
| stats_o/mean                   | 0.4465697  |
| stats_o/std                    | 0.03008511 |
| test/episodes                  | 640        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.61       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -3.6507554 |
| test/Q_plus_P                  | -3.6507554 |
| test/reward_per_eps            | -15.6      |
| test/steps                     | 25600      |
| train/episodes                 | 2560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.534      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00191   |
| train/info_shaping_reward_mean | -0.0636    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.6      |
| train/steps                    | 102400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 64         |
| stats_o/mean                   | 0.44652888 |
| stats_o/std                    | 0.03007587 |
| test/episodes                  | 650        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000657  |
| test/info_shaping_reward_mean  | -0.0408    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -2.5071275 |
| test/Q_plus_P                  | -2.5071275 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 26000      |
| train/episodes                 | 2600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.571      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00218   |
| train/info_shaping_reward_mean | -0.0597    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.1      |
| train/steps                    | 104000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.44648886  |
| stats_o/std                    | 0.030042507 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -2.7912874  |
| test/Q_plus_P                  | -2.7912874  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.555       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00219    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.44644842  |
| stats_o/std                    | 0.030043466 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.645       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00234    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -3.0483782  |
| test/Q_plus_P                  | -3.0483782  |
| test/reward_per_eps            | -14.2       |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.519       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.2       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.44642487  |
| stats_o/std                    | 0.030014692 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000672   |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -3.1829383  |
| test/Q_plus_P                  | -3.1829383  |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.529       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00219    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.9       |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.44637135  |
| stats_o/std                    | 0.030021949 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -2.9783895  |
| test/Q_plus_P                  | -2.9783895  |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.554       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.44633687 |
| stats_o/std                    | 0.03001511 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.58       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00126   |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -3.4918506 |
| test/Q_plus_P                  | -3.4918506 |
| test/reward_per_eps            | -16.8      |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00196   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.44630858 |
| stats_o/std                    | 0.02999028 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.647      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00166   |
| test/info_shaping_reward_mean  | -0.0488    |
| test/info_shaping_reward_min   | -0.236     |
| test/Q                         | -2.8443542 |
| test/Q_plus_P                  | -2.8443542 |
| test/reward_per_eps            | -14.1      |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.558      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00266   |
| train/info_shaping_reward_mean | -0.059     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.7      |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.4462794   |
| stats_o/std                    | 0.029973993 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -2.68892    |
| test/Q_plus_P                  | -2.68892    |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.539       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.44624028  |
| stats_o/std                    | 0.029961035 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00131    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -2.7592218  |
| test/Q_plus_P                  | -2.7592218  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.547       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0021     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.1       |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.44620433  |
| stats_o/std                    | 0.029947529 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00247    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -2.7941272  |
| test/Q_plus_P                  | -2.7941272  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.44617915  |
| stats_o/std                    | 0.029925952 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -3.0252616  |
| test/Q_plus_P                  | -3.0252616  |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.564       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00227    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.4       |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.4461496   |
| stats_o/std                    | 0.029920219 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000962   |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -2.8668602  |
| test/Q_plus_P                  | -2.8668602  |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.552       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00241    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.9       |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 76        |
| stats_o/mean                   | 0.4461218 |
| stats_o/std                    | 0.0299062 |
| test/episodes                  | 770       |
| test/info_is_success_max       | 1         |
| test/info_is_success_mean      | 0.642     |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.000295 |
| test/info_shaping_reward_mean  | -0.0504   |
| test/info_shaping_reward_min   | -0.262    |
| test/Q                         | -2.827425 |
| test/Q_plus_P                  | -2.827425 |
| test/reward_per_eps            | -14.3     |
| test/steps                     | 30800     |
| train/episodes                 | 3080      |
| train/info_is_success_max      | 1         |
| train/info_is_success_mean     | 0.564     |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.00231  |
| train/info_shaping_reward_mean | -0.059    |
| train/info_shaping_reward_min  | -0.24     |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -17.4     |
| train/steps                    | 123200    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.44608542  |
| stats_o/std                    | 0.029904798 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000694   |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -2.4572747  |
| test/Q_plus_P                  | -2.4572747  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.603       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0024     |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.4460665   |
| stats_o/std                    | 0.029884359 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000856   |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -2.4880767  |
| test/Q_plus_P                  | -2.4880767  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.44603094  |
| stats_o/std                    | 0.029878797 |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -2.3060844  |
| test/Q_plus_P                  | -2.3060844  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00212    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.44600585  |
| stats_o/std                    | 0.029854849 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -2.5999506  |
| test/Q_plus_P                  | -2.5999506  |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.44595882  |
| stats_o/std                    | 0.029848227 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000859   |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -2.5248115  |
| test/Q_plus_P                  | -2.5248115  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.44593355  |
| stats_o/std                    | 0.029833151 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00139    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -2.6159625  |
| test/Q_plus_P                  | -2.6159625  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.563       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 83         |
| stats_o/mean                   | 0.44590497 |
| stats_o/std                    | 0.02981648 |
| test/episodes                  | 840        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.675      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.0457    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -2.532752  |
| test/Q_plus_P                  | -2.532752  |
| test/reward_per_eps            | -13        |
| test/steps                     | 33600      |
| train/episodes                 | 3360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.564      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00237   |
| train/info_shaping_reward_mean | -0.059     |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -17.4      |
| train/steps                    | 134400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.44587615 |
| stats_o/std                    | 0.0298224  |
| test/episodes                  | 850        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.685      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00125   |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -2.466066  |
| test/Q_plus_P                  | -2.466066  |
| test/reward_per_eps            | -12.6      |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.515      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0025    |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.272     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -19.4      |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.4458449   |
| stats_o/std                    | 0.029821487 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000971   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -2.3826418  |
| test/Q_plus_P                  | -2.3826418  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.562       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00284    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.5       |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.44581857  |
| stats_o/std                    | 0.029813493 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.682       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -2.413875   |
| test/Q_plus_P                  | -2.413875   |
| test/reward_per_eps            | -12.7       |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.549       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00256    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.44579732  |
| stats_o/std                    | 0.029788963 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -2.31011    |
| test/Q_plus_P                  | -2.31011    |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.568       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00195    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.3       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.44577208  |
| stats_o/std                    | 0.029776774 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00148    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.658829   |
| test/Q_plus_P                  | -2.658829   |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.542       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.3       |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.4457498   |
| stats_o/std                    | 0.029756503 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000542   |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -1.9889197  |
| test/Q_plus_P                  | -1.9889197  |
| test/reward_per_eps            | -11         |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.57        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.4457173   |
| stats_o/std                    | 0.029736785 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.205      |
| test/Q                         | -2.2437088  |
| test/Q_plus_P                  | -2.2437088  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00236    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.4456979   |
| stats_o/std                    | 0.029720256 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.657       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -2.5834363  |
| test/Q_plus_P                  | -2.5834363  |
| test/reward_per_eps            | -13.7       |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0022     |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.44567695  |
| stats_o/std                    | 0.029711636 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -2.3919592  |
| test/Q_plus_P                  | -2.3919592  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.44565555  |
| stats_o/std                    | 0.029702583 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -2.217964   |
| test/Q_plus_P                  | -2.217964   |
| test/reward_per_eps            | -12         |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.578       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00217    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.44564214  |
| stats_o/std                    | 0.029678948 |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -2.513706   |
| test/Q_plus_P                  | -2.513706   |
| test/reward_per_eps            | -13         |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.44562808  |
| stats_o/std                    | 0.029654661 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -2.3759773  |
| test/Q_plus_P                  | -2.3759773  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.583       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.7       |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.4456127   |
| stats_o/std                    | 0.029651387 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.623       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00177    |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -2.7536628  |
| test/Q_plus_P                  | -2.7536628  |
| test/reward_per_eps            | -15.1       |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.577       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00238    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.9       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.44559076  |
| stats_o/std                    | 0.029640885 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00146    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -2.0968113  |
| test/Q_plus_P                  | -2.0968113  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00218    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.4455639   |
| stats_o/std                    | 0.029624084 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000865   |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -2.1587715  |
| test/Q_plus_P                  | -2.1587715  |
| test/reward_per_eps            | -12         |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00247    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.44554123  |
| stats_o/std                    | 0.029619334 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -2.0734608  |
| test/Q_plus_P                  | -2.0734608  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.4455206   |
| stats_o/std                    | 0.029614644 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00142    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -2.089463   |
| test/Q_plus_P                  | -2.089463   |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.4455144   |
| stats_o/std                    | 0.029598318 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00119    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -2.362549   |
| test/Q_plus_P                  | -2.362549   |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.537       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00242    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.5       |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.4455035   |
| stats_o/std                    | 0.029585918 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.64        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000959   |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.5914633  |
| test/Q_plus_P                  | -2.5914633  |
| test/reward_per_eps            | -14.4       |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00253    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.44549516  |
| stats_o/std                    | 0.029585266 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000447   |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -2.1575031  |
| test/Q_plus_P                  | -2.1575031  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.541       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -18.4       |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.44547734  |
| stats_o/std                    | 0.029571109 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -2.070298   |
| test/Q_plus_P                  | -2.070298   |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.44546342  |
| stats_o/std                    | 0.029543823 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00323    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.9690962  |
| test/Q_plus_P                  | -1.9690962  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00342    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.44544613  |
| stats_o/std                    | 0.029521829 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0433     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -2.1043768  |
| test/Q_plus_P                  | -2.1043768  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.4454285   |
| stats_o/std                    | 0.029507482 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00124    |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -2.4064472  |
| test/Q_plus_P                  | -2.4064472  |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.579       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0028     |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.44541416  |
| stats_o/std                    | 0.029497564 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000757   |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -2.2224884  |
| test/Q_plus_P                  | -2.2224884  |
| test/reward_per_eps            | -12         |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.581       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.8       |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.44540134  |
| stats_o/std                    | 0.029484218 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0444     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -2.412071   |
| test/Q_plus_P                  | -2.412071   |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.587       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.4453788   |
| stats_o/std                    | 0.029479504 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.667       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -2.431254   |
| test/Q_plus_P                  | -2.431254   |
| test/reward_per_eps            | -13.3       |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.588       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.5       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.44536686  |
| stats_o/std                    | 0.029470528 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.675       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000882   |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -2.4225419  |
| test/Q_plus_P                  | -2.4225419  |
| test/reward_per_eps            | -13         |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.574       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.44535446  |
| stats_o/std                    | 0.029456139 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000823   |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.84129    |
| test/Q_plus_P                  | -1.84129    |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.576       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17         |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.44534066  |
| stats_o/std                    | 0.029455377 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -2.168937   |
| test/Q_plus_P                  | -2.168937   |
| test/reward_per_eps            | -12         |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00272    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.44532725  |
| stats_o/std                    | 0.029440612 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.677       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000762   |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -2.2487216  |
| test/Q_plus_P                  | -2.2487216  |
| test/reward_per_eps            | -12.9       |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 115        |
| stats_o/mean                   | 0.44531587 |
| stats_o/std                    | 0.02942071 |
| test/episodes                  | 1160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.65       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000425  |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -2.496168  |
| test/Q_plus_P                  | -2.496168  |
| test/reward_per_eps            | -14        |
| test/steps                     | 46400      |
| train/episodes                 | 4640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.605      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00281   |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.8      |
| train/steps                    | 185600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.44529438  |
| stats_o/std                    | 0.029413521 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000434   |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -2.0930717  |
| test/Q_plus_P                  | -2.0930717  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.4452865   |
| stats_o/std                    | 0.029402792 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.893179   |
| test/Q_plus_P                  | -1.893179   |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.44526777  |
| stats_o/std                    | 0.029393151 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00138    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.208      |
| test/Q                         | -2.1580122  |
| test/Q_plus_P                  | -2.1580122  |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.44524446  |
| stats_o/std                    | 0.029388698 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000915   |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.6961445  |
| test/Q_plus_P                  | -1.6961445  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.44522488  |
| stats_o/std                    | 0.029376758 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0434     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.942068   |
| test/Q_plus_P                  | -1.942068   |
| test/reward_per_eps            | -12         |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00263    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.44521904  |
| stats_o/std                    | 0.029361099 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.66        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00116    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.36049    |
| test/Q_plus_P                  | -2.36049    |
| test/reward_per_eps            | -13.6       |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.525       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19         |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.4451969   |
| stats_o/std                    | 0.029352657 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.65        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00225    |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -2.5837355  |
| test/Q_plus_P                  | -2.5837355  |
| test/reward_per_eps            | -14         |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.591       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.44518986  |
| stats_o/std                    | 0.029330028 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00249    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -2.0399685  |
| test/Q_plus_P                  | -2.0399685  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.596       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.44517624  |
| stats_o/std                    | 0.029320298 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -2.264306   |
| test/Q_plus_P                  | -2.264306   |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0034     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.44516638  |
| stats_o/std                    | 0.029304544 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000825   |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -2.4247048  |
| test/Q_plus_P                  | -2.4247048  |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.44515154  |
| stats_o/std                    | 0.029287523 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.8793614  |
| test/Q_plus_P                  | -1.8793614  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.44513586  |
| stats_o/std                    | 0.029274078 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00178    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.7607654  |
| test/Q_plus_P                  | -1.7607654  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.054      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.44511735  |
| stats_o/std                    | 0.029261539 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -2.006695   |
| test/Q_plus_P                  | -2.006695   |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.4451106   |
| stats_o/std                    | 0.029246597 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.001      |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -2.1785014  |
| test/Q_plus_P                  | -2.1785014  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.44509104 |
| stats_o/std                    | 0.02924677 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.66       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00111   |
| test/info_shaping_reward_mean  | -0.0458    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -2.2952404 |
| test/Q_plus_P                  | -2.2952404 |
| test/reward_per_eps            | -13.6      |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00315   |
| train/info_shaping_reward_mean | -0.0633    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 131         |
| stats_o/mean                   | 0.44507846  |
| stats_o/std                    | 0.029232897 |
| test/episodes                  | 1320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000951   |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -2.2851028  |
| test/Q_plus_P                  | -2.2851028  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 52800       |
| train/episodes                 | 5280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 211200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.4450599   |
| stats_o/std                    | 0.029232554 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00101    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -2.1708374  |
| test/Q_plus_P                  | -2.1708374  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.608       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00295    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.44505146  |
| stats_o/std                    | 0.029228376 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -2.1896896  |
| test/Q_plus_P                  | -2.1896896  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.44504142  |
| stats_o/std                    | 0.029231498 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000995   |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -2.066728   |
| test/Q_plus_P                  | -2.066728   |
| test/reward_per_eps            | -12         |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.44503167  |
| stats_o/std                    | 0.029233756 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.6590219  |
| test/Q_plus_P                  | -1.6590219  |
| test/reward_per_eps            | -10         |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.571       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.1       |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.44501385  |
| stats_o/std                    | 0.029232897 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00115    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.7597983  |
| test/Q_plus_P                  | -1.7597983  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.6         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16         |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.44500148 |
| stats_o/std                    | 0.02922462 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.745      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000958  |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.7040815 |
| test/Q_plus_P                  | -1.7040815 |
| test/reward_per_eps            | -10.2      |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00329   |
| train/info_shaping_reward_mean | -0.0596    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.44499156  |
| stats_o/std                    | 0.029217869 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00173    |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -2.0175736  |
| test/Q_plus_P                  | -2.0175736  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.44497713  |
| stats_o/std                    | 0.029227696 |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -2.02596    |
| test/Q_plus_P                  | -2.02596    |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00227    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.44497117  |
| stats_o/std                    | 0.029226044 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.67        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -2.1906223  |
| test/Q_plus_P                  | -2.1906223  |
| test/reward_per_eps            | -13.2       |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.44496763  |
| stats_o/std                    | 0.029214308 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00128    |
| test/info_shaping_reward_mean  | -0.0412     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.6268317  |
| test/Q_plus_P                  | -1.6268317  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.589       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.4       |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.44494763  |
| stats_o/std                    | 0.029211318 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.080193   |
| test/Q_plus_P                  | -2.080193   |
| test/reward_per_eps            | -12         |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.4449285   |
| stats_o/std                    | 0.029214332 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00166    |
| test/info_shaping_reward_mean  | -0.0399     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.8104688  |
| test/Q_plus_P                  | -1.8104688  |
| test/reward_per_eps            | -10         |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00227    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.44491625  |
| stats_o/std                    | 0.029207908 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000767   |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -2.180526   |
| test/Q_plus_P                  | -2.180526   |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00199    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.44490314  |
| stats_o/std                    | 0.029204927 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000501   |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.7282481  |
| test/Q_plus_P                  | -1.7282481  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.619       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.4448948   |
| stats_o/std                    | 0.029204153 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000683   |
| test/info_shaping_reward_mean  | -0.0415     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.8399724  |
| test/Q_plus_P                  | -1.8399724  |
| test/reward_per_eps            | -11         |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.584       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.6       |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.44488308  |
| stats_o/std                    | 0.029191284 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.9650218  |
| test/Q_plus_P                  | -1.9650218  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00215    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.4448746   |
| stats_o/std                    | 0.029184876 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.6762482  |
| test/Q_plus_P                  | -1.6762482  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00231    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.4448668   |
| stats_o/std                    | 0.029168034 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00264    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.209      |
| test/Q                         | -1.8698865  |
| test/Q_plus_P                  | -1.8698865  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00244    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.44485915  |
| stats_o/std                    | 0.029161371 |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.642       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -2.408464   |
| test/Q_plus_P                  | -2.408464   |
| test/reward_per_eps            | -14.3       |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00251    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.44483876  |
| stats_o/std                    | 0.029161021 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.8115948  |
| test/Q_plus_P                  | -1.8115948  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.4448273   |
| stats_o/std                    | 0.029155137 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.8257211  |
| test/Q_plus_P                  | -1.8257211  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00228    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.444815    |
| stats_o/std                    | 0.029143332 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000427   |
| test/info_shaping_reward_mean  | -0.0373     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.6353831  |
| test/Q_plus_P                  | -1.6353831  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00219    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 154         |
| stats_o/mean                   | 0.44479677  |
| stats_o/std                    | 0.029133677 |
| test/episodes                  | 1550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0011     |
| test/info_shaping_reward_mean  | -0.0391     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.6753235  |
| test/Q_plus_P                  | -1.6753235  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 62000       |
| train/episodes                 | 6200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 248000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.44478872 |
| stats_o/std                    | 0.02913476 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.785      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000565  |
| test/info_shaping_reward_mean  | -0.0353    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -1.3018095 |
| test/Q_plus_P                  | -1.3018095 |
| test/reward_per_eps            | -8.6       |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.62       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00274   |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.4447756   |
| stats_o/std                    | 0.029126624 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000989   |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.5171325  |
| test/Q_plus_P                  | -1.5171325  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.44475877  |
| stats_o/std                    | 0.029121352 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000798   |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.8558738  |
| test/Q_plus_P                  | -1.8558738  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00303    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.44474256  |
| stats_o/std                    | 0.029114148 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00111    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.7927017  |
| test/Q_plus_P                  | -1.7927017  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00235    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.44473204 |
| stats_o/std                    | 0.02909563 |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00158   |
| test/info_shaping_reward_mean  | -0.0412    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.8331037 |
| test/Q_plus_P                  | -1.8331037 |
| test/reward_per_eps            | -11        |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00264   |
| train/info_shaping_reward_mean | -0.055     |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.44472876  |
| stats_o/std                    | 0.029083865 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000827   |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.7887285  |
| test/Q_plus_P                  | -1.7887285  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.4447243   |
| stats_o/std                    | 0.029068202 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0364     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.4716916  |
| test/Q_plus_P                  | -1.4716916  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.44471458  |
| stats_o/std                    | 0.029062541 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000788   |
| test/info_shaping_reward_mean  | -0.0417     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -1.7925103  |
| test/Q_plus_P                  | -1.7925103  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.44470665  |
| stats_o/std                    | 0.029043488 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.693       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00297    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.0324636  |
| test/Q_plus_P                  | -2.0324636  |
| test/reward_per_eps            | -12.3       |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00223    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.4446967   |
| stats_o/std                    | 0.029031537 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00102    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.1559248  |
| test/Q_plus_P                  | -2.1559248  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.4446851   |
| stats_o/std                    | 0.029020382 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000791   |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.5388598  |
| test/Q_plus_P                  | -1.5388598  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00275    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.4446769   |
| stats_o/std                    | 0.029008023 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0384     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.3834935  |
| test/Q_plus_P                  | -1.3834935  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.4446651   |
| stats_o/std                    | 0.028998464 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0016     |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.4770787  |
| test/Q_plus_P                  | -1.4770787  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00259    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.44465455  |
| stats_o/std                    | 0.028981602 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.0393     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.4047766  |
| test/Q_plus_P                  | -1.4047766  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.4446437  |
| stats_o/std                    | 0.02897224 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.715      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00115   |
| test/info_shaping_reward_mean  | -0.0444    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.7512735 |
| test/Q_plus_P                  | -1.7512735 |
| test/reward_per_eps            | -11.4      |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00275   |
| train/info_shaping_reward_mean | -0.0547    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.44463277  |
| stats_o/std                    | 0.028962443 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00109    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.6506091  |
| test/Q_plus_P                  | -1.6506091  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00261    |
| train/info_shaping_reward_mean | -0.0534     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 171        |
| stats_o/mean                   | 0.44461712 |
| stats_o/std                    | 0.02895843 |
| test/episodes                  | 1720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000886  |
| test/info_shaping_reward_mean  | -0.0435    |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -1.734191  |
| test/Q_plus_P                  | -1.734191  |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 68800      |
| train/episodes                 | 6880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0025    |
| train/info_shaping_reward_mean | -0.0589    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 275200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.44461152 |
| stats_o/std                    | 0.02894356 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000988  |
| test/info_shaping_reward_mean  | -0.0426    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.7452652 |
| test/Q_plus_P                  | -1.7452652 |
| test/reward_per_eps            | -11        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00224   |
| train/info_shaping_reward_mean | -0.0558    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.44460127  |
| stats_o/std                    | 0.028931895 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000571   |
| test/info_shaping_reward_mean  | -0.0356     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.2814546  |
| test/Q_plus_P                  | -1.2814546  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00229    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 174        |
| stats_o/mean                   | 0.44459248 |
| stats_o/std                    | 0.02892235 |
| test/episodes                  | 1750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000225  |
| test/info_shaping_reward_mean  | -0.0342    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -1.1209204 |
| test/Q_plus_P                  | -1.1209204 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 70000      |
| train/episodes                 | 7000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00259   |
| train/info_shaping_reward_mean | -0.056     |
| train/info_shaping_reward_min  | -0.266     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 280000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.4445807   |
| stats_o/std                    | 0.028912814 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000522   |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.0302258  |
| test/Q_plus_P                  | -2.0302258  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.44457328  |
| stats_o/std                    | 0.028895758 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.856818   |
| test/Q_plus_P                  | -1.856818   |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.44456336  |
| stats_o/std                    | 0.028891267 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.703       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000705   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.8738011  |
| test/Q_plus_P                  | -1.8738011  |
| test/reward_per_eps            | -11.9       |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00225    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.44455755 |
| stats_o/std                    | 0.02888    |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000541  |
| test/info_shaping_reward_mean  | -0.0395    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.4353942 |
| test/Q_plus_P                  | -1.4353942 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00265   |
| train/info_shaping_reward_mean | -0.0585    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.44455528  |
| stats_o/std                    | 0.028863125 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000387   |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.7400622  |
| test/Q_plus_P                  | -1.7400622  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.44454923  |
| stats_o/std                    | 0.028846994 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00184    |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.8192295  |
| test/Q_plus_P                  | -1.8192295  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0543     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.44454333  |
| stats_o/std                    | 0.028834634 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00084    |
| test/info_shaping_reward_mean  | -0.043      |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.562247   |
| test/Q_plus_P                  | -1.562247   |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0541     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.44453916  |
| stats_o/std                    | 0.028823482 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000663   |
| test/info_shaping_reward_mean  | -0.0411     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.6075844  |
| test/Q_plus_P                  | -1.6075844  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0549     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.4445344  |
| stats_o/std                    | 0.02880959 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00139   |
| test/info_shaping_reward_mean  | -0.0454    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.5356526 |
| test/Q_plus_P                  | -1.5356526 |
| test/reward_per_eps            | -11        |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00256   |
| train/info_shaping_reward_mean | -0.0524    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.44452417  |
| stats_o/std                    | 0.028799916 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000551   |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.4223595  |
| test/Q_plus_P                  | -1.4223595  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.44451687  |
| stats_o/std                    | 0.028791899 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000483   |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.7473689  |
| test/Q_plus_P                  | -1.7473689  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.444512    |
| stats_o/std                    | 0.028780885 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00145    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.6206409  |
| test/Q_plus_P                  | -1.6206409  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0548     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 187        |
| stats_o/mean                   | 0.44450462 |
| stats_o/std                    | 0.02876953 |
| test/episodes                  | 1880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00164   |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -1.6015375 |
| test/Q_plus_P                  | -1.6015375 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 75200      |
| train/episodes                 | 7520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00344   |
| train/info_shaping_reward_mean | -0.0594    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 300800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.44449592  |
| stats_o/std                    | 0.028766058 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000548   |
| test/info_shaping_reward_mean  | -0.0427     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -1.4677422  |
| test/Q_plus_P                  | -1.4677422  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00262    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.44449547  |
| stats_o/std                    | 0.028754488 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00166    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.0867673  |
| test/Q_plus_P                  | -1.0867673  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.4444879   |
| stats_o/std                    | 0.028749742 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00227    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.5838075  |
| test/Q_plus_P                  | -1.5838075  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00281    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.44448194  |
| stats_o/std                    | 0.028740374 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.5021691  |
| test/Q_plus_P                  | -1.5021691  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.695       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00249    |
| train/info_shaping_reward_mean | -0.0513     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.44447765  |
| stats_o/std                    | 0.028732842 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000711   |
| test/info_shaping_reward_mean  | -0.0405     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.3696271  |
| test/Q_plus_P                  | -1.3696271  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00278    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.4444749   |
| stats_o/std                    | 0.028716698 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.532       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00132    |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -2.3072462  |
| test/Q_plus_P                  | -2.3072462  |
| test/reward_per_eps            | -18.7       |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0518     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.44447136  |
| stats_o/std                    | 0.028712096 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00275    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.4682564  |
| test/Q_plus_P                  | -1.4682564  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.44446206  |
| stats_o/std                    | 0.028707953 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00152    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -1.761238   |
| test/Q_plus_P                  | -1.761238   |
| test/reward_per_eps            | -12         |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.44445288  |
| stats_o/std                    | 0.028696986 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000886   |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.4755086  |
| test/Q_plus_P                  | -1.4755086  |
| test/reward_per_eps            | -10         |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00288    |
| train/info_shaping_reward_mean | -0.0517     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.44444612  |
| stats_o/std                    | 0.028690556 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.4338108  |
| test/Q_plus_P                  | -1.4338108  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00233    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.44443837  |
| stats_o/std                    | 0.028685348 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000556   |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.6635214  |
| test/Q_plus_P                  | -1.6635214  |
| test/reward_per_eps            | -11         |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.709       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00274    |
| train/info_shaping_reward_mean | -0.053      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.44443145 |
| stats_o/std                    | 0.02867476 |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00228   |
| test/info_shaping_reward_mean  | -0.04      |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.168224  |
| test/Q_plus_P                  | -1.168224  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00335   |
| train/info_shaping_reward_mean | -0.0545    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.44443142 |
| stats_o/std                    | 0.02866348 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00342   |
| test/info_shaping_reward_mean  | -0.0394    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.144612  |
| test/Q_plus_P                  | -1.144612  |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00357   |
| train/info_shaping_reward_mean | -0.0561    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.44442555 |
| stats_o/std                    | 0.02865635 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.635      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -1.8032955 |
| test/Q_plus_P                  | -1.8032955 |
| test/reward_per_eps            | -14.6      |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.693      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00332   |
| train/info_shaping_reward_mean | -0.0531    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.44442365  |
| stats_o/std                    | 0.028639628 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0436     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.1795384  |
| test/Q_plus_P                  | -1.1795384  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0522     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.44441935  |
| stats_o/std                    | 0.028631926 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.63        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.002      |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.8472567  |
| test/Q_plus_P                  | -1.8472567  |
| test/reward_per_eps            | -14.8       |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.44441864  |
| stats_o/std                    | 0.028626308 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.68        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00118    |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.7061843  |
| test/Q_plus_P                  | -1.7061843  |
| test/reward_per_eps            | -12.8       |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 205        |
| stats_o/mean                   | 0.4444176  |
| stats_o/std                    | 0.02861555 |
| test/episodes                  | 2060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.667      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0021    |
| test/info_shaping_reward_mean  | -0.055     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -1.8543499 |
| test/Q_plus_P                  | -1.8543499 |
| test/reward_per_eps            | -13.3      |
| test/steps                     | 82400      |
| train/episodes                 | 8240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00262   |
| train/info_shaping_reward_mean | -0.0566    |
| train/info_shaping_reward_min  | -0.237     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 329600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.44440946  |
| stats_o/std                    | 0.028613374 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.672       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000503   |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.77352    |
| test/Q_plus_P                  | -1.77352    |
| test/reward_per_eps            | -13.1       |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 207         |
| stats_o/mean                   | 0.44440857  |
| stats_o/std                    | 0.028608471 |
| test/episodes                  | 2080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00146    |
| test/info_shaping_reward_mean  | -0.0414     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.2841145  |
| test/Q_plus_P                  | -1.2841145  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 83200       |
| train/episodes                 | 8320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 332800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.44440117  |
| stats_o/std                    | 0.028610043 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00302    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.5513593  |
| test/Q_plus_P                  | -1.5513593  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00276    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.44440007  |
| stats_o/std                    | 0.028609112 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000648   |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.4447562  |
| test/Q_plus_P                  | -1.4447562  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.44439784  |
| stats_o/std                    | 0.028599903 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.4382838  |
| test/Q_plus_P                  | -1.4382838  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00277    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.44439554 |
| stats_o/std                    | 0.02859467 |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00862   |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -1.3234972 |
| test/Q_plus_P                  | -1.3234972 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00258   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.4443915   |
| stats_o/std                    | 0.028583797 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00296    |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.5833025  |
| test/Q_plus_P                  | -1.5833025  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.44438875  |
| stats_o/std                    | 0.028577588 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00522    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.4561733  |
| test/Q_plus_P                  | -1.4561733  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.44438615  |
| stats_o/std                    | 0.028564824 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00197    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.6612983  |
| test/Q_plus_P                  | -1.6612983  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00368    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 215        |
| stats_o/mean                   | 0.44438615 |
| stats_o/std                    | 0.02855937 |
| test/episodes                  | 2160       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.21      |
| test/Q                         | -1.5375737 |
| test/Q_plus_P                  | -1.5375737 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 86400      |
| train/episodes                 | 8640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.591      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00285   |
| train/info_shaping_reward_mean | -0.0615    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.4      |
| train/steps                    | 345600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.44438037  |
| stats_o/std                    | 0.028560618 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000664   |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.6554358  |
| test/Q_plus_P                  | -1.6554358  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.4443754  |
| stats_o/std                    | 0.0285554  |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.767      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00347   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -1.3150933 |
| test/Q_plus_P                  | -1.3150933 |
| test/reward_per_eps            | -9.3       |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00282   |
| train/info_shaping_reward_mean | -0.0581    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 218        |
| stats_o/mean                   | 0.44436947 |
| stats_o/std                    | 0.02855484 |
| test/episodes                  | 2190       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00175   |
| test/info_shaping_reward_mean  | -0.0398    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.3316774 |
| test/Q_plus_P                  | -1.3316774 |
| test/reward_per_eps            | -9         |
| test/steps                     | 87600      |
| train/episodes                 | 8760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00304   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 350400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.44436452  |
| stats_o/std                    | 0.028546846 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00158    |
| test/info_shaping_reward_mean  | -0.0406     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.2806735  |
| test/Q_plus_P                  | -1.2806735  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.44435903  |
| stats_o/std                    | 0.028539328 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.423237   |
| test/Q_plus_P                  | -1.423237   |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.44435927  |
| stats_o/std                    | 0.028526353 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00153    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.6962947  |
| test/Q_plus_P                  | -1.6962947  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.44435766 |
| stats_o/std                    | 0.02851575 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.755      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00251   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -1.368698  |
| test/Q_plus_P                  | -1.368698  |
| test/reward_per_eps            | -9.8       |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00333   |
| train/info_shaping_reward_mean | -0.0589    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.44435236  |
| stats_o/std                    | 0.028507955 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00162    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.3056576  |
| test/Q_plus_P                  | -1.3056576  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0535     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.4443513   |
| stats_o/std                    | 0.028494382 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00436    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.96761954 |
| test/Q_plus_P                  | -0.96761954 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00375    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.44434604  |
| stats_o/std                    | 0.028484995 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00466    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.2210771  |
| test/Q_plus_P                  | -1.2210771  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0536     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.44434062  |
| stats_o/std                    | 0.028486282 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.715       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00231    |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.4652069  |
| test/Q_plus_P                  | -1.4652069  |
| test/reward_per_eps            | -11.4       |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00268    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.444338    |
| stats_o/std                    | 0.028485065 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000921   |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.6101925  |
| test/Q_plus_P                  | -1.6101925  |
| test/reward_per_eps            | -12         |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00273    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.4443356  |
| stats_o/std                    | 0.02847998 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.698      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00242   |
| test/info_shaping_reward_mean  | -0.0469    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -1.6887255 |
| test/Q_plus_P                  | -1.6887255 |
| test/reward_per_eps            | -12.1      |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.658      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00272   |
| train/info_shaping_reward_mean | -0.0581    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.7      |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.4443352   |
| stats_o/std                    | 0.028475888 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.69        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00122    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.7670662  |
| test/Q_plus_P                  | -1.7670662  |
| test/reward_per_eps            | -12.4       |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.44433057  |
| stats_o/std                    | 0.028468238 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0032     |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.4986835  |
| test/Q_plus_P                  | -1.4986835  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.44432747  |
| stats_o/std                    | 0.028458001 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.5144926  |
| test/Q_plus_P                  | -1.5144926  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.44432425  |
| stats_o/std                    | 0.028452607 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00312    |
| test/info_shaping_reward_mean  | -0.0401     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.2983209  |
| test/Q_plus_P                  | -1.2983209  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.44432175  |
| stats_o/std                    | 0.028447853 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00154    |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.3165191  |
| test/Q_plus_P                  | -1.3165191  |
| test/reward_per_eps            | -10         |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 234         |
| stats_o/mean                   | 0.44431958  |
| stats_o/std                    | 0.028441587 |
| test/episodes                  | 2350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00204    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.3754376  |
| test/Q_plus_P                  | -1.3754376  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 94000       |
| train/episodes                 | 9400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 376000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.4443172   |
| stats_o/std                    | 0.028431872 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.4024515  |
| test/Q_plus_P                  | -1.4024515  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0019     |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.44431624  |
| stats_o/std                    | 0.028426908 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00302    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.0866773  |
| test/Q_plus_P                  | -1.0866773  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00234    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.4443151   |
| stats_o/std                    | 0.028426008 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000474   |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.3784032  |
| test/Q_plus_P                  | -1.3784032  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00304    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.44431177  |
| stats_o/std                    | 0.028416952 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.3011491  |
| test/Q_plus_P                  | -1.3011491  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00214    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 239        |
| stats_o/mean                   | 0.4443114  |
| stats_o/std                    | 0.02840704 |
| test/episodes                  | 2400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00248   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -1.7522382 |
| test/Q_plus_P                  | -1.7522382 |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 96000      |
| train/episodes                 | 9600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0032    |
| train/info_shaping_reward_mean | -0.0559    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 384000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.444308    |
| stats_o/std                    | 0.028397014 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00338    |
| test/info_shaping_reward_mean  | -0.0472     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.485655   |
| test/Q_plus_P                  | -1.485655   |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.44430342 |
| stats_o/std                    | 0.0283928  |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00224   |
| test/info_shaping_reward_mean  | -0.0479    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -1.5136982 |
| test/Q_plus_P                  | -1.5136982 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00365   |
| train/info_shaping_reward_mean | -0.06      |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.44430557  |
| stats_o/std                    | 0.028385691 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.1895252  |
| test/Q_plus_P                  | -1.1895252  |
| test/reward_per_eps            | -9          |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.44430256  |
| stats_o/std                    | 0.028380813 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00192    |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.3225206  |
| test/Q_plus_P                  | -1.3225206  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00344    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.44430315  |
| stats_o/std                    | 0.028371865 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00366    |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.4983402  |
| test/Q_plus_P                  | -1.4983402  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.44429946  |
| stats_o/std                    | 0.028369239 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.277754   |
| test/Q_plus_P                  | -1.277754   |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.4442954   |
| stats_o/std                    | 0.028365506 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00473    |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.3275101  |
| test/Q_plus_P                  | -1.3275101  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00252    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.44429645  |
| stats_o/std                    | 0.028358096 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00277    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.1052582  |
| test/Q_plus_P                  | -1.1052582  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.44429135  |
| stats_o/std                    | 0.028358512 |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000606   |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.2061249  |
| test/Q_plus_P                  | -1.2061249  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00282    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.44429     |
| stats_o/std                    | 0.028350106 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.99398917 |
| test/Q_plus_P                  | -0.99398917 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.44428742  |
| stats_o/std                    | 0.028340632 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0046     |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.9163382  |
| test/Q_plus_P                  | -0.9163382  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.4442881   |
| stats_o/std                    | 0.028331531 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00159    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.2707638  |
| test/Q_plus_P                  | -1.2707638  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.44428352  |
| stats_o/std                    | 0.028326184 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0386     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.9923429  |
| test/Q_plus_P                  | -0.9923429  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00286    |
| train/info_shaping_reward_mean | -0.0537     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.44428146  |
| stats_o/std                    | 0.028320938 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00413    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.1931503  |
| test/Q_plus_P                  | -1.1931503  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00415    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.4442831   |
| stats_o/std                    | 0.028318532 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00192    |
| test/info_shaping_reward_mean  | -0.0514     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.3568795  |
| test/Q_plus_P                  | -1.3568795  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.44427732  |
| stats_o/std                    | 0.028313158 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00389    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.2249392  |
| test/Q_plus_P                  | -1.2249392  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0531     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.4442767   |
| stats_o/std                    | 0.028304286 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00259    |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.3376927  |
| test/Q_plus_P                  | -1.3376927  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0023     |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.44427693  |
| stats_o/std                    | 0.028301762 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00411    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.1396668  |
| test/Q_plus_P                  | -1.1396668  |
| test/reward_per_eps            | -10         |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.44427168  |
| stats_o/std                    | 0.028298205 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00136    |
| test/info_shaping_reward_mean  | -0.0398     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.0054808  |
| test/Q_plus_P                  | -1.0054808  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00246    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.44426572 |
| stats_o/std                    | 0.02829122 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.002     |
| test/info_shaping_reward_mean  | -0.0485    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.4238803 |
| test/Q_plus_P                  | -1.4238803 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.678      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00292   |
| train/info_shaping_reward_mean | -0.0554    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.44426572  |
| stats_o/std                    | 0.028281411 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00137    |
| test/info_shaping_reward_mean  | -0.0402     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.0191758  |
| test/Q_plus_P                  | -1.0191758  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.44426215  |
| stats_o/std                    | 0.028278692 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00073    |
| test/info_shaping_reward_mean  | -0.0368     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.0989747  |
| test/Q_plus_P                  | -1.0989747  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00291    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.44426373  |
| stats_o/std                    | 0.028273968 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00245    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -1.4095426  |
| test/Q_plus_P                  | -1.4095426  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00292    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.44426212  |
| stats_o/std                    | 0.028268864 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.2302492  |
| test/Q_plus_P                  | -1.2302492  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00359    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.44426098  |
| stats_o/std                    | 0.028262252 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00256    |
| test/info_shaping_reward_mean  | -0.04       |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.179202   |
| test/Q_plus_P                  | -1.179202   |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.44425985  |
| stats_o/std                    | 0.028261296 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.277927   |
| test/Q_plus_P                  | -1.277927   |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.44425723  |
| stats_o/std                    | 0.028253287 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00437    |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.4026425  |
| test/Q_plus_P                  | -1.4026425  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00289    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.44425014  |
| stats_o/std                    | 0.028262394 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00234    |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -1.2202028  |
| test/Q_plus_P                  | -1.2202028  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0032     |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.4442481   |
| stats_o/std                    | 0.028251886 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.5475007  |
| test/Q_plus_P                  | -1.5475007  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.724       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00305    |
| train/info_shaping_reward_mean | -0.0512     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11         |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.44424495  |
| stats_o/std                    | 0.028246857 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00191    |
| test/info_shaping_reward_mean  | -0.0419     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.0448931  |
| test/Q_plus_P                  | -1.0448931  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00396    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.44424298  |
| stats_o/std                    | 0.028240524 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00489    |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.3536403  |
| test/Q_plus_P                  | -1.3536403  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00258    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.44424     |
| stats_o/std                    | 0.028232941 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.4597676  |
| test/Q_plus_P                  | -1.4597676  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00287    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.44423547  |
| stats_o/std                    | 0.028227353 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.1640553  |
| test/Q_plus_P                  | -1.1640553  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0557     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.44423434 |
| stats_o/std                    | 0.02822086 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00222   |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -1.3528377 |
| test/Q_plus_P                  | -1.3528377 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00434   |
| train/info_shaping_reward_mean | -0.0596    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.44423223 |
| stats_o/std                    | 0.02821649 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00124   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.2110213 |
| test/Q_plus_P                  | -1.2110213 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00284   |
| train/info_shaping_reward_mean | -0.0579    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.4442316   |
| stats_o/std                    | 0.028211733 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00465    |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.2281091  |
| test/Q_plus_P                  | -1.2281091  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.44422832  |
| stats_o/std                    | 0.028211387 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.698       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00232    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.6175361  |
| test/Q_plus_P                  | -1.6175361  |
| test/reward_per_eps            | -12.1       |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.602       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.9       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.44422722  |
| stats_o/std                    | 0.028211303 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00175    |
| test/info_shaping_reward_mean  | -0.0445     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.3786521  |
| test/Q_plus_P                  | -1.3786521  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00479    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.44422784  |
| stats_o/std                    | 0.028202817 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00203    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.1392592  |
| test/Q_plus_P                  | -1.1392592  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.44422507  |
| stats_o/std                    | 0.028203556 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00192    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -1.1711805  |
| test/Q_plus_P                  | -1.1711805  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00271    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.44422302  |
| stats_o/std                    | 0.028195722 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.1607784  |
| test/Q_plus_P                  | -1.1607784  |
| test/reward_per_eps            | -9          |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.44421718  |
| stats_o/std                    | 0.028197162 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00833    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.207529   |
| test/Q_plus_P                  | -1.207529   |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00269    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.444216    |
| stats_o/std                    | 0.028189138 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00445    |
| test/info_shaping_reward_mean  | -0.0447     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.99851125 |
| test/Q_plus_P                  | -0.99851125 |
| test/reward_per_eps            | -8          |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00283    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.4442161   |
| stats_o/std                    | 0.028185861 |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00596    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0880313  |
| test/Q_plus_P                  | -1.0880313  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.4442136   |
| stats_o/std                    | 0.028181216 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00389    |
| test/info_shaping_reward_mean  | -0.044      |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -1.0995756  |
| test/Q_plus_P                  | -1.0995756  |
| test/reward_per_eps            | -9          |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00266    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.4442123   |
| stats_o/std                    | 0.028177008 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00199    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.9726445  |
| test/Q_plus_P                  | -0.9726445  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00267    |
| train/info_shaping_reward_mean | -0.0533     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.44421172 |
| stats_o/std                    | 0.02817198 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00174   |
| test/info_shaping_reward_mean  | -0.0429    |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -1.0172778 |
| test/Q_plus_P                  | -1.0172778 |
| test/reward_per_eps            | -8         |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.713      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00247   |
| train/info_shaping_reward_mean | -0.0524    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.5      |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.4442083  |
| stats_o/std                    | 0.0281721  |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00235   |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -1.1322546 |
| test/Q_plus_P                  | -1.1322546 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00303   |
| train/info_shaping_reward_mean | -0.0582    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.4442104   |
| stats_o/std                    | 0.028162017 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00429    |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.3818653  |
| test/Q_plus_P                  | -1.3818653  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00324    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.44420722  |
| stats_o/std                    | 0.028161176 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00462    |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.421795   |
| test/Q_plus_P                  | -1.421795   |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00483    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 290        |
| stats_o/mean                   | 0.4442073  |
| stats_o/std                    | 0.02815782 |
| test/episodes                  | 2910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.703      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00259   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -1.612187  |
| test/Q_plus_P                  | -1.612187  |
| test/reward_per_eps            | -11.9      |
| test/steps                     | 116400     |
| train/episodes                 | 11640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.671      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00313   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 465600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.44420823  |
| stats_o/std                    | 0.028151562 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000678   |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.2954402  |
| test/Q_plus_P                  | -1.2954402  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.44420674  |
| stats_o/std                    | 0.028143646 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00114    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.1242098  |
| test/Q_plus_P                  | -1.1242098  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00264    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.44420698  |
| stats_o/std                    | 0.028140785 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00724    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.2509291  |
| test/Q_plus_P                  | -1.2509291  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00338    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 294        |
| stats_o/mean                   | 0.4442016  |
| stats_o/std                    | 0.02813518 |
| test/episodes                  | 2950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00401   |
| test/info_shaping_reward_mean  | -0.0529    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -1.3448454 |
| test/Q_plus_P                  | -1.3448454 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 118000     |
| train/episodes                 | 11800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.706      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00301   |
| train/info_shaping_reward_mean | -0.0538    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.8      |
| train/steps                    | 472000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.444201    |
| stats_o/std                    | 0.028131938 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00616    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -1.1989785  |
| test/Q_plus_P                  | -1.1989785  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.44420254  |
| stats_o/std                    | 0.028123176 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00448    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.1873231  |
| test/Q_plus_P                  | -1.1873231  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.44420114 |
| stats_o/std                    | 0.02811917 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000882  |
| test/info_shaping_reward_mean  | -0.0503    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.4053699 |
| test/Q_plus_P                  | -1.4053699 |
| test/reward_per_eps            | -11        |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00334   |
| train/info_shaping_reward_mean | -0.0597    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.4442017   |
| stats_o/std                    | 0.028115466 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.012      |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.0212474  |
| test/Q_plus_P                  | -1.0212474  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.4442043   |
| stats_o/std                    | 0.028112581 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.627       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00414    |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.8078241  |
| test/Q_plus_P                  | -1.8078241  |
| test/reward_per_eps            | -14.9       |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.594       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00371    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.2       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.44420433  |
| stats_o/std                    | 0.028116532 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.5666106  |
| test/Q_plus_P                  | -1.5666106  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.4442041   |
| stats_o/std                    | 0.028111145 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00296    |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -1.3312428  |
| test/Q_plus_P                  | -1.3312428  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.44420302  |
| stats_o/std                    | 0.028106956 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0105     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.4322017  |
| test/Q_plus_P                  | -1.4322017  |
| test/reward_per_eps            | -11         |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00326    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.44420472 |
| stats_o/std                    | 0.0280993  |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.733      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00285   |
| test/info_shaping_reward_mean  | -0.0442    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -1.4021161 |
| test/Q_plus_P                  | -1.4021161 |
| test/reward_per_eps            | -10.7      |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00427   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.44420385  |
| stats_o/std                    | 0.028098518 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00411    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.1966136  |
| test/Q_plus_P                  | -1.1966136  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.44420195  |
| stats_o/std                    | 0.028093278 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0037     |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.217051   |
| test/Q_plus_P                  | -1.217051   |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00237    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.44420072  |
| stats_o/std                    | 0.028083853 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00422    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.98418754 |
| test/Q_plus_P                  | -0.98418754 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.44420195 |
| stats_o/std                    | 0.02808246 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00156   |
| test/info_shaping_reward_mean  | -0.0449    |
| test/info_shaping_reward_min   | -0.233     |
| test/Q                         | -1.0982894 |
| test/Q_plus_P                  | -1.0982894 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00329   |
| train/info_shaping_reward_mean | -0.0581    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.4441987   |
| stats_o/std                    | 0.028073302 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00531    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.2836956  |
| test/Q_plus_P                  | -1.2836956  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.44419944  |
| stats_o/std                    | 0.028064685 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00378    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0272205  |
| test/Q_plus_P                  | -1.0272205  |
| test/reward_per_eps            | -8          |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.44419882  |
| stats_o/std                    | 0.028063169 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00183    |
| test/info_shaping_reward_mean  | -0.0575     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.5946242  |
| test/Q_plus_P                  | -1.5946242  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.44419527 |
| stats_o/std                    | 0.02805812 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.708      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00143   |
| test/info_shaping_reward_mean  | -0.0581    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -1.5447628 |
| test/Q_plus_P                  | -1.5447628 |
| test/reward_per_eps            | -11.7      |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.699      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00376   |
| train/info_shaping_reward_mean | -0.0581    |
| train/info_shaping_reward_min  | -0.269     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12        |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.44419274  |
| stats_o/std                    | 0.028060714 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.2874544  |
| test/Q_plus_P                  | -1.2874544  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.44419268  |
| stats_o/std                    | 0.028054526 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0014     |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.1608472  |
| test/Q_plus_P                  | -1.1608472  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00318    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.44419184  |
| stats_o/std                    | 0.028048225 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00378    |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.055522   |
| test/Q_plus_P                  | -1.055522   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.4441916   |
| stats_o/std                    | 0.028045475 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00365    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.2651242  |
| test/Q_plus_P                  | -1.2651242  |
| test/reward_per_eps            | -10         |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00363    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.44418788  |
| stats_o/std                    | 0.028047016 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00289    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.2507485  |
| test/Q_plus_P                  | -1.2507485  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.44418505  |
| stats_o/std                    | 0.028044596 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00382    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.2380182  |
| test/Q_plus_P                  | -1.2380182  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00432    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.44418374  |
| stats_o/std                    | 0.028040325 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -1.1199135  |
| test/Q_plus_P                  | -1.1199135  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.44418502  |
| stats_o/std                    | 0.028040653 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00255    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.3165598  |
| test/Q_plus_P                  | -1.3165598  |
| test/reward_per_eps            | -10         |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.44418493  |
| stats_o/std                    | 0.028039468 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0021     |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.2056806  |
| test/Q_plus_P                  | -1.2056806  |
| test/reward_per_eps            | -10         |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.44418502  |
| stats_o/std                    | 0.028034708 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00683    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.0947447  |
| test/Q_plus_P                  | -1.0947447  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.44418576  |
| stats_o/std                    | 0.028028563 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00393    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.0283365  |
| test/Q_plus_P                  | -1.0283365  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.4441847  |
| stats_o/std                    | 0.02802632 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.743      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00295   |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.3046614 |
| test/Q_plus_P                  | -1.3046614 |
| test/reward_per_eps            | -10.3      |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00379   |
| train/info_shaping_reward_mean | -0.061     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.44418427  |
| stats_o/std                    | 0.028021453 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0034     |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -1.3334913  |
| test/Q_plus_P                  | -1.3334913  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.4441831   |
| stats_o/std                    | 0.028019024 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00752    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.9970893  |
| test/Q_plus_P                  | -0.9970893  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.44418237  |
| stats_o/std                    | 0.028010616 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.71        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00257    |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.5550635  |
| test/Q_plus_P                  | -1.5550635  |
| test/reward_per_eps            | -11.6       |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00334    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.4441788  |
| stats_o/std                    | 0.02800662 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.805      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00189   |
| test/info_shaping_reward_mean  | -0.0392    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -1.013188  |
| test/Q_plus_P                  | -1.013188  |
| test/reward_per_eps            | -7.8       |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.703      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00232   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.4441787   |
| stats_o/std                    | 0.028000405 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00176    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.1481502  |
| test/Q_plus_P                  | -1.1481502  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 329         |
| stats_o/mean                   | 0.4441788   |
| stats_o/std                    | 0.027996695 |
| test/episodes                  | 3300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00542    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.4513937  |
| test/Q_plus_P                  | -1.4513937  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 132000      |
| train/episodes                 | 13200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00257    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 528000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.4441782   |
| stats_o/std                    | 0.027994001 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00129    |
| test/info_shaping_reward_mean  | -0.0397     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.0430447  |
| test/Q_plus_P                  | -1.0430447  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.44417667  |
| stats_o/std                    | 0.027991513 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00214    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.0553284  |
| test/Q_plus_P                  | -1.0553284  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.44417822  |
| stats_o/std                    | 0.027986137 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.87533    |
| test/Q_plus_P                  | -0.87533    |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.44417867  |
| stats_o/std                    | 0.027987199 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00168    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.0312587  |
| test/Q_plus_P                  | -1.0312587  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00393    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 334        |
| stats_o/mean                   | 0.44417724 |
| stats_o/std                    | 0.0279792  |
| test/episodes                  | 3350       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00128   |
| test/info_shaping_reward_mean  | -0.0424    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.0539379 |
| test/Q_plus_P                  | -1.0539379 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 134000     |
| train/episodes                 | 13400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00262   |
| train/info_shaping_reward_mean | -0.0537    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 536000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.44417724 |
| stats_o/std                    | 0.02797813 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00187   |
| test/info_shaping_reward_mean  | -0.0399    |
| test/info_shaping_reward_min   | -0.217     |
| test/Q                         | -1.0614238 |
| test/Q_plus_P                  | -1.0614238 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.665      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00336   |
| train/info_shaping_reward_mean | -0.0601    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.4441756   |
| stats_o/std                    | 0.027975097 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000894   |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -1.1669555  |
| test/Q_plus_P                  | -1.1669555  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.44417536  |
| stats_o/std                    | 0.027969077 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000693   |
| test/info_shaping_reward_mean  | -0.0396     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0304499  |
| test/Q_plus_P                  | -1.0304499  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0041     |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.44417346  |
| stats_o/std                    | 0.027969211 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00302    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.3637357  |
| test/Q_plus_P                  | -1.3637357  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00385    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.44417056  |
| stats_o/std                    | 0.027967507 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.2097751  |
| test/Q_plus_P                  | -1.2097751  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00309    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.44416794  |
| stats_o/std                    | 0.027963424 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00272    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.4199386  |
| test/Q_plus_P                  | -1.4199386  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.4441687   |
| stats_o/std                    | 0.027956352 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.1996013  |
| test/Q_plus_P                  | -1.1996013  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.44416642  |
| stats_o/std                    | 0.027954511 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000946   |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.4122224  |
| test/Q_plus_P                  | -1.4122224  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0029     |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.44416425  |
| stats_o/std                    | 0.027948974 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00399    |
| test/info_shaping_reward_mean  | -0.0584     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -1.6337324  |
| test/Q_plus_P                  | -1.6337324  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00426    |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.44416413  |
| stats_o/std                    | 0.027945906 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00738    |
| test/info_shaping_reward_mean  | -0.0432     |
| test/info_shaping_reward_min   | -0.212      |
| test/Q                         | -1.1422377  |
| test/Q_plus_P                  | -1.1422377  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00332    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.44416502  |
| stats_o/std                    | 0.027938897 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.4187889  |
| test/Q_plus_P                  | -1.4187889  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.4441645   |
| stats_o/std                    | 0.027933491 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00193    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.9358637  |
| test/Q_plus_P                  | -0.9358637  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.4441636   |
| stats_o/std                    | 0.027929455 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00443    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -1.3919961  |
| test/Q_plus_P                  | -1.3919961  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.4441612   |
| stats_o/std                    | 0.027925802 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00135    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.2655405  |
| test/Q_plus_P                  | -1.2655405  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.4441614   |
| stats_o/std                    | 0.027920177 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.0839853  |
| test/Q_plus_P                  | -1.0839853  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.44416162  |
| stats_o/std                    | 0.027914168 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00468    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.0394973  |
| test/Q_plus_P                  | -1.0394973  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.44416332  |
| stats_o/std                    | 0.027908199 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00468    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.1218443  |
| test/Q_plus_P                  | -1.1218443  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00404    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.44416186  |
| stats_o/std                    | 0.027901158 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.1380373  |
| test/Q_plus_P                  | -1.1380373  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00353    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 353        |
| stats_o/mean                   | 0.44416344 |
| stats_o/std                    | 0.02789285 |
| test/episodes                  | 3540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00169   |
| test/info_shaping_reward_mean  | -0.0483    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -1.0085385 |
| test/Q_plus_P                  | -1.0085385 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 141600     |
| train/episodes                 | 14160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.704      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00428   |
| train/info_shaping_reward_mean | -0.0561    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.8      |
| train/steps                    | 566400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.44416392  |
| stats_o/std                    | 0.027888231 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00313    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.2898207  |
| test/Q_plus_P                  | -1.2898207  |
| test/reward_per_eps            | -10         |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.44416022  |
| stats_o/std                    | 0.027885886 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00133    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.2158291  |
| test/Q_plus_P                  | -1.2158291  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00299    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.44415972  |
| stats_o/std                    | 0.027882429 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.745       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00315    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.3589877  |
| test/Q_plus_P                  | -1.3589877  |
| test/reward_per_eps            | -10.2       |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.44415894  |
| stats_o/std                    | 0.027879871 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00261    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.0765077  |
| test/Q_plus_P                  | -1.0765077  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0031     |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.44416103  |
| stats_o/std                    | 0.027877046 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00172    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.9682222  |
| test/Q_plus_P                  | -0.9682222  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00418    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.44416177  |
| stats_o/std                    | 0.027869286 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00222    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.4132957  |
| test/Q_plus_P                  | -1.4132957  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.056      |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.44415927  |
| stats_o/std                    | 0.027868614 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00104    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.96253884 |
| test/Q_plus_P                  | -0.96253884 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.44415638  |
| stats_o/std                    | 0.027864521 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00189    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.2220246  |
| test/Q_plus_P                  | -1.2220246  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00315    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.44415772  |
| stats_o/std                    | 0.027864704 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00389    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.3630509  |
| test/Q_plus_P                  | -1.3630509  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.614       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.44415796  |
| stats_o/std                    | 0.027862353 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.725       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00291    |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.2867776  |
| test/Q_plus_P                  | -1.2867776  |
| test/reward_per_eps            | -11         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00384    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.44415757  |
| stats_o/std                    | 0.027861642 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000874   |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.0375054  |
| test/Q_plus_P                  | -1.0375054  |
| test/reward_per_eps            | -9          |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.44415712  |
| stats_o/std                    | 0.027859561 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00196    |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.0781139  |
| test/Q_plus_P                  | -1.0781139  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00307    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.44415495  |
| stats_o/std                    | 0.027857928 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00198    |
| test/info_shaping_reward_mean  | -0.0448     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.9082378  |
| test/Q_plus_P                  | -0.9082378  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.44415346  |
| stats_o/std                    | 0.027857535 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00436    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -1.2551203  |
| test/Q_plus_P                  | -1.2551203  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00346    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.4441539   |
| stats_o/std                    | 0.027850688 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000911   |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.2321264  |
| test/Q_plus_P                  | -1.2321264  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00349    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.4441532   |
| stats_o/std                    | 0.027850276 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0015     |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.8844474  |
| test/Q_plus_P                  | -0.8844474  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00321    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.44415084  |
| stats_o/std                    | 0.027846465 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00338    |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.991002   |
| test/Q_plus_P                  | -0.991002   |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00279    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.44415116  |
| stats_o/std                    | 0.027843555 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00667    |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.93064743 |
| test/Q_plus_P                  | -0.93064743 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00466    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.4441493   |
| stats_o/std                    | 0.027843878 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00211    |
| test/info_shaping_reward_mean  | -0.0439     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.0944773  |
| test/Q_plus_P                  | -1.0944773  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0035     |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.44414654 |
| stats_o/std                    | 0.02784203 |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00144   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -1.2561412 |
| test/Q_plus_P                  | -1.2561412 |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00448   |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.444147   |
| stats_o/std                    | 0.02783931 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.735      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000752  |
| test/info_shaping_reward_mean  | -0.0491    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.3334655 |
| test/Q_plus_P                  | -1.3334655 |
| test/reward_per_eps            | -10.6      |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.655      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0578    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.4441478   |
| stats_o/std                    | 0.027833862 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000863   |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.2618499  |
| test/Q_plus_P                  | -1.2618499  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.44414365  |
| stats_o/std                    | 0.027831942 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0013     |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.1732084  |
| test/Q_plus_P                  | -1.1732084  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.44414166  |
| stats_o/std                    | 0.027830632 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.0453     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.3476447  |
| test/Q_plus_P                  | -1.3476447  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00372    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.44414398 |
| stats_o/std                    | 0.02783034 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.74       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00179   |
| test/info_shaping_reward_mean  | -0.046     |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.2806109 |
| test/Q_plus_P                  | -1.2806109 |
| test/reward_per_eps            | -10.4      |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00338   |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.44414458  |
| stats_o/std                    | 0.027827198 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00143    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.9674224  |
| test/Q_plus_P                  | -0.9674224  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00347    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.44414476  |
| stats_o/std                    | 0.027822955 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00309    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.265826   |
| test/Q_plus_P                  | -1.265826   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00239    |
| train/info_shaping_reward_mean | -0.0566     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.4441419   |
| stats_o/std                    | 0.027821267 |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00547    |
| test/info_shaping_reward_mean  | -0.0514     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.4279801  |
| test/Q_plus_P                  | -1.4279801  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00243    |
| train/info_shaping_reward_mean | -0.0542     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 382         |
| stats_o/mean                   | 0.44414008  |
| stats_o/std                    | 0.027820274 |
| test/episodes                  | 3830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00169    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -1.2789502  |
| test/Q_plus_P                  | -1.2789502  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 153200      |
| train/episodes                 | 15320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00351    |
| train/info_shaping_reward_mean | -0.057      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 612800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.44414222  |
| stats_o/std                    | 0.027813816 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.718       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00286    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.5236039  |
| test/Q_plus_P                  | -1.5236039  |
| test/reward_per_eps            | -11.3       |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0545     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.44414058  |
| stats_o/std                    | 0.027814165 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000842   |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.234662   |
| test/Q_plus_P                  | -1.234662   |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00434    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.4441394   |
| stats_o/std                    | 0.027812956 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00195    |
| test/info_shaping_reward_mean  | -0.0438     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.0379362  |
| test/Q_plus_P                  | -1.0379362  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00395    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.4441376   |
| stats_o/std                    | 0.027812898 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00188    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -1.2478893  |
| test/Q_plus_P                  | -1.2478893  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00424    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.44413555  |
| stats_o/std                    | 0.027807841 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00252    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.1728702  |
| test/Q_plus_P                  | -1.1728702  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.717       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.052      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.44413504  |
| stats_o/std                    | 0.02780331  |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0031     |
| test/info_shaping_reward_mean  | -0.039      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.89852554 |
| test/Q_plus_P                  | -0.89852554 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00386    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.44413617  |
| stats_o/std                    | 0.027799854 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00332    |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.9988313  |
| test/Q_plus_P                  | -0.9988313  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.627       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00341    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.44413555 |
| stats_o/std                    | 0.02779953 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.71       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00305   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -1.4817348 |
| test/Q_plus_P                  | -1.4817348 |
| test/reward_per_eps            | -11.6      |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00356   |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.44413462 |
| stats_o/std                    | 0.02779383 |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000584  |
| test/info_shaping_reward_mean  | -0.0465    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -1.2344851 |
| test/Q_plus_P                  | -1.2344851 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.715      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00293   |
| train/info_shaping_reward_mean | -0.0543    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.4441327   |
| stats_o/std                    | 0.027792213 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00717    |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.2303064  |
| test/Q_plus_P                  | -1.2303064  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.44413233  |
| stats_o/std                    | 0.027786398 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00237    |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.1145799  |
| test/Q_plus_P                  | -1.1145799  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.003      |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.44412962  |
| stats_o/std                    | 0.027786238 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0048     |
| test/info_shaping_reward_mean  | -0.0441     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.023893   |
| test/Q_plus_P                  | -1.023893   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00356    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.44413114  |
| stats_o/std                    | 0.027782977 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.9679851  |
| test/Q_plus_P                  | -0.9679851  |
| test/reward_per_eps            | -8          |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.721       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00364    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.44413128  |
| stats_o/std                    | 0.027780117 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000979   |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.1993744  |
| test/Q_plus_P                  | -1.1993744  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00407    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.44413033 |
| stats_o/std                    | 0.02777519 |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.772      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00232   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -1.0625225 |
| test/Q_plus_P                  | -1.0625225 |
| test/reward_per_eps            | -9.1       |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.7        |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.0573    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12        |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.44413158  |
| stats_o/std                    | 0.027773539 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00654    |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.4714617  |
| test/Q_plus_P                  | -1.4714617  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.44413444  |
| stats_o/std                    | 0.027769513 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00392    |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.9031388  |
| test/Q_plus_P                  | -0.9031388  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00491    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.44413432  |
| stats_o/std                    | 0.027765067 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.757       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00347    |
| test/info_shaping_reward_mean  | -0.0475     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.2317351  |
| test/Q_plus_P                  | -1.2317351  |
| test/reward_per_eps            | -9.7        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.44413254  |
| stats_o/std                    | 0.027760878 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00594    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.9046219  |
| test/Q_plus_P                  | -0.9046219  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00491    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.4441335   |
| stats_o/std                    | 0.027756318 |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00682    |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.0736747  |
| test/Q_plus_P                  | -1.0736747  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00302    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.44413543  |
| stats_o/std                    | 0.027750907 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00125    |
| test/info_shaping_reward_mean  | -0.0421     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.0030015  |
| test/Q_plus_P                  | -1.0030015  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00379    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.4441359   |
| stats_o/std                    | 0.027746232 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00893    |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -1.4643745  |
| test/Q_plus_P                  | -1.4643745  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00361    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.44413552  |
| stats_o/std                    | 0.027740253 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00599    |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -1.4153003  |
| test/Q_plus_P                  | -1.4153003  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 406        |
| stats_o/mean                   | 0.44413552 |
| stats_o/std                    | 0.02773921 |
| test/episodes                  | 4070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00379   |
| test/info_shaping_reward_mean  | -0.0431    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -1.0792483 |
| test/Q_plus_P                  | -1.0792483 |
| test/reward_per_eps            | -9         |
| test/steps                     | 162800     |
| train/episodes                 | 16280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00424   |
| train/info_shaping_reward_mean | -0.0585    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 651200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.44413295  |
| stats_o/std                    | 0.027736098 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00665    |
| test/info_shaping_reward_mean  | -0.0407     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.76801944 |
| test/Q_plus_P                  | -0.76801944 |
| test/reward_per_eps            | -7          |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00293    |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.44413146  |
| stats_o/std                    | 0.027732572 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0058     |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.3171594  |
| test/Q_plus_P                  | -1.3171594  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.4441286   |
| stats_o/std                    | 0.027733356 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00517    |
| test/info_shaping_reward_mean  | -0.0416     |
| test/info_shaping_reward_min   | -0.207      |
| test/Q                         | -0.79567635 |
| test/Q_plus_P                  | -0.79567635 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.4441272   |
| stats_o/std                    | 0.027730033 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00235    |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.1653981  |
| test/Q_plus_P                  | -1.1653981  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.705       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00329    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.4441258   |
| stats_o/std                    | 0.027726993 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00437    |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.91243386 |
| test/Q_plus_P                  | -0.91243386 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.44412577  |
| stats_o/std                    | 0.027724741 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0028     |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.9795325  |
| test/Q_plus_P                  | -0.9795325  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00369    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.4441265   |
| stats_o/std                    | 0.027722357 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00471    |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.9645577  |
| test/Q_plus_P                  | -0.9645577  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.713       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0036     |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.5       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.4441248   |
| stats_o/std                    | 0.027718985 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.9047303  |
| test/Q_plus_P                  | -0.9047303  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00377    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.4441255   |
| stats_o/std                    | 0.027713696 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00157    |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.1282705  |
| test/Q_plus_P                  | -1.1282705  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00357    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.444127    |
| stats_o/std                    | 0.027708052 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0012     |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.2616341  |
| test/Q_plus_P                  | -1.2616341  |
| test/reward_per_eps            | -10         |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00333    |
| train/info_shaping_reward_mean | -0.0556     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.4441272   |
| stats_o/std                    | 0.027704382 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00147    |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.3144133  |
| test/Q_plus_P                  | -1.3144133  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.44412962  |
| stats_o/std                    | 0.027699867 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000391   |
| test/info_shaping_reward_mean  | -0.041      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.0693073  |
| test/Q_plus_P                  | -1.0693073  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.44412994  |
| stats_o/std                    | 0.027698956 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00362    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.987523   |
| test/Q_plus_P                  | -0.987523   |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00396    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.44413137  |
| stats_o/std                    | 0.027694328 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00427    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.2984906  |
| test/Q_plus_P                  | -1.2984906  |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00482    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.4441323   |
| stats_o/std                    | 0.027691444 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00327    |
| test/info_shaping_reward_mean  | -0.0409     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.8819134  |
| test/Q_plus_P                  | -0.8819134  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.444133    |
| stats_o/std                    | 0.027688108 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00954    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.0361938  |
| test/Q_plus_P                  | -1.0361938  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00348    |
| train/info_shaping_reward_mean | -0.0562     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.44413137  |
| stats_o/std                    | 0.027688237 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00401    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.9354772  |
| test/Q_plus_P                  | -0.9354772  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.4441317   |
| stats_o/std                    | 0.027681917 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00342    |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.9530553  |
| test/Q_plus_P                  | -0.9530553  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.718       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.44413224  |
| stats_o/std                    | 0.027680561 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.006      |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7290879  |
| test/Q_plus_P                  | -0.7290879  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00367    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.44413373  |
| stats_o/std                    | 0.027680168 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00108    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.3649926  |
| test/Q_plus_P                  | -1.3649926  |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00409    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.44413555  |
| stats_o/std                    | 0.027678097 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000952   |
| test/info_shaping_reward_mean  | -0.0404     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.0773573  |
| test/Q_plus_P                  | -1.0773573  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00431    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.4441339   |
| stats_o/std                    | 0.027675392 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00121    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.2022268  |
| test/Q_plus_P                  | -1.2022268  |
| test/reward_per_eps            | -10         |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00354    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.44413307  |
| stats_o/std                    | 0.027673366 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.695       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00201    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -1.5134465  |
| test/Q_plus_P                  | -1.5134465  |
| test/reward_per_eps            | -12.2       |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.44413257  |
| stats_o/std                    | 0.027672617 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000979   |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.1259086  |
| test/Q_plus_P                  | -1.1259086  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00294    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.44412938  |
| stats_o/std                    | 0.027672209 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.73        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00796    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.3546414  |
| test/Q_plus_P                  | -1.3546414  |
| test/reward_per_eps            | -10.8       |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0037     |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.44412908  |
| stats_o/std                    | 0.027668998 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00367    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.9632268  |
| test/Q_plus_P                  | -0.9632268  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00391    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 433        |
| stats_o/mean                   | 0.44412994 |
| stats_o/std                    | 0.0276656  |
| test/episodes                  | 4340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.765      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00294   |
| test/info_shaping_reward_mean  | -0.0476    |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -1.1514766 |
| test/Q_plus_P                  | -1.1514766 |
| test/reward_per_eps            | -9.4       |
| test/steps                     | 173600     |
| train/episodes                 | 17360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00351   |
| train/info_shaping_reward_mean | -0.0577    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 694400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.44412962  |
| stats_o/std                    | 0.027667118 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00671    |
| test/info_shaping_reward_mean  | -0.0429     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.9722739  |
| test/Q_plus_P                  | -0.9722739  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.44413123  |
| stats_o/std                    | 0.027665116 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00278    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.3169408  |
| test/Q_plus_P                  | -1.3169408  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00388    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.44413292  |
| stats_o/std                    | 0.027663445 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00123    |
| test/info_shaping_reward_mean  | -0.0422     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.0575271  |
| test/Q_plus_P                  | -1.0575271  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00531    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.44413427  |
| stats_o/std                    | 0.027658468 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00314    |
| test/info_shaping_reward_mean  | -0.0469     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.0723214  |
| test/Q_plus_P                  | -1.0723214  |
| test/reward_per_eps            | -9          |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00265    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.44413197  |
| stats_o/std                    | 0.027659103 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00156    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.25458    |
| test/Q_plus_P                  | -1.25458    |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00339    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.44413182  |
| stats_o/std                    | 0.027658226 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00343    |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.202      |
| test/Q                         | -0.9560186  |
| test/Q_plus_P                  | -0.9560186  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00365    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.44412956  |
| stats_o/std                    | 0.027657153 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00408    |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.8400385  |
| test/Q_plus_P                  | -0.8400385  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00335    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.44412827  |
| stats_o/std                    | 0.027649805 |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00693    |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0506048  |
| test/Q_plus_P                  | -1.0506048  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0041     |
| train/info_shaping_reward_mean | -0.0538     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.44412908  |
| stats_o/std                    | 0.027648412 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00322    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.9537705  |
| test/Q_plus_P                  | -0.9537705  |
| test/reward_per_eps            | -8          |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.44413054  |
| stats_o/std                    | 0.027641095 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00246    |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0105497  |
| test/Q_plus_P                  | -1.0105497  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.44413114  |
| stats_o/std                    | 0.027637752 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000827   |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.0594026  |
| test/Q_plus_P                  | -1.0594026  |
| test/reward_per_eps            | -9          |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00317    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.4441326   |
| stats_o/std                    | 0.027636034 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00273    |
| test/info_shaping_reward_mean  | -0.0413     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.9578975  |
| test/Q_plus_P                  | -0.9578975  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 446        |
| stats_o/mean                   | 0.4441329  |
| stats_o/std                    | 0.02763144 |
| test/episodes                  | 4470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00655   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -1.2101169 |
| test/Q_plus_P                  | -1.2101169 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 178800     |
| train/episodes                 | 17880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00328   |
| train/info_shaping_reward_mean | -0.0555    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 715200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.44413435 |
| stats_o/std                    | 0.0276246  |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00874   |
| test/info_shaping_reward_mean  | -0.0516    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -0.9257721 |
| test/Q_plus_P                  | -0.9257721 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.715      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00507   |
| train/info_shaping_reward_mean | -0.0545    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.4      |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.4441357   |
| stats_o/std                    | 0.027623123 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00433    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.1308064  |
| test/Q_plus_P                  | -1.1308064  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00475    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.44413698  |
| stats_o/std                    | 0.027621284 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0043     |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.9610665  |
| test/Q_plus_P                  | -0.9610665  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00474    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.4441357  |
| stats_o/std                    | 0.02761875 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00297   |
| test/info_shaping_reward_mean  | -0.0462    |
| test/info_shaping_reward_min   | -0.249     |
| test/Q                         | -1.0852478 |
| test/Q_plus_P                  | -1.0852478 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00383   |
| train/info_shaping_reward_mean | -0.0582    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.44413617  |
| stats_o/std                    | 0.027611045 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00438    |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.3191336  |
| test/Q_plus_P                  | -1.3191336  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00622    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.444136    |
| stats_o/std                    | 0.027609967 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0041     |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.0314765  |
| test/Q_plus_P                  | -1.0314765  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.44413695 |
| stats_o/std                    | 0.02760609 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.795      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000268  |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -0.9184725 |
| test/Q_plus_P                  | -0.9184725 |
| test/reward_per_eps            | -8.2       |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.688      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0054    |
| train/info_shaping_reward_mean | -0.0584    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.5      |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.4441373  |
| stats_o/std                    | 0.02760274 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00325   |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -1.0981793 |
| test/Q_plus_P                  | -1.0981793 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.709      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00492   |
| train/info_shaping_reward_mean | -0.0567    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.6      |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.4441372   |
| stats_o/std                    | 0.027599646 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00141    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.8642429  |
| test/Q_plus_P                  | -0.8642429  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00313    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.44413614 |
| stats_o/std                    | 0.0275967  |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00105   |
| test/info_shaping_reward_mean  | -0.0482    |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -1.0240718 |
| test/Q_plus_P                  | -1.0240718 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00307   |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.44413576  |
| stats_o/std                    | 0.027595846 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00368    |
| test/info_shaping_reward_mean  | -0.0442     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.0372515  |
| test/Q_plus_P                  | -1.0372515  |
| test/reward_per_eps            | -9          |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.7         |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00327    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.4441339   |
| stats_o/std                    | 0.027594304 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00073    |
| test/info_shaping_reward_mean  | -0.0452     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -1.1281888  |
| test/Q_plus_P                  | -1.1281888  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00316    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.44413492  |
| stats_o/std                    | 0.027590336 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00271    |
| test/info_shaping_reward_mean  | -0.0369     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.68538564 |
| test/Q_plus_P                  | -0.68538564 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00337    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.44413486  |
| stats_o/std                    | 0.027590202 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00555    |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.9507218  |
| test/Q_plus_P                  | -0.9507218  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00383    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.44413686  |
| stats_o/std                    | 0.027584812 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0104     |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.9066284  |
| test/Q_plus_P                  | -0.9066284  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00338    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.44413838  |
| stats_o/std                    | 0.027579175 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00662    |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.1027435  |
| test/Q_plus_P                  | -1.1027435  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00355    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.44413754  |
| stats_o/std                    | 0.02757671  |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0148     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.92500246 |
| test/Q_plus_P                  | -0.92500246 |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00422    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 464        |
| stats_o/mean                   | 0.4441388  |
| stats_o/std                    | 0.02757194 |
| test/episodes                  | 4650       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00884   |
| test/info_shaping_reward_mean  | -0.0521    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -1.0211754 |
| test/Q_plus_P                  | -1.0211754 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 186000     |
| train/episodes                 | 18600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00487   |
| train/info_shaping_reward_mean | -0.0617    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 744000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.44413897  |
| stats_o/std                    | 0.02756849  |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00645    |
| test/info_shaping_reward_mean  | -0.0489     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.80918103 |
| test/Q_plus_P                  | -0.80918103 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.703       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.44414017  |
| stats_o/std                    | 0.027563736 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00573    |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.1479682  |
| test/Q_plus_P                  | -1.1479682  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 467         |
| stats_o/mean                   | 0.44413927  |
| stats_o/std                    | 0.027563231 |
| test/episodes                  | 4680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.705       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00338    |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.3581789  |
| test/Q_plus_P                  | -1.3581789  |
| test/reward_per_eps            | -11.8       |
| test/steps                     | 187200      |
| train/episodes                 | 18720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00596    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 748800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.44413996  |
| stats_o/std                    | 0.027559927 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00213    |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -1.3665193  |
| test/Q_plus_P                  | -1.3665193  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0569     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.44414032  |
| stats_o/std                    | 0.027560918 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0123     |
| test/info_shaping_reward_mean  | -0.0621     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.3240695  |
| test/Q_plus_P                  | -1.3240695  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.006      |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.44414106 |
| stats_o/std                    | 0.02755911 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0128    |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -1.0157058 |
| test/Q_plus_P                  | -1.0157058 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.677      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00537   |
| train/info_shaping_reward_mean | -0.0616    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.44414163 |
| stats_o/std                    | 0.02755679 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00936   |
| test/info_shaping_reward_mean  | -0.0545    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.1922009 |
| test/Q_plus_P                  | -1.1922009 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00317   |
| train/info_shaping_reward_mean | -0.0584    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.44414178 |
| stats_o/std                    | 0.02755748 |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0019    |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -1.1207215 |
| test/Q_plus_P                  | -1.1207215 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.004     |
| train/info_shaping_reward_mean | -0.0606    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.44414166 |
| stats_o/std                    | 0.02755328 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0102    |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -1.0782315 |
| test/Q_plus_P                  | -1.0782315 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.71       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0056    |
| train/info_shaping_reward_mean | -0.0572    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.6      |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 474        |
| stats_o/mean                   | 0.4441415  |
| stats_o/std                    | 0.02755045 |
| test/episodes                  | 4750       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0103    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -1.2376909 |
| test/Q_plus_P                  | -1.2376909 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 190000     |
| train/episodes                 | 19000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.696      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00396   |
| train/info_shaping_reward_mean | -0.0608    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.2      |
| train/steps                    | 760000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.44414106  |
| stats_o/std                    | 0.02754757  |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00677    |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.93197244 |
| test/Q_plus_P                  | -0.93197244 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.719       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.2       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.44414207  |
| stats_o/std                    | 0.027545251 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0223     |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.2035832  |
| test/Q_plus_P                  | -1.2035832  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.44414222  |
| stats_o/std                    | 0.027541777 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0196     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8710008  |
| test/Q_plus_P                  | -0.8710008  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00319    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.44414365 |
| stats_o/std                    | 0.02753743 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0127    |
| test/info_shaping_reward_mean  | -0.059     |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -1.2226559 |
| test/Q_plus_P                  | -1.2226559 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0603    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.44414428  |
| stats_o/std                    | 0.027536944 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.011      |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.9306129  |
| test/Q_plus_P                  | -0.9306129  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00378    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.44414547  |
| stats_o/std                    | 0.027533008 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0056     |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -1.1666937  |
| test/Q_plus_P                  | -1.1666937  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00443    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.44414636  |
| stats_o/std                    | 0.027530335 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00216    |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.1697273  |
| test/Q_plus_P                  | -1.1697273  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00298    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.444146    |
| stats_o/std                    | 0.027528597 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0166     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.0588361  |
| test/Q_plus_P                  | -1.0588361  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00525    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.44414613 |
| stats_o/std                    | 0.02752333 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00249   |
| test/info_shaping_reward_mean  | -0.0446    |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -1.1534346 |
| test/Q_plus_P                  | -1.1534346 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.701      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.057     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.44414508  |
| stats_o/std                    | 0.027522802 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00957    |
| test/info_shaping_reward_mean  | -0.0481     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.9395589  |
| test/Q_plus_P                  | -0.9395589  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00297    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.44414595  |
| stats_o/std                    | 0.027521374 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00195    |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9106093  |
| test/Q_plus_P                  | -0.9106093  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00521    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.44414628  |
| stats_o/std                    | 0.027517902 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00443    |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.1286198  |
| test/Q_plus_P                  | -1.1286198  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.44414762  |
| stats_o/std                    | 0.027515551 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00766    |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.9257638  |
| test/Q_plus_P                  | -0.9257638  |
| test/reward_per_eps            | -8          |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00441    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.44414818  |
| stats_o/std                    | 0.027515657 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00926    |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.2283003  |
| test/Q_plus_P                  | -1.2283003  |
| test/reward_per_eps            | -10         |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.4441489   |
| stats_o/std                    | 0.02751255  |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0072     |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.97238207 |
| test/Q_plus_P                  | -0.97238207 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00461    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.44414923  |
| stats_o/std                    | 0.027511664 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00948    |
| test/info_shaping_reward_mean  | -0.0612     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.1354645  |
| test/Q_plus_P                  | -1.1354645  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00399    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.44415143  |
| stats_o/std                    | 0.027507072 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0118     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.2624524  |
| test/Q_plus_P                  | -1.2624524  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.44415212  |
| stats_o/std                    | 0.027505117 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00728    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.98968714 |
| test/Q_plus_P                  | -0.98968714 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00562    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 493        |
| stats_o/mean                   | 0.44415355 |
| stats_o/std                    | 0.02750506 |
| test/episodes                  | 4940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.752      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00613   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -1.2556895 |
| test/Q_plus_P                  | -1.2556895 |
| test/reward_per_eps            | -9.9       |
| test/steps                     | 197600     |
| train/episodes                 | 19760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.629      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00386   |
| train/info_shaping_reward_mean | -0.0648    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 790400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.44415554  |
| stats_o/std                    | 0.027501097 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0153     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.0991275  |
| test/Q_plus_P                  | -1.0991275  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.004      |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.4441569   |
| stats_o/std                    | 0.027499206 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -1.0323651  |
| test/Q_plus_P                  | -1.0323651  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.44415843  |
| stats_o/std                    | 0.027497277 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00814    |
| test/info_shaping_reward_mean  | -0.057      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.0877825  |
| test/Q_plus_P                  | -1.0877825  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00837    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.44415793  |
| stats_o/std                    | 0.027495809 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.728       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0171     |
| test/info_shaping_reward_mean  | -0.0604     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.2635666  |
| test/Q_plus_P                  | -1.2635666  |
| test/reward_per_eps            | -10.9       |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00431    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.44415784 |
| stats_o/std                    | 0.02749467 |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.75       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.018     |
| test/info_shaping_reward_mean  | -0.0563    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -1.2006768 |
| test/Q_plus_P                  | -1.2006768 |
| test/reward_per_eps            | -10        |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00567   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.44415876  |
| stats_o/std                    | 0.027492711 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.9162793  |
| test/Q_plus_P                  | -0.9162793  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00498    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 500        |
| stats_o/mean                   | 0.44415984 |
| stats_o/std                    | 0.02748788 |
| test/episodes                  | 5010       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.738      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00724   |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -1.1801558 |
| test/Q_plus_P                  | -1.1801558 |
| test/reward_per_eps            | -10.5      |
| test/steps                     | 200400     |
| train/episodes                 | 20040      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00752   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 801600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.44416043  |
| stats_o/std                    | 0.027486391 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.688       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00443    |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.4748676  |
| test/Q_plus_P                  | -1.4748676  |
| test/reward_per_eps            | -12.5       |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00428    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.44416013  |
| stats_o/std                    | 0.027484635 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00297    |
| test/info_shaping_reward_mean  | -0.048      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.245972   |
| test/Q_plus_P                  | -1.245972   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.44415966  |
| stats_o/std                    | 0.027481144 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00548    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.9849076  |
| test/Q_plus_P                  | -0.9849076  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00423    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 504        |
| stats_o/mean                   | 0.4441594  |
| stats_o/std                    | 0.02748151 |
| test/episodes                  | 5050       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.728      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0145    |
| test/info_shaping_reward_mean  | -0.0558    |
| test/info_shaping_reward_min   | -0.224     |
| test/Q                         | -1.3819209 |
| test/Q_plus_P                  | -1.3819209 |
| test/reward_per_eps            | -10.9      |
| test/steps                     | 202000     |
| train/episodes                 | 20200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00514   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.257     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 808000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.44416043 |
| stats_o/std                    | 0.02747825 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.748      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00515   |
| test/info_shaping_reward_mean  | -0.0554    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -1.334675  |
| test/Q_plus_P                  | -1.334675  |
| test/reward_per_eps            | -10.1      |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.682      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00496   |
| train/info_shaping_reward_mean | -0.0583    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.44415948  |
| stats_o/std                    | 0.027477639 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0145     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0473577  |
| test/Q_plus_P                  | -1.0473577  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00518    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.44415972  |
| stats_o/std                    | 0.027479952 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00616    |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.0393043  |
| test/Q_plus_P                  | -1.0393043  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00532    |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.44415927  |
| stats_o/std                    | 0.027476905 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00384    |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.2340506  |
| test/Q_plus_P                  | -1.2340506  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.44415942 |
| stats_o/std                    | 0.02747742 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.76       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00963   |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.22      |
| test/Q                         | -1.1536822 |
| test/Q_plus_P                  | -1.1536822 |
| test/reward_per_eps            | -9.6       |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.692      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00458   |
| train/info_shaping_reward_mean | -0.0591    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.3      |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.44415867  |
| stats_o/std                    | 0.027476048 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0119     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.88229483 |
| test/Q_plus_P                  | -0.88229483 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00463    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.44415942  |
| stats_o/std                    | 0.027474098 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00815    |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.1558869  |
| test/Q_plus_P                  | -1.1558869  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00331    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.4441605   |
| stats_o/std                    | 0.027471805 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0075     |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -1.079762   |
| test/Q_plus_P                  | -1.079762   |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00459    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.4441602   |
| stats_o/std                    | 0.027470859 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0155     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.090232   |
| test/Q_plus_P                  | -1.090232   |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00451    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.44416058  |
| stats_o/std                    | 0.027469419 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.0225222  |
| test/Q_plus_P                  | -1.0225222  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00366    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.44416094  |
| stats_o/std                    | 0.027465282 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.72        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00674    |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -1.2971555  |
| test/Q_plus_P                  | -1.2971555  |
| test/reward_per_eps            | -11.2       |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00401    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.44416192  |
| stats_o/std                    | 0.027460292 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0063     |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.9865347  |
| test/Q_plus_P                  | -0.9865347  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00392    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.44416365  |
| stats_o/std                    | 0.027456941 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00728    |
| test/info_shaping_reward_mean  | -0.0574     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.2302588  |
| test/Q_plus_P                  | -1.2302588  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00576    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.44416654  |
| stats_o/std                    | 0.027454505 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00185    |
| test/info_shaping_reward_mean  | -0.0462     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.179916   |
| test/Q_plus_P                  | -1.179916   |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00558    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.4441668   |
| stats_o/std                    | 0.027452191 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00529    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -1.0341474  |
| test/Q_plus_P                  | -1.0341474  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00393    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.44416884  |
| stats_o/std                    | 0.027448421 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00489    |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.9036426  |
| test/Q_plus_P                  | -0.9036426  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0559     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.44417015  |
| stats_o/std                    | 0.027444564 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.0480126  |
| test/Q_plus_P                  | -1.0480126  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00408    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.4441717   |
| stats_o/std                    | 0.027442662 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00801    |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.8971441  |
| test/Q_plus_P                  | -0.8971441  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00663    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.44417217  |
| stats_o/std                    | 0.027437167 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0035     |
| test/info_shaping_reward_mean  | -0.0458     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.0245714  |
| test/Q_plus_P                  | -1.0245714  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0049     |
| train/info_shaping_reward_mean | -0.0564     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.444172    |
| stats_o/std                    | 0.027434943 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00345    |
| test/info_shaping_reward_mean  | -0.045      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.0218525  |
| test/Q_plus_P                  | -1.0218525  |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00387    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.44417322  |
| stats_o/std                    | 0.027429258 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00105    |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -1.2188965  |
| test/Q_plus_P                  | -1.2188965  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00526    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 526        |
| stats_o/mean                   | 0.44417366 |
| stats_o/std                    | 0.02742627 |
| test/episodes                  | 5270       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.000998  |
| test/info_shaping_reward_mean  | -0.0461    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -0.8226412 |
| test/Q_plus_P                  | -0.8226412 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 210800     |
| train/episodes                 | 21080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.646      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00433   |
| train/info_shaping_reward_mean | -0.0575    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 843200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.44417366  |
| stats_o/std                    | 0.027423687 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00268    |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -1.020707   |
| test/Q_plus_P                  | -1.020707   |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00328    |
| train/info_shaping_reward_mean | -0.0554     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.44417405  |
| stats_o/std                    | 0.027420953 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00732    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0493106  |
| test/Q_plus_P                  | -1.0493106  |
| test/reward_per_eps            | -9          |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.44417515  |
| stats_o/std                    | 0.027418965 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00607    |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -1.0231742  |
| test/Q_plus_P                  | -1.0231742  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00508    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.44417462  |
| stats_o/std                    | 0.027419632 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00812    |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.0673571  |
| test/Q_plus_P                  | -1.0673571  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0063     |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.44417408  |
| stats_o/std                    | 0.027418582 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00394    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -1.0495185  |
| test/Q_plus_P                  | -1.0495185  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00406    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.44417593  |
| stats_o/std                    | 0.027416194 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0112     |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.112835   |
| test/Q_plus_P                  | -1.112835   |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00523    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.44417575 |
| stats_o/std                    | 0.02741167 |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.78       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0116    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -1.0397079 |
| test/Q_plus_P                  | -1.0397079 |
| test/reward_per_eps            | -8.8       |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00408   |
| train/info_shaping_reward_mean | -0.0589    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.4441739   |
| stats_o/std                    | 0.027411483 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0132     |
| test/info_shaping_reward_mean  | -0.0514     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.8225746  |
| test/Q_plus_P                  | -0.8225746  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00599    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.4441756   |
| stats_o/std                    | 0.027411455 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00398    |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.2247328  |
| test/Q_plus_P                  | -1.2247328  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00419    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.44417593  |
| stats_o/std                    | 0.027408041 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00499    |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -1.058403   |
| test/Q_plus_P                  | -1.058403   |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00402    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.4441749   |
| stats_o/std                    | 0.027407788 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0037     |
| test/info_shaping_reward_mean  | -0.057      |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -1.2172886  |
| test/Q_plus_P                  | -1.2172886  |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00465    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.44417417  |
| stats_o/std                    | 0.027408011 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.713       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00625    |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -1.41714    |
| test/Q_plus_P                  | -1.41714    |
| test/reward_per_eps            | -11.5       |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00449    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.44417557  |
| stats_o/std                    | 0.027408587 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.775       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0207     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -1.0793353  |
| test/Q_plus_P                  | -1.0793353  |
| test/reward_per_eps            | -9          |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00507    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.44417545  |
| stats_o/std                    | 0.027405215 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0025     |
| test/info_shaping_reward_mean  | -0.0473     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.01972    |
| test/Q_plus_P                  | -1.01972    |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00397    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.4441756   |
| stats_o/std                    | 0.027402477 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00424    |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.1555883  |
| test/Q_plus_P                  | -1.1555883  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00296    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.44417575 |
| stats_o/std                    | 0.02740007 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.757      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00108   |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -1.2488971 |
| test/Q_plus_P                  | -1.2488971 |
| test/reward_per_eps            | -9.7       |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.676      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00464   |
| train/info_shaping_reward_mean | -0.0602    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 543        |
| stats_o/mean                   | 0.44417715 |
| stats_o/std                    | 0.02739761 |
| test/episodes                  | 5440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.787      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0135    |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.221     |
| test/Q                         | -0.9779094 |
| test/Q_plus_P                  | -0.9779094 |
| test/reward_per_eps            | -8.5       |
| test/steps                     | 217600     |
| train/episodes                 | 21760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0624    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 870400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.44417772  |
| stats_o/std                    | 0.027394189 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.7         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0056     |
| test/info_shaping_reward_mean  | -0.0593     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.5026914  |
| test/Q_plus_P                  | -1.5026914  |
| test/reward_per_eps            | -12         |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00405    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.44417778  |
| stats_o/std                    | 0.027392715 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00829    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.1745639  |
| test/Q_plus_P                  | -1.1745639  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00381    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.44417885  |
| stats_o/std                    | 0.027390406 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.76        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00423    |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -1.2224569  |
| test/Q_plus_P                  | -1.2224569  |
| test/reward_per_eps            | -9.6        |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00486    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.4441775   |
| stats_o/std                    | 0.027390933 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00134    |
| test/info_shaping_reward_mean  | -0.0395     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.0773573  |
| test/Q_plus_P                  | -1.0773573  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.706       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00389    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.44417706  |
| stats_o/std                    | 0.027387949 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00182    |
| test/info_shaping_reward_mean  | -0.0428     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.90024835 |
| test/Q_plus_P                  | -0.90024835 |
| test/reward_per_eps            | -8          |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.704       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.8       |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.44417763  |
| stats_o/std                    | 0.02738346  |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00254    |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.93966764 |
| test/Q_plus_P                  | -0.93966764 |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00373    |
| train/info_shaping_reward_mean | -0.0552     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.4441781   |
| stats_o/std                    | 0.027384171 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.743       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00533    |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -1.4025995  |
| test/Q_plus_P                  | -1.4025995  |
| test/reward_per_eps            | -10.3       |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00393    |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.44417813 |
| stats_o/std                    | 0.02738572 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.777      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00452   |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -1.0793011 |
| test/Q_plus_P                  | -1.0793011 |
| test/reward_per_eps            | -8.9       |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.686      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00382   |
| train/info_shaping_reward_mean | -0.0618    |
| train/info_shaping_reward_min  | -0.275     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.4441775   |
| stats_o/std                    | 0.027383594 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.74        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00523    |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.3314759  |
| test/Q_plus_P                  | -1.3314759  |
| test/reward_per_eps            | -10.4       |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.44417766  |
| stats_o/std                    | 0.027379772 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00723    |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.3911943  |
| test/Q_plus_P                  | -1.3911943  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00418    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.44417796  |
| stats_o/std                    | 0.027378246 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00263    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.2308735  |
| test/Q_plus_P                  | -1.2308735  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00417    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.4441795   |
| stats_o/std                    | 0.027376294 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0058     |
| test/info_shaping_reward_mean  | -0.0424     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.9085699  |
| test/Q_plus_P                  | -0.9085699  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00523    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.44418144  |
| stats_o/std                    | 0.027377805 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00272    |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -1.13887    |
| test/Q_plus_P                  | -1.13887    |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0062     |
| train/info_shaping_reward_mean | -0.0689     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.44418278  |
| stats_o/std                    | 0.027375452 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00755    |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.1331695  |
| test/Q_plus_P                  | -1.1331695  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.688       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00306    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.44418374  |
| stats_o/std                    | 0.027375212 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00417    |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.87765986 |
| test/Q_plus_P                  | -0.87765986 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00456    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.4441855   |
| stats_o/std                    | 0.027373863 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.003      |
| test/info_shaping_reward_mean  | -0.046      |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.9932244  |
| test/Q_plus_P                  | -0.9932244  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00473    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.44418487  |
| stats_o/std                    | 0.027372679 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00576    |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -1.0775863  |
| test/Q_plus_P                  | -1.0775863  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0045     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.44418612  |
| stats_o/std                    | 0.027369203 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0141     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -1.0519559  |
| test/Q_plus_P                  | -1.0519559  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00365    |
| train/info_shaping_reward_mean | -0.0565     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.44418713  |
| stats_o/std                    | 0.027365586 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00523    |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -1.0193876  |
| test/Q_plus_P                  | -1.0193876  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00435    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.44418645  |
| stats_o/std                    | 0.027365422 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00365    |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.98978007 |
| test/Q_plus_P                  | -0.98978007 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.708       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00382    |
| train/info_shaping_reward_mean | -0.0558     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.44418594  |
| stats_o/std                    | 0.027364371 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.752       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00637    |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -1.238164   |
| test/Q_plus_P                  | -1.238164   |
| test/reward_per_eps            | -9.9        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.691       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00448    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.44418645  |
| stats_o/std                    | 0.027361646 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.708       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0136     |
| test/info_shaping_reward_mean  | -0.059      |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -1.3965373  |
| test/Q_plus_P                  | -1.3965373  |
| test/reward_per_eps            | -11.7       |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00445    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.44418824  |
| stats_o/std                    | 0.027356036 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.785       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0126     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.0331507  |
| test/Q_plus_P                  | -1.0331507  |
| test/reward_per_eps            | -8.6        |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00457    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.44418958  |
| stats_o/std                    | 0.027352935 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00149    |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.0194046  |
| test/Q_plus_P                  | -1.0194046  |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00522    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 568        |
| stats_o/mean                   | 0.44418955 |
| stats_o/std                    | 0.02735032 |
| test/episodes                  | 5690       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.79       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00346   |
| test/info_shaping_reward_mean  | -0.0484    |
| test/info_shaping_reward_min   | -0.218     |
| test/Q                         | -0.9701119 |
| test/Q_plus_P                  | -0.9701119 |
| test/reward_per_eps            | -8.4       |
| test/steps                     | 227600     |
| train/episodes                 | 22760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.703      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00491   |
| train/info_shaping_reward_mean | -0.0574    |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -11.9      |
| train/steps                    | 910400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.44418934  |
| stats_o/std                    | 0.027346775 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.723       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.000826   |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -1.3641309  |
| test/Q_plus_P                  | -1.3641309  |
| test/reward_per_eps            | -11.1       |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00325    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.444188    |
| stats_o/std                    | 0.027344162 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00535    |
| test/info_shaping_reward_mean  | -0.0482     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.9510428  |
| test/Q_plus_P                  | -0.9510428  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0042     |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.44418755  |
| stats_o/std                    | 0.027341036 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00945    |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -1.0984825  |
| test/Q_plus_P                  | -1.0984825  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.44418916  |
| stats_o/std                    | 0.027336782 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.733       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00719    |
| test/info_shaping_reward_mean  | -0.0586     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -1.2629881  |
| test/Q_plus_P                  | -1.2629881  |
| test/reward_per_eps            | -10.7       |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0039     |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.4441917   |
| stats_o/std                    | 0.027336238 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0185     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.9182072  |
| test/Q_plus_P                  | -0.9182072  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.597       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00496    |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.4441919   |
| stats_o/std                    | 0.027333817 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.782       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0102     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0694312  |
| test/Q_plus_P                  | -1.0694312  |
| test/reward_per_eps            | -8.7        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00599    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.4441923   |
| stats_o/std                    | 0.027330935 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.77        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.008      |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -1.0903327  |
| test/Q_plus_P                  | -1.0903327  |
| test/reward_per_eps            | -9.2        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.702       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00358    |
| train/info_shaping_reward_mean | -0.0553     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.9       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 576        |
| stats_o/mean                   | 0.44419345 |
| stats_o/std                    | 0.02733165 |
| test/episodes                  | 5770       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.782      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00314   |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -1.0973369 |
| test/Q_plus_P                  | -1.0973369 |
| test/reward_per_eps            | -8.7       |
| test/steps                     | 230800     |
| train/episodes                 | 23080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00385   |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 923200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.444194   |
| stats_o/std                    | 0.02732677 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00408   |
| test/info_shaping_reward_mean  | -0.0478    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.7723582 |
| test/Q_plus_P                  | -0.7723582 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00415   |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.44419298  |
| stats_o/std                    | 0.027327215 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00363    |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.94981885 |
| test/Q_plus_P                  | -0.94981885 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00604    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.44419324  |
| stats_o/std                    | 0.027322508 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.762       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00164    |
| test/info_shaping_reward_mean  | -0.057      |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.1202704  |
| test/Q_plus_P                  | -1.1202704  |
| test/reward_per_eps            | -9.5        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00487    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.4441949   |
| stats_o/std                    | 0.027319299 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.735       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00915    |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -1.2430781  |
| test/Q_plus_P                  | -1.2430781  |
| test/reward_per_eps            | -10.6       |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00592    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.44419315 |
| stats_o/std                    | 0.02732011 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.77       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00435   |
| test/info_shaping_reward_mean  | -0.0585    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -1.1097953 |
| test/Q_plus_P                  | -1.1097953 |
| test/reward_per_eps            | -9.2       |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00425   |
| train/info_shaping_reward_mean | -0.0622    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.44419456  |
| stats_o/std                    | 0.027321173 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00664    |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.824685   |
| test/Q_plus_P                  | -0.824685   |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00526    |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.44419563  |
| stats_o/std                    | 0.027316974 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.75        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0165     |
| test/info_shaping_reward_mean  | -0.0591     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -1.2681413  |
| test/Q_plus_P                  | -1.2681413  |
| test/reward_per_eps            | -10         |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0044     |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.44419524  |
| stats_o/std                    | 0.027314944 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.772       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0127     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.0172905  |
| test/Q_plus_P                  | -1.0172905  |
| test/reward_per_eps            | -9.1        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00565    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.44419637  |
| stats_o/std                    | 0.027312564 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0256     |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.84652627 |
| test/Q_plus_P                  | -0.84652627 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00596    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.44419622  |
| stats_o/std                    | 0.027309628 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0118     |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -1.0272286  |
| test/Q_plus_P                  | -1.0272286  |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.717       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00301    |
| train/info_shaping_reward_mean | -0.0551     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.3       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.4441973   |
| stats_o/std                    | 0.027309565 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00945    |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.98200554 |
| test/Q_plus_P                  | -0.98200554 |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00457    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.4441973   |
| stats_o/std                    | 0.027305707 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.765       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0222     |
| test/info_shaping_reward_mean  | -0.0578     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.0820714  |
| test/Q_plus_P                  | -1.0820714  |
| test/reward_per_eps            | -9.4        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00487    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.44419765  |
| stats_o/std                    | 0.027301794 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0197     |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8586859  |
| test/Q_plus_P                  | -0.8586859  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00537    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.44419837  |
| stats_o/std                    | 0.02730073  |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0145     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.84487784 |
| test/Q_plus_P                  | -0.84487784 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00534    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.4441981   |
| stats_o/std                    | 0.027299589 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0019     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.9275033  |
| test/Q_plus_P                  | -0.9275033  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00444    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.44419822 |
| stats_o/std                    | 0.02729549 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.775      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00594   |
| test/info_shaping_reward_mean  | -0.0504    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -1.039786  |
| test/Q_plus_P                  | -1.039786  |
| test/reward_per_eps            | -9         |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.674      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00384   |
| train/info_shaping_reward_mean | -0.0566    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.4441986   |
| stats_o/std                    | 0.027291425 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.685       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00107    |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -1.5352311  |
| test/Q_plus_P                  | -1.5352311  |
| test/reward_per_eps            | -12.6       |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00374    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.44419873 |
| stats_o/std                    | 0.02728876 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.718      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00244   |
| test/info_shaping_reward_mean  | -0.0555    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -1.3873395 |
| test/Q_plus_P                  | -1.3873395 |
| test/reward_per_eps            | -11.3      |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00314   |
| train/info_shaping_reward_mean | -0.0585    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.44419837 |
| stats_o/std                    | 0.02728691 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.725      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.014     |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -1.4317986 |
| test/Q_plus_P                  | -1.4317986 |
| test/reward_per_eps            | -11        |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.683      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0033    |
| train/info_shaping_reward_mean | -0.0603    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.7      |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.44419834  |
| stats_o/std                    | 0.027285852 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00911    |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.98548734 |
| test/Q_plus_P                  | -0.98548734 |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.707       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00421    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.7       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.44419864  |
| stats_o/std                    | 0.027282866 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.777       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00774    |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -1.0406393  |
| test/Q_plus_P                  | -1.0406393  |
| test/reward_per_eps            | -8.9        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00439    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.44419873  |
| stats_o/std                    | 0.027279632 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.755       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0127     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -1.2565373  |
| test/Q_plus_P                  | -1.2565373  |
| test/reward_per_eps            | -9.8        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00427    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.4441987  |
| stats_o/std                    | 0.02727951 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.762      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00287   |
| test/info_shaping_reward_mean  | -0.0493    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -1.1177049 |
| test/Q_plus_P                  | -1.1177049 |
| test/reward_per_eps            | -9.5       |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00474   |
| train/info_shaping_reward_mean | -0.064     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
