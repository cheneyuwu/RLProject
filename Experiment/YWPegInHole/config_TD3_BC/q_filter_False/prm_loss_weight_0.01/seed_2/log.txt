Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC/q_filter_False/prm_loss_weight_0.01/seed_2
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.4404584   |
| stats_o/std                    | 0.037735824 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0425      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00521    |
| test/info_shaping_reward_mean  | -0.0841     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -1.3024591  |
| test/Q_plus_P                  | -1.3024591  |
| test/reward_per_eps            | -38.3       |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0676     |
| train/info_shaping_reward_mean | -0.143      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.44237438  |
| stats_o/std                    | 0.036370512 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0525      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00226    |
| test/info_shaping_reward_mean  | -0.0855     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -1.6393052  |
| test/Q_plus_P                  | -1.6393052  |
| test/reward_per_eps            | -37.9       |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0609     |
| train/info_shaping_reward_mean | -0.118      |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.44322565  |
| stats_o/std                    | 0.035532907 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.172       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00323    |
| test/info_shaping_reward_mean  | -0.079      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -1.9019504  |
| test/Q_plus_P                  | -1.9019504  |
| test/reward_per_eps            | -33.1       |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0163      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0616     |
| train/info_shaping_reward_mean | -0.124      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.4       |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.44288847  |
| stats_o/std                    | 0.034465607 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0975      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00718    |
| test/info_shaping_reward_mean  | -0.0939     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -2.2990568  |
| test/Q_plus_P                  | -2.2990568  |
| test/reward_per_eps            | -36.1       |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00125     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0657     |
| train/info_shaping_reward_mean | -0.129      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 4          |
| stats_o/mean                   | 0.44159982 |
| stats_o/std                    | 0.03463575 |
| test/episodes                  | 50         |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.165      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0124    |
| test/info_shaping_reward_mean  | -0.0933    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -2.5431075 |
| test/Q_plus_P                  | -2.5431075 |
| test/reward_per_eps            | -33.4      |
| test/steps                     | 2000       |
| train/episodes                 | 200        |
| train/info_is_success_max      | 0.1        |
| train/info_is_success_mean     | 0.0144     |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0707    |
| train/info_shaping_reward_mean | -0.163     |
| train/info_shaping_reward_min  | -0.274     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -39.4      |
| train/steps                    | 8000       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.43939468  |
| stats_o/std                    | 0.035324525 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.045       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.127      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -3.1177535  |
| test/Q_plus_P                  | -3.1177535  |
| test/reward_per_eps            | -38.2       |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0842     |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 6           |
| stats_o/mean                   | 0.43777135  |
| stats_o/std                    | 0.035642978 |
| test/episodes                  | 70          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.343       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00144    |
| test/info_shaping_reward_mean  | -0.0759     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -2.9316058  |
| test/Q_plus_P                  | -2.9316058  |
| test/reward_per_eps            | -26.3       |
| test/steps                     | 2800        |
| train/episodes                 | 280         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0875     |
| train/info_shaping_reward_mean | -0.188      |
| train/info_shaping_reward_min  | -0.293      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 11200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 7           |
| stats_o/mean                   | 0.4356375   |
| stats_o/std                    | 0.035873603 |
| test/episodes                  | 80          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0475      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00431    |
| test/info_shaping_reward_mean  | -0.101      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -3.7969074  |
| test/Q_plus_P                  | -3.7969074  |
| test/reward_per_eps            | -38.1       |
| test/steps                     | 3200        |
| train/episodes                 | 320         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0862     |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.289      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 12800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.43414438  |
| stats_o/std                    | 0.035645127 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.19        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00372    |
| test/info_shaping_reward_mean  | -0.0888     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -3.8542206  |
| test/Q_plus_P                  | -3.8542206  |
| test/reward_per_eps            | -32.4       |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0851     |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 9          |
| stats_o/mean                   | 0.43238714 |
| stats_o/std                    | 0.03579486 |
| test/episodes                  | 100        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0698    |
| test/info_shaping_reward_mean  | -0.095     |
| test/info_shaping_reward_min   | -0.24      |
| test/Q                         | -4.4850416 |
| test/Q_plus_P                  | -4.4850416 |
| test/reward_per_eps            | -40        |
| test/steps                     | 4000       |
| train/episodes                 | 400        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0819    |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 16000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.4312457   |
| stats_o/std                    | 0.035626363 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0685     |
| test/info_shaping_reward_mean  | -0.0936     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -4.6300173  |
| test/Q_plus_P                  | -4.6300173  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0687     |
| train/info_shaping_reward_mean | -0.159      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.43072906  |
| stats_o/std                    | 0.035342462 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0683     |
| test/info_shaping_reward_mean  | -0.0876     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -5.3225193  |
| test/Q_plus_P                  | -5.3225193  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0564     |
| train/info_shaping_reward_mean | -0.137      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.43036386  |
| stats_o/std                    | 0.034959313 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0575      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0048     |
| test/info_shaping_reward_mean  | -0.0829     |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -5.3555555  |
| test/Q_plus_P                  | -5.3555555  |
| test/reward_per_eps            | -37.7       |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0571     |
| train/info_shaping_reward_mean | -0.127      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.4298072   |
| stats_o/std                    | 0.035000008 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0635     |
| test/info_shaping_reward_mean  | -0.0901     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -5.2486134  |
| test/Q_plus_P                  | -5.2486134  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0548     |
| train/info_shaping_reward_mean | -0.139      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 14         |
| stats_o/mean                   | 0.42948368 |
| stats_o/std                    | 0.03511684 |
| test/episodes                  | 150        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0627    |
| test/info_shaping_reward_mean  | -0.0849    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -5.844243  |
| test/Q_plus_P                  | -5.844243  |
| test/reward_per_eps            | -40        |
| test/steps                     | 6000       |
| train/episodes                 | 600        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0552    |
| train/info_shaping_reward_mean | -0.128     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 24000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.42966112  |
| stats_o/std                    | 0.034640823 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0638     |
| test/info_shaping_reward_mean  | -0.0835     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -5.995719   |
| test/Q_plus_P                  | -5.995719   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0566     |
| train/info_shaping_reward_mean | -0.0915     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 16         |
| stats_o/mean                   | 0.42983603 |
| stats_o/std                    | 0.0342605  |
| test/episodes                  | 170        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0646    |
| test/info_shaping_reward_mean  | -0.0832    |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -6.3863606 |
| test/Q_plus_P                  | -6.3863606 |
| test/reward_per_eps            | -40        |
| test/steps                     | 6800       |
| train/episodes                 | 680        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0578    |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 27200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.43002477  |
| stats_o/std                    | 0.033926807 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0633     |
| test/info_shaping_reward_mean  | -0.0857     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -6.9155602  |
| test/Q_plus_P                  | -6.9155602  |
| test/reward_per_eps            | -40         |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0566     |
| train/info_shaping_reward_mean | -0.0869     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 18         |
| stats_o/mean                   | 0.43021885 |
| stats_o/std                    | 0.03353678 |
| test/episodes                  | 190        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0682    |
| test/info_shaping_reward_mean  | -0.0857    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -7.238109  |
| test/Q_plus_P                  | -7.238109  |
| test/reward_per_eps            | -40        |
| test/steps                     | 7600       |
| train/episodes                 | 760        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0563    |
| train/info_shaping_reward_mean | -0.088     |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 30400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.4303122  |
| stats_o/std                    | 0.03310292 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0684    |
| test/info_shaping_reward_mean  | -0.084     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -7.37743   |
| test/Q_plus_P                  | -7.37743   |
| test/reward_per_eps            | -40        |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0571    |
| train/info_shaping_reward_mean | -0.0867    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.4305283  |
| stats_o/std                    | 0.03280596 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0658    |
| test/info_shaping_reward_mean  | -0.083     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -7.690421  |
| test/Q_plus_P                  | -7.690421  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0581    |
| train/info_shaping_reward_mean | -0.09      |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.43066588  |
| stats_o/std                    | 0.032543737 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0687     |
| test/info_shaping_reward_mean  | -0.0827     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -8.133827   |
| test/Q_plus_P                  | -8.133827   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.056      |
| train/info_shaping_reward_mean | -0.0889     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.4308239   |
| stats_o/std                    | 0.032388784 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0662     |
| test/info_shaping_reward_mean  | -0.0812     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -8.449543   |
| test/Q_plus_P                  | -8.449543   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0571     |
| train/info_shaping_reward_mean | -0.0926     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.43113458  |
| stats_o/std                    | 0.032448787 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0658     |
| test/info_shaping_reward_mean  | -0.0882     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -8.430742   |
| test/Q_plus_P                  | -8.430742   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0559     |
| train/info_shaping_reward_mean | -0.0942     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.43160164  |
| stats_o/std                    | 0.032805804 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0681     |
| test/info_shaping_reward_mean  | -0.0985     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -7.6373234  |
| test/Q_plus_P                  | -7.6373234  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0607     |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 25          |
| stats_o/mean                   | 0.4322448   |
| stats_o/std                    | 0.033331946 |
| test/episodes                  | 260         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0555     |
| test/info_shaping_reward_mean  | -0.0851     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -6.722068   |
| test/Q_plus_P                  | -6.722068   |
| test/reward_per_eps            | -40         |
| test/steps                     | 10400       |
| train/episodes                 | 1040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0563     |
| train/info_shaping_reward_mean | -0.0992     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 41600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.43307808  |
| stats_o/std                    | 0.033980813 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0554     |
| test/info_shaping_reward_mean  | -0.0833     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -6.3915653  |
| test/Q_plus_P                  | -6.3915653  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0539     |
| train/info_shaping_reward_mean | -0.101      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.43378368  |
| stats_o/std                    | 0.034462888 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0568     |
| test/info_shaping_reward_mean  | -0.0834     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -6.467191   |
| test/Q_plus_P                  | -6.467191   |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0541     |
| train/info_shaping_reward_mean | -0.0992     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 28          |
| stats_o/mean                   | 0.4344325   |
| stats_o/std                    | 0.034838542 |
| test/episodes                  | 290         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0521     |
| test/info_shaping_reward_mean  | -0.0798     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -5.574447   |
| test/Q_plus_P                  | -5.574447   |
| test/reward_per_eps            | -40         |
| test/steps                     | 11600       |
| train/episodes                 | 1160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0527     |
| train/info_shaping_reward_mean | -0.0939     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 46400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.4349943  |
| stats_o/std                    | 0.03510835 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0518    |
| test/info_shaping_reward_mean  | -0.0801    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -6.40964   |
| test/Q_plus_P                  | -6.40964   |
| test/reward_per_eps            | -40        |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0514    |
| train/info_shaping_reward_mean | -0.0878    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.4355428  |
| stats_o/std                    | 0.03535686 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0519    |
| test/info_shaping_reward_mean  | -0.0755    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -5.6519165 |
| test/Q_plus_P                  | -5.6519165 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0519    |
| train/info_shaping_reward_mean | -0.0889    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.43601206  |
| stats_o/std                    | 0.035583477 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0513     |
| test/info_shaping_reward_mean  | -0.0766     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -5.8306923  |
| test/Q_plus_P                  | -5.8306923  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0521     |
| train/info_shaping_reward_mean | -0.0903     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.43648675  |
| stats_o/std                    | 0.035764974 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0505     |
| test/info_shaping_reward_mean  | -0.0797     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -5.7786884  |
| test/Q_plus_P                  | -5.7786884  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0517     |
| train/info_shaping_reward_mean | -0.0864     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 33         |
| stats_o/mean                   | 0.43693224 |
| stats_o/std                    | 0.03591993 |
| test/episodes                  | 340        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0516    |
| test/info_shaping_reward_mean  | -0.0766    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -5.8212585 |
| test/Q_plus_P                  | -5.8212585 |
| test/reward_per_eps            | -40        |
| test/steps                     | 13600      |
| train/episodes                 | 1360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0523    |
| train/info_shaping_reward_mean | -0.0877    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 54400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 34         |
| stats_o/mean                   | 0.4373697  |
| stats_o/std                    | 0.03605533 |
| test/episodes                  | 350        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0515    |
| test/info_shaping_reward_mean  | -0.08      |
| test/info_shaping_reward_min   | -0.241     |
| test/Q                         | -6.1942554 |
| test/Q_plus_P                  | -6.1942554 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14000      |
| train/episodes                 | 1400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0519    |
| train/info_shaping_reward_mean | -0.0865    |
| train/info_shaping_reward_min  | -0.234     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 56000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 35          |
| stats_o/mean                   | 0.4377801   |
| stats_o/std                    | 0.036191445 |
| test/episodes                  | 360         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0504     |
| test/info_shaping_reward_mean  | -0.0808     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -6.3965864  |
| test/Q_plus_P                  | -6.3965864  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14400       |
| train/episodes                 | 1440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0522     |
| train/info_shaping_reward_mean | -0.0917     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 57600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 36          |
| stats_o/mean                   | 0.43819693  |
| stats_o/std                    | 0.036294382 |
| test/episodes                  | 370         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0521     |
| test/info_shaping_reward_mean  | -0.0761     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -6.704136   |
| test/Q_plus_P                  | -6.704136   |
| test/reward_per_eps            | -40         |
| test/steps                     | 14800       |
| train/episodes                 | 1480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0522     |
| train/info_shaping_reward_mean | -0.0866     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 59200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.4385759   |
| stats_o/std                    | 0.036379486 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0521     |
| test/info_shaping_reward_mean  | -0.0774     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -7.0824184  |
| test/Q_plus_P                  | -7.0824184  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.052      |
| train/info_shaping_reward_mean | -0.0868     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.4389268   |
| stats_o/std                    | 0.036434077 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0513     |
| test/info_shaping_reward_mean  | -0.0794     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -7.372507   |
| test/Q_plus_P                  | -7.372507   |
| test/reward_per_eps            | -40         |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0519     |
| train/info_shaping_reward_mean | -0.0857     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.43927643  |
| stats_o/std                    | 0.036491595 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0549     |
| test/info_shaping_reward_mean  | -0.0772     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -7.607921   |
| test/Q_plus_P                  | -7.607921   |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0523     |
| train/info_shaping_reward_mean | -0.0862     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.43959606  |
| stats_o/std                    | 0.036536638 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0534     |
| test/info_shaping_reward_mean  | -0.0772     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -8.00585    |
| test/Q_plus_P                  | -8.00585    |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0525     |
| train/info_shaping_reward_mean | -0.088      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.43990064  |
| stats_o/std                    | 0.036569536 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0537     |
| test/info_shaping_reward_mean  | -0.0766     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -8.384839   |
| test/Q_plus_P                  | -8.384839   |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0529     |
| train/info_shaping_reward_mean | -0.0856     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.4402195   |
| stats_o/std                    | 0.036600914 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0555     |
| test/info_shaping_reward_mean  | -0.0822     |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -8.823731   |
| test/Q_plus_P                  | -8.823731   |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0531     |
| train/info_shaping_reward_mean | -0.0884     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 43          |
| stats_o/mean                   | 0.44052923  |
| stats_o/std                    | 0.036651198 |
| test/episodes                  | 440         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0548     |
| test/info_shaping_reward_mean  | -0.0824     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -9.067699   |
| test/Q_plus_P                  | -9.067699   |
| test/reward_per_eps            | -40         |
| test/steps                     | 17600       |
| train/episodes                 | 1760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0533     |
| train/info_shaping_reward_mean | -0.092      |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 70400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.4408201   |
| stats_o/std                    | 0.036683083 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0548     |
| test/info_shaping_reward_mean  | -0.0818     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -9.386491   |
| test/Q_plus_P                  | -9.386491   |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0534     |
| train/info_shaping_reward_mean | -0.0927     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.44112268  |
| stats_o/std                    | 0.036724027 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0551     |
| test/info_shaping_reward_mean  | -0.0791     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -9.566885   |
| test/Q_plus_P                  | -9.566885   |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0537     |
| train/info_shaping_reward_mean | -0.0951     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.44143793  |
| stats_o/std                    | 0.036794513 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0561     |
| test/info_shaping_reward_mean  | -0.0969     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -10.092477  |
| test/Q_plus_P                  | -10.092477  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.055      |
| train/info_shaping_reward_mean | -0.104      |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.4418035  |
| stats_o/std                    | 0.03683756 |
| test/episodes                  | 480        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0585    |
| test/info_shaping_reward_mean  | -0.109     |
| test/info_shaping_reward_min   | -0.222     |
| test/Q                         | -10.456784 |
| test/Q_plus_P                  | -10.456784 |
| test/reward_per_eps            | -40        |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.056     |
| train/info_shaping_reward_mean | -0.106     |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.4421625   |
| stats_o/std                    | 0.036895722 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.103      |
| test/info_shaping_reward_mean  | -0.134      |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -10.836035  |
| test/Q_plus_P                  | -10.836035  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0579     |
| train/info_shaping_reward_mean | -0.111      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.44259682  |
| stats_o/std                    | 0.037039038 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0928     |
| test/info_shaping_reward_mean  | -0.147      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -11.205714  |
| test/Q_plus_P                  | -11.205714  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0633     |
| train/info_shaping_reward_mean | -0.126      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 50         |
| stats_o/mean                   | 0.4430194  |
| stats_o/std                    | 0.03720594 |
| test/episodes                  | 510        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0639    |
| test/info_shaping_reward_mean  | -0.194     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -11.536565 |
| test/Q_plus_P                  | -11.536565 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20400      |
| train/episodes                 | 2040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0629    |
| train/info_shaping_reward_mean | -0.133     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 81600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.44341984 |
| stats_o/std                    | 0.0373441  |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0695    |
| test/info_shaping_reward_mean  | -0.136     |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -11.682148 |
| test/Q_plus_P                  | -11.682148 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.068     |
| train/info_shaping_reward_mean | -0.132     |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.44385645  |
| stats_o/std                    | 0.037521917 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.055       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.181      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -11.558237  |
| test/Q_plus_P                  | -11.558237  |
| test/reward_per_eps            | -37.8       |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0721     |
| train/info_shaping_reward_mean | -0.144      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.4443902   |
| stats_o/std                    | 0.037926197 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0667     |
| test/info_shaping_reward_mean  | -0.132      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -12.331006  |
| test/Q_plus_P                  | -12.331006  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0839     |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.282      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.44491288  |
| stats_o/std                    | 0.038297158 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0972     |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -12.619072  |
| test/Q_plus_P                  | -12.619072  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0798     |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 55         |
| stats_o/mean                   | 0.44549105 |
| stats_o/std                    | 0.03881668 |
| test/episodes                  | 560        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.182      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.1       |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -9.933028  |
| test/Q_plus_P                  | -9.933028  |
| test/reward_per_eps            | -32.7      |
| test/steps                     | 22400      |
| train/episodes                 | 2240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0961    |
| train/info_shaping_reward_mean | -0.198     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 89600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.44586095  |
| stats_o/std                    | 0.039095573 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0693     |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -13.127146  |
| test/Q_plus_P                  | -13.127146  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0729     |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.44607806  |
| stats_o/std                    | 0.039125543 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0681     |
| test/info_shaping_reward_mean  | -0.1        |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -11.679997  |
| test/Q_plus_P                  | -11.679997  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0623     |
| train/info_shaping_reward_mean | -0.121      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.44623426  |
| stats_o/std                    | 0.039065894 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.0725      |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00738    |
| test/info_shaping_reward_mean  | -0.0902     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -10.281485  |
| test/Q_plus_P                  | -10.281485  |
| test/reward_per_eps            | -37.1       |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.0175      |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0597     |
| train/info_shaping_reward_mean | -0.109      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.3       |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.44639385  |
| stats_o/std                    | 0.038999837 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.237       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0154     |
| test/info_shaping_reward_mean  | -0.0747     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -8.139271   |
| test/Q_plus_P                  | -8.139271   |
| test/reward_per_eps            | -30.5       |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00688     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0576     |
| train/info_shaping_reward_mean | -0.105      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.7       |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.4465109   |
| stats_o/std                    | 0.038994443 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.435       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0118     |
| test/info_shaping_reward_mean  | -0.0649     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -5.5573716  |
| test/Q_plus_P                  | -5.5573716  |
| test/reward_per_eps            | -22.6       |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 0.7         |
| train/info_is_success_mean     | 0.197       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0309     |
| train/info_shaping_reward_mean | -0.0997     |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -32.1       |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.4465846   |
| stats_o/std                    | 0.038843248 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.472       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0153     |
| test/info_shaping_reward_mean  | -0.0615     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -5.229178   |
| test/Q_plus_P                  | -5.229178   |
| test/reward_per_eps            | -21.1       |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 0.9         |
| train/info_is_success_mean     | 0.257       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0167     |
| train/info_shaping_reward_mean | -0.084      |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -29.7       |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.44660547  |
| stats_o/std                    | 0.038700517 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.6         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0268     |
| test/info_shaping_reward_mean  | -0.0647     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -4.70058    |
| test/Q_plus_P                  | -4.70058    |
| test/reward_per_eps            | -16         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.376       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0136     |
| train/info_shaping_reward_mean | -0.0785     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -24.9       |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.44662198  |
| stats_o/std                    | 0.038553547 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.458       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00738    |
| test/info_shaping_reward_mean  | -0.0669     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -5.480927   |
| test/Q_plus_P                  | -5.480927   |
| test/reward_per_eps            | -21.7       |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.453       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0133     |
| train/info_shaping_reward_mean | -0.0752     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.9       |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.44661668  |
| stats_o/std                    | 0.038414083 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.662       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00663    |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -3.4349983  |
| test/Q_plus_P                  | -3.4349983  |
| test/reward_per_eps            | -13.5       |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.511       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0148     |
| train/info_shaping_reward_mean | -0.0725     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -19.6       |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.44662046  |
| stats_o/std                    | 0.038288414 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.748       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0066     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -2.1438758  |
| test/Q_plus_P                  | -2.1438758  |
| test/reward_per_eps            | -10.1       |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.422       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00773    |
| train/info_shaping_reward_mean | -0.0745     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -23.1       |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.44661212  |
| stats_o/std                    | 0.038162466 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.767       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0114     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.4697741  |
| test/Q_plus_P                  | -1.4697741  |
| test/reward_per_eps            | -9.3        |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.499       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0736     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -20.1       |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.44661286  |
| stats_o/std                    | 0.038036723 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0135     |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -1.8090522  |
| test/Q_plus_P                  | -1.8090522  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.452       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0141     |
| train/info_shaping_reward_mean | -0.0778     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -21.9       |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 68         |
| stats_o/mean                   | 0.4466084  |
| stats_o/std                    | 0.03789906 |
| test/episodes                  | 690        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00419   |
| test/info_shaping_reward_mean  | -0.0472    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -1.6675849 |
| test/Q_plus_P                  | -1.6675849 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 27600      |
| train/episodes                 | 2760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.537      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00777   |
| train/info_shaping_reward_mean | -0.0696    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -18.5      |
| train/steps                    | 110400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 69          |
| stats_o/mean                   | 0.44659624  |
| stats_o/std                    | 0.037768442 |
| test/episodes                  | 700         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.79        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0103     |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -1.641484   |
| test/Q_plus_P                  | -1.641484   |
| test/reward_per_eps            | -8.4        |
| test/steps                     | 28000       |
| train/episodes                 | 2800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.555       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00772    |
| train/info_shaping_reward_mean | -0.0688     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.8       |
| train/steps                    | 112000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.4465921   |
| stats_o/std                    | 0.037654977 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00664    |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -1.3884993  |
| test/Q_plus_P                  | -1.3884993  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.569       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00826    |
| train/info_shaping_reward_mean | -0.0703     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -17.2       |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 71          |
| stats_o/mean                   | 0.44658586  |
| stats_o/std                    | 0.037510302 |
| test/episodes                  | 720         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00696    |
| test/info_shaping_reward_mean  | -0.0443     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -1.0863682  |
| test/Q_plus_P                  | -1.0863682  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 28800       |
| train/episodes                 | 2880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00544    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.227      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 115200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.4465681   |
| stats_o/std                    | 0.037386507 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -1.0308146  |
| test/Q_plus_P                  | -1.0308146  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00731    |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.44654584  |
| stats_o/std                    | 0.037261125 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0129     |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -1.0744952  |
| test/Q_plus_P                  | -1.0744952  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.682       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00598    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.44652045  |
| stats_o/std                    | 0.037134532 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00978    |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -1.0425408  |
| test/Q_plus_P                  | -1.0425408  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00697    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.4465036   |
| stats_o/std                    | 0.03700783  |
| test/episodes                  | 760         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00996    |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.87436545 |
| test/Q_plus_P                  | -0.87436545 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00523    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.446496    |
| stats_o/std                    | 0.036891486 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00859    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -1.0421659  |
| test/Q_plus_P                  | -1.0421659  |
| test/reward_per_eps            | -8          |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00754    |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.4464681   |
| stats_o/std                    | 0.036781013 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0173     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -1.0192817  |
| test/Q_plus_P                  | -1.0192817  |
| test/reward_per_eps            | -8          |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0108     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.4464455   |
| stats_o/std                    | 0.036682233 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.847789   |
| test/Q_plus_P                  | -0.847789   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00737    |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 79          |
| stats_o/mean                   | 0.44642782  |
| stats_o/std                    | 0.03656304  |
| test/episodes                  | 800         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0128     |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.82767236 |
| test/Q_plus_P                  | -0.82767236 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 32000       |
| train/episodes                 | 3200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00781    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 128000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.44640782  |
| stats_o/std                    | 0.036456376 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0202     |
| test/info_shaping_reward_mean  | -0.0575     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.84471667 |
| test/Q_plus_P                  | -0.84471667 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00753    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 81         |
| stats_o/mean                   | 0.446383   |
| stats_o/std                    | 0.03634683 |
| test/episodes                  | 820        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0106    |
| test/info_shaping_reward_mean  | -0.0537    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.7717451 |
| test/Q_plus_P                  | -0.7717451 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 32800      |
| train/episodes                 | 3280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00624   |
| train/info_shaping_reward_mean | -0.0615    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 131200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 82         |
| stats_o/mean                   | 0.44636878 |
| stats_o/std                    | 0.03624323 |
| test/episodes                  | 830        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0108    |
| test/info_shaping_reward_mean  | -0.0535    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -0.8252139 |
| test/Q_plus_P                  | -0.8252139 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 33200      |
| train/episodes                 | 3320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.635      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00988   |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 132800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.44634318  |
| stats_o/std                    | 0.036133833 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0161     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.8156931  |
| test/Q_plus_P                  | -0.8156931  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0087     |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.44632435  |
| stats_o/std                    | 0.036025435 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00811    |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.75562423 |
| test/Q_plus_P                  | -0.75562423 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00644    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.44628975 |
| stats_o/std                    | 0.0359251  |
| test/episodes                  | 860        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0193    |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -0.844487  |
| test/Q_plus_P                  | -0.844487  |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00607   |
| train/info_shaping_reward_mean | -0.0637    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.44627222  |
| stats_o/std                    | 0.035822224 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0108     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.9052775  |
| test/Q_plus_P                  | -0.9052775  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0119     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.44625926  |
| stats_o/std                    | 0.035716325 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00371    |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.74166846 |
| test/Q_plus_P                  | -0.74166846 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00559    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 88         |
| stats_o/mean                   | 0.44624445 |
| stats_o/std                    | 0.03560931 |
| test/episodes                  | 890        |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0207    |
| test/info_shaping_reward_mean  | -0.0525    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -0.8588913 |
| test/Q_plus_P                  | -0.8588913 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 35600      |
| train/episodes                 | 3560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0096    |
| train/info_shaping_reward_mean | -0.062     |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 142400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.4462316   |
| stats_o/std                    | 0.035518955 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0133     |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.8951967  |
| test/Q_plus_P                  | -0.8951967  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00754    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.44622484  |
| stats_o/std                    | 0.035418637 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0199     |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.872358   |
| test/Q_plus_P                  | -0.872358   |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00739    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.4462177   |
| stats_o/std                    | 0.035318453 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0146     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.769294   |
| test/Q_plus_P                  | -0.769294   |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00736    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 92          |
| stats_o/mean                   | 0.4462136   |
| stats_o/std                    | 0.035221595 |
| test/episodes                  | 930         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.787       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0186     |
| test/info_shaping_reward_mean  | -0.0593     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.81668884 |
| test/Q_plus_P                  | -0.81668884 |
| test/reward_per_eps            | -8.5        |
| test/steps                     | 37200       |
| train/episodes                 | 3720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 148800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.44621888  |
| stats_o/std                    | 0.035133105 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0255     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.76056427 |
| test/Q_plus_P                  | -0.76056427 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0127     |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 94          |
| stats_o/mean                   | 0.44619724  |
| stats_o/std                    | 0.03504964  |
| test/episodes                  | 950         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0224     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.72625655 |
| test/Q_plus_P                  | -0.72625655 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 38000       |
| train/episodes                 | 3800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0122     |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 152000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.44619158  |
| stats_o/std                    | 0.034952603 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0273     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7313611  |
| test/Q_plus_P                  | -0.7313611  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 96          |
| stats_o/mean                   | 0.44617677  |
| stats_o/std                    | 0.034872714 |
| test/episodes                  | 970         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0256     |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.7277938  |
| test/Q_plus_P                  | -0.7277938  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 38800       |
| train/episodes                 | 3880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.606       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00991    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 155200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.44615424  |
| stats_o/std                    | 0.034800112 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0253     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.6396725  |
| test/Q_plus_P                  | -0.6396725  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00985    |
| train/info_shaping_reward_mean | -0.0665     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.44614443  |
| stats_o/std                    | 0.034723245 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0594     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.8172958  |
| test/Q_plus_P                  | -0.8172958  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0078     |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.44612804 |
| stats_o/std                    | 0.03464138 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0284    |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -0.6429678 |
| test/Q_plus_P                  | -0.6429678 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0104    |
| train/info_shaping_reward_mean | -0.0645    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.44612095  |
| stats_o/std                    | 0.034556177 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.029      |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.8022735  |
| test/Q_plus_P                  | -0.8022735  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00896    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.44610155  |
| stats_o/std                    | 0.034485485 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0228     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.60420597 |
| test/Q_plus_P                  | -0.60420597 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.697       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.4460911   |
| stats_o/std                    | 0.03440958  |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0304     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.64019275 |
| test/Q_plus_P                  | -0.64019275 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0114     |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.4460857   |
| stats_o/std                    | 0.034337297 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0217     |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.6838684  |
| test/Q_plus_P                  | -0.6838684  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0114     |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.44607165  |
| stats_o/std                    | 0.034268077 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0293     |
| test/info_shaping_reward_mean  | -0.0587     |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -0.74673307 |
| test/Q_plus_P                  | -0.74673307 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00621    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.44607314  |
| stats_o/std                    | 0.03418679  |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0246     |
| test/info_shaping_reward_mean  | -0.0605     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.68443877 |
| test/Q_plus_P                  | -0.68443877 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0119     |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.44606176  |
| stats_o/std                    | 0.03412153  |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.78        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0274     |
| test/info_shaping_reward_mean  | -0.0633     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.79944915 |
| test/Q_plus_P                  | -0.79944915 |
| test/reward_per_eps            | -8.8        |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00937    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.44605747  |
| stats_o/std                    | 0.03404135  |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0236     |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.74176574 |
| test/Q_plus_P                  | -0.74176574 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0145     |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.44605204 |
| stats_o/std                    | 0.03396753 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.029     |
| test/info_shaping_reward_mean  | -0.0616    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -0.8341524 |
| test/Q_plus_P                  | -0.8341524 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.656      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0099    |
| train/info_shaping_reward_mean | -0.0622    |
| train/info_shaping_reward_min  | -0.243     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.44604647  |
| stats_o/std                    | 0.03390179  |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.738       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0238     |
| test/info_shaping_reward_mean  | -0.0631     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.89571834 |
| test/Q_plus_P                  | -0.89571834 |
| test/reward_per_eps            | -10.5       |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0662     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.4460467   |
| stats_o/std                    | 0.033830743 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0243     |
| test/info_shaping_reward_mean  | -0.0595     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.80303264 |
| test/Q_plus_P                  | -0.80303264 |
| test/reward_per_eps            | -7          |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0138     |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.4460404   |
| stats_o/std                    | 0.033758055 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0312     |
| test/info_shaping_reward_mean  | -0.0611     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.8967906  |
| test/Q_plus_P                  | -0.8967906  |
| test/reward_per_eps            | -8          |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00789    |
| train/info_shaping_reward_mean | -0.0648     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.44603026  |
| stats_o/std                    | 0.033694543 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0333     |
| test/info_shaping_reward_mean  | -0.0611     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7682934  |
| test/Q_plus_P                  | -0.7682934  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0656     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.44602993  |
| stats_o/std                    | 0.033633422 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0247     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.7195912  |
| test/Q_plus_P                  | -0.7195912  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.598       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.016      |
| train/info_shaping_reward_mean | -0.0684     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.1       |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 114        |
| stats_o/mean                   | 0.44601548 |
| stats_o/std                    | 0.0335643  |
| test/episodes                  | 1150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0317    |
| test/info_shaping_reward_mean  | -0.0574    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.8015793 |
| test/Q_plus_P                  | -0.8015793 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 46000      |
| train/episodes                 | 4600       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00678   |
| train/info_shaping_reward_mean | -0.0597    |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 184000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.44601014  |
| stats_o/std                    | 0.033505898 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0299     |
| test/info_shaping_reward_mean  | -0.0612     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.8710229  |
| test/Q_plus_P                  | -0.8710229  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.611       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0141     |
| train/info_shaping_reward_mean | -0.0677     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 116        |
| stats_o/mean                   | 0.446009   |
| stats_o/std                    | 0.03343558 |
| test/episodes                  | 1170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0304    |
| test/info_shaping_reward_mean  | -0.0564    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -0.7571168 |
| test/Q_plus_P                  | -0.7571168 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 46800      |
| train/episodes                 | 4680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.68       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0114    |
| train/info_shaping_reward_mean | -0.0609    |
| train/info_shaping_reward_min  | -0.235     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 187200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.44600293  |
| stats_o/std                    | 0.03337181  |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0313     |
| test/info_shaping_reward_mean  | -0.0598     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.83425325 |
| test/Q_plus_P                  | -0.83425325 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0137     |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.4460013   |
| stats_o/std                    | 0.033312462 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0315     |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.7558713  |
| test/Q_plus_P                  | -0.7558713  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0145     |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.4460025   |
| stats_o/std                    | 0.033247914 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0281     |
| test/info_shaping_reward_mean  | -0.0613     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.9489297  |
| test/Q_plus_P                  | -0.9489297  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0122     |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.44599542 |
| stats_o/std                    | 0.03318393 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0329    |
| test/info_shaping_reward_mean  | -0.0593    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -0.7969001 |
| test/Q_plus_P                  | -0.7969001 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0142    |
| train/info_shaping_reward_mean | -0.0643    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 121        |
| stats_o/mean                   | 0.4459828  |
| stats_o/std                    | 0.033135   |
| test/episodes                  | 1220       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0287    |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.229     |
| test/Q                         | -0.6703671 |
| test/Q_plus_P                  | -0.6703671 |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 48800      |
| train/episodes                 | 4880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.603      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.012     |
| train/info_shaping_reward_mean | -0.0673    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.9      |
| train/steps                    | 195200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.44597116  |
| stats_o/std                    | 0.03307557  |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0309     |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.78084254 |
| test/Q_plus_P                  | -0.78084254 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.44596955  |
| stats_o/std                    | 0.033016916 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0325     |
| test/info_shaping_reward_mean  | -0.0588     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8073196  |
| test/Q_plus_P                  | -0.8073196  |
| test/reward_per_eps            | -7          |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0117     |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.44596395  |
| stats_o/std                    | 0.032959364 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0274     |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.68716544 |
| test/Q_plus_P                  | -0.68716544 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0123     |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.4459535   |
| stats_o/std                    | 0.032905202 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0325     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.7008182  |
| test/Q_plus_P                  | -0.7008182  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00946    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.4459431   |
| stats_o/std                    | 0.032852788 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0282     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.7736602  |
| test/Q_plus_P                  | -0.7736602  |
| test/reward_per_eps            | -7          |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00575    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.4459372   |
| stats_o/std                    | 0.032796238 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0342     |
| test/info_shaping_reward_mean  | -0.0587     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.83311486 |
| test/Q_plus_P                  | -0.83311486 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00954    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.4459255  |
| stats_o/std                    | 0.03275137 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0318    |
| test/info_shaping_reward_mean  | -0.0611    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -0.8868278 |
| test/Q_plus_P                  | -0.8868278 |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.64       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.012     |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.264     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.44591162  |
| stats_o/std                    | 0.03270201  |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0331     |
| test/info_shaping_reward_mean  | -0.0584     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.80966705 |
| test/Q_plus_P                  | -0.80966705 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0153     |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.44590747  |
| stats_o/std                    | 0.032653008 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0362     |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.7566022  |
| test/Q_plus_P                  | -0.7566022  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0168     |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.4458989  |
| stats_o/std                    | 0.03260738 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0239    |
| test/info_shaping_reward_mean  | -0.0531    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -0.7583717 |
| test/Q_plus_P                  | -0.7583717 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.67       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0122    |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.44589233  |
| stats_o/std                    | 0.032551683 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0349     |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.7987124  |
| test/Q_plus_P                  | -0.7987124  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0109     |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.44588685  |
| stats_o/std                    | 0.032497052 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0352     |
| test/info_shaping_reward_mean  | -0.0579     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.66356665 |
| test/Q_plus_P                  | -0.66356665 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00962    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.44588336  |
| stats_o/std                    | 0.03245386  |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0311     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.78432167 |
| test/Q_plus_P                  | -0.78432167 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0141     |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.44588062 |
| stats_o/std                    | 0.03240108 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0311    |
| test/info_shaping_reward_mean  | -0.0544    |
| test/info_shaping_reward_min   | -0.227     |
| test/Q                         | -0.6610605 |
| test/Q_plus_P                  | -0.6610605 |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.593      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0157    |
| train/info_shaping_reward_mean | -0.066     |
| train/info_shaping_reward_min  | -0.24      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -16.3      |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.44588175  |
| stats_o/std                    | 0.032349396 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0336     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.702243   |
| test/Q_plus_P                  | -0.702243   |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00993    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.44587973  |
| stats_o/std                    | 0.032294046 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0308     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.6859629  |
| test/Q_plus_P                  | -0.6859629  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00974    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.4458818  |
| stats_o/std                    | 0.03225698 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0336    |
| test/info_shaping_reward_mean  | -0.0589    |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -0.7581442 |
| test/Q_plus_P                  | -0.7581442 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.619      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0101    |
| train/info_shaping_reward_mean | -0.0667    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.2      |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 139         |
| stats_o/mean                   | 0.44587818  |
| stats_o/std                    | 0.03221211  |
| test/episodes                  | 1400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.036      |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.66194534 |
| test/Q_plus_P                  | -0.66194534 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 56000       |
| train/episodes                 | 5600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0135     |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 224000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.4458721   |
| stats_o/std                    | 0.032165032 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0324     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7185642  |
| test/Q_plus_P                  | -0.7185642  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0116     |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 141        |
| stats_o/mean                   | 0.44587198 |
| stats_o/std                    | 0.03211959 |
| test/episodes                  | 1420       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0378    |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.255     |
| test/Q                         | -0.6672251 |
| test/Q_plus_P                  | -0.6672251 |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 56800      |
| train/episodes                 | 5680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.616      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0156    |
| train/info_shaping_reward_mean | -0.065     |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.3      |
| train/steps                    | 227200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.44587252  |
| stats_o/std                    | 0.032071743 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0329     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.75247675 |
| test/Q_plus_P                  | -0.75247675 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0121     |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.44586802  |
| stats_o/std                    | 0.032028552 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0345     |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.7519963  |
| test/Q_plus_P                  | -0.7519963  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.44586983  |
| stats_o/std                    | 0.03198013  |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0168     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.75837046 |
| test/Q_plus_P                  | -0.75837046 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0135     |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.44586036  |
| stats_o/std                    | 0.03194     |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0329     |
| test/info_shaping_reward_mean  | -0.0587     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.80215424 |
| test/Q_plus_P                  | -0.80215424 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.4458653  |
| stats_o/std                    | 0.03189064 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0371    |
| test/info_shaping_reward_mean  | -0.0583    |
| test/info_shaping_reward_min   | -0.232     |
| test/Q                         | -0.7218203 |
| test/Q_plus_P                  | -0.7218203 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0129    |
| train/info_shaping_reward_mean | -0.063     |
| train/info_shaping_reward_min  | -0.242     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.44586277  |
| stats_o/std                    | 0.031845927 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0394     |
| test/info_shaping_reward_mean  | -0.0613     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.8257115  |
| test/Q_plus_P                  | -0.8257115  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0184     |
| train/info_shaping_reward_mean | -0.0653     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.44585964 |
| stats_o/std                    | 0.03180207 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0318    |
| test/info_shaping_reward_mean  | -0.0586    |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -0.859733  |
| test/Q_plus_P                  | -0.859733  |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.631      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0135    |
| train/info_shaping_reward_mean | -0.0651    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.8      |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.44585136 |
| stats_o/std                    | 0.03175916 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0309    |
| test/info_shaping_reward_mean  | -0.0568    |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -0.8198367 |
| test/Q_plus_P                  | -0.8198367 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.661      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0134    |
| train/info_shaping_reward_mean | -0.0629    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 150         |
| stats_o/mean                   | 0.44583705  |
| stats_o/std                    | 0.03172432  |
| test/episodes                  | 1510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.74129343 |
| test/Q_plus_P                  | -0.74129343 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 60400       |
| train/episodes                 | 6040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0137     |
| train/info_shaping_reward_mean | -0.0666     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 241600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.44583866  |
| stats_o/std                    | 0.031675924 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.792       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0215     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.8618026  |
| test/Q_plus_P                  | -0.8618026  |
| test/reward_per_eps            | -8.3        |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0131     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 152         |
| stats_o/mean                   | 0.44582692  |
| stats_o/std                    | 0.031638656 |
| test/episodes                  | 1530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0325     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.72745055 |
| test/Q_plus_P                  | -0.72745055 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 61200       |
| train/episodes                 | 6120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0117     |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 244800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 153        |
| stats_o/mean                   | 0.44582042 |
| stats_o/std                    | 0.03159633 |
| test/episodes                  | 1540       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.036     |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -0.8251123 |
| test/Q_plus_P                  | -0.8251123 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 61600      |
| train/episodes                 | 6160       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.671      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0118    |
| train/info_shaping_reward_mean | -0.0621    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 246400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.4458135  |
| stats_o/std                    | 0.03156266 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0337    |
| test/info_shaping_reward_mean  | -0.0566    |
| test/info_shaping_reward_min   | -0.234     |
| test/Q                         | -0.8445187 |
| test/Q_plus_P                  | -0.8445187 |
| test/reward_per_eps            | -7         |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0152    |
| train/info_shaping_reward_mean | -0.0656    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 155        |
| stats_o/mean                   | 0.44581404 |
| stats_o/std                    | 0.03152346 |
| test/episodes                  | 1560       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0232    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -0.7624876 |
| test/Q_plus_P                  | -0.7624876 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 62400      |
| train/episodes                 | 6240       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.61       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.013     |
| train/info_shaping_reward_mean | -0.0652    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 249600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 156         |
| stats_o/mean                   | 0.44581017  |
| stats_o/std                    | 0.031487826 |
| test/episodes                  | 1570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.033      |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.71772635 |
| test/Q_plus_P                  | -0.71772635 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 62800       |
| train/episodes                 | 6280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0127     |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 251200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.44580948  |
| stats_o/std                    | 0.031444598 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0365     |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.211      |
| test/Q                         | -0.6510903  |
| test/Q_plus_P                  | -0.6510903  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0138     |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.44580498  |
| stats_o/std                    | 0.03141112  |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0249     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.74595827 |
| test/Q_plus_P                  | -0.74595827 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0134     |
| train/info_shaping_reward_mean | -0.0655     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.44579354  |
| stats_o/std                    | 0.031379808 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0195     |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.7340019  |
| test/Q_plus_P                  | -0.7340019  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0138     |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 160         |
| stats_o/mean                   | 0.44579935  |
| stats_o/std                    | 0.031336594 |
| test/episodes                  | 1610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0336     |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.7681257  |
| test/Q_plus_P                  | -0.7681257  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 64400       |
| train/episodes                 | 6440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0155     |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 257600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.44579503  |
| stats_o/std                    | 0.031299274 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0237     |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.80754423 |
| test/Q_plus_P                  | -0.80754423 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0112     |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 162        |
| stats_o/mean                   | 0.4457948  |
| stats_o/std                    | 0.03126177 |
| test/episodes                  | 1630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.041     |
| test/info_shaping_reward_mean  | -0.0602    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -0.7394491 |
| test/Q_plus_P                  | -0.7394491 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 65200      |
| train/episodes                 | 6520       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0157    |
| train/info_shaping_reward_mean | -0.0621    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 260800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.4457934   |
| stats_o/std                    | 0.031225761 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0334     |
| test/info_shaping_reward_mean  | -0.0568     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7902928  |
| test/Q_plus_P                  | -0.7902928  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.44578502  |
| stats_o/std                    | 0.031184196 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.035      |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.80617815 |
| test/Q_plus_P                  | -0.80617815 |
| test/reward_per_eps            | -7          |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00982    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.44577786 |
| stats_o/std                    | 0.03114863 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0302    |
| test/info_shaping_reward_mean  | -0.0522    |
| test/info_shaping_reward_min   | -0.25      |
| test/Q                         | -0.6489814 |
| test/Q_plus_P                  | -0.6489814 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.662      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00749   |
| train/info_shaping_reward_mean | -0.0614    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.445771    |
| stats_o/std                    | 0.031116849 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0361     |
| test/info_shaping_reward_mean  | -0.0576     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.797106   |
| test/Q_plus_P                  | -0.797106   |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.44576153  |
| stats_o/std                    | 0.031085096 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.033      |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.7791488  |
| test/Q_plus_P                  | -0.7791488  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0129     |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.4457674   |
| stats_o/std                    | 0.031043245 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0327     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.8292111  |
| test/Q_plus_P                  | -0.8292111  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0138     |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.44576466 |
| stats_o/std                    | 0.03101112 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0286    |
| test/info_shaping_reward_mean  | -0.0538    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -0.7187264 |
| test/Q_plus_P                  | -0.7187264 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.641      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00931   |
| train/info_shaping_reward_mean | -0.0634    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.4457576   |
| stats_o/std                    | 0.030979248 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0325     |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.73293495 |
| test/Q_plus_P                  | -0.73293495 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0152     |
| train/info_shaping_reward_mean | -0.0642     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.44575298  |
| stats_o/std                    | 0.030942777 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.029      |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.7706056  |
| test/Q_plus_P                  | -0.7706056  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0108     |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 172         |
| stats_o/mean                   | 0.44575295  |
| stats_o/std                    | 0.030915266 |
| test/episodes                  | 1730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0326     |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.73755026 |
| test/Q_plus_P                  | -0.73755026 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 69200       |
| train/episodes                 | 6920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00874    |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 276800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.44574925  |
| stats_o/std                    | 0.030879907 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.72868896 |
| test/Q_plus_P                  | -0.72868896 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.618       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.44573998  |
| stats_o/std                    | 0.030850342 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0326     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7833791  |
| test/Q_plus_P                  | -0.7833791  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.71        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00698    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.6       |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.44573092  |
| stats_o/std                    | 0.030823378 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0324     |
| test/info_shaping_reward_mean  | -0.0582     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.83349454 |
| test/Q_plus_P                  | -0.83349454 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00908    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.4457289   |
| stats_o/std                    | 0.030786693 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.8294211  |
| test/Q_plus_P                  | -0.8294211  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.011      |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.44572774  |
| stats_o/std                    | 0.030755984 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0362     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.73033315 |
| test/Q_plus_P                  | -0.73033315 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.609       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0145     |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.6       |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.4457297   |
| stats_o/std                    | 0.030720664 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00636    |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.82116663 |
| test/Q_plus_P                  | -0.82116663 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0132     |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.44571838  |
| stats_o/std                    | 0.030691257 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0292     |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7668261  |
| test/Q_plus_P                  | -0.7668261  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.44571206  |
| stats_o/std                    | 0.030664451 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0358     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.70183617 |
| test/Q_plus_P                  | -0.70183617 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00959    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.44570717  |
| stats_o/std                    | 0.030637803 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0314     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.76268744 |
| test/Q_plus_P                  | -0.76268744 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0114     |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 182         |
| stats_o/mean                   | 0.44570172  |
| stats_o/std                    | 0.030607874 |
| test/episodes                  | 1830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0293     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.8590993  |
| test/Q_plus_P                  | -0.8590993  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 73200       |
| train/episodes                 | 7320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0123     |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 292800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.44569728  |
| stats_o/std                    | 0.030579096 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0294     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.73903537 |
| test/Q_plus_P                  | -0.73903537 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 184        |
| stats_o/mean                   | 0.44569418 |
| stats_o/std                    | 0.03054355 |
| test/episodes                  | 1850       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.815      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0289    |
| test/info_shaping_reward_mean  | -0.0567    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -0.8987419 |
| test/Q_plus_P                  | -0.8987419 |
| test/reward_per_eps            | -7.4       |
| test/steps                     | 74000      |
| train/episodes                 | 7400       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00954   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.226     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 296000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.44569325  |
| stats_o/std                    | 0.030513398 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.74526525 |
| test/Q_plus_P                  | -0.74526525 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.011      |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.4456894  |
| stats_o/std                    | 0.0304942  |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0289    |
| test/info_shaping_reward_mean  | -0.0543    |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -0.7690388 |
| test/Q_plus_P                  | -0.7690388 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00761   |
| train/info_shaping_reward_mean | -0.0638    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.445685    |
| stats_o/std                    | 0.030470645 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0303     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7572953  |
| test/Q_plus_P                  | -0.7572953  |
| test/reward_per_eps            | -7          |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0151     |
| train/info_shaping_reward_mean | -0.0659     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.445675    |
| stats_o/std                    | 0.03045355  |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0319     |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.76835525 |
| test/Q_plus_P                  | -0.76835525 |
| test/reward_per_eps            | -7          |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0122     |
| train/info_shaping_reward_mean | -0.0667     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.44567034  |
| stats_o/std                    | 0.030425863 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0251     |
| test/info_shaping_reward_mean  | -0.0569     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.8368827  |
| test/Q_plus_P                  | -0.8368827  |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.44566873  |
| stats_o/std                    | 0.0304017   |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0251     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.71385646 |
| test/Q_plus_P                  | -0.71385646 |
| test/reward_per_eps            | -7          |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00965    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.4456639  |
| stats_o/std                    | 0.03037736 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0316    |
| test/info_shaping_reward_mean  | -0.0577    |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -0.8282089 |
| test/Q_plus_P                  | -0.8282089 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.014     |
| train/info_shaping_reward_mean | -0.0631    |
| train/info_shaping_reward_min  | -0.246     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.44565928  |
| stats_o/std                    | 0.030352237 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0279     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.206      |
| test/Q                         | -0.6766197  |
| test/Q_plus_P                  | -0.6766197  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00623    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 193        |
| stats_o/mean                   | 0.44565675 |
| stats_o/std                    | 0.03032702 |
| test/episodes                  | 1940       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0319    |
| test/info_shaping_reward_mean  | -0.0572    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -0.8226193 |
| test/Q_plus_P                  | -0.8226193 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 77600      |
| train/episodes                 | 7760       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.648      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00746   |
| train/info_shaping_reward_mean | -0.0658    |
| train/info_shaping_reward_min  | -0.268     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 310400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 194         |
| stats_o/mean                   | 0.44565046  |
| stats_o/std                    | 0.030300213 |
| test/episodes                  | 1950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0316     |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.8156082  |
| test/Q_plus_P                  | -0.8156082  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 78000       |
| train/episodes                 | 7800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.01       |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 312000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.4456469   |
| stats_o/std                    | 0.030274024 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0333     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.7035724  |
| test/Q_plus_P                  | -0.7035724  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.604       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0133     |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 196         |
| stats_o/mean                   | 0.44563603  |
| stats_o/std                    | 0.030251404 |
| test/episodes                  | 1970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0234     |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.8046474  |
| test/Q_plus_P                  | -0.8046474  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 78800       |
| train/episodes                 | 7880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0118     |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 315200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.44562662  |
| stats_o/std                    | 0.030230192 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0349     |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.6970469  |
| test/Q_plus_P                  | -0.6970469  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0113     |
| train/info_shaping_reward_mean | -0.0635     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.4456248   |
| stats_o/std                    | 0.030202625 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0351     |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.7458039  |
| test/Q_plus_P                  | -0.7458039  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0136     |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.44562244  |
| stats_o/std                    | 0.030181363 |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0368     |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.74139136 |
| test/Q_plus_P                  | -0.74139136 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.013      |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.44561648  |
| stats_o/std                    | 0.030163476 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0339     |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.7761692  |
| test/Q_plus_P                  | -0.7761692  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0134     |
| train/info_shaping_reward_mean | -0.066      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 201         |
| stats_o/mean                   | 0.445613    |
| stats_o/std                    | 0.030137697 |
| test/episodes                  | 2020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0315     |
| test/info_shaping_reward_mean  | -0.057      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.792962   |
| test/Q_plus_P                  | -0.792962   |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 80800       |
| train/episodes                 | 8080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.012      |
| train/info_shaping_reward_mean | -0.0647     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 323200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.4456142   |
| stats_o/std                    | 0.030110257 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0285     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7368332  |
| test/Q_plus_P                  | -0.7368332  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0125     |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.44561103  |
| stats_o/std                    | 0.030086562 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0301     |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.8038134  |
| test/Q_plus_P                  | -0.8038134  |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.624       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00928    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.44560984  |
| stats_o/std                    | 0.0300638   |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0326     |
| test/info_shaping_reward_mean  | -0.0581     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.79515237 |
| test/Q_plus_P                  | -0.79515237 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0141     |
| train/info_shaping_reward_mean | -0.0658     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.44560817  |
| stats_o/std                    | 0.030036082 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0304     |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.85532534 |
| test/Q_plus_P                  | -0.85532534 |
| test/reward_per_eps            | -8          |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.44560447  |
| stats_o/std                    | 0.030011848 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0293     |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.7122375  |
| test/Q_plus_P                  | -0.7122375  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.4455969  |
| stats_o/std                    | 0.02999037 |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.82       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0339    |
| test/info_shaping_reward_mean  | -0.0584    |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -0.7964943 |
| test/Q_plus_P                  | -0.7964943 |
| test/reward_per_eps            | -7.2       |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.672      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00982   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.1      |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.4455938   |
| stats_o/std                    | 0.029966922 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0285     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.7514468  |
| test/Q_plus_P                  | -0.7514468  |
| test/reward_per_eps            | -7          |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0136     |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.44558802  |
| stats_o/std                    | 0.029943258 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0297     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.73882383 |
| test/Q_plus_P                  | -0.73882383 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0111     |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.44558367  |
| stats_o/std                    | 0.029921437 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0261     |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.6830844  |
| test/Q_plus_P                  | -0.6830844  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.4455757   |
| stats_o/std                    | 0.029900886 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0327     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.7599553  |
| test/Q_plus_P                  | -0.7599553  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.44557726  |
| stats_o/std                    | 0.029883213 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0308     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7506903  |
| test/Q_plus_P                  | -0.7506903  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00755    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.445572    |
| stats_o/std                    | 0.029863475 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0327     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.7285087  |
| test/Q_plus_P                  | -0.7285087  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0121     |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.4455713   |
| stats_o/std                    | 0.029842442 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.81612325 |
| test/Q_plus_P                  | -0.81612325 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00627    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.44556525  |
| stats_o/std                    | 0.029822929 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0289     |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.87595975 |
| test/Q_plus_P                  | -0.87595975 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0145     |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.4455589  |
| stats_o/std                    | 0.02980391 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.83       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0344    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.7508841 |
| test/Q_plus_P                  | -0.7508841 |
| test/reward_per_eps            | -6.8       |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00731   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.44555274  |
| stats_o/std                    | 0.029784337 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0301     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.71764255 |
| test/Q_plus_P                  | -0.71764255 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00922    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.44555044  |
| stats_o/std                    | 0.029758165 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0313     |
| test/info_shaping_reward_mean  | -0.0589     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.885112   |
| test/Q_plus_P                  | -0.885112   |
| test/reward_per_eps            | -8          |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00767    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.44554815  |
| stats_o/std                    | 0.029737974 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0232     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.7192364  |
| test/Q_plus_P                  | -0.7192364  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.44554138  |
| stats_o/std                    | 0.02972026  |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0363     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.80410933 |
| test/Q_plus_P                  | -0.80410933 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.44552624  |
| stats_o/std                    | 0.02970978  |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0262     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.74620354 |
| test/Q_plus_P                  | -0.74620354 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00837    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 222         |
| stats_o/mean                   | 0.4455203   |
| stats_o/std                    | 0.02969826  |
| test/episodes                  | 2230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0291     |
| test/info_shaping_reward_mean  | -0.0558     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.85525763 |
| test/Q_plus_P                  | -0.85525763 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 89200       |
| train/episodes                 | 8920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00796    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 356800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.44551373  |
| stats_o/std                    | 0.029683009 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0233     |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7566826  |
| test/Q_plus_P                  | -0.7566826  |
| test/reward_per_eps            | -7          |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00606    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 224         |
| stats_o/mean                   | 0.44550785  |
| stats_o/std                    | 0.029662235 |
| test/episodes                  | 2250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0321     |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.76251495 |
| test/Q_plus_P                  | -0.76251495 |
| test/reward_per_eps            | -7          |
| test/steps                     | 90000       |
| train/episodes                 | 9000        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 360000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.4455097   |
| stats_o/std                    | 0.029642416 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0312     |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7372353  |
| test/Q_plus_P                  | -0.7372353  |
| test/reward_per_eps            | -7          |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00764    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.44550416  |
| stats_o/std                    | 0.029620113 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7471087  |
| test/Q_plus_P                  | -0.7471087  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00798    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.4455012   |
| stats_o/std                    | 0.029602481 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0303     |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.8051027  |
| test/Q_plus_P                  | -0.8051027  |
| test/reward_per_eps            | -7          |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00586    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.44549945  |
| stats_o/std                    | 0.029577693 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.802       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0249     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.87305033 |
| test/Q_plus_P                  | -0.87305033 |
| test/reward_per_eps            | -7.9        |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.011      |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.44549537  |
| stats_o/std                    | 0.029556675 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0252     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -0.8019123  |
| test/Q_plus_P                  | -0.8019123  |
| test/reward_per_eps            | -7          |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.635       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0117     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.4454899   |
| stats_o/std                    | 0.029537685 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.8110392  |
| test/Q_plus_P                  | -0.8110392  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00837    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.4454886   |
| stats_o/std                    | 0.029518375 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.0521     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.86400527 |
| test/Q_plus_P                  | -0.86400527 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00824    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.44548604  |
| stats_o/std                    | 0.029502748 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.85        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0349     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.6350545  |
| test/Q_plus_P                  | -0.6350545  |
| test/reward_per_eps            | -6          |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.01       |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.44548476  |
| stats_o/std                    | 0.029483385 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0334     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.7515156  |
| test/Q_plus_P                  | -0.7515156  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 234         |
| stats_o/mean                   | 0.44547832  |
| stats_o/std                    | 0.029465357 |
| test/episodes                  | 2350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0278     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.76926583 |
| test/Q_plus_P                  | -0.76926583 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 94000       |
| train/episodes                 | 9400        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0111     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 376000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.4454782   |
| stats_o/std                    | 0.029444942 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0334     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.73645484 |
| test/Q_plus_P                  | -0.73645484 |
| test/reward_per_eps            | -7          |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 236        |
| stats_o/mean                   | 0.44547585 |
| stats_o/std                    | 0.02943069 |
| test/episodes                  | 2370       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0282    |
| test/info_shaping_reward_mean  | -0.0526    |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -0.7384804 |
| test/Q_plus_P                  | -0.7384804 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 94800      |
| train/episodes                 | 9480       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.653      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00941   |
| train/info_shaping_reward_mean | -0.0628    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 379200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.44547346  |
| stats_o/std                    | 0.029409772 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0354     |
| test/info_shaping_reward_mean  | -0.0575     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.854832   |
| test/Q_plus_P                  | -0.854832   |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.44546783  |
| stats_o/std                    | 0.029392472 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0198     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.84187365 |
| test/Q_plus_P                  | -0.84187365 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00868    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.445465    |
| stats_o/std                    | 0.029376924 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0287     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.8674523  |
| test/Q_plus_P                  | -0.8674523  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00646    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.4454619   |
| stats_o/std                    | 0.029360719 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0265     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.79713666 |
| test/Q_plus_P                  | -0.79713666 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0123     |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.44545218  |
| stats_o/std                    | 0.029349098 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0152     |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.68132186 |
| test/Q_plus_P                  | -0.68132186 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00782    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.273      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.4454503  |
| stats_o/std                    | 0.02932807 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.802      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0293    |
| test/info_shaping_reward_mean  | -0.0553    |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -0.830346  |
| test/Q_plus_P                  | -0.830346  |
| test/reward_per_eps            | -7.9       |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.66       |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00876   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.44544467  |
| stats_o/std                    | 0.029313758 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0275     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.7913771  |
| test/Q_plus_P                  | -0.7913771  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00613    |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 244         |
| stats_o/mean                   | 0.4454434   |
| stats_o/std                    | 0.029299317 |
| test/episodes                  | 2450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0327     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.7253671  |
| test/Q_plus_P                  | -0.7253671  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 98000       |
| train/episodes                 | 9800        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 392000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.44544363  |
| stats_o/std                    | 0.029283024 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.8459079  |
| test/Q_plus_P                  | -0.8459079  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.011      |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.44543755  |
| stats_o/std                    | 0.029268228 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0363     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.76248765 |
| test/Q_plus_P                  | -0.76248765 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.44543353  |
| stats_o/std                    | 0.02924975  |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0314     |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.77178156 |
| test/Q_plus_P                  | -0.77178156 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0094     |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.4454296   |
| stats_o/std                    | 0.02923735  |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0304     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.86394143 |
| test/Q_plus_P                  | -0.86394143 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0126     |
| train/info_shaping_reward_mean | -0.0657     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.4454228   |
| stats_o/std                    | 0.029223979 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.7168152  |
| test/Q_plus_P                  | -0.7168152  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00887    |
| train/info_shaping_reward_mean | -0.0654     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.4454188   |
| stats_o/std                    | 0.029205883 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0314     |
| test/info_shaping_reward_mean  | -0.059      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.83223456 |
| test/Q_plus_P                  | -0.83223456 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0121     |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 251         |
| stats_o/mean                   | 0.4454173   |
| stats_o/std                    | 0.029186701 |
| test/episodes                  | 2520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0294     |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.65119153 |
| test/Q_plus_P                  | -0.65119153 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 100800      |
| train/episodes                 | 10080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 403200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.44541165  |
| stats_o/std                    | 0.0291719   |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0308     |
| test/info_shaping_reward_mean  | -0.0577     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.88207936 |
| test/Q_plus_P                  | -0.88207936 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.01       |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.44540796  |
| stats_o/std                    | 0.029157124 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0326     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.802897   |
| test/Q_plus_P                  | -0.802897   |
| test/reward_per_eps            | -7          |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.4454036   |
| stats_o/std                    | 0.029144036 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0332     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.7265192  |
| test/Q_plus_P                  | -0.7265192  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00963    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.44540122  |
| stats_o/std                    | 0.029125452 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0295     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.83486265 |
| test/Q_plus_P                  | -0.83486265 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0126     |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.4454029   |
| stats_o/std                    | 0.02910824  |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.73431313 |
| test/Q_plus_P                  | -0.73431313 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0149     |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.4454023   |
| stats_o/std                    | 0.029091954 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0307     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.7756982  |
| test/Q_plus_P                  | -0.7756982  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0126     |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.44540063  |
| stats_o/std                    | 0.029074343 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0348     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.6783135  |
| test/Q_plus_P                  | -0.6783135  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 259        |
| stats_o/mean                   | 0.44539365 |
| stats_o/std                    | 0.02906513 |
| test/episodes                  | 2600       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0318    |
| test/info_shaping_reward_mean  | -0.0519    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -0.6896081 |
| test/Q_plus_P                  | -0.6896081 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 104000     |
| train/episodes                 | 10400      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.636      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00804   |
| train/info_shaping_reward_mean | -0.0639    |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.6      |
| train/steps                    | 416000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.44539174  |
| stats_o/std                    | 0.029049205 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0317     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.77053344 |
| test/Q_plus_P                  | -0.77053344 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0144     |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.44538963  |
| stats_o/std                    | 0.029036624 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0261     |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.74803686 |
| test/Q_plus_P                  | -0.74803686 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00706    |
| train/info_shaping_reward_mean | -0.063      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.44538817  |
| stats_o/std                    | 0.029021649 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7241706  |
| test/Q_plus_P                  | -0.7241706  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.4453828   |
| stats_o/std                    | 0.029006975 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0309     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.81970733 |
| test/Q_plus_P                  | -0.81970733 |
| test/reward_per_eps            | -7          |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.01       |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.44537786  |
| stats_o/std                    | 0.028996466 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0305     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.74519753 |
| test/Q_plus_P                  | -0.74519753 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0651     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 265         |
| stats_o/mean                   | 0.44537482  |
| stats_o/std                    | 0.028981827 |
| test/episodes                  | 2660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0319     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7835232  |
| test/Q_plus_P                  | -0.7835232  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 106400      |
| train/episodes                 | 10640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00797    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 425600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.4453716   |
| stats_o/std                    | 0.028967785 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.73015404 |
| test/Q_plus_P                  | -0.73015404 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00905    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.44536805  |
| stats_o/std                    | 0.028953867 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0319     |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.8300972  |
| test/Q_plus_P                  | -0.8300972  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00803    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.44536713  |
| stats_o/std                    | 0.028942712 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0271     |
| test/info_shaping_reward_mean  | -0.058      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.8405536  |
| test/Q_plus_P                  | -0.8405536  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0128     |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.44536427  |
| stats_o/std                    | 0.028931133 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0296     |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.7141809  |
| test/Q_plus_P                  | -0.7141809  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0131     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.4453607   |
| stats_o/std                    | 0.028920792 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0339     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.215      |
| test/Q                         | -0.7158039  |
| test/Q_plus_P                  | -0.7158039  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0106     |
| train/info_shaping_reward_mean | -0.065      |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.44535998  |
| stats_o/std                    | 0.028904783 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0315     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.8666465  |
| test/Q_plus_P                  | -0.8666465  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0121     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 272        |
| stats_o/mean                   | 0.4453609  |
| stats_o/std                    | 0.02888868 |
| test/episodes                  | 2730       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.845      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0239    |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -0.6836624 |
| test/Q_plus_P                  | -0.6836624 |
| test/reward_per_eps            | -6.2       |
| test/steps                     | 109200     |
| train/episodes                 | 10920      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.671      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0132    |
| train/info_shaping_reward_mean | -0.0626    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 436800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.44536063  |
| stats_o/std                    | 0.028874286 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0244     |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7698656  |
| test/Q_plus_P                  | -0.7698656  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.445357    |
| stats_o/std                    | 0.028858634 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.70297945 |
| test/Q_plus_P                  | -0.70297945 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00811    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.44535515  |
| stats_o/std                    | 0.028845193 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.023      |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.84446996 |
| test/Q_plus_P                  | -0.84446996 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0108     |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 276         |
| stats_o/mean                   | 0.44535267  |
| stats_o/std                    | 0.028834129 |
| test/episodes                  | 2770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0286     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.77984613 |
| test/Q_plus_P                  | -0.77984613 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 110800      |
| train/episodes                 | 11080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00984    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 443200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.44535157  |
| stats_o/std                    | 0.028821172 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0294     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7541773  |
| test/Q_plus_P                  | -0.7541773  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00585    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.44534716  |
| stats_o/std                    | 0.028807731 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0317     |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.9216267  |
| test/Q_plus_P                  | -0.9216267  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0126     |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.44534755  |
| stats_o/std                    | 0.028794317 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0265     |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.6819174  |
| test/Q_plus_P                  | -0.6819174  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.44534183  |
| stats_o/std                    | 0.028783137 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0312     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.7521603  |
| test/Q_plus_P                  | -0.7521603  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0118     |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.445341    |
| stats_o/std                    | 0.028767714 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0312     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.7945279  |
| test/Q_plus_P                  | -0.7945279  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00855    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.4453403   |
| stats_o/std                    | 0.028754681 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0324     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.742821   |
| test/Q_plus_P                  | -0.742821   |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 283         |
| stats_o/mean                   | 0.445343    |
| stats_o/std                    | 0.028739078 |
| test/episodes                  | 2840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0337     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.71591973 |
| test/Q_plus_P                  | -0.71591973 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 113600      |
| train/episodes                 | 11360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00803    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 454400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.44533885  |
| stats_o/std                    | 0.028727254 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0268     |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.70495546 |
| test/Q_plus_P                  | -0.70495546 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00814    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 285        |
| stats_o/mean                   | 0.4453361  |
| stats_o/std                    | 0.0287166  |
| test/episodes                  | 2860       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0246    |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -0.8188884 |
| test/Q_plus_P                  | -0.8188884 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 114400     |
| train/episodes                 | 11440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.654      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00754   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.245     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.8      |
| train/steps                    | 457600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.44533312  |
| stats_o/std                    | 0.028704636 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0276     |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.71620834 |
| test/Q_plus_P                  | -0.71620834 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00778    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.4453344   |
| stats_o/std                    | 0.028690396 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0249     |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.71159995 |
| test/Q_plus_P                  | -0.71159995 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00764    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.44532752  |
| stats_o/std                    | 0.028677186 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0282     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.69671875 |
| test/Q_plus_P                  | -0.69671875 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0061     |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.44532034  |
| stats_o/std                    | 0.028669847 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.8579499  |
| test/Q_plus_P                  | -0.8579499  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.628       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00719    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.4453192   |
| stats_o/std                    | 0.028652323 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.69474876 |
| test/Q_plus_P                  | -0.69474876 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00909    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.44531444  |
| stats_o/std                    | 0.028644452 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.026      |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -0.7757005  |
| test/Q_plus_P                  | -0.7757005  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0139     |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.4453086   |
| stats_o/std                    | 0.02863243  |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0267     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.75510806 |
| test/Q_plus_P                  | -0.75510806 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.011      |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.44530657  |
| stats_o/std                    | 0.028620057 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0258     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.761327   |
| test/Q_plus_P                  | -0.761327   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00864    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.44530413  |
| stats_o/std                    | 0.028606035 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0329     |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.86608666 |
| test/Q_plus_P                  | -0.86608666 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00724    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.4453043   |
| stats_o/std                    | 0.028590992 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0293     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.7502782  |
| test/Q_plus_P                  | -0.7502782  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00712    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 296        |
| stats_o/mean                   | 0.44530272 |
| stats_o/std                    | 0.02857931 |
| test/episodes                  | 2970       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.032     |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.235     |
| test/Q                         | -0.7170233 |
| test/Q_plus_P                  | -0.7170233 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 118800     |
| train/episodes                 | 11880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.632      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00706   |
| train/info_shaping_reward_mean | -0.0617    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 475200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.44529986  |
| stats_o/std                    | 0.028572837 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0294     |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7716242  |
| test/Q_plus_P                  | -0.7716242  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00665    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.267      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.44529697  |
| stats_o/std                    | 0.028559139 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0252     |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.7878963  |
| test/Q_plus_P                  | -0.7878963  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00842    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.44529638  |
| stats_o/std                    | 0.028549189 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0248     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.8423544  |
| test/Q_plus_P                  | -0.8423544  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00808    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.44529533  |
| stats_o/std                    | 0.028538017 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0224     |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.7173735  |
| test/Q_plus_P                  | -0.7173735  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00714    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.4452931   |
| stats_o/std                    | 0.028529273 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0314     |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.7522242  |
| test/Q_plus_P                  | -0.7522242  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00775    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.44528887  |
| stats_o/std                    | 0.028519055 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0222     |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.8188246  |
| test/Q_plus_P                  | -0.8188246  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00928    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.44528338  |
| stats_o/std                    | 0.028513914 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0266     |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.7526515  |
| test/Q_plus_P                  | -0.7526515  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00966    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.44527814  |
| stats_o/std                    | 0.028503159 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0218     |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.8430652  |
| test/Q_plus_P                  | -0.8430652  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 305         |
| stats_o/mean                   | 0.4452716   |
| stats_o/std                    | 0.028493049 |
| test/episodes                  | 3060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0267     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7633092  |
| test/Q_plus_P                  | -0.7633092  |
| test/reward_per_eps            | -7          |
| test/steps                     | 122400      |
| train/episodes                 | 12240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.012      |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 489600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.44527152 |
| stats_o/std                    | 0.02848193 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0283    |
| test/info_shaping_reward_mean  | -0.0509    |
| test/info_shaping_reward_min   | -0.237     |
| test/Q                         | -0.7504994 |
| test/Q_plus_P                  | -0.7504994 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.623      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0073    |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.1      |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.44527003  |
| stats_o/std                    | 0.028470619 |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0275     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.7802298  |
| test/Q_plus_P                  | -0.7802298  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.44526646  |
| stats_o/std                    | 0.028465612 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0238     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -0.8359118  |
| test/Q_plus_P                  | -0.8359118  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00972    |
| train/info_shaping_reward_mean | -0.0646     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.44526318  |
| stats_o/std                    | 0.028456524 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0185     |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.8125932  |
| test/Q_plus_P                  | -0.8125932  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00617    |
| train/info_shaping_reward_mean | -0.0578     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.44526184  |
| stats_o/std                    | 0.02845168  |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.76767385 |
| test/Q_plus_P                  | -0.76767385 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00615    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.268      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.44526112  |
| stats_o/std                    | 0.028439887 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0254     |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.7646856  |
| test/Q_plus_P                  | -0.7646856  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00661    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.44526026  |
| stats_o/std                    | 0.028428873 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0293     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.76902527 |
| test/Q_plus_P                  | -0.76902527 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.44525692  |
| stats_o/std                    | 0.028419489 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0189     |
| test/info_shaping_reward_mean  | -0.0474     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.8139294  |
| test/Q_plus_P                  | -0.8139294  |
| test/reward_per_eps            | -7          |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00673    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 314        |
| stats_o/mean                   | 0.44525483 |
| stats_o/std                    | 0.02841002 |
| test/episodes                  | 3150       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0237    |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -0.7977876 |
| test/Q_plus_P                  | -0.7977876 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 126000     |
| train/episodes                 | 12600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.638      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00651   |
| train/info_shaping_reward_mean | -0.0619    |
| train/info_shaping_reward_min  | -0.259     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.5      |
| train/steps                    | 504000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.44525468  |
| stats_o/std                    | 0.028395234 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0287     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.8216531  |
| test/Q_plus_P                  | -0.8216531  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.684       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00859    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 316        |
| stats_o/mean                   | 0.44525647 |
| stats_o/std                    | 0.02838125 |
| test/episodes                  | 3170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0215    |
| test/info_shaping_reward_mean  | -0.0501    |
| test/info_shaping_reward_min   | -0.225     |
| test/Q                         | -0.7926638 |
| test/Q_plus_P                  | -0.7926638 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 126800     |
| train/episodes                 | 12680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.645      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00918   |
| train/info_shaping_reward_mean | -0.0616    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 507200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.44525412  |
| stats_o/std                    | 0.028375639 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.021      |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.7645974  |
| test/Q_plus_P                  | -0.7645974  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00607    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.4452523   |
| stats_o/std                    | 0.028365312 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0216     |
| test/info_shaping_reward_mean  | -0.0464     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.7306145  |
| test/Q_plus_P                  | -0.7306145  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00577    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 319        |
| stats_o/mean                   | 0.44525066 |
| stats_o/std                    | 0.02835855 |
| test/episodes                  | 3200       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.026     |
| test/info_shaping_reward_mean  | -0.0547    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -0.8538949 |
| test/Q_plus_P                  | -0.8538949 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 128000     |
| train/episodes                 | 12800      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.643      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.011     |
| train/info_shaping_reward_mean | -0.0646    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.3      |
| train/steps                    | 512000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.445248    |
| stats_o/std                    | 0.028347306 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0295     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.69639635 |
| test/Q_plus_P                  | -0.69639635 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00889    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.4452462   |
| stats_o/std                    | 0.028340088 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0228     |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.7871141  |
| test/Q_plus_P                  | -0.7871141  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00757    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.44524047  |
| stats_o/std                    | 0.028330857 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0318     |
| test/info_shaping_reward_mean  | -0.0567     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.8202626  |
| test/Q_plus_P                  | -0.8202626  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0067     |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 323        |
| stats_o/mean                   | 0.44523808 |
| stats_o/std                    | 0.02831955 |
| test/episodes                  | 3240       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.818      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.024     |
| test/info_shaping_reward_mean  | -0.0494    |
| test/info_shaping_reward_min   | -0.244     |
| test/Q                         | -0.7937243 |
| test/Q_plus_P                  | -0.7937243 |
| test/reward_per_eps            | -7.3       |
| test/steps                     | 129600     |
| train/episodes                 | 12960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.664      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00942   |
| train/info_shaping_reward_mean | -0.0586    |
| train/info_shaping_reward_min  | -0.229     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.4      |
| train/steps                    | 518400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.44523206  |
| stats_o/std                    | 0.028313043 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0266     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.72014624 |
| test/Q_plus_P                  | -0.72014624 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00685    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.44523248  |
| stats_o/std                    | 0.028303156 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.23       |
| test/Q                         | -0.8518538  |
| test/Q_plus_P                  | -0.8518538  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00623    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.44523183  |
| stats_o/std                    | 0.028293783 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0175     |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.852066   |
| test/Q_plus_P                  | -0.852066   |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00746    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.44523156  |
| stats_o/std                    | 0.028285412 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0185     |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.7213362  |
| test/Q_plus_P                  | -0.7213362  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00665    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.4452326   |
| stats_o/std                    | 0.028276483 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.802963   |
| test/Q_plus_P                  | -0.802963   |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.008      |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.44523022 |
| stats_o/std                    | 0.02826807 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.019     |
| test/info_shaping_reward_mean  | -0.0466    |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -0.8159978 |
| test/Q_plus_P                  | -0.8159978 |
| test/reward_per_eps            | -7         |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.679      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00762   |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.26      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.44522974  |
| stats_o/std                    | 0.028255342 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0224     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.84540755 |
| test/Q_plus_P                  | -0.84540755 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00745    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.44522688  |
| stats_o/std                    | 0.028247135 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0258     |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.77624696 |
| test/Q_plus_P                  | -0.77624696 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0085     |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.44522238  |
| stats_o/std                    | 0.028238127 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0156     |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.7547289  |
| test/Q_plus_P                  | -0.7547289  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00548    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.44522393  |
| stats_o/std                    | 0.028225385 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0253     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.8456036  |
| test/Q_plus_P                  | -0.8456036  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00642    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.44522157  |
| stats_o/std                    | 0.028215693 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0232     |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.8426641  |
| test/Q_plus_P                  | -0.8426641  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00636    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.44521675 |
| stats_o/std                    | 0.02820613 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0265    |
| test/info_shaping_reward_mean  | -0.0536    |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -0.8472499 |
| test/Q_plus_P                  | -0.8472499 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.684      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00641   |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.6      |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.44521722  |
| stats_o/std                    | 0.028198197 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0227     |
| test/info_shaping_reward_mean  | -0.0478     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.6805106  |
| test/Q_plus_P                  | -0.6805106  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0096     |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.44521734  |
| stats_o/std                    | 0.028190397 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0257     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.7674064  |
| test/Q_plus_P                  | -0.7674064  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00773    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 338         |
| stats_o/mean                   | 0.44521502  |
| stats_o/std                    | 0.028183237 |
| test/episodes                  | 3390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0263     |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.241      |
| test/Q                         | -0.7435507  |
| test/Q_plus_P                  | -0.7435507  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 135600      |
| train/episodes                 | 13560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00646    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 542400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 339         |
| stats_o/mean                   | 0.44521058  |
| stats_o/std                    | 0.028175794 |
| test/episodes                  | 3400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0297     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.82745993 |
| test/Q_plus_P                  | -0.82745993 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 136000      |
| train/episodes                 | 13600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00943    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 544000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.44521114  |
| stats_o/std                    | 0.028170066 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0315     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.73047227 |
| test/Q_plus_P                  | -0.73047227 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00694    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 341         |
| stats_o/mean                   | 0.44521102  |
| stats_o/std                    | 0.028158836 |
| test/episodes                  | 3420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0293     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7677173  |
| test/Q_plus_P                  | -0.7677173  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 136800      |
| train/episodes                 | 13680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 547200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.44520614  |
| stats_o/std                    | 0.028153108 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0282     |
| test/info_shaping_reward_mean  | -0.0514     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.7248471  |
| test/Q_plus_P                  | -0.7248471  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00779    |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 343        |
| stats_o/mean                   | 0.44520068 |
| stats_o/std                    | 0.02814568 |
| test/episodes                  | 3440       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.00915   |
| test/info_shaping_reward_mean  | -0.0452    |
| test/info_shaping_reward_min   | -0.228     |
| test/Q                         | -0.7315281 |
| test/Q_plus_P                  | -0.7315281 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 137600     |
| train/episodes                 | 13760      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00638   |
| train/info_shaping_reward_mean | -0.0587    |
| train/info_shaping_reward_min  | -0.258     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 550400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.44520235  |
| stats_o/std                    | 0.028134957 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0271     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.8471342  |
| test/Q_plus_P                  | -0.8471342  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00756    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.44520262  |
| stats_o/std                    | 0.028128013 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0256     |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7440698  |
| test/Q_plus_P                  | -0.7440698  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00739    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.44520092  |
| stats_o/std                    | 0.028118992 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0206     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7841587  |
| test/Q_plus_P                  | -0.7841587  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00784    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 347         |
| stats_o/mean                   | 0.4451972   |
| stats_o/std                    | 0.028115371 |
| test/episodes                  | 3480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0254     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.8068207  |
| test/Q_plus_P                  | -0.8068207  |
| test/reward_per_eps            | -7          |
| test/steps                     | 139200      |
| train/episodes                 | 13920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0086     |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 556800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.44519567  |
| stats_o/std                    | 0.028104804 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0213     |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.8472848  |
| test/Q_plus_P                  | -0.8472848  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00583    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.224      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 349         |
| stats_o/mean                   | 0.44519415  |
| stats_o/std                    | 0.028096626 |
| test/episodes                  | 3500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0234     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.9448662  |
| test/Q_plus_P                  | -0.9448662  |
| test/reward_per_eps            | -8          |
| test/steps                     | 140000      |
| train/episodes                 | 14000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0077     |
| train/info_shaping_reward_mean | -0.0641     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 560000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.4451923   |
| stats_o/std                    | 0.028090818 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00949    |
| test/info_shaping_reward_mean  | -0.0425     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.78265    |
| test/Q_plus_P                  | -0.78265    |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0088     |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.44519112  |
| stats_o/std                    | 0.028080856 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0183     |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.7081811  |
| test/Q_plus_P                  | -0.7081811  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00817    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.44519225  |
| stats_o/std                    | 0.028074212 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0267     |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.69398355 |
| test/Q_plus_P                  | -0.69398355 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00902    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.44518754  |
| stats_o/std                    | 0.028062128 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0289     |
| test/info_shaping_reward_mean  | -0.053      |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.7727931  |
| test/Q_plus_P                  | -0.7727931  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00852    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.44518435  |
| stats_o/std                    | 0.028053984 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0131     |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.7736156  |
| test/Q_plus_P                  | -0.7736156  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.689       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00775    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.44518328  |
| stats_o/std                    | 0.028050633 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.8681916  |
| test/Q_plus_P                  | -0.8681916  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.593       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0063     |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -16.3       |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.44518164  |
| stats_o/std                    | 0.028041624 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0247     |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.8255354  |
| test/Q_plus_P                  | -0.8255354  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.44517884  |
| stats_o/std                    | 0.028033668 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0197     |
| test/info_shaping_reward_mean  | -0.0485     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.8127003  |
| test/Q_plus_P                  | -0.8127003  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00792    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.44517824  |
| stats_o/std                    | 0.028022839 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.0437     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.6969337  |
| test/Q_plus_P                  | -0.6969337  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.68        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0052     |
| train/info_shaping_reward_mean | -0.0544     |
| train/info_shaping_reward_min  | -0.228      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.44517684  |
| stats_o/std                    | 0.028016292 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0209     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.8007965  |
| test/Q_plus_P                  | -0.8007965  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00733    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 360         |
| stats_o/mean                   | 0.4451785   |
| stats_o/std                    | 0.028008558 |
| test/episodes                  | 3610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0263     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.9135879  |
| test/Q_plus_P                  | -0.9135879  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 144400      |
| train/episodes                 | 14440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 577600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.44517636  |
| stats_o/std                    | 0.027999528 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.022      |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.73308253 |
| test/Q_plus_P                  | -0.73308253 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0086     |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 362        |
| stats_o/mean                   | 0.44517374 |
| stats_o/std                    | 0.02799358 |
| test/episodes                  | 3630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.812      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0184    |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -0.8840297 |
| test/Q_plus_P                  | -0.8840297 |
| test/reward_per_eps            | -7.5       |
| test/steps                     | 145200     |
| train/episodes                 | 14520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00937   |
| train/info_shaping_reward_mean | -0.0608    |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 580800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.44517326  |
| stats_o/std                    | 0.027984088 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0177     |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.86056006 |
| test/Q_plus_P                  | -0.86056006 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0083     |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.44516996  |
| stats_o/std                    | 0.027979406 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0186     |
| test/info_shaping_reward_mean  | -0.0461     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.6891167  |
| test/Q_plus_P                  | -0.6891167  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00742    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.44516575  |
| stats_o/std                    | 0.02797073  |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0277     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.79452616 |
| test/Q_plus_P                  | -0.79452616 |
| test/reward_per_eps            | -7          |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.44516537  |
| stats_o/std                    | 0.027963916 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.025      |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.7314786  |
| test/Q_plus_P                  | -0.7314786  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00545    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.27       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 367         |
| stats_o/mean                   | 0.44516256  |
| stats_o/std                    | 0.027958678 |
| test/episodes                  | 3680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0203     |
| test/info_shaping_reward_mean  | -0.0451     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.6659879  |
| test/Q_plus_P                  | -0.6659879  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 147200      |
| train/episodes                 | 14720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00641    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 588800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.44516417  |
| stats_o/std                    | 0.027948804 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0248     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.8596978  |
| test/Q_plus_P                  | -0.8596978  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00659    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 369         |
| stats_o/mean                   | 0.4451613   |
| stats_o/std                    | 0.027942061 |
| test/episodes                  | 3700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0295     |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.222      |
| test/Q                         | -0.7532742  |
| test/Q_plus_P                  | -0.7532742  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 148000      |
| train/episodes                 | 14800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00835    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 592000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.4451593   |
| stats_o/std                    | 0.027935617 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.7409643  |
| test/Q_plus_P                  | -0.7409643  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00896    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.44516     |
| stats_o/std                    | 0.027929002 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0206     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.78032714 |
| test/Q_plus_P                  | -0.78032714 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00961    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.44515774  |
| stats_o/std                    | 0.027924048 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.024      |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.890344   |
| test/Q_plus_P                  | -0.890344   |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00673    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.44515586  |
| stats_o/std                    | 0.027913822 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.78318673 |
| test/Q_plus_P                  | -0.78318673 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00724    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.4451535   |
| stats_o/std                    | 0.027903797 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0197     |
| test/info_shaping_reward_mean  | -0.0456     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.6510424  |
| test/Q_plus_P                  | -0.6510424  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00781    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 375         |
| stats_o/mean                   | 0.44514957  |
| stats_o/std                    | 0.027898593 |
| test/episodes                  | 3760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0199     |
| test/info_shaping_reward_mean  | -0.049      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.80094635 |
| test/Q_plus_P                  | -0.80094635 |
| test/reward_per_eps            | -7          |
| test/steps                     | 150400      |
| train/episodes                 | 15040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00485    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 601600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 376         |
| stats_o/mean                   | 0.44514716  |
| stats_o/std                    | 0.027889803 |
| test/episodes                  | 3770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0145     |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.8829048  |
| test/Q_plus_P                  | -0.8829048  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 150800      |
| train/episodes                 | 15080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.685       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00782    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 603200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.4451444   |
| stats_o/std                    | 0.02788174  |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0266     |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.66906255 |
| test/Q_plus_P                  | -0.66906255 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00661    |
| train/info_shaping_reward_mean | -0.0567     |
| train/info_shaping_reward_min  | -0.229      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.44514084  |
| stats_o/std                    | 0.027877191 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0201     |
| test/info_shaping_reward_mean  | -0.0459     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.7444975  |
| test/Q_plus_P                  | -0.7444975  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00527    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.4451392   |
| stats_o/std                    | 0.027870506 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0233     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.78598315 |
| test/Q_plus_P                  | -0.78598315 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00537    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.44513968  |
| stats_o/std                    | 0.027864618 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0199     |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7465535  |
| test/Q_plus_P                  | -0.7465535  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00912    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 381         |
| stats_o/mean                   | 0.44513747  |
| stats_o/std                    | 0.02785687  |
| test/episodes                  | 3820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0293     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.213      |
| test/Q                         | -0.68615943 |
| test/Q_plus_P                  | -0.68615943 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 152800      |
| train/episodes                 | 15280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00596    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 611200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 382         |
| stats_o/mean                   | 0.44513693  |
| stats_o/std                    | 0.027851395 |
| test/episodes                  | 3830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.63416326 |
| test/Q_plus_P                  | -0.63416326 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 153200      |
| train/episodes                 | 15320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0079     |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 612800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.44513658  |
| stats_o/std                    | 0.027841555 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0313     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7775373  |
| test/Q_plus_P                  | -0.7775373  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00766    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.44513562  |
| stats_o/std                    | 0.027832618 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0157     |
| test/info_shaping_reward_mean  | -0.0457     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.68339485 |
| test/Q_plus_P                  | -0.68339485 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00629    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.44513237  |
| stats_o/std                    | 0.027829371 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0247     |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.74964166 |
| test/Q_plus_P                  | -0.74964166 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00897    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.44513202  |
| stats_o/std                    | 0.027821982 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0269     |
| test/info_shaping_reward_mean  | -0.0563     |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -0.87234515 |
| test/Q_plus_P                  | -0.87234515 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00886    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.44513014  |
| stats_o/std                    | 0.027812479 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0258     |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.79261154 |
| test/Q_plus_P                  | -0.79261154 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0098     |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.4451293  |
| stats_o/std                    | 0.02780664 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0269    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -0.7609514 |
| test/Q_plus_P                  | -0.7609514 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.663      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00943   |
| train/info_shaping_reward_mean | -0.0607    |
| train/info_shaping_reward_min  | -0.263     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.5      |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.4451258   |
| stats_o/std                    | 0.027801014 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8305929  |
| test/Q_plus_P                  | -0.8305929  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.645       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00738    |
| train/info_shaping_reward_mean | -0.06       |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 390         |
| stats_o/mean                   | 0.445123    |
| stats_o/std                    | 0.027796194 |
| test/episodes                  | 3910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0235     |
| test/info_shaping_reward_mean  | -0.0483     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -0.7162612  |
| test/Q_plus_P                  | -0.7162612  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 156400      |
| train/episodes                 | 15640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00481    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 625600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.44512364 |
| stats_o/std                    | 0.02778931 |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0239    |
| test/info_shaping_reward_mean  | -0.0513    |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -0.7896875 |
| test/Q_plus_P                  | -0.7896875 |
| test/reward_per_eps            | -7         |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.649      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00615   |
| train/info_shaping_reward_mean | -0.0595    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.44512162  |
| stats_o/std                    | 0.027780792 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0263     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.8686878  |
| test/Q_plus_P                  | -0.8686878  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00879    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.44512117  |
| stats_o/std                    | 0.027774407 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0206     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.8469563  |
| test/Q_plus_P                  | -0.8469563  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00842    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.4451212   |
| stats_o/std                    | 0.027765429 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0299     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.7954276  |
| test/Q_plus_P                  | -0.7954276  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0123     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.4451188   |
| stats_o/std                    | 0.027761145 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0241     |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.7605585  |
| test/Q_plus_P                  | -0.7605585  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00736    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.44511804  |
| stats_o/std                    | 0.027751626 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0257     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.80436206 |
| test/Q_plus_P                  | -0.80436206 |
| test/reward_per_eps            | -7          |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00889    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.44511652  |
| stats_o/std                    | 0.027747015 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0171     |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.6879667  |
| test/Q_plus_P                  | -0.6879667  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00429    |
| train/info_shaping_reward_mean | -0.0575     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 398        |
| stats_o/mean                   | 0.4451141  |
| stats_o/std                    | 0.02774181 |
| test/episodes                  | 3990       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.835      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0148    |
| test/info_shaping_reward_mean  | -0.0455    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -0.7246769 |
| test/Q_plus_P                  | -0.7246769 |
| test/reward_per_eps            | -6.6       |
| test/steps                     | 159600     |
| train/episodes                 | 15960      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.667      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00652   |
| train/info_shaping_reward_mean | -0.0579    |
| train/info_shaping_reward_min  | -0.239     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 638400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.4451108   |
| stats_o/std                    | 0.027734568 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0212     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7474987  |
| test/Q_plus_P                  | -0.7474987  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00846    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 400         |
| stats_o/mean                   | 0.4451102   |
| stats_o/std                    | 0.027730366 |
| test/episodes                  | 4010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0193     |
| test/info_shaping_reward_mean  | -0.0468     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.725248   |
| test/Q_plus_P                  | -0.725248   |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 160400      |
| train/episodes                 | 16040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00741    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 641600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.4451103   |
| stats_o/std                    | 0.027723966 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.0463     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.7433052  |
| test/Q_plus_P                  | -0.7433052  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00578    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.44510937  |
| stats_o/std                    | 0.027718158 |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0216     |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.94865435 |
| test/Q_plus_P                  | -0.94865435 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00632    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.44510695  |
| stats_o/std                    | 0.027711885 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0271     |
| test/info_shaping_reward_mean  | -0.0548     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.8745987  |
| test/Q_plus_P                  | -0.8745987  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.65        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00625    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 404         |
| stats_o/mean                   | 0.44510496  |
| stats_o/std                    | 0.027704151 |
| test/episodes                  | 4050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0147     |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.9262418  |
| test/Q_plus_P                  | -0.9262418  |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 162000      |
| train/episodes                 | 16200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00673    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 648000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 405        |
| stats_o/mean                   | 0.44510248 |
| stats_o/std                    | 0.0276985  |
| test/episodes                  | 4060       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.838      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0253    |
| test/info_shaping_reward_mean  | -0.0502    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.722417  |
| test/Q_plus_P                  | -0.722417  |
| test/reward_per_eps            | -6.5       |
| test/steps                     | 162400     |
| train/episodes                 | 16240      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00542   |
| train/info_shaping_reward_mean | -0.0585    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14        |
| train/steps                    | 649600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.44510257  |
| stats_o/std                    | 0.027687976 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0158     |
| test/info_shaping_reward_mean  | -0.0467     |
| test/info_shaping_reward_min   | -0.232      |
| test/Q                         | -0.7700041  |
| test/Q_plus_P                  | -0.7700041  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.701       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00524    |
| train/info_shaping_reward_mean | -0.0561     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.44510174  |
| stats_o/std                    | 0.027681166 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0271     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.8249484  |
| test/Q_plus_P                  | -0.8249484  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00942    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.44510102  |
| stats_o/std                    | 0.027672058 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0294     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7418586  |
| test/Q_plus_P                  | -0.7418586  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00791    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.44509825  |
| stats_o/std                    | 0.02766812  |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0311     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.226      |
| test/Q                         | -0.74807554 |
| test/Q_plus_P                  | -0.74807554 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00981    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.44509414  |
| stats_o/std                    | 0.027664764 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0247     |
| test/info_shaping_reward_mean  | -0.0506     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.7818003  |
| test/Q_plus_P                  | -0.7818003  |
| test/reward_per_eps            | -7          |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00736    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.44509092 |
| stats_o/std                    | 0.02766108 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.025     |
| test/info_shaping_reward_mean  | -0.0495    |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -0.685557  |
| test/Q_plus_P                  | -0.685557  |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.677      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00848   |
| train/info_shaping_reward_mean | -0.0604    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.9      |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.4450897   |
| stats_o/std                    | 0.027656682 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0291     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.84170395 |
| test/Q_plus_P                  | -0.84170395 |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.679       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.0572     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.44508895  |
| stats_o/std                    | 0.027651474 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0241     |
| test/info_shaping_reward_mean  | -0.0512     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7093303  |
| test/Q_plus_P                  | -0.7093303  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00842    |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.44508585  |
| stats_o/std                    | 0.027649753 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0066     |
| test/info_shaping_reward_mean  | -0.0426     |
| test/info_shaping_reward_min   | -0.217      |
| test/Q                         | -0.6688414  |
| test/Q_plus_P                  | -0.6688414  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00608    |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.44508567  |
| stats_o/std                    | 0.027641706 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0275     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.85224444 |
| test/Q_plus_P                  | -0.85224444 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00842    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.4450846  |
| stats_o/std                    | 0.02763618 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.823      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0218    |
| test/info_shaping_reward_mean  | -0.0505    |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -0.8073755 |
| test/Q_plus_P                  | -0.8073755 |
| test/reward_per_eps            | -7.1       |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.666      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00708   |
| train/info_shaping_reward_mean | -0.0588    |
| train/info_shaping_reward_min  | -0.25      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.3      |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.44508258  |
| stats_o/std                    | 0.027631858 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.795       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0226     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.894019   |
| test/Q_plus_P                  | -0.894019   |
| test/reward_per_eps            | -8.2        |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.4450785   |
| stats_o/std                    | 0.027625578 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0271     |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.72916937 |
| test/Q_plus_P                  | -0.72916937 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00654    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.4450784   |
| stats_o/std                    | 0.027616253 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0205     |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.8509624  |
| test/Q_plus_P                  | -0.8509624  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00645    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.4450796   |
| stats_o/std                    | 0.027606806 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0278     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.8338871  |
| test/Q_plus_P                  | -0.8338871  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00846    |
| train/info_shaping_reward_mean | -0.0568     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.44507587  |
| stats_o/std                    | 0.027601695 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0268     |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.7965422  |
| test/Q_plus_P                  | -0.7965422  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0063     |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.44507357  |
| stats_o/std                    | 0.027597478 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0242     |
| test/info_shaping_reward_mean  | -0.0499     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.68023    |
| test/Q_plus_P                  | -0.68023    |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00581    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.44507346  |
| stats_o/std                    | 0.027589539 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0301     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.7304652  |
| test/Q_plus_P                  | -0.7304652  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.642       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00894    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.44506907  |
| stats_o/std                    | 0.02758498  |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.014      |
| test/info_shaping_reward_mean  | -0.0465     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.74648976 |
| test/Q_plus_P                  | -0.74648976 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00817    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.44506478  |
| stats_o/std                    | 0.027579395 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.0517     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.76778495 |
| test/Q_plus_P                  | -0.76778495 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.698       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00467    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.1       |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 426         |
| stats_o/mean                   | 0.44506243  |
| stats_o/std                    | 0.027572393 |
| test/episodes                  | 4270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0253     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.85164416 |
| test/Q_plus_P                  | -0.85164416 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 170800      |
| train/episodes                 | 17080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.667       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0067     |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 683200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.44506076  |
| stats_o/std                    | 0.027565522 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0251     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7849664  |
| test/Q_plus_P                  | -0.7849664  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.637       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0083     |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.44506046  |
| stats_o/std                    | 0.027559375 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0284     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.80524427 |
| test/Q_plus_P                  | -0.80524427 |
| test/reward_per_eps            | -7          |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 429        |
| stats_o/mean                   | 0.44505987 |
| stats_o/std                    | 0.02755283 |
| test/episodes                  | 4300       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0266    |
| test/info_shaping_reward_mean  | -0.0539    |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -0.7923025 |
| test/Q_plus_P                  | -0.7923025 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 172000     |
| train/episodes                 | 17200      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.675      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00761   |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.255     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13        |
| train/steps                    | 688000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.44505897  |
| stats_o/std                    | 0.027549475 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.00721    |
| test/info_shaping_reward_mean  | -0.0498     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.8829004  |
| test/Q_plus_P                  | -0.8829004  |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.644       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00816    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.44505727  |
| stats_o/std                    | 0.027540999 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0238     |
| test/info_shaping_reward_mean  | -0.058      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.88410056 |
| test/Q_plus_P                  | -0.88410056 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00672    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 432         |
| stats_o/mean                   | 0.44505575  |
| stats_o/std                    | 0.027536908 |
| test/episodes                  | 4330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.029      |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.7267533  |
| test/Q_plus_P                  | -0.7267533  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 173200      |
| train/episodes                 | 17320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00484    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 692800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.4450545   |
| stats_o/std                    | 0.027528329 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0295     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.708274   |
| test/Q_plus_P                  | -0.708274   |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.69        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00752    |
| train/info_shaping_reward_mean | -0.0571     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.4       |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.4450558   |
| stats_o/std                    | 0.027521351 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0294     |
| test/info_shaping_reward_mean  | -0.0551     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.77164865 |
| test/Q_plus_P                  | -0.77164865 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.683       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00874    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.7       |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.445056    |
| stats_o/std                    | 0.027516799 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0325     |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.7813429  |
| test/Q_plus_P                  | -0.7813429  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00893    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 436         |
| stats_o/mean                   | 0.44505545  |
| stats_o/std                    | 0.027511522 |
| test/episodes                  | 4370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.7947538  |
| test/Q_plus_P                  | -0.7947538  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 174800      |
| train/episodes                 | 17480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00766    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 699200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 437         |
| stats_o/mean                   | 0.44505465  |
| stats_o/std                    | 0.027506417 |
| test/episodes                  | 4380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0272     |
| test/info_shaping_reward_mean  | -0.055      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.7517858  |
| test/Q_plus_P                  | -0.7517858  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 175200      |
| train/episodes                 | 17520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00892    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 700800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.44505346  |
| stats_o/std                    | 0.027498545 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0346     |
| test/info_shaping_reward_mean  | -0.0564     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.84518266 |
| test/Q_plus_P                  | -0.84518266 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00843    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 439        |
| stats_o/mean                   | 0.44505265 |
| stats_o/std                    | 0.02749042 |
| test/episodes                  | 4400       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.825      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0283    |
| test/info_shaping_reward_mean  | -0.0527    |
| test/info_shaping_reward_min   | -0.23      |
| test/Q                         | -0.7810074 |
| test/Q_plus_P                  | -0.7810074 |
| test/reward_per_eps            | -7         |
| test/steps                     | 176000     |
| train/episodes                 | 17600      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0074    |
| train/info_shaping_reward_mean | -0.0591    |
| train/info_shaping_reward_min  | -0.225     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 704000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.44505236  |
| stats_o/std                    | 0.027484326 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0296     |
| test/info_shaping_reward_mean  | -0.0549     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.7529583  |
| test/Q_plus_P                  | -0.7529583  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 441         |
| stats_o/mean                   | 0.4450517   |
| stats_o/std                    | 0.02747661  |
| test/episodes                  | 4420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.032      |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.82293725 |
| test/Q_plus_P                  | -0.82293725 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 176800      |
| train/episodes                 | 17680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00647    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 707200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.44505194  |
| stats_o/std                    | 0.027473008 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0317     |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -0.762265   |
| test/Q_plus_P                  | -0.762265   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00533    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.4450499   |
| stats_o/std                    | 0.027467169 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0313     |
| test/info_shaping_reward_mean  | -0.0555     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.80722505 |
| test/Q_plus_P                  | -0.80722505 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00884    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.44504967  |
| stats_o/std                    | 0.02745956  |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0317     |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.88043475 |
| test/Q_plus_P                  | -0.88043475 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00832    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.44504988  |
| stats_o/std                    | 0.027455486 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0253     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.8042952  |
| test/Q_plus_P                  | -0.8042952  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00736    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.44505003  |
| stats_o/std                    | 0.027446518 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.033      |
| test/info_shaping_reward_mean  | -0.0542     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.7523427  |
| test/Q_plus_P                  | -0.7523427  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00955    |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.44504675  |
| stats_o/std                    | 0.027443327 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0292     |
| test/info_shaping_reward_mean  | -0.0505     |
| test/info_shaping_reward_min   | -0.236      |
| test/Q                         | -0.62154335 |
| test/Q_plus_P                  | -0.62154335 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00998    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 448         |
| stats_o/mean                   | 0.4450438   |
| stats_o/std                    | 0.027439274 |
| test/episodes                  | 4490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0291     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.67067987 |
| test/Q_plus_P                  | -0.67067987 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 179600      |
| train/episodes                 | 17960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00886    |
| train/info_shaping_reward_mean | -0.0634     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 718400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.445043    |
| stats_o/std                    | 0.02743466  |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0257     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.82699937 |
| test/Q_plus_P                  | -0.82699937 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00784    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 450         |
| stats_o/mean                   | 0.44504336  |
| stats_o/std                    | 0.027427414 |
| test/episodes                  | 4510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.019      |
| test/info_shaping_reward_mean  | -0.0515     |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -0.84233975 |
| test/Q_plus_P                  | -0.84233975 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 180400      |
| train/episodes                 | 18040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0086     |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 721600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.4450402   |
| stats_o/std                    | 0.027422639 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0258     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.9120393  |
| test/Q_plus_P                  | -0.9120393  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.659       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00594    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 452         |
| stats_o/mean                   | 0.44503868  |
| stats_o/std                    | 0.027416436 |
| test/episodes                  | 4530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.033      |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -0.8046954  |
| test/Q_plus_P                  | -0.8046954  |
| test/reward_per_eps            | -7          |
| test/steps                     | 181200      |
| train/episodes                 | 18120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0058     |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 724800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.44503888  |
| stats_o/std                    | 0.027408056 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0291     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.76308775 |
| test/Q_plus_P                  | -0.76308775 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.44503698  |
| stats_o/std                    | 0.027400086 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.028      |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.78804564 |
| test/Q_plus_P                  | -0.78804564 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0124     |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.44503748  |
| stats_o/std                    | 0.02739116  |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0281     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.73412323 |
| test/Q_plus_P                  | -0.73412323 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.44503677  |
| stats_o/std                    | 0.027384274 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0227     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.7185271  |
| test/Q_plus_P                  | -0.7185271  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00845    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.44503137  |
| stats_o/std                    | 0.027382238 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.026      |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.7637016  |
| test/Q_plus_P                  | -0.7637016  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00742    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.44502833  |
| stats_o/std                    | 0.027379602 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0183     |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.7287123  |
| test/Q_plus_P                  | -0.7287123  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00517    |
| train/info_shaping_reward_mean | -0.0579     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.44502917  |
| stats_o/std                    | 0.027374223 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0206     |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.8589505  |
| test/Q_plus_P                  | -0.8589505  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00883    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.4450295   |
| stats_o/std                    | 0.027368426 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0182     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.8158521  |
| test/Q_plus_P                  | -0.8158521  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00743    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.44502974  |
| stats_o/std                    | 0.027361212 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0237     |
| test/info_shaping_reward_mean  | -0.0497     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.71066415 |
| test/Q_plus_P                  | -0.71066415 |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00817    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.44503033 |
| stats_o/std                    | 0.02735036 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0224    |
| test/info_shaping_reward_mean  | -0.0475    |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -0.6985463 |
| test/Q_plus_P                  | -0.6985463 |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.697      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00803   |
| train/info_shaping_reward_mean | -0.0565    |
| train/info_shaping_reward_min  | -0.244     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.1      |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 463         |
| stats_o/mean                   | 0.4450288   |
| stats_o/std                    | 0.027343877 |
| test/episodes                  | 4640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0238     |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.80274314 |
| test/Q_plus_P                  | -0.80274314 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 185600      |
| train/episodes                 | 18560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00791    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 742400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.44502842  |
| stats_o/std                    | 0.027337337 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.026      |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.80420524 |
| test/Q_plus_P                  | -0.80420524 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00552    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.44502783  |
| stats_o/std                    | 0.027332926 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0218     |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7231623  |
| test/Q_plus_P                  | -0.7231623  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00579    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.44502652  |
| stats_o/std                    | 0.027327033 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0247     |
| test/info_shaping_reward_mean  | -0.0557     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.8438164  |
| test/Q_plus_P                  | -0.8438164  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.623       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0625     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.44502428 |
| stats_o/std                    | 0.02732279 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.833      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0185    |
| test/info_shaping_reward_mean  | -0.049     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -0.7470662 |
| test/Q_plus_P                  | -0.7470662 |
| test/reward_per_eps            | -6.7       |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.647      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00997   |
| train/info_shaping_reward_mean | -0.0613    |
| train/info_shaping_reward_min  | -0.249     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.1      |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.4450223   |
| stats_o/std                    | 0.02731728  |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0256     |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.71536636 |
| test/Q_plus_P                  | -0.71536636 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00662    |
| train/info_shaping_reward_mean | -0.0601     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.44502315  |
| stats_o/std                    | 0.027315343 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0273     |
| test/info_shaping_reward_mean  | -0.0511     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.63278353 |
| test/Q_plus_P                  | -0.63278353 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.612       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00931    |
| train/info_shaping_reward_mean | -0.0649     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.44501972  |
| stats_o/std                    | 0.027314754 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0289     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.6949156  |
| test/Q_plus_P                  | -0.6949156  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00814    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.44501844 |
| stats_o/std                    | 0.02731034 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.8        |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0236    |
| test/info_shaping_reward_mean  | -0.0548    |
| test/info_shaping_reward_min   | -0.231     |
| test/Q                         | -0.9205461 |
| test/Q_plus_P                  | -0.9205461 |
| test/reward_per_eps            | -8         |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00643   |
| train/info_shaping_reward_mean | -0.0606    |
| train/info_shaping_reward_min  | -0.248     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.44501913  |
| stats_o/std                    | 0.027305173 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0168     |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.76985437 |
| test/Q_plus_P                  | -0.76985437 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.666       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00567    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.44501588  |
| stats_o/std                    | 0.027304763 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0175     |
| test/info_shaping_reward_mean  | -0.0476     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.74369735 |
| test/Q_plus_P                  | -0.74369735 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.64        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0061     |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.44501472  |
| stats_o/std                    | 0.027302423 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0308     |
| test/info_shaping_reward_mean  | -0.0573     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.84601396 |
| test/Q_plus_P                  | -0.84601396 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00777    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 475         |
| stats_o/mean                   | 0.4450134   |
| stats_o/std                    | 0.027298683 |
| test/episodes                  | 4760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0226     |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.75380003 |
| test/Q_plus_P                  | -0.75380003 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 190400      |
| train/episodes                 | 19040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00691    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 761600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.44501284  |
| stats_o/std                    | 0.027293885 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.03       |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -0.75651354 |
| test/Q_plus_P                  | -0.75651354 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00865    |
| train/info_shaping_reward_mean | -0.0582     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.44501016  |
| stats_o/std                    | 0.027290842 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0319     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.79070336 |
| test/Q_plus_P                  | -0.79070336 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.699       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00723    |
| train/info_shaping_reward_mean | -0.058      |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12         |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 478         |
| stats_o/mean                   | 0.44500875  |
| stats_o/std                    | 0.027285919 |
| test/episodes                  | 4790        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0245     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -0.8123275  |
| test/Q_plus_P                  | -0.8123275  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 191600      |
| train/episodes                 | 19160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00996    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 766400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.4450084   |
| stats_o/std                    | 0.027281066 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0308     |
| test/info_shaping_reward_mean  | -0.0528     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.64218974 |
| test/Q_plus_P                  | -0.64218974 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0072     |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.263      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.44500828  |
| stats_o/std                    | 0.027275683 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.797       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0284     |
| test/info_shaping_reward_mean  | -0.0587     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.9406717  |
| test/Q_plus_P                  | -0.9406717  |
| test/reward_per_eps            | -8.1        |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00802    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 481         |
| stats_o/mean                   | 0.44500962  |
| stats_o/std                    | 0.027268136 |
| test/episodes                  | 4820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0232     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -0.7460054  |
| test/Q_plus_P                  | -0.7460054  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 192800      |
| train/episodes                 | 19280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.665       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00876    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 771200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.4450077   |
| stats_o/std                    | 0.027264573 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0194     |
| test/info_shaping_reward_mean  | -0.0455     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.7463604  |
| test/Q_plus_P                  | -0.7463604  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0098     |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 483         |
| stats_o/mean                   | 0.44500485  |
| stats_o/std                    | 0.027261585 |
| test/episodes                  | 4840        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0322     |
| test/info_shaping_reward_mean  | -0.0583     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.8817939  |
| test/Q_plus_P                  | -0.8817939  |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 193600      |
| train/episodes                 | 19360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00569    |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 774400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.44500598  |
| stats_o/std                    | 0.027256278 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.807       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0297     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.8375911  |
| test/Q_plus_P                  | -0.8375911  |
| test/reward_per_eps            | -7.7        |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00416    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.44500574  |
| stats_o/std                    | 0.02725301  |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0127     |
| test/info_shaping_reward_mean  | -0.0488     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.79437554 |
| test/Q_plus_P                  | -0.79437554 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00616    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.44500616  |
| stats_o/std                    | 0.027247675 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.024      |
| test/info_shaping_reward_mean  | -0.0493     |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -0.85515535 |
| test/Q_plus_P                  | -0.85515535 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.692       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00821    |
| train/info_shaping_reward_mean | -0.0563     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 487         |
| stats_o/mean                   | 0.44500372  |
| stats_o/std                    | 0.027240554 |
| test/episodes                  | 4880        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0137     |
| test/info_shaping_reward_mean  | -0.0435     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.670624   |
| test/Q_plus_P                  | -0.670624   |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 195200      |
| train/episodes                 | 19520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.714       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00613    |
| train/info_shaping_reward_mean | -0.0546     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -11.4       |
| train/steps                    | 780800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.44500145  |
| stats_o/std                    | 0.027238803 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.805       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0209     |
| test/info_shaping_reward_mean  | -0.0554     |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -0.86828065 |
| test/Q_plus_P                  | -0.86828065 |
| test/reward_per_eps            | -7.8        |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00758    |
| train/info_shaping_reward_mean | -0.0638     |
| train/info_shaping_reward_min  | -0.265      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 489         |
| stats_o/mean                   | 0.44500077  |
| stats_o/std                    | 0.027233588 |
| test/episodes                  | 4900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.74949104 |
| test/Q_plus_P                  | -0.74949104 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 196000      |
| train/episodes                 | 19600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0101     |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.234      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 784000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.44500116  |
| stats_o/std                    | 0.027229132 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0234     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.7795098  |
| test/Q_plus_P                  | -0.7795098  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00753    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.4449962   |
| stats_o/std                    | 0.027225772 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0295     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.252      |
| test/Q                         | -0.77323824 |
| test/Q_plus_P                  | -0.77323824 |
| test/reward_per_eps            | -7          |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00866    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 492         |
| stats_o/mean                   | 0.44499627  |
| stats_o/std                    | 0.027218657 |
| test/episodes                  | 4930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.848       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0207     |
| test/info_shaping_reward_mean  | -0.0446     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.61425036 |
| test/Q_plus_P                  | -0.61425036 |
| test/reward_per_eps            | -6.1        |
| test/steps                     | 197200      |
| train/episodes                 | 19720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.694       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00732    |
| train/info_shaping_reward_mean | -0.0577     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 788800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.44499373  |
| stats_o/std                    | 0.027215632 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0264     |
| test/info_shaping_reward_mean  | -0.0535     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.8292348  |
| test/Q_plus_P                  | -0.8292348  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00804    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.272      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.44499424  |
| stats_o/std                    | 0.02720998  |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0243     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.81066537 |
| test/Q_plus_P                  | -0.81066537 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0629     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.44499466  |
| stats_o/std                    | 0.027204687 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0261     |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.7343338  |
| test/Q_plus_P                  | -0.7343338  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00604    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.44499323  |
| stats_o/std                    | 0.027201893 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0317     |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.8051128  |
| test/Q_plus_P                  | -0.8051128  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0078     |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.44499376  |
| stats_o/std                    | 0.027196107 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0256     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7644338  |
| test/Q_plus_P                  | -0.7644338  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00744    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.44499385  |
| stats_o/std                    | 0.027190901 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0266     |
| test/info_shaping_reward_mean  | -0.0504     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.6734073  |
| test/Q_plus_P                  | -0.6734073  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00662    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.44499102  |
| stats_o/std                    | 0.027187489 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0281     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -0.87093526 |
| test/Q_plus_P                  | -0.87093526 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00618    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.44498876  |
| stats_o/std                    | 0.027181186 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0253     |
| test/info_shaping_reward_mean  | -0.0491     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.69605505 |
| test/Q_plus_P                  | -0.69605505 |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.681       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00667    |
| train/info_shaping_reward_mean | -0.0576     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.8       |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 501         |
| stats_o/mean                   | 0.44498754  |
| stats_o/std                    | 0.027177453 |
| test/episodes                  | 5020        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0282     |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -0.7698147  |
| test/Q_plus_P                  | -0.7698147  |
| test/reward_per_eps            | -7          |
| test/steps                     | 200800      |
| train/episodes                 | 20080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00803    |
| train/info_shaping_reward_mean | -0.0618     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 803200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.44498846  |
| stats_o/std                    | 0.027172498 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0229     |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6424921  |
| test/Q_plus_P                  | -0.6424921  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.648       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00588    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 503         |
| stats_o/mean                   | 0.4449894   |
| stats_o/std                    | 0.027166301 |
| test/episodes                  | 5040        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0252     |
| test/info_shaping_reward_mean  | -0.0509     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.78171945 |
| test/Q_plus_P                  | -0.78171945 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 201600      |
| train/episodes                 | 20160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00799    |
| train/info_shaping_reward_mean | -0.0621     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 806400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.4449896   |
| stats_o/std                    | 0.027162818 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0209     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.8283255  |
| test/Q_plus_P                  | -0.8283255  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00751    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.44498956  |
| stats_o/std                    | 0.027154528 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0259     |
| test/info_shaping_reward_mean  | -0.0519     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.7562601  |
| test/Q_plus_P                  | -0.7562601  |
| test/reward_per_eps            | -7          |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.696       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00555    |
| train/info_shaping_reward_mean | -0.0547     |
| train/info_shaping_reward_min  | -0.231      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.2       |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 506        |
| stats_o/mean                   | 0.4449875  |
| stats_o/std                    | 0.02714991 |
| test/episodes                  | 5070       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0248    |
| test/info_shaping_reward_mean  | -0.0542    |
| test/info_shaping_reward_min   | -0.243     |
| test/Q                         | -0.7608577 |
| test/Q_plus_P                  | -0.7608577 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 202800     |
| train/episodes                 | 20280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.651      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00719   |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.253     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.9      |
| train/steps                    | 811200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 507         |
| stats_o/mean                   | 0.44498566  |
| stats_o/std                    | 0.027144054 |
| test/episodes                  | 5080        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0254     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.76146126 |
| test/Q_plus_P                  | -0.76146126 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 203200      |
| train/episodes                 | 20320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0113     |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 812800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.44498372  |
| stats_o/std                    | 0.027139125 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0257     |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.8412418  |
| test/Q_plus_P                  | -0.8412418  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.693       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00864    |
| train/info_shaping_reward_mean | -0.0583     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.3       |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.4449835   |
| stats_o/std                    | 0.027137354 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0306     |
| test/info_shaping_reward_mean  | -0.0562     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.80161124 |
| test/Q_plus_P                  | -0.80161124 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.653       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00832    |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.44498277  |
| stats_o/std                    | 0.027133934 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0212     |
| test/info_shaping_reward_mean  | -0.0518     |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -0.7624371  |
| test/Q_plus_P                  | -0.7624371  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00847    |
| train/info_shaping_reward_mean | -0.0639     |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.4449828   |
| stats_o/std                    | 0.027125997 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0235     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -0.7081369  |
| test/Q_plus_P                  | -0.7081369  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00737    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.235      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.44498217  |
| stats_o/std                    | 0.027121201 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.035      |
| test/info_shaping_reward_mean  | -0.0571     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.81600386 |
| test/Q_plus_P                  | -0.81600386 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.672       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00897    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.44498172  |
| stats_o/std                    | 0.027116014 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0298     |
| test/info_shaping_reward_mean  | -0.0546     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.7406769  |
| test/Q_plus_P                  | -0.7406769  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00828    |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 514         |
| stats_o/mean                   | 0.44498205  |
| stats_o/std                    | 0.027111018 |
| test/episodes                  | 5150        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0266     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.8456023  |
| test/Q_plus_P                  | -0.8456023  |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 206000      |
| train/episodes                 | 20600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.617       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00972    |
| train/info_shaping_reward_mean | -0.064      |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 824000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.44498286  |
| stats_o/std                    | 0.027105622 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0289     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.7568856  |
| test/Q_plus_P                  | -0.7568856  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0055     |
| train/info_shaping_reward_mean | -0.0604     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.4449823   |
| stats_o/std                    | 0.027102994 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0266     |
| test/info_shaping_reward_mean  | -0.0552     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.80294937 |
| test/Q_plus_P                  | -0.80294937 |
| test/reward_per_eps            | -7          |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00692    |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.44498265  |
| stats_o/std                    | 0.027096836 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0243     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.6886275  |
| test/Q_plus_P                  | -0.6886275  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0074     |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 518         |
| stats_o/mean                   | 0.44498006  |
| stats_o/std                    | 0.027093515 |
| test/episodes                  | 5190        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0215     |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.7538003  |
| test/Q_plus_P                  | -0.7538003  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 207600      |
| train/episodes                 | 20760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.687       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00639    |
| train/info_shaping_reward_mean | -0.0585     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.5       |
| train/steps                    | 830400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 519         |
| stats_o/mean                   | 0.44498083  |
| stats_o/std                    | 0.027088925 |
| test/episodes                  | 5200        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0327     |
| test/info_shaping_reward_mean  | -0.0565     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.8129184  |
| test/Q_plus_P                  | -0.8129184  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 208000      |
| train/episodes                 | 20800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0105     |
| train/info_shaping_reward_mean | -0.0624     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 832000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.44498074  |
| stats_o/std                    | 0.027088933 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0245     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -0.780787   |
| test/Q_plus_P                  | -0.780787   |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.607       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0108     |
| train/info_shaping_reward_mean | -0.0664     |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.7       |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.44497982  |
| stats_o/std                    | 0.027085507 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0357     |
| test/info_shaping_reward_mean  | -0.0588     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.84003663 |
| test/Q_plus_P                  | -0.84003663 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.674       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00802    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.251      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.4449781   |
| stats_o/std                    | 0.027082166 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0268     |
| test/info_shaping_reward_mean  | -0.0537     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.7804968  |
| test/Q_plus_P                  | -0.7804968  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.686       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00451    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.6       |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.44497752  |
| stats_o/std                    | 0.02708058  |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0208     |
| test/info_shaping_reward_mean  | -0.0492     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.76385367 |
| test/Q_plus_P                  | -0.76385367 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00715    |
| train/info_shaping_reward_mean | -0.0605     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.44497693  |
| stats_o/std                    | 0.027075915 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0309     |
| test/info_shaping_reward_mean  | -0.0529     |
| test/info_shaping_reward_min   | -0.221      |
| test/Q                         | -0.7220453  |
| test/Q_plus_P                  | -0.7220453  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00646    |
| train/info_shaping_reward_mean | -0.0595     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.44497576  |
| stats_o/std                    | 0.027073057 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0248     |
| test/info_shaping_reward_mean  | -0.0501     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.747286   |
| test/Q_plus_P                  | -0.747286   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00915    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.44497475  |
| stats_o/std                    | 0.027067462 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0249     |
| test/info_shaping_reward_mean  | -0.0544     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.7658797  |
| test/Q_plus_P                  | -0.7658797  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00974    |
| train/info_shaping_reward_mean | -0.0645     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.44497347  |
| stats_o/std                    | 0.027063029 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0194     |
| test/info_shaping_reward_mean  | -0.0496     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.67170477 |
| test/Q_plus_P                  | -0.67170477 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.655       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00749    |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.444973    |
| stats_o/std                    | 0.027060134 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0239     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.219      |
| test/Q                         | -0.75029176 |
| test/Q_plus_P                  | -0.75029176 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0079     |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.4449729   |
| stats_o/std                    | 0.027053513 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0183     |
| test/info_shaping_reward_mean  | -0.0487     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.81854814 |
| test/Q_plus_P                  | -0.81854814 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00889    |
| train/info_shaping_reward_mean | -0.061      |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.44497228  |
| stats_o/std                    | 0.027050672 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0125     |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.7575062  |
| test/Q_plus_P                  | -0.7575062  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00844    |
| train/info_shaping_reward_mean | -0.0608     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.4449723  |
| stats_o/std                    | 0.02704493 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.807      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0294    |
| test/info_shaping_reward_mean  | -0.057     |
| test/info_shaping_reward_min   | -0.246     |
| test/Q                         | -0.8677556 |
| test/Q_plus_P                  | -0.8677556 |
| test/reward_per_eps            | -7.7       |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.659      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0078    |
| train/info_shaping_reward_mean | -0.0593    |
| train/info_shaping_reward_min  | -0.241     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.6      |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.4449713   |
| stats_o/std                    | 0.02704337  |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0292     |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -0.81199104 |
| test/Q_plus_P                  | -0.81199104 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.639       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00994    |
| train/info_shaping_reward_mean | -0.0631     |
| train/info_shaping_reward_min  | -0.257      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.44496933  |
| stats_o/std                    | 0.027038367 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0196     |
| test/info_shaping_reward_mean  | -0.0471     |
| test/info_shaping_reward_min   | -0.223      |
| test/Q                         | -0.74732804 |
| test/Q_plus_P                  | -0.74732804 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.647       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00688    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.4449689   |
| stats_o/std                    | 0.027036177 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0218     |
| test/info_shaping_reward_mean  | -0.0484     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.6645111  |
| test/Q_plus_P                  | -0.6645111  |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.605       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00805    |
| train/info_shaping_reward_mean | -0.0636     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.8       |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.44496536  |
| stats_o/std                    | 0.027032727 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0284     |
| test/info_shaping_reward_mean  | -0.0536     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.7510332  |
| test/Q_plus_P                  | -0.7510332  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00539    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.44496235  |
| stats_o/std                    | 0.027031826 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0301     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.67021716 |
| test/Q_plus_P                  | -0.67021716 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.625       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.253      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15         |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.4449607   |
| stats_o/std                    | 0.027027445 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0187     |
| test/info_shaping_reward_mean  | -0.0477     |
| test/info_shaping_reward_min   | -0.233      |
| test/Q                         | -0.73055047 |
| test/Q_plus_P                  | -0.73055047 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00702    |
| train/info_shaping_reward_mean | -0.0596     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.44495928  |
| stats_o/std                    | 0.027023695 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0255     |
| test/info_shaping_reward_mean  | -0.0508     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.67423964 |
| test/Q_plus_P                  | -0.67423964 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.677       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00956    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.44495746  |
| stats_o/std                    | 0.02702409  |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0219     |
| test/info_shaping_reward_mean  | -0.0559     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.84624916 |
| test/Q_plus_P                  | -0.84624916 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00902    |
| train/info_shaping_reward_mean | -0.0637     |
| train/info_shaping_reward_min  | -0.271      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 540         |
| stats_o/mean                   | 0.44495633  |
| stats_o/std                    | 0.027018817 |
| test/episodes                  | 5410        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.031      |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.24       |
| test/Q                         | -0.7540545  |
| test/Q_plus_P                  | -0.7540545  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 216400      |
| train/episodes                 | 21640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.668       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00974    |
| train/info_shaping_reward_mean | -0.0589     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.3       |
| train/steps                    | 865600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.44495657  |
| stats_o/std                    | 0.027016155 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0269     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.231      |
| test/Q                         | -0.78878486 |
| test/Q_plus_P                  | -0.78878486 |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.615       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.4       |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.44495526 |
| stats_o/std                    | 0.0270106  |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0298    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.238     |
| test/Q                         | -0.8456483 |
| test/Q_plus_P                  | -0.8456483 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.689      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00832   |
| train/info_shaping_reward_mean | -0.057     |
| train/info_shaping_reward_min  | -0.256     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.4      |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.4449558   |
| stats_o/std                    | 0.027004631 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.019      |
| test/info_shaping_reward_mean  | -0.05       |
| test/info_shaping_reward_min   | -0.224      |
| test/Q                         | -0.786684   |
| test/Q_plus_P                  | -0.786684   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0107     |
| train/info_shaping_reward_mean | -0.062      |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.44495353  |
| stats_o/std                    | 0.027001379 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0219     |
| test/info_shaping_reward_mean  | -0.0449     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.6412425  |
| test/Q_plus_P                  | -0.6412425  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00843    |
| train/info_shaping_reward_mean | -0.0619     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.44495228  |
| stats_o/std                    | 0.026997896 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.81        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0276     |
| test/info_shaping_reward_mean  | -0.0541     |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -0.88939726 |
| test/Q_plus_P                  | -0.88939726 |
| test/reward_per_eps            | -7.6        |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.622       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00857    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.1       |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.44495204  |
| stats_o/std                    | 0.0269939   |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0241     |
| test/info_shaping_reward_mean  | -0.0525     |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -0.87205404 |
| test/Q_plus_P                  | -0.87205404 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.634       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00989    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.237      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.44495025  |
| stats_o/std                    | 0.026990866 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0299     |
| test/info_shaping_reward_mean  | -0.0543     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.7797489  |
| test/Q_plus_P                  | -0.7797489  |
| test/reward_per_eps            | -7          |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0115     |
| train/info_shaping_reward_mean | -0.0633     |
| train/info_shaping_reward_min  | -0.254      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 548         |
| stats_o/mean                   | 0.44495174  |
| stats_o/std                    | 0.026986198 |
| test/episodes                  | 5490        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0305     |
| test/info_shaping_reward_mean  | -0.0566     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.7643297  |
| test/Q_plus_P                  | -0.7643297  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 219600      |
| train/episodes                 | 21960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.646       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00919    |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.238      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.2       |
| train/steps                    | 878400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.44495177  |
| stats_o/std                    | 0.026983207 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0295     |
| test/info_shaping_reward_mean  | -0.0531     |
| test/info_shaping_reward_min   | -0.245      |
| test/Q                         | -0.73648703 |
| test/Q_plus_P                  | -0.73648703 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00702    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 550         |
| stats_o/mean                   | 0.4449512   |
| stats_o/std                    | 0.026976489 |
| test/episodes                  | 5510        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0183     |
| test/info_shaping_reward_mean  | -0.0466     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.721629   |
| test/Q_plus_P                  | -0.721629   |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 220400      |
| train/episodes                 | 22040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00888    |
| train/info_shaping_reward_mean | -0.0588     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 881600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.44495156 |
| stats_o/std                    | 0.0269723  |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0252    |
| test/info_shaping_reward_mean  | -0.0511    |
| test/info_shaping_reward_min   | -0.242     |
| test/Q                         | -0.7698514 |
| test/Q_plus_P                  | -0.7698514 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.633      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00755   |
| train/info_shaping_reward_mean | -0.0612    |
| train/info_shaping_reward_min  | -0.254     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.7      |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.44495034  |
| stats_o/std                    | 0.026967973 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0275     |
| test/info_shaping_reward_mean  | -0.0539     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.80741316 |
| test/Q_plus_P                  | -0.80741316 |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0103     |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.233      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.44495127  |
| stats_o/std                    | 0.02696398  |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0232     |
| test/info_shaping_reward_mean  | -0.0524     |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -0.83503425 |
| test/Q_plus_P                  | -0.83503425 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.641       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00999    |
| train/info_shaping_reward_mean | -0.0602     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.4       |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 554         |
| stats_o/mean                   | 0.4449505   |
| stats_o/std                    | 0.026960403 |
| test/episodes                  | 5550        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0233     |
| test/info_shaping_reward_mean  | -0.0526     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.85862434 |
| test/Q_plus_P                  | -0.85862434 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 222000      |
| train/episodes                 | 22200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.626       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0104     |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.9       |
| train/steps                    | 888000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.44494954  |
| stats_o/std                    | 0.026954079 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0277     |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.7413199  |
| test/Q_plus_P                  | -0.7413199  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00746    |
| train/info_shaping_reward_mean | -0.0555     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.44494936 |
| stats_o/std                    | 0.02694992 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.828      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0325    |
| test/info_shaping_reward_mean  | -0.0549    |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -0.7653582 |
| test/Q_plus_P                  | -0.7653582 |
| test/reward_per_eps            | -6.9       |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.644      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0112    |
| train/info_shaping_reward_mean | -0.0622    |
| train/info_shaping_reward_min  | -0.247     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.2      |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.4449499   |
| stats_o/std                    | 0.026946448 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.84        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0306     |
| test/info_shaping_reward_mean  | -0.0532     |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.6643357  |
| test/Q_plus_P                  | -0.6643357  |
| test/reward_per_eps            | -6.4        |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.654       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0114     |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.44495097  |
| stats_o/std                    | 0.026944183 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.812       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0374     |
| test/info_shaping_reward_mean  | -0.0585     |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -0.85766184 |
| test/Q_plus_P                  | -0.85766184 |
| test/reward_per_eps            | -7.5        |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00855    |
| train/info_shaping_reward_mean | -0.0615     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.44495067  |
| stats_o/std                    | 0.026937893 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.035      |
| test/info_shaping_reward_mean  | -0.0572     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.80447716 |
| test/Q_plus_P                  | -0.80447716 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.63        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00839    |
| train/info_shaping_reward_mean | -0.0622     |
| train/info_shaping_reward_min  | -0.239      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.44494918  |
| stats_o/std                    | 0.026933802 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.68112594 |
| test/Q_plus_P                  | -0.68112594 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.643       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00764    |
| train/info_shaping_reward_mean | -0.0597     |
| train/info_shaping_reward_min  | -0.232      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.3       |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.44494864  |
| stats_o/std                    | 0.026929403 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0268     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.227      |
| test/Q                         | -0.6756535  |
| test/Q_plus_P                  | -0.6756535  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00762    |
| train/info_shaping_reward_mean | -0.0617     |
| train/info_shaping_reward_min  | -0.252      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.44494963  |
| stats_o/std                    | 0.026924148 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0165     |
| test/info_shaping_reward_mean  | -0.0431     |
| test/info_shaping_reward_min   | -0.216      |
| test/Q                         | -0.6325385  |
| test/Q_plus_P                  | -0.6325385  |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.629       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00589    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.444948    |
| stats_o/std                    | 0.026918942 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0204     |
| test/info_shaping_reward_mean  | -0.052      |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.81315285 |
| test/Q_plus_P                  | -0.81315285 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.675       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00916    |
| train/info_shaping_reward_mean | -0.0598     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.44494823  |
| stats_o/std                    | 0.02691388  |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0135     |
| test/info_shaping_reward_mean  | -0.051      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.76103705 |
| test/Q_plus_P                  | -0.76103705 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00983    |
| train/info_shaping_reward_mean | -0.0581     |
| train/info_shaping_reward_min  | -0.226      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.4449481   |
| stats_o/std                    | 0.026909458 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0283     |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.243      |
| test/Q                         | -0.73030543 |
| test/Q_plus_P                  | -0.73030543 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.616       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.006      |
| train/info_shaping_reward_mean | -0.0627     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.3       |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.44494864  |
| stats_o/std                    | 0.026906708 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0351     |
| test/info_shaping_reward_mean  | -0.056      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.7481894  |
| test/Q_plus_P                  | -0.7481894  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.631       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0106     |
| train/info_shaping_reward_mean | -0.0644     |
| train/info_shaping_reward_min  | -0.259      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.8       |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.44494772  |
| stats_o/std                    | 0.026902333 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0302     |
| test/info_shaping_reward_mean  | -0.0556     |
| test/info_shaping_reward_min   | -0.235      |
| test/Q                         | -0.8392629  |
| test/Q_plus_P                  | -0.8392629  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.671       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00644    |
| train/info_shaping_reward_mean | -0.0573     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.44494691  |
| stats_o/std                    | 0.026898375 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0241     |
| test/info_shaping_reward_mean  | -0.0507     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.71325    |
| test/Q_plus_P                  | -0.71325    |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.662       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00811    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.44494697  |
| stats_o/std                    | 0.026894527 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0294     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.237      |
| test/Q                         | -0.73895806 |
| test/Q_plus_P                  | -0.73895806 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00942    |
| train/info_shaping_reward_mean | -0.0606     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.4449469   |
| stats_o/std                    | 0.026888384 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0241     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.22       |
| test/Q                         | -0.82513434 |
| test/Q_plus_P                  | -0.82513434 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.656       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00855    |
| train/info_shaping_reward_mean | -0.0613     |
| train/info_shaping_reward_min  | -0.243      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.8       |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.4449465   |
| stats_o/std                    | 0.0268846   |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.843       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0199     |
| test/info_shaping_reward_mean  | -0.047      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -0.68511474 |
| test/Q_plus_P                  | -0.68511474 |
| test/reward_per_eps            | -6.3        |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.669       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00849    |
| train/info_shaping_reward_mean | -0.0611     |
| train/info_shaping_reward_min  | -0.256      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.4449443   |
| stats_o/std                    | 0.026881306 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0278     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.238      |
| test/Q                         | -0.7796036  |
| test/Q_plus_P                  | -0.7796036  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0075     |
| train/info_shaping_reward_mean | -0.0594     |
| train/info_shaping_reward_min  | -0.242      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 573         |
| stats_o/mean                   | 0.44494578  |
| stats_o/std                    | 0.026875531 |
| test/episodes                  | 5740        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0258     |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -0.77652395 |
| test/Q_plus_P                  | -0.77652395 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 229600      |
| train/episodes                 | 22960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00652    |
| train/info_shaping_reward_mean | -0.059      |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 918400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.44494608  |
| stats_o/std                    | 0.026872113 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0165     |
| test/info_shaping_reward_mean  | -0.0479     |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -0.75782275 |
| test/Q_plus_P                  | -0.75782275 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.657       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00699    |
| train/info_shaping_reward_mean | -0.0592     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.44494638  |
| stats_o/std                    | 0.026869042 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0225     |
| test/info_shaping_reward_mean  | -0.0486     |
| test/info_shaping_reward_min   | -0.229      |
| test/Q                         | -0.7825886  |
| test/Q_plus_P                  | -0.7825886  |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.633       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00962    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.4449463   |
| stats_o/std                    | 0.026866486 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0309     |
| test/info_shaping_reward_mean  | -0.0553     |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -0.82320684 |
| test/Q_plus_P                  | -0.82320684 |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00983    |
| train/info_shaping_reward_mean | -0.0603     |
| train/info_shaping_reward_min  | -0.262      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 577         |
| stats_o/mean                   | 0.44494548  |
| stats_o/std                    | 0.02686279  |
| test/episodes                  | 5780        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0313     |
| test/info_shaping_reward_mean  | -0.0561     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.77087736 |
| test/Q_plus_P                  | -0.77087736 |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 231200      |
| train/episodes                 | 23120       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.621       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00763    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.2       |
| train/steps                    | 924800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.44494522 |
| stats_o/std                    | 0.02685777 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.84       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0247    |
| test/info_shaping_reward_mean  | -0.0496    |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -0.6914139 |
| test/Q_plus_P                  | -0.6914139 |
| test/reward_per_eps            | -6.4       |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.639      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00757   |
| train/info_shaping_reward_mean | -0.0614    |
| train/info_shaping_reward_min  | -0.238     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -14.4      |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.4449451   |
| stats_o/std                    | 0.026855601 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0217     |
| test/info_shaping_reward_mean  | -0.0503     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.74073404 |
| test/Q_plus_P                  | -0.74073404 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.676       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0098     |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.24       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13         |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 580         |
| stats_o/mean                   | 0.44494417  |
| stats_o/std                    | 0.026851004 |
| test/episodes                  | 5810        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0301     |
| test/info_shaping_reward_mean  | -0.0545     |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -0.7998042  |
| test/Q_plus_P                  | -0.7998042  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 232400      |
| train/episodes                 | 23240       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0102     |
| train/info_shaping_reward_mean | -0.0607     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 929600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 581         |
| stats_o/mean                   | 0.44494367  |
| stats_o/std                    | 0.026846712 |
| test/episodes                  | 5820        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0226     |
| test/info_shaping_reward_mean  | -0.0495     |
| test/info_shaping_reward_min   | -0.228      |
| test/Q                         | -0.69708776 |
| test/Q_plus_P                  | -0.69708776 |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 232800      |
| train/episodes                 | 23280       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.67        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00847    |
| train/info_shaping_reward_mean | -0.0584     |
| train/info_shaping_reward_min  | -0.247      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.2       |
| train/steps                    | 931200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.44494203  |
| stats_o/std                    | 0.026845047 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0233     |
| test/info_shaping_reward_mean  | -0.0513     |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -0.77005243 |
| test/Q_plus_P                  | -0.77005243 |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.651       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00944    |
| train/info_shaping_reward_mean | -0.0609     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14         |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 583        |
| stats_o/mean                   | 0.44494045 |
| stats_o/std                    | 0.02684214 |
| test/episodes                  | 5840       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.845      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0219    |
| test/info_shaping_reward_mean  | -0.0524    |
| test/info_shaping_reward_min   | -0.214     |
| test/Q                         | -0.6448744 |
| test/Q_plus_P                  | -0.6448744 |
| test/reward_per_eps            | -6.2       |
| test/steps                     | 233600     |
| train/episodes                 | 23360      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.669      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.00795   |
| train/info_shaping_reward_mean | -0.0599    |
| train/info_shaping_reward_min  | -0.251     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -13.2      |
| train/steps                    | 934400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.4449413   |
| stats_o/std                    | 0.026837999 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.024      |
| test/info_shaping_reward_mean  | -0.0522     |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -0.7444875  |
| test/Q_plus_P                  | -0.7444875  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.649       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0112     |
| train/info_shaping_reward_mean | -0.0612     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.1       |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.44494018  |
| stats_o/std                    | 0.026836691 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.828       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.026      |
| test/info_shaping_reward_mean  | -0.0533     |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -0.8164746  |
| test/Q_plus_P                  | -0.8164746  |
| test/reward_per_eps            | -6.9        |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.636       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00942    |
| train/info_shaping_reward_mean | -0.0626     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.6       |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.4449384   |
| stats_o/std                    | 0.026834138 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.02       |
| test/info_shaping_reward_mean  | -0.0494     |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -0.73028684 |
| test/Q_plus_P                  | -0.73028684 |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.664       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00645    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.258      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.4       |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.44493905 |
| stats_o/std                    | 0.02683173 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.81       |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0305    |
| test/info_shaping_reward_mean  | -0.0573    |
| test/info_shaping_reward_min   | -0.262     |
| test/Q                         | -0.8585123 |
| test/Q_plus_P                  | -0.8585123 |
| test/reward_per_eps            | -7.6       |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.681      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0103    |
| train/info_shaping_reward_mean | -0.0608    |
| train/info_shaping_reward_min  | -0.252     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -12.8      |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.44493738  |
| stats_o/std                    | 0.026831688 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.83        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.016      |
| test/info_shaping_reward_mean  | -0.0454     |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -0.7199173  |
| test/Q_plus_P                  | -0.7199173  |
| test/reward_per_eps            | -6.8        |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.613       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00765    |
| train/info_shaping_reward_mean | -0.0632     |
| train/info_shaping_reward_min  | -0.255      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -15.5       |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.44493857  |
| stats_o/std                    | 0.026827998 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.818       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0242     |
| test/info_shaping_reward_mean  | -0.0527     |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -0.8207148  |
| test/Q_plus_P                  | -0.8207148  |
| test/reward_per_eps            | -7.3        |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.638       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00738    |
| train/info_shaping_reward_mean | -0.0628     |
| train/info_shaping_reward_min  | -0.26       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.5       |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.44493994  |
| stats_o/std                    | 0.026825408 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.833       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0288     |
| test/info_shaping_reward_mean  | -0.0514     |
| test/info_shaping_reward_min   | -0.256      |
| test/Q                         | -0.7217657  |
| test/Q_plus_P                  | -0.7217657  |
| test/reward_per_eps            | -6.7        |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00914    |
| train/info_shaping_reward_mean | -0.0614     |
| train/info_shaping_reward_min  | -0.25       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.4449393  |
| stats_o/std                    | 0.02682124 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 1          |
| test/info_is_success_mean      | 0.843      |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0346    |
| test/info_shaping_reward_mean  | -0.0559    |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -0.7061287 |
| test/Q_plus_P                  | -0.7061287 |
| test/reward_per_eps            | -6.3       |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 1          |
| train/info_is_success_mean     | 0.611      |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0121    |
| train/info_shaping_reward_mean | -0.0655    |
| train/info_shaping_reward_min  | -0.261     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -15.6      |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.44493827  |
| stats_o/std                    | 0.026818214 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.825       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0177     |
| test/info_shaping_reward_mean  | -0.0523     |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -0.78654206 |
| test/Q_plus_P                  | -0.78654206 |
| test/reward_per_eps            | -7          |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.661       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00885    |
| train/info_shaping_reward_mean | -0.0586     |
| train/info_shaping_reward_min  | -0.246      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.44493783  |
| stats_o/std                    | 0.026814163 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.845       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0281     |
| test/info_shaping_reward_mean  | -0.0502     |
| test/info_shaping_reward_min   | -0.225      |
| test/Q                         | -0.66322374 |
| test/Q_plus_P                  | -0.66322374 |
| test/reward_per_eps            | -6.2        |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.632       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00794    |
| train/info_shaping_reward_mean | -0.0623     |
| train/info_shaping_reward_min  | -0.248      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -14.7       |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 594         |
| stats_o/mean                   | 0.44493604  |
| stats_o/std                    | 0.026811255 |
| test/episodes                  | 5950        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.823       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0304     |
| test/info_shaping_reward_mean  | -0.0538     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.7754528  |
| test/Q_plus_P                  | -0.7754528  |
| test/reward_per_eps            | -7.1        |
| test/steps                     | 238000      |
| train/episodes                 | 23800       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.66        |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00847    |
| train/info_shaping_reward_mean | -0.0593     |
| train/info_shaping_reward_min  | -0.249      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.6       |
| train/steps                    | 952000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 595         |
| stats_o/mean                   | 0.44493628  |
| stats_o/std                    | 0.026804633 |
| test/episodes                  | 5960        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.8         |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0212     |
| test/info_shaping_reward_mean  | -0.0547     |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -0.9047351  |
| test/Q_plus_P                  | -0.9047351  |
| test/reward_per_eps            | -8          |
| test/steps                     | 238400      |
| train/episodes                 | 23840       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.658       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00653    |
| train/info_shaping_reward_mean | -0.0574     |
| train/info_shaping_reward_min  | -0.236      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.7       |
| train/steps                    | 953600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.44493484  |
| stats_o/std                    | 0.02679869  |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.82        |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0241     |
| test/info_shaping_reward_mean  | -0.0534     |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -0.81703186 |
| test/Q_plus_P                  | -0.81703186 |
| test/reward_per_eps            | -7.2        |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.678       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00615    |
| train/info_shaping_reward_mean | -0.0591     |
| train/info_shaping_reward_min  | -0.244      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -12.9       |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.44493422  |
| stats_o/std                    | 0.026795773 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.838       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0277     |
| test/info_shaping_reward_mean  | -0.0516     |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -0.6989869  |
| test/Q_plus_P                  | -0.6989869  |
| test/reward_per_eps            | -6.5        |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.652       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00856    |
| train/info_shaping_reward_mean | -0.0616     |
| train/info_shaping_reward_min  | -0.245      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.9       |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.4449354   |
| stats_o/std                    | 0.026789987 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.815       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.024      |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -0.853736   |
| test/Q_plus_P                  | -0.853736   |
| test/reward_per_eps            | -7.4        |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.663       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00783    |
| train/info_shaping_reward_mean | -0.0587     |
| train/info_shaping_reward_min  | -0.23       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.5       |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 599         |
| stats_o/mean                   | 0.44493428  |
| stats_o/std                    | 0.026787007 |
| test/episodes                  | 6000        |
| test/info_is_success_max       | 1           |
| test/info_is_success_mean      | 0.835       |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0308     |
| test/info_shaping_reward_mean  | -0.054      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -0.7012369  |
| test/Q_plus_P                  | -0.7012369  |
| test/reward_per_eps            | -6.6        |
| test/steps                     | 240000      |
| train/episodes                 | 24000       |
| train/info_is_success_max      | 1           |
| train/info_is_success_mean     | 0.673       |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.00969    |
| train/info_shaping_reward_mean | -0.0599     |
| train/info_shaping_reward_min  | -0.241      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -13.1       |
| train/steps                    | 960000      |
------------------------------------------------
Saving latest policy.
