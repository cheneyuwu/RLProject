Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC_Init/seed_3
epoch: 19 policy initialization loss: 0.0035745713394135237
epoch: 39 policy initialization loss: 0.0031518633477389812
epoch: 59 policy initialization loss: 0.0015274245524778962
epoch: 79 policy initialization loss: 0.0013037442695349455
epoch: 99 policy initialization loss: 0.0010330297518521547
epoch: 119 policy initialization loss: 0.0009728107834234834
epoch: 139 policy initialization loss: 0.0005408127326518297
epoch: 159 policy initialization loss: 0.002509636804461479
epoch: 179 policy initialization loss: 0.0006077354773879051
epoch: 199 policy initialization loss: 0.0005617794813588262
epoch: 219 policy initialization loss: 0.00042115006363019347
epoch: 239 policy initialization loss: 0.0003152354038320482
epoch: 259 policy initialization loss: 0.0009414755622856319
epoch: 279 policy initialization loss: 0.0003336051886435598
epoch: 299 policy initialization loss: 0.00038583087734878063
epoch: 319 policy initialization loss: 0.0002618719881866127
epoch: 339 policy initialization loss: 0.00024655103334225714
epoch: 359 policy initialization loss: 0.00021965213818475604
epoch: 379 policy initialization loss: 0.00012465591134969145
epoch: 399 policy initialization loss: 0.00016033725114539266
epoch: 419 policy initialization loss: 0.00034689740277826786
epoch: 439 policy initialization loss: 0.0002481166447978467
epoch: 459 policy initialization loss: 6.789635517634451e-05
epoch: 479 policy initialization loss: 0.0005091862403787673
epoch: 499 policy initialization loss: 0.0001315905392402783
epoch: 519 policy initialization loss: 0.00014133274089545012
epoch: 539 policy initialization loss: 0.00015268297283910215
epoch: 559 policy initialization loss: 6.987962842686102e-05
epoch: 579 policy initialization loss: 0.0002457383379805833
epoch: 599 policy initialization loss: 6.968503294046968e-05
epoch: 619 policy initialization loss: 0.00010479388583917171
epoch: 639 policy initialization loss: 0.0007772564422339201
epoch: 659 policy initialization loss: 0.00023412748123519123
epoch: 679 policy initialization loss: 0.00013300549471750855
epoch: 699 policy initialization loss: 0.0001022608412313275
epoch: 719 policy initialization loss: 0.00012062144378433004
epoch: 739 policy initialization loss: 8.706409425940365e-05
epoch: 759 policy initialization loss: 7.062630902510136e-05
epoch: 779 policy initialization loss: 0.00018488786008674651
epoch: 799 policy initialization loss: 0.00023745890939608216
epoch: 819 policy initialization loss: 3.8469122955575585e-05
epoch: 839 policy initialization loss: 0.00014210886729415506
epoch: 859 policy initialization loss: 0.002078611170873046
epoch: 879 policy initialization loss: 0.0004742695018649101
epoch: 899 policy initialization loss: 6.344551366055384e-05
epoch: 919 policy initialization loss: 4.96630382258445e-05
epoch: 939 policy initialization loss: 0.00012086845526937395
epoch: 959 policy initialization loss: 7.786737114656717e-05
epoch: 979 policy initialization loss: 0.0001606353762326762
epoch: 999 policy initialization loss: 2.859143933164887e-05
epoch: 1019 policy initialization loss: 1.929416976054199e-05
epoch: 1039 policy initialization loss: 2.6007026463048533e-05
epoch: 1059 policy initialization loss: 1.0434902833367232e-05
epoch: 1079 policy initialization loss: 6.05715140409302e-05
epoch: 1099 policy initialization loss: 0.0001548493019072339
epoch: 1119 policy initialization loss: 4.692358925240114e-05
epoch: 1139 policy initialization loss: 4.5390821469482034e-05
epoch: 1159 policy initialization loss: 2.4064573153737e-05
epoch: 1179 policy initialization loss: 0.00011280985927442089
epoch: 1199 policy initialization loss: 0.00017272122204303741
epoch: 1219 policy initialization loss: 4.1251965740229934e-05
epoch: 1239 policy initialization loss: 5.7572844525566325e-05
epoch: 1259 policy initialization loss: 3.1925872463034466e-05
epoch: 1279 policy initialization loss: 4.2111147195100784e-05
epoch: 1299 policy initialization loss: 0.0003773854114115238
epoch: 1319 policy initialization loss: 4.344800254330039e-05
epoch: 1339 policy initialization loss: 2.667903754627332e-05
epoch: 1359 policy initialization loss: 1.9038006939808838e-05
epoch: 1379 policy initialization loss: 3.683134855236858e-05
epoch: 1399 policy initialization loss: 2.609400326036848e-05
epoch: 1419 policy initialization loss: 1.226551194122294e-05
epoch: 1439 policy initialization loss: 0.001964844297617674
epoch: 1459 policy initialization loss: 0.00010550162551226094
epoch: 1479 policy initialization loss: 4.4809727114625275e-05
epoch: 1499 policy initialization loss: 4.7732555685797706e-05
epoch: 1519 policy initialization loss: 9.346977094537579e-06
epoch: 1539 policy initialization loss: 2.2014108253642917e-05
epoch: 1559 policy initialization loss: 0.0021378356032073498
epoch: 1579 policy initialization loss: 1.0222403943771496e-05
epoch: 1599 policy initialization loss: 0.0001588190789334476
epoch: 1619 policy initialization loss: 1.9833811165881343e-05
epoch: 1639 policy initialization loss: 4.890196942142211e-05
epoch: 1659 policy initialization loss: 0.000305808411212638
epoch: 1679 policy initialization loss: 6.999026663834229e-05
epoch: 1699 policy initialization loss: 3.060518065467477e-05
epoch: 1719 policy initialization loss: 1.093997161660809e-05
epoch: 1739 policy initialization loss: 4.3820014980155975e-05
epoch: 1759 policy initialization loss: 3.321181793580763e-05
epoch: 1779 policy initialization loss: 8.22912079456728e-06
epoch: 1799 policy initialization loss: 1.437805713067064e-05
epoch: 1819 policy initialization loss: 0.00043711840407922864
epoch: 1839 policy initialization loss: 4.985496343579143e-05
epoch: 1859 policy initialization loss: 2.535016756155528e-05
epoch: 1879 policy initialization loss: 2.5059764084289782e-05
epoch: 1899 policy initialization loss: 9.771088662091643e-05
epoch: 1919 policy initialization loss: 4.753139728563838e-05
epoch: 1939 policy initialization loss: 0.0019033757271245122
epoch: 1959 policy initialization loss: 7.767257557134144e-06
epoch: 1979 policy initialization loss: 0.00014718546299263835
epoch: 1999 policy initialization loss: 4.446407547220588e-05
Saving initial policy.
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.44322062  |
| stats_o/std                    | 0.038418707 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0707     |
| test/info_shaping_reward_mean  | -0.125      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -1.3112056  |
| test/Q_plus_P                  | -1.3112056  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0819     |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.43968943  |
| stats_o/std                    | 0.038813222 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.102      |
| test/info_shaping_reward_mean  | -0.176      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -1.6345079  |
| test/Q_plus_P                  | -1.6345079  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.077      |
| train/info_shaping_reward_mean | -0.163      |
| train/info_shaping_reward_min  | -0.275      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 2          |
| stats_o/mean                   | 0.43497992 |
| stats_o/std                    | 0.04228275 |
| test/episodes                  | 30         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.108     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -1.9760419 |
| test/Q_plus_P                  | -1.9760419 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1200       |
| train/episodes                 | 120        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0979    |
| train/info_shaping_reward_mean | -0.204     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 4800       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 3           |
| stats_o/mean                   | 0.4327487   |
| stats_o/std                    | 0.042273775 |
| test/episodes                  | 40          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.108      |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -2.3603263  |
| test/Q_plus_P                  | -2.3603263  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1600        |
| train/episodes                 | 160         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.109      |
| train/info_shaping_reward_mean | -0.194      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 6400        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.43113554  |
| stats_o/std                    | 0.041714907 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.105      |
| test/info_shaping_reward_mean  | -0.172      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -2.7758038  |
| test/Q_plus_P                  | -2.7758038  |
| test/reward_per_eps            | -40         |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0921     |
| train/info_shaping_reward_mean | -0.182      |
| train/info_shaping_reward_min  | -0.276      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 5           |
| stats_o/mean                   | 0.43069708  |
| stats_o/std                    | 0.041160904 |
| test/episodes                  | 60          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.103      |
| test/info_shaping_reward_mean  | -0.152      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -3.145222   |
| test/Q_plus_P                  | -3.145222   |
| test/reward_per_eps            | -40         |
| test/steps                     | 2400        |
| train/episodes                 | 240         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0889     |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.278      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 9600        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.4304222  |
| stats_o/std                    | 0.04058782 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0963    |
| test/info_shaping_reward_mean  | -0.144     |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -3.5876129 |
| test/Q_plus_P                  | -3.5876129 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.088     |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.271     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.43026498 |
| stats_o/std                    | 0.04054671 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0993    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -3.965392  |
| test/Q_plus_P                  | -3.965392  |
| test/reward_per_eps            | -40        |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0904    |
| train/info_shaping_reward_mean | -0.179     |
| train/info_shaping_reward_min  | -0.28      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 8           |
| stats_o/mean                   | 0.4304885   |
| stats_o/std                    | 0.040226538 |
| test/episodes                  | 90          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.1        |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -4.397386   |
| test/Q_plus_P                  | -4.397386   |
| test/reward_per_eps            | -40         |
| test/steps                     | 3600        |
| train/episodes                 | 360         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0827     |
| train/info_shaping_reward_mean | -0.171      |
| train/info_shaping_reward_min  | -0.278      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 14400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.43031642  |
| stats_o/std                    | 0.039982717 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0816     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -4.7955427  |
| test/Q_plus_P                  | -4.7955427  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0814     |
| train/info_shaping_reward_mean | -0.166      |
| train/info_shaping_reward_min  | -0.269      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.4303304   |
| stats_o/std                    | 0.039766204 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0992     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -5.1818376  |
| test/Q_plus_P                  | -5.1818376  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0843     |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 11         |
| stats_o/mean                   | 0.43004218 |
| stats_o/std                    | 0.03948989 |
| test/episodes                  | 120        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0848    |
| test/info_shaping_reward_mean  | -0.149     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -5.571709  |
| test/Q_plus_P                  | -5.571709  |
| test/reward_per_eps            | -40        |
| test/steps                     | 4800       |
| train/episodes                 | 480        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0792    |
| train/info_shaping_reward_mean | -0.166     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 19200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 12         |
| stats_o/mean                   | 0.42965457 |
| stats_o/std                    | 0.03943525 |
| test/episodes                  | 130        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0893    |
| test/info_shaping_reward_mean  | -0.152     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -5.979917  |
| test/Q_plus_P                  | -5.979917  |
| test/reward_per_eps            | -40        |
| test/steps                     | 5200       |
| train/episodes                 | 520        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0767    |
| train/info_shaping_reward_mean | -0.17      |
| train/info_shaping_reward_min  | -0.282     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 20800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.4295201   |
| stats_o/std                    | 0.039567526 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0924     |
| test/info_shaping_reward_mean  | -0.144      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -6.375299   |
| test/Q_plus_P                  | -6.375299   |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0888     |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.42934886  |
| stats_o/std                    | 0.039443195 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0898     |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -6.758663   |
| test/Q_plus_P                  | -6.758663   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.081      |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.283      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.42913035  |
| stats_o/std                    | 0.039434936 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0894     |
| test/info_shaping_reward_mean  | -0.155      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -7.1711597  |
| test/Q_plus_P                  | -7.1711597  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0724     |
| train/info_shaping_reward_mean | -0.172      |
| train/info_shaping_reward_min  | -0.283      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.42918527  |
| stats_o/std                    | 0.039253056 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.094      |
| test/info_shaping_reward_mean  | -0.144      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -7.5241127  |
| test/Q_plus_P                  | -7.5241127  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0775     |
| train/info_shaping_reward_mean | -0.157      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.42902842  |
| stats_o/std                    | 0.039167944 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.085      |
| test/info_shaping_reward_mean  | -0.151      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -7.9094243  |
| test/Q_plus_P                  | -7.9094243  |
| test/reward_per_eps            | -40         |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0739     |
| train/info_shaping_reward_mean | -0.167      |
| train/info_shaping_reward_min  | -0.293      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.42891034  |
| stats_o/std                    | 0.039225947 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0726     |
| test/info_shaping_reward_mean  | -0.138      |
| test/info_shaping_reward_min   | -0.253      |
| test/Q                         | -8.29188    |
| test/Q_plus_P                  | -8.29188    |
| test/reward_per_eps            | -40         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0745     |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.293      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 19          |
| stats_o/mean                   | 0.42850935  |
| stats_o/std                    | 0.039087508 |
| test/episodes                  | 200         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0914     |
| test/info_shaping_reward_mean  | -0.148      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -8.670111   |
| test/Q_plus_P                  | -8.670111   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8000        |
| train/episodes                 | 800         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0681     |
| train/info_shaping_reward_mean | -0.161      |
| train/info_shaping_reward_min  | -0.261      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 32000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 20         |
| stats_o/mean                   | 0.42811763 |
| stats_o/std                    | 0.03897185 |
| test/episodes                  | 210        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0788    |
| test/info_shaping_reward_mean  | -0.153     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -9.0585785 |
| test/Q_plus_P                  | -9.0585785 |
| test/reward_per_eps            | -40        |
| test/steps                     | 8400       |
| train/episodes                 | 840        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0861    |
| train/info_shaping_reward_mean | -0.177     |
| train/info_shaping_reward_min  | -0.284     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 33600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.42778382  |
| stats_o/std                    | 0.038891133 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0947     |
| test/info_shaping_reward_mean  | -0.159      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -9.394165   |
| test/Q_plus_P                  | -9.394165   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0827     |
| train/info_shaping_reward_mean | -0.178      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 22          |
| stats_o/mean                   | 0.42784965  |
| stats_o/std                    | 0.038915385 |
| test/episodes                  | 230         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.101      |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -9.793504   |
| test/Q_plus_P                  | -9.793504   |
| test/reward_per_eps            | -40         |
| test/steps                     | 9200        |
| train/episodes                 | 920         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0887     |
| train/info_shaping_reward_mean | -0.176      |
| train/info_shaping_reward_min  | -0.281      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 36800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 23          |
| stats_o/mean                   | 0.42765108  |
| stats_o/std                    | 0.038886078 |
| test/episodes                  | 240         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0955     |
| test/info_shaping_reward_mean  | -0.163      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -10.137814  |
| test/Q_plus_P                  | -10.137814  |
| test/reward_per_eps            | -40         |
| test/steps                     | 9600        |
| train/episodes                 | 960         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0929     |
| train/info_shaping_reward_mean | -0.179      |
| train/info_shaping_reward_min  | -0.274      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 38400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.42733192  |
| stats_o/std                    | 0.038759135 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0892     |
| test/info_shaping_reward_mean  | -0.155      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -10.5028    |
| test/Q_plus_P                  | -10.5028    |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0839     |
| train/info_shaping_reward_mean | -0.174      |
| train/info_shaping_reward_min  | -0.264      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.42712986 |
| stats_o/std                    | 0.0387441  |
| test/episodes                  | 260        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0926    |
| test/info_shaping_reward_mean  | -0.164     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -10.845342 |
| test/Q_plus_P                  | -10.845342 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0809    |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.291     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 26          |
| stats_o/mean                   | 0.42708123  |
| stats_o/std                    | 0.038632255 |
| test/episodes                  | 270         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0897     |
| test/info_shaping_reward_mean  | -0.143      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -11.176697  |
| test/Q_plus_P                  | -11.176697  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10800       |
| train/episodes                 | 1080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0744     |
| train/info_shaping_reward_mean | -0.165      |
| train/info_shaping_reward_min  | -0.28       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 43200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.42684552  |
| stats_o/std                    | 0.038655754 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0933     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -11.5340805 |
| test/Q_plus_P                  | -11.5340805 |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.102      |
| train/info_shaping_reward_mean | -0.186      |
| train/info_shaping_reward_min  | -0.289      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.42652595 |
| stats_o/std                    | 0.03869373 |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0923    |
| test/info_shaping_reward_mean  | -0.156     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -11.876169 |
| test/Q_plus_P                  | -11.876169 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0887    |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.42622468 |
| stats_o/std                    | 0.03862117 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0951    |
| test/info_shaping_reward_mean  | -0.157     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -12.196606 |
| test/Q_plus_P                  | -12.196606 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0849    |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.284     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 30          |
| stats_o/mean                   | 0.42594954  |
| stats_o/std                    | 0.038562614 |
| test/episodes                  | 310         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0989     |
| test/info_shaping_reward_mean  | -0.173      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -12.555249  |
| test/Q_plus_P                  | -12.555249  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12400       |
| train/episodes                 | 1240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0946     |
| train/info_shaping_reward_mean | -0.181      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 49600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 31         |
| stats_o/mean                   | 0.4256011  |
| stats_o/std                    | 0.03848954 |
| test/episodes                  | 320        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0916    |
| test/info_shaping_reward_mean  | -0.154     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -12.884614 |
| test/Q_plus_P                  | -12.884614 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12800      |
| train/episodes                 | 1280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0984    |
| train/info_shaping_reward_mean | -0.18      |
| train/info_shaping_reward_min  | -0.262     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 51200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.42539752  |
| stats_o/std                    | 0.038481753 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0905     |
| test/info_shaping_reward_mean  | -0.161      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -13.209223  |
| test/Q_plus_P                  | -13.209223  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0918     |
| train/info_shaping_reward_mean | -0.184      |
| train/info_shaping_reward_min  | -0.294      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.42523178  |
| stats_o/std                    | 0.038431697 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0976     |
| test/info_shaping_reward_mean  | -0.179      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -13.544539  |
| test/Q_plus_P                  | -13.544539  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0894     |
| train/info_shaping_reward_mean | -0.191      |
| train/info_shaping_reward_min  | -0.293      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.42502865  |
| stats_o/std                    | 0.038416974 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0944     |
| test/info_shaping_reward_mean  | -0.169      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -13.839029  |
| test/Q_plus_P                  | -13.839029  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.1        |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.291      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.42469287 |
| stats_o/std                    | 0.03840475 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0968    |
| test/info_shaping_reward_mean  | -0.163     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -14.169743 |
| test/Q_plus_P                  | -14.169743 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.099     |
| train/info_shaping_reward_mean | -0.19      |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.42450336 |
| stats_o/std                    | 0.03830082 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0943    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -14.468576 |
| test/Q_plus_P                  | -14.468576 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.182     |
| train/info_shaping_reward_min  | -0.276     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 37         |
| stats_o/mean                   | 0.42433655 |
| stats_o/std                    | 0.03822655 |
| test/episodes                  | 380        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0994    |
| test/info_shaping_reward_mean  | -0.168     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -14.786834 |
| test/Q_plus_P                  | -14.786834 |
| test/reward_per_eps            | -40        |
| test/steps                     | 15200      |
| train/episodes                 | 1520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0977    |
| train/info_shaping_reward_mean | -0.181     |
| train/info_shaping_reward_min  | -0.272     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 60800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 38          |
| stats_o/mean                   | 0.4240749   |
| stats_o/std                    | 0.038283408 |
| test/episodes                  | 390         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.095      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -15.09479   |
| test/Q_plus_P                  | -15.09479   |
| test/reward_per_eps            | -40         |
| test/steps                     | 15600       |
| train/episodes                 | 1560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0963     |
| train/info_shaping_reward_mean | -0.201      |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 62400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.42386332  |
| stats_o/std                    | 0.038249586 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.106      |
| test/info_shaping_reward_mean  | -0.188      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -15.394335  |
| test/Q_plus_P                  | -15.394335  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.103      |
| train/info_shaping_reward_mean | -0.193      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.42366     |
| stats_o/std                    | 0.038217615 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0988     |
| test/info_shaping_reward_mean  | -0.168      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -15.689345  |
| test/Q_plus_P                  | -15.689345  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.106      |
| train/info_shaping_reward_mean | -0.189      |
| train/info_shaping_reward_min  | -0.29       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.42359746  |
| stats_o/std                    | 0.038192317 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.11       |
| test/info_shaping_reward_mean  | -0.184      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -15.998584  |
| test/Q_plus_P                  | -15.998584  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.101      |
| train/info_shaping_reward_mean | -0.188      |
| train/info_shaping_reward_min  | -0.286      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.42341158  |
| stats_o/std                    | 0.038200025 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.099      |
| test/info_shaping_reward_mean  | -0.166      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -16.277962  |
| test/Q_plus_P                  | -16.277962  |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.1        |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.279      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.4232035  |
| stats_o/std                    | 0.03820706 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.108     |
| test/info_shaping_reward_mean  | -0.167     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -16.572336 |
| test/Q_plus_P                  | -16.572336 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.106     |
| train/info_shaping_reward_mean | -0.199     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.42306295  |
| stats_o/std                    | 0.038139578 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.11       |
| test/info_shaping_reward_mean  | -0.162      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -16.859743  |
| test/Q_plus_P                  | -16.859743  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.188      |
| train/info_shaping_reward_min  | -0.266      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 45         |
| stats_o/mean                   | 0.422984   |
| stats_o/std                    | 0.03814412 |
| test/episodes                  | 460        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0944    |
| test/info_shaping_reward_mean  | -0.179     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -17.150366 |
| test/Q_plus_P                  | -17.150366 |
| test/reward_per_eps            | -40        |
| test/steps                     | 18400      |
| train/episodes                 | 1840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.194     |
| train/info_shaping_reward_min  | -0.279     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 73600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 46          |
| stats_o/mean                   | 0.42288688  |
| stats_o/std                    | 0.038108524 |
| test/episodes                  | 470         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.12       |
| test/info_shaping_reward_mean  | -0.196      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -17.427963  |
| test/Q_plus_P                  | -17.427963  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18800       |
| train/episodes                 | 1880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0997     |
| train/info_shaping_reward_mean | -0.183      |
| train/info_shaping_reward_min  | -0.277      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 75200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 47         |
| stats_o/mean                   | 0.42281532 |
| stats_o/std                    | 0.0381564  |
| test/episodes                  | 480        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.106     |
| test/info_shaping_reward_mean  | -0.185     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -17.68381  |
| test/Q_plus_P                  | -17.68381  |
| test/reward_per_eps            | -40        |
| test/steps                     | 19200      |
| train/episodes                 | 1920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.199     |
| train/info_shaping_reward_min  | -0.292     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 76800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 48         |
| stats_o/mean                   | 0.42265356 |
| stats_o/std                    | 0.03815951 |
| test/episodes                  | 490        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.106     |
| test/info_shaping_reward_mean  | -0.187     |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -17.96834  |
| test/Q_plus_P                  | -17.96834  |
| test/reward_per_eps            | -40        |
| test/steps                     | 19600      |
| train/episodes                 | 1960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.202     |
| train/info_shaping_reward_min  | -0.283     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 78400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 49          |
| stats_o/mean                   | 0.42244616  |
| stats_o/std                    | 0.038152777 |
| test/episodes                  | 500         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.102      |
| test/info_shaping_reward_mean  | -0.18       |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -18.248728  |
| test/Q_plus_P                  | -18.248728  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20000       |
| train/episodes                 | 2000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.111      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 80000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.42223677  |
| stats_o/std                    | 0.038137976 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.103      |
| test/info_shaping_reward_mean  | -0.175      |
| test/info_shaping_reward_min   | -0.247      |
| test/Q                         | -18.510332  |
| test/Q_plus_P                  | -18.510332  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.105      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 51          |
| stats_o/mean                   | 0.42203668  |
| stats_o/std                    | 0.038142297 |
| test/episodes                  | 520         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.126      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -18.766468  |
| test/Q_plus_P                  | -18.766468  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20800       |
| train/episodes                 | 2080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.207      |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 83200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 52         |
| stats_o/mean                   | 0.421908   |
| stats_o/std                    | 0.03812697 |
| test/episodes                  | 530        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.099     |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -19.039515 |
| test/Q_plus_P                  | -19.039515 |
| test/reward_per_eps            | -40        |
| test/steps                     | 21200      |
| train/episodes                 | 2120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0968    |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.281     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 84800      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.42174527  |
| stats_o/std                    | 0.038130056 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.105      |
| test/info_shaping_reward_mean  | -0.181      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -19.278982  |
| test/Q_plus_P                  | -19.278982  |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.101      |
| train/info_shaping_reward_mean | -0.202      |
| train/info_shaping_reward_min  | -0.289      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 54          |
| stats_o/mean                   | 0.42155072  |
| stats_o/std                    | 0.038172554 |
| test/episodes                  | 550         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.102      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -19.535908  |
| test/Q_plus_P                  | -19.535908  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22000       |
| train/episodes                 | 2200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.21       |
| train/info_shaping_reward_min  | -0.289      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 88000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.42132163  |
| stats_o/std                    | 0.038203586 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.105      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -19.77553   |
| test/Q_plus_P                  | -19.77553   |
| test/reward_per_eps            | -40         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.208      |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 56          |
| stats_o/mean                   | 0.42116925  |
| stats_o/std                    | 0.038178705 |
| test/episodes                  | 570         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -20.042992  |
| test/Q_plus_P                  | -20.042992  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22800       |
| train/episodes                 | 2280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.109      |
| train/info_shaping_reward_mean | -0.197      |
| train/info_shaping_reward_min  | -0.279      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 91200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.42091492  |
| stats_o/std                    | 0.038227484 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.106      |
| test/info_shaping_reward_mean  | -0.19       |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -20.280981  |
| test/Q_plus_P                  | -20.280981  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.42075682  |
| stats_o/std                    | 0.038313035 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.131      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -20.51594   |
| test/Q_plus_P                  | -20.51594   |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.213      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.42071345  |
| stats_o/std                    | 0.038332608 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -20.77232   |
| test/Q_plus_P                  | -20.77232   |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.211      |
| train/info_shaping_reward_min  | -0.294      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 60         |
| stats_o/mean                   | 0.4207416  |
| stats_o/std                    | 0.03835149 |
| test/episodes                  | 610        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -21.010395 |
| test/Q_plus_P                  | -21.010395 |
| test/reward_per_eps            | -40        |
| test/steps                     | 24400      |
| train/episodes                 | 2440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.206     |
| train/info_shaping_reward_min  | -0.297     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 97600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 61         |
| stats_o/mean                   | 0.4206777  |
| stats_o/std                    | 0.03835861 |
| test/episodes                  | 620        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -21.23672  |
| test/Q_plus_P                  | -21.23672  |
| test/reward_per_eps            | -40        |
| test/steps                     | 24800      |
| train/episodes                 | 2480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.208     |
| train/info_shaping_reward_min  | -0.298     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 99200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.42059898  |
| stats_o/std                    | 0.038403932 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -21.47554   |
| test/Q_plus_P                  | -21.47554   |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.4204878   |
| stats_o/std                    | 0.038444594 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.102      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -21.698975  |
| test/Q_plus_P                  | -21.698975  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.108      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.42041814  |
| stats_o/std                    | 0.038480394 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.11       |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -21.939053  |
| test/Q_plus_P                  | -21.939053  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.42031088  |
| stats_o/std                    | 0.038482044 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -22.15418   |
| test/Q_plus_P                  | -22.15418   |
| test/reward_per_eps            | -40         |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.4201734   |
| stats_o/std                    | 0.038605265 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.105      |
| test/info_shaping_reward_mean  | -0.17       |
| test/info_shaping_reward_min   | -0.239      |
| test/Q                         | -22.387684  |
| test/Q_plus_P                  | -22.387684  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 67          |
| stats_o/mean                   | 0.4200021   |
| stats_o/std                    | 0.038667887 |
| test/episodes                  | 680         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -22.560614  |
| test/Q_plus_P                  | -22.560614  |
| test/reward_per_eps            | -40         |
| test/steps                     | 27200       |
| train/episodes                 | 2720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 108800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.41984642  |
| stats_o/std                    | 0.038769357 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -22.755684  |
| test/Q_plus_P                  | -22.755684  |
| test/reward_per_eps            | -40         |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.4196978  |
| stats_o/std                    | 0.03882699 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.128     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -23.000893 |
| test/Q_plus_P                  | -23.000893 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 70          |
| stats_o/mean                   | 0.41950336  |
| stats_o/std                    | 0.038848095 |
| test/episodes                  | 710         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -23.184717  |
| test/Q_plus_P                  | -23.184717  |
| test/reward_per_eps            | -40         |
| test/steps                     | 28400       |
| train/episodes                 | 2840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 113600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.4193652  |
| stats_o/std                    | 0.03888428 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.111     |
| test/info_shaping_reward_mean  | -0.191     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -23.404192 |
| test/Q_plus_P                  | -23.404192 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.41932812  |
| stats_o/std                    | 0.038935836 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -23.62982   |
| test/Q_plus_P                  | -23.62982   |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.4192256   |
| stats_o/std                    | 0.038946733 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.121      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -23.806856  |
| test/Q_plus_P                  | -23.806856  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 74         |
| stats_o/mean                   | 0.41911468 |
| stats_o/std                    | 0.03894401 |
| test/episodes                  | 750        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.117     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -24.051731 |
| test/Q_plus_P                  | -24.051731 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30000      |
| train/episodes                 | 3000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 120000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 75         |
| stats_o/mean                   | 0.41898644 |
| stats_o/std                    | 0.03895594 |
| test/episodes                  | 760        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -24.194668 |
| test/Q_plus_P                  | -24.194668 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30400      |
| train/episodes                 | 3040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 121600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 76         |
| stats_o/mean                   | 0.41882905 |
| stats_o/std                    | 0.03900336 |
| test/episodes                  | 770        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -24.407217 |
| test/Q_plus_P                  | -24.407217 |
| test/reward_per_eps            | -40        |
| test/steps                     | 30800      |
| train/episodes                 | 3080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 123200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 77          |
| stats_o/mean                   | 0.41875303  |
| stats_o/std                    | 0.039014958 |
| test/episodes                  | 780         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -24.57707   |
| test/Q_plus_P                  | -24.57707   |
| test/reward_per_eps            | -40         |
| test/steps                     | 31200       |
| train/episodes                 | 3120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 124800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 78         |
| stats_o/mean                   | 0.41870305 |
| stats_o/std                    | 0.03902223 |
| test/episodes                  | 790        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.119     |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -24.7925   |
| test/Q_plus_P                  | -24.7925   |
| test/reward_per_eps            | -40        |
| test/steps                     | 31600      |
| train/episodes                 | 3160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 126400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.41860533 |
| stats_o/std                    | 0.03906067 |
| test/episodes                  | 800        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -24.975615 |
| test/Q_plus_P                  | -24.975615 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 80          |
| stats_o/mean                   | 0.41857052  |
| stats_o/std                    | 0.039061252 |
| test/episodes                  | 810         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -25.15252   |
| test/Q_plus_P                  | -25.15252   |
| test/reward_per_eps            | -40         |
| test/steps                     | 32400       |
| train/episodes                 | 3240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 129600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.4185109   |
| stats_o/std                    | 0.039103415 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -25.343487  |
| test/Q_plus_P                  | -25.343487  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.4185041   |
| stats_o/std                    | 0.039107937 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.127      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -25.521843  |
| test/Q_plus_P                  | -25.521843  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.41848335  |
| stats_o/std                    | 0.039105993 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -25.746479  |
| test/Q_plus_P                  | -25.746479  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 84          |
| stats_o/mean                   | 0.41846249  |
| stats_o/std                    | 0.039126582 |
| test/episodes                  | 850         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -25.885984  |
| test/Q_plus_P                  | -25.885984  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34000       |
| train/episodes                 | 3400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 136000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 85         |
| stats_o/mean                   | 0.41847196 |
| stats_o/std                    | 0.03914882 |
| test/episodes                  | 860        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.123     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -26.070732 |
| test/Q_plus_P                  | -26.070732 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34400      |
| train/episodes                 | 3440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 137600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.41843477  |
| stats_o/std                    | 0.039127264 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -26.23153   |
| test/Q_plus_P                  | -26.23153   |
| test/reward_per_eps            | -40         |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.21       |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 87          |
| stats_o/mean                   | 0.41829744  |
| stats_o/std                    | 0.039169207 |
| test/episodes                  | 880         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.122      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -26.40924   |
| test/Q_plus_P                  | -26.40924   |
| test/reward_per_eps            | -40         |
| test/steps                     | 35200       |
| train/episodes                 | 3520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 140800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.41824892  |
| stats_o/std                    | 0.039207477 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -26.600492  |
| test/Q_plus_P                  | -26.600492  |
| test/reward_per_eps            | -40         |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 89          |
| stats_o/mean                   | 0.41810215  |
| stats_o/std                    | 0.039266806 |
| test/episodes                  | 900         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -26.760859  |
| test/Q_plus_P                  | -26.760859  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36000       |
| train/episodes                 | 3600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 144000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.41801783  |
| stats_o/std                    | 0.039257787 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -26.895576  |
| test/Q_plus_P                  | -26.895576  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 91         |
| stats_o/mean                   | 0.4178939  |
| stats_o/std                    | 0.03925734 |
| test/episodes                  | 920        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -27.090239 |
| test/Q_plus_P                  | -27.090239 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36800      |
| train/episodes                 | 3680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 147200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.41779557 |
| stats_o/std                    | 0.03930055 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -27.23435  |
| test/Q_plus_P                  | -27.23435  |
| test/reward_per_eps            | -40        |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.41777274  |
| stats_o/std                    | 0.039299812 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -27.364616  |
| test/Q_plus_P                  | -27.364616  |
| test/reward_per_eps            | -40         |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.41770074 |
| stats_o/std                    | 0.03929631 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -27.515348 |
| test/Q_plus_P                  | -27.515348 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.41765735  |
| stats_o/std                    | 0.039308157 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -27.697529  |
| test/Q_plus_P                  | -27.697529  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.41761544 |
| stats_o/std                    | 0.03930633 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.136     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -27.842274 |
| test/Q_plus_P                  | -27.842274 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 97          |
| stats_o/mean                   | 0.41756102  |
| stats_o/std                    | 0.039305728 |
| test/episodes                  | 980         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -28.009882  |
| test/Q_plus_P                  | -28.009882  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39200       |
| train/episodes                 | 3920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 156800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.41754636  |
| stats_o/std                    | 0.039331667 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.122      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -28.133207  |
| test/Q_plus_P                  | -28.133207  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 99          |
| stats_o/mean                   | 0.4174938   |
| stats_o/std                    | 0.039327566 |
| test/episodes                  | 1000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.186      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -28.299614  |
| test/Q_plus_P                  | -28.299614  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40000       |
| train/episodes                 | 4000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 160000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 100        |
| stats_o/mean                   | 0.41750038 |
| stats_o/std                    | 0.03937505 |
| test/episodes                  | 1010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -28.415327 |
| test/Q_plus_P                  | -28.415327 |
| test/reward_per_eps            | -40        |
| test/steps                     | 40400      |
| train/episodes                 | 4040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 161600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.41739753  |
| stats_o/std                    | 0.039413188 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -28.54079   |
| test/Q_plus_P                  | -28.54079   |
| test/reward_per_eps            | -40         |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.41733804  |
| stats_o/std                    | 0.039436795 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -28.702871  |
| test/Q_plus_P                  | -28.702871  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.41727403  |
| stats_o/std                    | 0.039460827 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.121      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -28.83321   |
| test/Q_plus_P                  | -28.83321   |
| test/reward_per_eps            | -40         |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.41725358  |
| stats_o/std                    | 0.039487477 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0956     |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -28.978535  |
| test/Q_plus_P                  | -28.978535  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 105        |
| stats_o/mean                   | 0.41728663 |
| stats_o/std                    | 0.03947799 |
| test/episodes                  | 1060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.12      |
| test/info_shaping_reward_mean  | -0.192     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -29.131084 |
| test/Q_plus_P                  | -29.131084 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42400      |
| train/episodes                 | 4240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.286     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 169600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 106        |
| stats_o/mean                   | 0.41723606 |
| stats_o/std                    | 0.03950695 |
| test/episodes                  | 1070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.119     |
| test/info_shaping_reward_mean  | -0.209     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -29.257063 |
| test/Q_plus_P                  | -29.257063 |
| test/reward_per_eps            | -40        |
| test/steps                     | 42800      |
| train/episodes                 | 4280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 171200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 107        |
| stats_o/mean                   | 0.41713968 |
| stats_o/std                    | 0.03954008 |
| test/episodes                  | 1080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -29.352354 |
| test/Q_plus_P                  | -29.352354 |
| test/reward_per_eps            | -40        |
| test/steps                     | 43200      |
| train/episodes                 | 4320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 172800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 108         |
| stats_o/mean                   | 0.41707158  |
| stats_o/std                    | 0.039554622 |
| test/episodes                  | 1090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -29.509253  |
| test/Q_plus_P                  | -29.509253  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43600       |
| train/episodes                 | 4360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 174400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 109         |
| stats_o/mean                   | 0.41698977  |
| stats_o/std                    | 0.039559953 |
| test/episodes                  | 1100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -29.64945   |
| test/Q_plus_P                  | -29.64945   |
| test/reward_per_eps            | -40         |
| test/steps                     | 44000       |
| train/episodes                 | 4400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 176000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 110         |
| stats_o/mean                   | 0.4169393   |
| stats_o/std                    | 0.039582763 |
| test/episodes                  | 1110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.104      |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -29.801085  |
| test/Q_plus_P                  | -29.801085  |
| test/reward_per_eps            | -40         |
| test/steps                     | 44400       |
| train/episodes                 | 4440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 177600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 111        |
| stats_o/mean                   | 0.41691217 |
| stats_o/std                    | 0.03958316 |
| test/episodes                  | 1120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -29.9733   |
| test/Q_plus_P                  | -29.9733   |
| test/reward_per_eps            | -40        |
| test/steps                     | 44800      |
| train/episodes                 | 4480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 179200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 112        |
| stats_o/mean                   | 0.41694155 |
| stats_o/std                    | 0.03962167 |
| test/episodes                  | 1130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -29.966963 |
| test/Q_plus_P                  | -29.966963 |
| test/reward_per_eps            | -40        |
| test/steps                     | 45200      |
| train/episodes                 | 4520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 180800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.41690794  |
| stats_o/std                    | 0.039647724 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -30.120352  |
| test/Q_plus_P                  | -30.120352  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.41684905  |
| stats_o/std                    | 0.039680526 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -30.290665  |
| test/Q_plus_P                  | -30.290665  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.41673458  |
| stats_o/std                    | 0.039683785 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -30.40517   |
| test/Q_plus_P                  | -30.40517   |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.41666546  |
| stats_o/std                    | 0.039689038 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -30.456396  |
| test/Q_plus_P                  | -30.456396  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 117         |
| stats_o/mean                   | 0.41661683  |
| stats_o/std                    | 0.039698217 |
| test/episodes                  | 1180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.224      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -30.62609   |
| test/Q_plus_P                  | -30.62609   |
| test/reward_per_eps            | -40         |
| test/steps                     | 47200       |
| train/episodes                 | 4720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 188800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.41663697  |
| stats_o/std                    | 0.039705988 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -30.751738  |
| test/Q_plus_P                  | -30.751738  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.11       |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.41652536  |
| stats_o/std                    | 0.039735813 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.126      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -30.86383   |
| test/Q_plus_P                  | -30.86383   |
| test/reward_per_eps            | -40         |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 120        |
| stats_o/mean                   | 0.41646114 |
| stats_o/std                    | 0.03974471 |
| test/episodes                  | 1210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -30.90332  |
| test/Q_plus_P                  | -30.90332  |
| test/reward_per_eps            | -40        |
| test/steps                     | 48400      |
| train/episodes                 | 4840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 193600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.41636875  |
| stats_o/std                    | 0.039768185 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -31.048578  |
| test/Q_plus_P                  | -31.048578  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 122         |
| stats_o/mean                   | 0.4163139   |
| stats_o/std                    | 0.039797544 |
| test/episodes                  | 1230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -31.159273  |
| test/Q_plus_P                  | -31.159273  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49200       |
| train/episodes                 | 4920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 196800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.41629264  |
| stats_o/std                    | 0.039824564 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -31.284758  |
| test/Q_plus_P                  | -31.284758  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.41620326  |
| stats_o/std                    | 0.039830316 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -31.382252  |
| test/Q_plus_P                  | -31.382252  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 125         |
| stats_o/mean                   | 0.41617194  |
| stats_o/std                    | 0.039858494 |
| test/episodes                  | 1260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -31.514868  |
| test/Q_plus_P                  | -31.514868  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50400       |
| train/episodes                 | 5040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 201600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.41609094  |
| stats_o/std                    | 0.039861448 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -31.618984  |
| test/Q_plus_P                  | -31.618984  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.41605258  |
| stats_o/std                    | 0.039898384 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -31.69967   |
| test/Q_plus_P                  | -31.69967   |
| test/reward_per_eps            | -40         |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 128         |
| stats_o/mean                   | 0.41601315  |
| stats_o/std                    | 0.039915312 |
| test/episodes                  | 1290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -31.81072   |
| test/Q_plus_P                  | -31.81072   |
| test/reward_per_eps            | -40         |
| test/steps                     | 51600       |
| train/episodes                 | 5160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 206400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 129        |
| stats_o/mean                   | 0.41597065 |
| stats_o/std                    | 0.03990577 |
| test/episodes                  | 1300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -31.926935 |
| test/Q_plus_P                  | -31.926935 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52000      |
| train/episodes                 | 5200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 208000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 130         |
| stats_o/mean                   | 0.41593155  |
| stats_o/std                    | 0.039960142 |
| test/episodes                  | 1310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -31.990732  |
| test/Q_plus_P                  | -31.990732  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52400       |
| train/episodes                 | 5240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 209600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.41585538 |
| stats_o/std                    | 0.03997646 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.135     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -32.091465 |
| test/Q_plus_P                  | -32.091465 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.4158148   |
| stats_o/std                    | 0.039991774 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -32.174072  |
| test/Q_plus_P                  | -32.174072  |
| test/reward_per_eps            | -40         |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 133         |
| stats_o/mean                   | 0.41579843  |
| stats_o/std                    | 0.040015597 |
| test/episodes                  | 1340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -32.28293   |
| test/Q_plus_P                  | -32.28293   |
| test/reward_per_eps            | -40         |
| test/steps                     | 53600       |
| train/episodes                 | 5360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 214400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.41580328  |
| stats_o/std                    | 0.040027402 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.111      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -32.432167  |
| test/Q_plus_P                  | -32.432167  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 135        |
| stats_o/mean                   | 0.41578403 |
| stats_o/std                    | 0.04001723 |
| test/episodes                  | 1360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -32.48254  |
| test/Q_plus_P                  | -32.48254  |
| test/reward_per_eps            | -40        |
| test/steps                     | 54400      |
| train/episodes                 | 5440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 217600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 136        |
| stats_o/mean                   | 0.41577342 |
| stats_o/std                    | 0.04001795 |
| test/episodes                  | 1370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -32.580677 |
| test/Q_plus_P                  | -32.580677 |
| test/reward_per_eps            | -40        |
| test/steps                     | 54800      |
| train/episodes                 | 5480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 219200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 137        |
| stats_o/mean                   | 0.415772   |
| stats_o/std                    | 0.04007392 |
| test/episodes                  | 1380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -32.6589   |
| test/Q_plus_P                  | -32.6589   |
| test/reward_per_eps            | -40        |
| test/steps                     | 55200      |
| train/episodes                 | 5520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 220800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 138         |
| stats_o/mean                   | 0.41569093  |
| stats_o/std                    | 0.040086903 |
| test/episodes                  | 1390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -32.763744  |
| test/Q_plus_P                  | -32.763744  |
| test/reward_per_eps            | -40         |
| test/steps                     | 55600       |
| train/episodes                 | 5560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 222400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.41564283 |
| stats_o/std                    | 0.04009505 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.335     |
| test/Q                         | -32.802807 |
| test/Q_plus_P                  | -32.802807 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 140         |
| stats_o/mean                   | 0.4156064   |
| stats_o/std                    | 0.040083174 |
| test/episodes                  | 1410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -32.91954   |
| test/Q_plus_P                  | -32.91954   |
| test/reward_per_eps            | -40         |
| test/steps                     | 56400       |
| train/episodes                 | 5640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 225600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.41560102  |
| stats_o/std                    | 0.040104833 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -32.990185  |
| test/Q_plus_P                  | -32.990185  |
| test/reward_per_eps            | -40         |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 142        |
| stats_o/mean                   | 0.41555747 |
| stats_o/std                    | 0.04009374 |
| test/episodes                  | 1430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.112     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -33.120663 |
| test/Q_plus_P                  | -33.120663 |
| test/reward_per_eps            | -40        |
| test/steps                     | 57200      |
| train/episodes                 | 5720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 228800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 143        |
| stats_o/mean                   | 0.41550422 |
| stats_o/std                    | 0.0400819  |
| test/episodes                  | 1440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.111     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -33.14192  |
| test/Q_plus_P                  | -33.14192  |
| test/reward_per_eps            | -40        |
| test/steps                     | 57600      |
| train/episodes                 | 5760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 230400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 144        |
| stats_o/mean                   | 0.41549277 |
| stats_o/std                    | 0.04009972 |
| test/episodes                  | 1450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.121     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -33.25976  |
| test/Q_plus_P                  | -33.25976  |
| test/reward_per_eps            | -40        |
| test/steps                     | 58000      |
| train/episodes                 | 5800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 232000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.41546413  |
| stats_o/std                    | 0.040096316 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -33.34826   |
| test/Q_plus_P                  | -33.34826   |
| test/reward_per_eps            | -40         |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 146         |
| stats_o/mean                   | 0.415447    |
| stats_o/std                    | 0.040105853 |
| test/episodes                  | 1470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -33.389515  |
| test/Q_plus_P                  | -33.389515  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58800       |
| train/episodes                 | 5880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 235200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 147         |
| stats_o/mean                   | 0.41543218  |
| stats_o/std                    | 0.040138204 |
| test/episodes                  | 1480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -33.535557  |
| test/Q_plus_P                  | -33.535557  |
| test/reward_per_eps            | -40         |
| test/steps                     | 59200       |
| train/episodes                 | 5920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 236800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 148         |
| stats_o/mean                   | 0.41542456  |
| stats_o/std                    | 0.040145714 |
| test/episodes                  | 1490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.251      |
| test/Q                         | -33.623184  |
| test/Q_plus_P                  | -33.623184  |
| test/reward_per_eps            | -40         |
| test/steps                     | 59600       |
| train/episodes                 | 5960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 238400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 149        |
| stats_o/mean                   | 0.41537443 |
| stats_o/std                    | 0.04016844 |
| test/episodes                  | 1500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -33.699223 |
| test/Q_plus_P                  | -33.699223 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60000      |
| train/episodes                 | 6000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 240000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.41532788 |
| stats_o/std                    | 0.04020703 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.32      |
| test/Q                         | -33.74677  |
| test/Q_plus_P                  | -33.74677  |
| test/reward_per_eps            | -40        |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.4153149   |
| stats_o/std                    | 0.040236156 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -33.872684  |
| test/Q_plus_P                  | -33.872684  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.41527906 |
| stats_o/std                    | 0.04025772 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -33.92748  |
| test/Q_plus_P                  | -33.92748  |
| test/reward_per_eps            | -40        |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.41532314  |
| stats_o/std                    | 0.040272716 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -33.997654  |
| test/Q_plus_P                  | -33.997654  |
| test/reward_per_eps            | -40         |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.41533127 |
| stats_o/std                    | 0.04028529 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.124     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -34.02987  |
| test/Q_plus_P                  | -34.02987  |
| test/reward_per_eps            | -40        |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.41534355  |
| stats_o/std                    | 0.040302854 |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.13       |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -34.12645   |
| test/Q_plus_P                  | -34.12645   |
| test/reward_per_eps            | -40         |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.4152638  |
| stats_o/std                    | 0.04029655 |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -34.197254 |
| test/Q_plus_P                  | -34.197254 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.4152452   |
| stats_o/std                    | 0.040265273 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -34.318222  |
| test/Q_plus_P                  | -34.318222  |
| test/reward_per_eps            | -40         |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.21       |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.41525218  |
| stats_o/std                    | 0.040279586 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -34.376827  |
| test/Q_plus_P                  | -34.376827  |
| test/reward_per_eps            | -40         |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 159        |
| stats_o/mean                   | 0.41523513 |
| stats_o/std                    | 0.04030816 |
| test/episodes                  | 1600       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -34.438526 |
| test/Q_plus_P                  | -34.438526 |
| test/reward_per_eps            | -40        |
| test/steps                     | 64000      |
| train/episodes                 | 6400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 256000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.41524574 |
| stats_o/std                    | 0.04032551 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -34.54321  |
| test/Q_plus_P                  | -34.54321  |
| test/reward_per_eps            | -40        |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.4152356   |
| stats_o/std                    | 0.040343527 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -34.54946   |
| test/Q_plus_P                  | -34.54946   |
| test/reward_per_eps            | -40         |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.41519925  |
| stats_o/std                    | 0.040336315 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -34.635075  |
| test/Q_plus_P                  | -34.635075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 163         |
| stats_o/mean                   | 0.41518602  |
| stats_o/std                    | 0.040346984 |
| test/episodes                  | 1640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -34.68411   |
| test/Q_plus_P                  | -34.68411   |
| test/reward_per_eps            | -40         |
| test/steps                     | 65600       |
| train/episodes                 | 6560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 262400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.41517213  |
| stats_o/std                    | 0.040347755 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -34.810112  |
| test/Q_plus_P                  | -34.810112  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 165         |
| stats_o/mean                   | 0.41516146  |
| stats_o/std                    | 0.040360577 |
| test/episodes                  | 1660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -34.84354   |
| test/Q_plus_P                  | -34.84354   |
| test/reward_per_eps            | -40         |
| test/steps                     | 66400       |
| train/episodes                 | 6640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 265600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.4151317   |
| stats_o/std                    | 0.040346008 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -34.933403  |
| test/Q_plus_P                  | -34.933403  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 167         |
| stats_o/mean                   | 0.41514066  |
| stats_o/std                    | 0.040359695 |
| test/episodes                  | 1680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -34.96621   |
| test/Q_plus_P                  | -34.96621   |
| test/reward_per_eps            | -40         |
| test/steps                     | 67200       |
| train/episodes                 | 6720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 268800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 168         |
| stats_o/mean                   | 0.41514623  |
| stats_o/std                    | 0.040375818 |
| test/episodes                  | 1690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -35.001083  |
| test/Q_plus_P                  | -35.001083  |
| test/reward_per_eps            | -40         |
| test/steps                     | 67600       |
| train/episodes                 | 6760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 270400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 169        |
| stats_o/mean                   | 0.41518995 |
| stats_o/std                    | 0.04037926 |
| test/episodes                  | 1700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -35.08943  |
| test/Q_plus_P                  | -35.08943  |
| test/reward_per_eps            | -40        |
| test/steps                     | 68000      |
| train/episodes                 | 6800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 272000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 170         |
| stats_o/mean                   | 0.4151946   |
| stats_o/std                    | 0.040389907 |
| test/episodes                  | 1710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.19       |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -35.19436   |
| test/Q_plus_P                  | -35.19436   |
| test/reward_per_eps            | -40         |
| test/steps                     | 68400       |
| train/episodes                 | 6840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 273600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.41517162  |
| stats_o/std                    | 0.040390108 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.116      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -35.116756  |
| test/Q_plus_P                  | -35.116756  |
| test/reward_per_eps            | -40         |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.4151394  |
| stats_o/std                    | 0.04039096 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -35.311897 |
| test/Q_plus_P                  | -35.311897 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 173        |
| stats_o/mean                   | 0.41512713 |
| stats_o/std                    | 0.04042374 |
| test/episodes                  | 1740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -35.333054 |
| test/Q_plus_P                  | -35.333054 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69600      |
| train/episodes                 | 6960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 278400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.4150847   |
| stats_o/std                    | 0.040424407 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -35.386753  |
| test/Q_plus_P                  | -35.386753  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 175        |
| stats_o/mean                   | 0.41509452 |
| stats_o/std                    | 0.04042661 |
| test/episodes                  | 1760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -35.42568  |
| test/Q_plus_P                  | -35.42568  |
| test/reward_per_eps            | -40        |
| test/steps                     | 70400      |
| train/episodes                 | 7040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 281600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.41513076  |
| stats_o/std                    | 0.040429115 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -35.478054  |
| test/Q_plus_P                  | -35.478054  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.41512093  |
| stats_o/std                    | 0.040436357 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -35.537647  |
| test/Q_plus_P                  | -35.537647  |
| test/reward_per_eps            | -40         |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 178        |
| stats_o/mean                   | 0.41511643 |
| stats_o/std                    | 0.04043393 |
| test/episodes                  | 1790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.119     |
| test/info_shaping_reward_mean  | -0.193     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -35.567734 |
| test/Q_plus_P                  | -35.567734 |
| test/reward_per_eps            | -40        |
| test/steps                     | 71600      |
| train/episodes                 | 7160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 286400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 179        |
| stats_o/mean                   | 0.41512704 |
| stats_o/std                    | 0.04045377 |
| test/episodes                  | 1800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -35.633667 |
| test/Q_plus_P                  | -35.633667 |
| test/reward_per_eps            | -40        |
| test/steps                     | 72000      |
| train/episodes                 | 7200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 288000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 180         |
| stats_o/mean                   | 0.41515514  |
| stats_o/std                    | 0.040467396 |
| test/episodes                  | 1810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -35.704506  |
| test/Q_plus_P                  | -35.704506  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72400       |
| train/episodes                 | 7240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 289600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.4151622   |
| stats_o/std                    | 0.040473595 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -35.714252  |
| test/Q_plus_P                  | -35.714252  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.41511628 |
| stats_o/std                    | 0.04049102 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -35.793407 |
| test/Q_plus_P                  | -35.793407 |
| test/reward_per_eps            | -40        |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 183         |
| stats_o/mean                   | 0.41516247  |
| stats_o/std                    | 0.040496163 |
| test/episodes                  | 1840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.118      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -35.8626    |
| test/Q_plus_P                  | -35.8626    |
| test/reward_per_eps            | -40         |
| test/steps                     | 73600       |
| train/episodes                 | 7360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 294400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.41518494  |
| stats_o/std                    | 0.040510453 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.124      |
| test/info_shaping_reward_mean  | -0.202      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -35.92567   |
| test/Q_plus_P                  | -35.92567   |
| test/reward_per_eps            | -40         |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.41520476  |
| stats_o/std                    | 0.040497463 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -35.956608  |
| test/Q_plus_P                  | -35.956608  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 186         |
| stats_o/mean                   | 0.41518232  |
| stats_o/std                    | 0.040479664 |
| test/episodes                  | 1870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.258      |
| test/Q                         | -35.995922  |
| test/Q_plus_P                  | -35.995922  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74800       |
| train/episodes                 | 7480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 299200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.4151766   |
| stats_o/std                    | 0.040490966 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -36.083355  |
| test/Q_plus_P                  | -36.083355  |
| test/reward_per_eps            | -40         |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 188        |
| stats_o/mean                   | 0.41518638 |
| stats_o/std                    | 0.04049537 |
| test/episodes                  | 1890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.085     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -36.10774  |
| test/Q_plus_P                  | -36.10774  |
| test/reward_per_eps            | -40        |
| test/steps                     | 75600      |
| train/episodes                 | 7560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 302400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.41518986  |
| stats_o/std                    | 0.040514864 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0807     |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -36.15669   |
| test/Q_plus_P                  | -36.15669   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 190         |
| stats_o/mean                   | 0.4151741   |
| stats_o/std                    | 0.040494725 |
| test/episodes                  | 1910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.105      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -36.214703  |
| test/Q_plus_P                  | -36.214703  |
| test/reward_per_eps            | -40         |
| test/steps                     | 76400       |
| train/episodes                 | 7640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.206      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 305600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 191        |
| stats_o/mean                   | 0.41518176 |
| stats_o/std                    | 0.04049493 |
| test/episodes                  | 1920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -36.242794 |
| test/Q_plus_P                  | -36.242794 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76800      |
| train/episodes                 | 7680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 307200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.4151849   |
| stats_o/std                    | 0.040488485 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -36.27251   |
| test/Q_plus_P                  | -36.27251   |
| test/reward_per_eps            | -40         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.41520488  |
| stats_o/std                    | 0.040483627 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.102      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -36.38185   |
| test/Q_plus_P                  | -36.38185   |
| test/reward_per_eps            | -40         |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.4151741  |
| stats_o/std                    | 0.04048134 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0742    |
| test/info_shaping_reward_mean  | -0.178     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -36.356537 |
| test/Q_plus_P                  | -36.356537 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.41517687  |
| stats_o/std                    | 0.040479068 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -36.433407  |
| test/Q_plus_P                  | -36.433407  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.41514698 |
| stats_o/std                    | 0.04048551 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -36.44509  |
| test/Q_plus_P                  | -36.44509  |
| test/reward_per_eps            | -40        |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.122     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 197        |
| stats_o/mean                   | 0.4151359  |
| stats_o/std                    | 0.04049233 |
| test/episodes                  | 1980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -36.4816   |
| test/Q_plus_P                  | -36.4816   |
| test/reward_per_eps            | -40        |
| test/steps                     | 79200      |
| train/episodes                 | 7920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 316800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.415131    |
| stats_o/std                    | 0.040499106 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.127      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -36.531025  |
| test/Q_plus_P                  | -36.531025  |
| test/reward_per_eps            | -40         |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 199         |
| stats_o/mean                   | 0.41508964  |
| stats_o/std                    | 0.040492024 |
| test/episodes                  | 2000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -36.56814   |
| test/Q_plus_P                  | -36.56814   |
| test/reward_per_eps            | -40         |
| test/steps                     | 80000       |
| train/episodes                 | 8000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 320000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 200         |
| stats_o/mean                   | 0.4150729   |
| stats_o/std                    | 0.040487975 |
| test/episodes                  | 2010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -36.63787   |
| test/Q_plus_P                  | -36.63787   |
| test/reward_per_eps            | -40         |
| test/steps                     | 80400       |
| train/episodes                 | 8040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 321600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.41501823 |
| stats_o/std                    | 0.04048647 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -36.6856   |
| test/Q_plus_P                  | -36.6856   |
| test/reward_per_eps            | -40        |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 202        |
| stats_o/mean                   | 0.41498426 |
| stats_o/std                    | 0.04049732 |
| test/episodes                  | 2030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.197     |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -36.69685  |
| test/Q_plus_P                  | -36.69685  |
| test/reward_per_eps            | -40        |
| test/steps                     | 81200      |
| train/episodes                 | 8120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 324800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.41498628  |
| stats_o/std                    | 0.040499765 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -36.70867   |
| test/Q_plus_P                  | -36.70867   |
| test/reward_per_eps            | -40         |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 204         |
| stats_o/mean                   | 0.4149466   |
| stats_o/std                    | 0.040500943 |
| test/episodes                  | 2050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.112      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -36.71603   |
| test/Q_plus_P                  | -36.71603   |
| test/reward_per_eps            | -40         |
| test/steps                     | 82000       |
| train/episodes                 | 8200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 328000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.41496754  |
| stats_o/std                    | 0.040508486 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.116      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -36.813988  |
| test/Q_plus_P                  | -36.813988  |
| test/reward_per_eps            | -40         |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 206        |
| stats_o/mean                   | 0.41495797 |
| stats_o/std                    | 0.04051465 |
| test/episodes                  | 2070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.127     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -36.855083 |
| test/Q_plus_P                  | -36.855083 |
| test/reward_per_eps            | -40        |
| test/steps                     | 82800      |
| train/episodes                 | 8280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 331200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 207        |
| stats_o/mean                   | 0.41491783 |
| stats_o/std                    | 0.0405106  |
| test/episodes                  | 2080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.102     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -36.86388  |
| test/Q_plus_P                  | -36.86388  |
| test/reward_per_eps            | -40        |
| test/steps                     | 83200      |
| train/episodes                 | 8320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 332800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.41491094  |
| stats_o/std                    | 0.040513966 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -36.904453  |
| test/Q_plus_P                  | -36.904453  |
| test/reward_per_eps            | -40         |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 209        |
| stats_o/mean                   | 0.41486704 |
| stats_o/std                    | 0.04050919 |
| test/episodes                  | 2100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.124     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -37.004303 |
| test/Q_plus_P                  | -37.004303 |
| test/reward_per_eps            | -40        |
| test/steps                     | 84000      |
| train/episodes                 | 8400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 336000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 210         |
| stats_o/mean                   | 0.4148365   |
| stats_o/std                    | 0.040511034 |
| test/episodes                  | 2110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.108      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -36.99547   |
| test/Q_plus_P                  | -36.99547   |
| test/reward_per_eps            | -40         |
| test/steps                     | 84400       |
| train/episodes                 | 8440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 337600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 211        |
| stats_o/mean                   | 0.41483358 |
| stats_o/std                    | 0.0405209  |
| test/episodes                  | 2120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -37.015038 |
| test/Q_plus_P                  | -37.015038 |
| test/reward_per_eps            | -40        |
| test/steps                     | 84800      |
| train/episodes                 | 8480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 339200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.41482472  |
| stats_o/std                    | 0.040524207 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -37.05668   |
| test/Q_plus_P                  | -37.05668   |
| test/reward_per_eps            | -40         |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.41483638  |
| stats_o/std                    | 0.040526535 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -37.115036  |
| test/Q_plus_P                  | -37.115036  |
| test/reward_per_eps            | -40         |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.41482663  |
| stats_o/std                    | 0.040520072 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -37.13351   |
| test/Q_plus_P                  | -37.13351   |
| test/reward_per_eps            | -40         |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.4148424   |
| stats_o/std                    | 0.040533938 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -37.20496   |
| test/Q_plus_P                  | -37.20496   |
| test/reward_per_eps            | -40         |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 216         |
| stats_o/mean                   | 0.41481796  |
| stats_o/std                    | 0.040526412 |
| test/episodes                  | 2170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -37.214794  |
| test/Q_plus_P                  | -37.214794  |
| test/reward_per_eps            | -40         |
| test/steps                     | 86800       |
| train/episodes                 | 8680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 347200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 217        |
| stats_o/mean                   | 0.41482076 |
| stats_o/std                    | 0.04051741 |
| test/episodes                  | 2180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -37.181152 |
| test/Q_plus_P                  | -37.181152 |
| test/reward_per_eps            | -40        |
| test/steps                     | 87200      |
| train/episodes                 | 8720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 348800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.41484228  |
| stats_o/std                    | 0.040531524 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -37.32691   |
| test/Q_plus_P                  | -37.32691   |
| test/reward_per_eps            | -40         |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 219        |
| stats_o/mean                   | 0.4148494  |
| stats_o/std                    | 0.04053316 |
| test/episodes                  | 2200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -37.32522  |
| test/Q_plus_P                  | -37.32522  |
| test/reward_per_eps            | -40        |
| test/steps                     | 88000      |
| train/episodes                 | 8800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 352000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 220        |
| stats_o/mean                   | 0.41484913 |
| stats_o/std                    | 0.04055239 |
| test/episodes                  | 2210       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -37.4235   |
| test/Q_plus_P                  | -37.4235   |
| test/reward_per_eps            | -40        |
| test/steps                     | 88400      |
| train/episodes                 | 8840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 353600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.41484734  |
| stats_o/std                    | 0.040566165 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -37.360626  |
| test/Q_plus_P                  | -37.360626  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.41480133 |
| stats_o/std                    | 0.04058433 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.14      |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -37.403946 |
| test/Q_plus_P                  | -37.403946 |
| test/reward_per_eps            | -40        |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.4147952   |
| stats_o/std                    | 0.040589135 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -37.40565   |
| test/Q_plus_P                  | -37.40565   |
| test/reward_per_eps            | -40         |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.41478813 |
| stats_o/std                    | 0.0405851  |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.324     |
| test/Q                         | -37.463505 |
| test/Q_plus_P                  | -37.463505 |
| test/reward_per_eps            | -40        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 225         |
| stats_o/mean                   | 0.4147822   |
| stats_o/std                    | 0.040590044 |
| test/episodes                  | 2260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -37.494587  |
| test/Q_plus_P                  | -37.494587  |
| test/reward_per_eps            | -40         |
| test/steps                     | 90400       |
| train/episodes                 | 9040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 361600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.41474542  |
| stats_o/std                    | 0.040598664 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -37.559296  |
| test/Q_plus_P                  | -37.559296  |
| test/reward_per_eps            | -40         |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 227        |
| stats_o/mean                   | 0.41471028 |
| stats_o/std                    | 0.04060969 |
| test/episodes                  | 2280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.209     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.346     |
| test/Q                         | -37.60732  |
| test/Q_plus_P                  | -37.60732  |
| test/reward_per_eps            | -40        |
| test/steps                     | 91200      |
| train/episodes                 | 9120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 364800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 228        |
| stats_o/mean                   | 0.41468987 |
| stats_o/std                    | 0.04062688 |
| test/episodes                  | 2290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -37.572166 |
| test/Q_plus_P                  | -37.572166 |
| test/reward_per_eps            | -40        |
| test/steps                     | 91600      |
| train/episodes                 | 9160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 366400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 229         |
| stats_o/mean                   | 0.4146788   |
| stats_o/std                    | 0.040639363 |
| test/episodes                  | 2300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -37.60265   |
| test/Q_plus_P                  | -37.60265   |
| test/reward_per_eps            | -40         |
| test/steps                     | 92000       |
| train/episodes                 | 9200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 368000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 230        |
| stats_o/mean                   | 0.41468766 |
| stats_o/std                    | 0.04064337 |
| test/episodes                  | 2310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -37.65168  |
| test/Q_plus_P                  | -37.65168  |
| test/reward_per_eps            | -40        |
| test/steps                     | 92400      |
| train/episodes                 | 9240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 369600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 231         |
| stats_o/mean                   | 0.41467214  |
| stats_o/std                    | 0.040641703 |
| test/episodes                  | 2320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -37.677246  |
| test/Q_plus_P                  | -37.677246  |
| test/reward_per_eps            | -40         |
| test/steps                     | 92800       |
| train/episodes                 | 9280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 371200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.4146875   |
| stats_o/std                    | 0.040641714 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -37.69766   |
| test/Q_plus_P                  | -37.69766   |
| test/reward_per_eps            | -40         |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 233         |
| stats_o/mean                   | 0.41467476  |
| stats_o/std                    | 0.040661186 |
| test/episodes                  | 2340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.217      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -37.789936  |
| test/Q_plus_P                  | -37.789936  |
| test/reward_per_eps            | -40         |
| test/steps                     | 93600       |
| train/episodes                 | 9360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 374400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 234        |
| stats_o/mean                   | 0.41466984 |
| stats_o/std                    | 0.04067383 |
| test/episodes                  | 2350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -37.79742  |
| test/Q_plus_P                  | -37.79742  |
| test/reward_per_eps            | -40        |
| test/steps                     | 94000      |
| train/episodes                 | 9400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 376000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.41467103  |
| stats_o/std                    | 0.040671375 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -37.79994   |
| test/Q_plus_P                  | -37.79994   |
| test/reward_per_eps            | -40         |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.41463825  |
| stats_o/std                    | 0.040675893 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -37.832306  |
| test/Q_plus_P                  | -37.832306  |
| test/reward_per_eps            | -40         |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.41463605  |
| stats_o/std                    | 0.040685054 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.206      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -37.906593  |
| test/Q_plus_P                  | -37.906593  |
| test/reward_per_eps            | -40         |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.41459194  |
| stats_o/std                    | 0.040693544 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -37.873886  |
| test/Q_plus_P                  | -37.873886  |
| test/reward_per_eps            | -40         |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.41458368  |
| stats_o/std                    | 0.040699344 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -37.941906  |
| test/Q_plus_P                  | -37.941906  |
| test/reward_per_eps            | -40         |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 240         |
| stats_o/mean                   | 0.41457632  |
| stats_o/std                    | 0.040715005 |
| test/episodes                  | 2410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -37.95185   |
| test/Q_plus_P                  | -37.95185   |
| test/reward_per_eps            | -40         |
| test/steps                     | 96400       |
| train/episodes                 | 9640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 385600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 241        |
| stats_o/mean                   | 0.41455302 |
| stats_o/std                    | 0.04072732 |
| test/episodes                  | 2420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.269     |
| test/info_shaping_reward_min   | -0.35      |
| test/Q                         | -37.978565 |
| test/Q_plus_P                  | -37.978565 |
| test/reward_per_eps            | -40        |
| test/steps                     | 96800      |
| train/episodes                 | 9680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 387200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 242         |
| stats_o/mean                   | 0.4145249   |
| stats_o/std                    | 0.040734407 |
| test/episodes                  | 2430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.352      |
| test/Q                         | -37.979137  |
| test/Q_plus_P                  | -37.979137  |
| test/reward_per_eps            | -40         |
| test/steps                     | 97200       |
| train/episodes                 | 9720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 388800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 243         |
| stats_o/mean                   | 0.41451204  |
| stats_o/std                    | 0.040742107 |
| test/episodes                  | 2440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -38.0262    |
| test/Q_plus_P                  | -38.0262    |
| test/reward_per_eps            | -40         |
| test/steps                     | 97600       |
| train/episodes                 | 9760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 390400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.4144969  |
| stats_o/std                    | 0.04075301 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -37.98544  |
| test/Q_plus_P                  | -37.98544  |
| test/reward_per_eps            | -40        |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 245        |
| stats_o/mean                   | 0.41448    |
| stats_o/std                    | 0.04075273 |
| test/episodes                  | 2460       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.213     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.321     |
| test/Q                         | -38.07343  |
| test/Q_plus_P                  | -38.07343  |
| test/reward_per_eps            | -40        |
| test/steps                     | 98400      |
| train/episodes                 | 9840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 393600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 246        |
| stats_o/mean                   | 0.4144721  |
| stats_o/std                    | 0.04075614 |
| test/episodes                  | 2470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -38.03914  |
| test/Q_plus_P                  | -38.03914  |
| test/reward_per_eps            | -40        |
| test/steps                     | 98800      |
| train/episodes                 | 9880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 395200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 247         |
| stats_o/mean                   | 0.4144577   |
| stats_o/std                    | 0.040755127 |
| test/episodes                  | 2480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -38.10185   |
| test/Q_plus_P                  | -38.10185   |
| test/reward_per_eps            | -40         |
| test/steps                     | 99200       |
| train/episodes                 | 9920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 396800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 248        |
| stats_o/mean                   | 0.41443276 |
| stats_o/std                    | 0.04076708 |
| test/episodes                  | 2490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.172     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -38.156803 |
| test/Q_plus_P                  | -38.156803 |
| test/reward_per_eps            | -40        |
| test/steps                     | 99600      |
| train/episodes                 | 9960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 398400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 249         |
| stats_o/mean                   | 0.4144034   |
| stats_o/std                    | 0.040785093 |
| test/episodes                  | 2500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -38.16255   |
| test/Q_plus_P                  | -38.16255   |
| test/reward_per_eps            | -40         |
| test/steps                     | 100000      |
| train/episodes                 | 10000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 400000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.414417    |
| stats_o/std                    | 0.040816925 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -38.166924  |
| test/Q_plus_P                  | -38.166924  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.41439724 |
| stats_o/std                    | 0.04083291 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -38.193726 |
| test/Q_plus_P                  | -38.193726 |
| test/reward_per_eps            | -40        |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.261     |
| train/info_shaping_reward_min  | -0.371     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 252        |
| stats_o/mean                   | 0.41440496 |
| stats_o/std                    | 0.04083616 |
| test/episodes                  | 2530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.326     |
| test/Q                         | -38.24751  |
| test/Q_plus_P                  | -38.24751  |
| test/reward_per_eps            | -40        |
| test/steps                     | 101200     |
| train/episodes                 | 10120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 404800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 253        |
| stats_o/mean                   | 0.41438445 |
| stats_o/std                    | 0.04083499 |
| test/episodes                  | 2540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -38.240784 |
| test/Q_plus_P                  | -38.240784 |
| test/reward_per_eps            | -40        |
| test/steps                     | 101600     |
| train/episodes                 | 10160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 406400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.41437006  |
| stats_o/std                    | 0.040840898 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -38.284073  |
| test/Q_plus_P                  | -38.284073  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.41434717  |
| stats_o/std                    | 0.040844757 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -38.280544  |
| test/Q_plus_P                  | -38.280544  |
| test/reward_per_eps            | -40         |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 256        |
| stats_o/mean                   | 0.4143373  |
| stats_o/std                    | 0.04086069 |
| test/episodes                  | 2570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -38.242504 |
| test/Q_plus_P                  | -38.242504 |
| test/reward_per_eps            | -40        |
| test/steps                     | 102800     |
| train/episodes                 | 10280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 411200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.41435012  |
| stats_o/std                    | 0.040849544 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -38.29457   |
| test/Q_plus_P                  | -38.29457   |
| test/reward_per_eps            | -40         |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 258         |
| stats_o/mean                   | 0.41434482  |
| stats_o/std                    | 0.040865485 |
| test/episodes                  | 2590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -38.35596   |
| test/Q_plus_P                  | -38.35596   |
| test/reward_per_eps            | -40         |
| test/steps                     | 103600      |
| train/episodes                 | 10360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 414400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.41432986  |
| stats_o/std                    | 0.040859196 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -38.424854  |
| test/Q_plus_P                  | -38.424854  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.41433355  |
| stats_o/std                    | 0.040858645 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -38.387234  |
| test/Q_plus_P                  | -38.387234  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 261         |
| stats_o/mean                   | 0.41431472  |
| stats_o/std                    | 0.040864058 |
| test/episodes                  | 2620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -38.39465   |
| test/Q_plus_P                  | -38.39465   |
| test/reward_per_eps            | -40         |
| test/steps                     | 104800      |
| train/episodes                 | 10480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 419200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 262        |
| stats_o/mean                   | 0.41429773 |
| stats_o/std                    | 0.04087324 |
| test/episodes                  | 2630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.202     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -38.356728 |
| test/Q_plus_P                  | -38.356728 |
| test/reward_per_eps            | -40        |
| test/steps                     | 105200     |
| train/episodes                 | 10520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 420800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.41426528  |
| stats_o/std                    | 0.040878635 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -38.450302  |
| test/Q_plus_P                  | -38.450302  |
| test/reward_per_eps            | -40         |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.4142518   |
| stats_o/std                    | 0.040873215 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.204      |
| test/info_shaping_reward_mean  | -0.271      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -38.4074    |
| test/Q_plus_P                  | -38.4074    |
| test/reward_per_eps            | -40         |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.41425177 |
| stats_o/std                    | 0.04087377 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -38.472706 |
| test/Q_plus_P                  | -38.472706 |
| test/reward_per_eps            | -40        |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.4142398   |
| stats_o/std                    | 0.040893808 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.265      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -38.498466  |
| test/Q_plus_P                  | -38.498466  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 267         |
| stats_o/mean                   | 0.41423634  |
| stats_o/std                    | 0.040899877 |
| test/episodes                  | 2680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -38.502495  |
| test/Q_plus_P                  | -38.502495  |
| test/reward_per_eps            | -40         |
| test/steps                     | 107200      |
| train/episodes                 | 10720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 428800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.4142343   |
| stats_o/std                    | 0.040893607 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -38.56876   |
| test/Q_plus_P                  | -38.56876   |
| test/reward_per_eps            | -40         |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.41422907  |
| stats_o/std                    | 0.040906563 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -38.530636  |
| test/Q_plus_P                  | -38.530636  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.41422966  |
| stats_o/std                    | 0.040906556 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -38.620514  |
| test/Q_plus_P                  | -38.620514  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.41421768  |
| stats_o/std                    | 0.040903907 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -38.637535  |
| test/Q_plus_P                  | -38.637535  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.414199    |
| stats_o/std                    | 0.040879574 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -38.573734  |
| test/Q_plus_P                  | -38.573734  |
| test/reward_per_eps            | -40         |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.3        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 273        |
| stats_o/mean                   | 0.41417453 |
| stats_o/std                    | 0.04087172 |
| test/episodes                  | 2740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -38.61455  |
| test/Q_plus_P                  | -38.61455  |
| test/reward_per_eps            | -40        |
| test/steps                     | 109600     |
| train/episodes                 | 10960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 438400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 274        |
| stats_o/mean                   | 0.41416523 |
| stats_o/std                    | 0.04087068 |
| test/episodes                  | 2750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.198     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -38.641415 |
| test/Q_plus_P                  | -38.641415 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110000     |
| train/episodes                 | 11000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 440000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 275         |
| stats_o/mean                   | 0.41416574  |
| stats_o/std                    | 0.040871512 |
| test/episodes                  | 2760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -38.638016  |
| test/Q_plus_P                  | -38.638016  |
| test/reward_per_eps            | -40         |
| test/steps                     | 110400      |
| train/episodes                 | 11040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 441600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.4141545  |
| stats_o/std                    | 0.04088136 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -38.675323 |
| test/Q_plus_P                  | -38.675323 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 277         |
| stats_o/mean                   | 0.41416     |
| stats_o/std                    | 0.040906027 |
| test/episodes                  | 2780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -38.680187  |
| test/Q_plus_P                  | -38.680187  |
| test/reward_per_eps            | -40         |
| test/steps                     | 111200      |
| train/episodes                 | 11120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 444800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 278        |
| stats_o/mean                   | 0.41414905 |
| stats_o/std                    | 0.04090244 |
| test/episodes                  | 2790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -38.6908   |
| test/Q_plus_P                  | -38.6908   |
| test/reward_per_eps            | -40        |
| test/steps                     | 111600     |
| train/episodes                 | 11160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 446400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 279        |
| stats_o/mean                   | 0.41412497 |
| stats_o/std                    | 0.04089697 |
| test/episodes                  | 2800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -38.706985 |
| test/Q_plus_P                  | -38.706985 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112000     |
| train/episodes                 | 11200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 448000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 280        |
| stats_o/mean                   | 0.41412863 |
| stats_o/std                    | 0.04090286 |
| test/episodes                  | 2810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -38.731937 |
| test/Q_plus_P                  | -38.731937 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112400     |
| train/episodes                 | 11240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 449600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 281        |
| stats_o/mean                   | 0.41411135 |
| stats_o/std                    | 0.04090183 |
| test/episodes                  | 2820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -38.734936 |
| test/Q_plus_P                  | -38.734936 |
| test/reward_per_eps            | -40        |
| test/steps                     | 112800     |
| train/episodes                 | 11280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 451200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 282         |
| stats_o/mean                   | 0.4141259   |
| stats_o/std                    | 0.040907893 |
| test/episodes                  | 2830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -38.849945  |
| test/Q_plus_P                  | -38.849945  |
| test/reward_per_eps            | -40         |
| test/steps                     | 113200      |
| train/episodes                 | 11320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 452800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.4141142  |
| stats_o/std                    | 0.04092696 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.189     |
| test/info_shaping_reward_mean  | -0.268     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -38.752876 |
| test/Q_plus_P                  | -38.752876 |
| test/reward_per_eps            | -40        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 284        |
| stats_o/mean                   | 0.414093   |
| stats_o/std                    | 0.04093306 |
| test/episodes                  | 2850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -38.78507  |
| test/Q_plus_P                  | -38.78507  |
| test/reward_per_eps            | -40        |
| test/steps                     | 114000     |
| train/episodes                 | 11400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.183     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 456000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.41406652  |
| stats_o/std                    | 0.040931217 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.211      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -38.790146  |
| test/Q_plus_P                  | -38.790146  |
| test/reward_per_eps            | -40         |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 286         |
| stats_o/mean                   | 0.41405177  |
| stats_o/std                    | 0.040939637 |
| test/episodes                  | 2870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -38.81002   |
| test/Q_plus_P                  | -38.81002   |
| test/reward_per_eps            | -40         |
| test/steps                     | 114800      |
| train/episodes                 | 11480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 459200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 287         |
| stats_o/mean                   | 0.4140526   |
| stats_o/std                    | 0.040946264 |
| test/episodes                  | 2880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -38.825512  |
| test/Q_plus_P                  | -38.825512  |
| test/reward_per_eps            | -40         |
| test/steps                     | 115200      |
| train/episodes                 | 11520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 460800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.41405573  |
| stats_o/std                    | 0.040946133 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -38.797035  |
| test/Q_plus_P                  | -38.797035  |
| test/reward_per_eps            | -40         |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.4140779   |
| stats_o/std                    | 0.040955167 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -38.825947  |
| test/Q_plus_P                  | -38.825947  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.41406843  |
| stats_o/std                    | 0.040954106 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -38.91716   |
| test/Q_plus_P                  | -38.91716   |
| test/reward_per_eps            | -40         |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 291        |
| stats_o/mean                   | 0.4140627  |
| stats_o/std                    | 0.04095414 |
| test/episodes                  | 2920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.119     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -38.861446 |
| test/Q_plus_P                  | -38.861446 |
| test/reward_per_eps            | -40        |
| test/steps                     | 116800     |
| train/episodes                 | 11680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 467200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 292        |
| stats_o/mean                   | 0.41407442 |
| stats_o/std                    | 0.04095371 |
| test/episodes                  | 2930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.196     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -38.948647 |
| test/Q_plus_P                  | -38.948647 |
| test/reward_per_eps            | -40        |
| test/steps                     | 117200     |
| train/episodes                 | 11720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 468800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 293        |
| stats_o/mean                   | 0.41405988 |
| stats_o/std                    | 0.04095472 |
| test/episodes                  | 2940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.2       |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -38.970192 |
| test/Q_plus_P                  | -38.970192 |
| test/reward_per_eps            | -40        |
| test/steps                     | 117600     |
| train/episodes                 | 11760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 470400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.41404054  |
| stats_o/std                    | 0.040960696 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -38.928383  |
| test/Q_plus_P                  | -38.928383  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 295         |
| stats_o/mean                   | 0.41401073  |
| stats_o/std                    | 0.040953588 |
| test/episodes                  | 2960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -38.93386   |
| test/Q_plus_P                  | -38.93386   |
| test/reward_per_eps            | -40         |
| test/steps                     | 118400      |
| train/episodes                 | 11840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 473600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.41396353  |
| stats_o/std                    | 0.040955137 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -38.947998  |
| test/Q_plus_P                  | -38.947998  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.369      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 297         |
| stats_o/mean                   | 0.41396198  |
| stats_o/std                    | 0.040953565 |
| test/episodes                  | 2980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.344      |
| test/Q                         | -38.93117   |
| test/Q_plus_P                  | -38.93117   |
| test/reward_per_eps            | -40         |
| test/steps                     | 119200      |
| train/episodes                 | 11920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 476800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.41395023  |
| stats_o/std                    | 0.040968586 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -38.968025  |
| test/Q_plus_P                  | -38.968025  |
| test/reward_per_eps            | -40         |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.377      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 299        |
| stats_o/mean                   | 0.41392216 |
| stats_o/std                    | 0.04096843 |
| test/episodes                  | 3000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.066006 |
| test/Q_plus_P                  | -39.066006 |
| test/reward_per_eps            | -40        |
| test/steps                     | 120000     |
| train/episodes                 | 12000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 480000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 300         |
| stats_o/mean                   | 0.41390705  |
| stats_o/std                    | 0.040967543 |
| test/episodes                  | 3010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -38.992554  |
| test/Q_plus_P                  | -38.992554  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120400      |
| train/episodes                 | 12040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 481600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 301        |
| stats_o/mean                   | 0.41389573 |
| stats_o/std                    | 0.04097255 |
| test/episodes                  | 3020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -39.002804 |
| test/Q_plus_P                  | -39.002804 |
| test/reward_per_eps            | -40        |
| test/steps                     | 120800     |
| train/episodes                 | 12080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 483200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.41387388  |
| stats_o/std                    | 0.040965777 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -38.98395   |
| test/Q_plus_P                  | -38.98395   |
| test/reward_per_eps            | -40         |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 303        |
| stats_o/mean                   | 0.41386148 |
| stats_o/std                    | 0.04096159 |
| test/episodes                  | 3040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -38.97612  |
| test/Q_plus_P                  | -38.97612  |
| test/reward_per_eps            | -40        |
| test/steps                     | 121600     |
| train/episodes                 | 12160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 486400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.41384724  |
| stats_o/std                    | 0.040959854 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.194      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.027725  |
| test/Q_plus_P                  | -39.027725  |
| test/reward_per_eps            | -40         |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.41383913 |
| stats_o/std                    | 0.04095345 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -39.04034  |
| test/Q_plus_P                  | -39.04034  |
| test/reward_per_eps            | -40        |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 306        |
| stats_o/mean                   | 0.41383907 |
| stats_o/std                    | 0.04094862 |
| test/episodes                  | 3070       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.35      |
| test/Q                         | -39.08065  |
| test/Q_plus_P                  | -39.08065  |
| test/reward_per_eps            | -40        |
| test/steps                     | 122800     |
| train/episodes                 | 12280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 491200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 307         |
| stats_o/mean                   | 0.4138342   |
| stats_o/std                    | 0.040953007 |
| test/episodes                  | 3080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -39.065865  |
| test/Q_plus_P                  | -39.065865  |
| test/reward_per_eps            | -40         |
| test/steps                     | 123200      |
| train/episodes                 | 12320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 492800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.41380537  |
| stats_o/std                    | 0.040961426 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.197365  |
| test/Q_plus_P                  | -39.197365  |
| test/reward_per_eps            | -40         |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 309        |
| stats_o/mean                   | 0.4138228  |
| stats_o/std                    | 0.04095572 |
| test/episodes                  | 3100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -39.123886 |
| test/Q_plus_P                  | -39.123886 |
| test/reward_per_eps            | -40        |
| test/steps                     | 124000     |
| train/episodes                 | 12400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 496000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 310        |
| stats_o/mean                   | 0.413802   |
| stats_o/std                    | 0.04096824 |
| test/episodes                  | 3110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.117783 |
| test/Q_plus_P                  | -39.117783 |
| test/reward_per_eps            | -40        |
| test/steps                     | 124400     |
| train/episodes                 | 12440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.376     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 497600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 311         |
| stats_o/mean                   | 0.4138075   |
| stats_o/std                    | 0.040995024 |
| test/episodes                  | 3120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.10646   |
| test/Q_plus_P                  | -39.10646   |
| test/reward_per_eps            | -40         |
| test/steps                     | 124800      |
| train/episodes                 | 12480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.272      |
| train/info_shaping_reward_min  | -0.4        |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 499200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 312        |
| stats_o/mean                   | 0.41380683 |
| stats_o/std                    | 0.04100087 |
| test/episodes                  | 3130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.1206   |
| test/Q_plus_P                  | -39.1206   |
| test/reward_per_eps            | -40        |
| test/steps                     | 125200     |
| train/episodes                 | 12520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 500800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.41378987  |
| stats_o/std                    | 0.040997528 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.132366  |
| test/Q_plus_P                  | -39.132366  |
| test/reward_per_eps            | -40         |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.41377807  |
| stats_o/std                    | 0.040996064 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.155117  |
| test/Q_plus_P                  | -39.155117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.4137639   |
| stats_o/std                    | 0.040986326 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.142994  |
| test/Q_plus_P                  | -39.142994  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.41377366  |
| stats_o/std                    | 0.040992312 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.13       |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.192123  |
| test/Q_plus_P                  | -39.192123  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 317        |
| stats_o/mean                   | 0.4137882  |
| stats_o/std                    | 0.04099163 |
| test/episodes                  | 3180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.167564 |
| test/Q_plus_P                  | -39.167564 |
| test/reward_per_eps            | -40        |
| test/steps                     | 127200     |
| train/episodes                 | 12720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 508800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 318         |
| stats_o/mean                   | 0.4137905   |
| stats_o/std                    | 0.041000057 |
| test/episodes                  | 3190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.163628  |
| test/Q_plus_P                  | -39.163628  |
| test/reward_per_eps            | -40         |
| test/steps                     | 127600      |
| train/episodes                 | 12760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 510400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.4137948   |
| stats_o/std                    | 0.040997434 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.221      |
| test/info_shaping_reward_mean  | -0.272      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -39.175846  |
| test/Q_plus_P                  | -39.175846  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.41378048  |
| stats_o/std                    | 0.041010402 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.15919   |
| test/Q_plus_P                  | -39.15919   |
| test/reward_per_eps            | -40         |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.384      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.41377082  |
| stats_o/std                    | 0.041010838 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.192802  |
| test/Q_plus_P                  | -39.192802  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 322        |
| stats_o/mean                   | 0.41377282 |
| stats_o/std                    | 0.04101089 |
| test/episodes                  | 3230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.198     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.184036 |
| test/Q_plus_P                  | -39.184036 |
| test/reward_per_eps            | -40        |
| test/steps                     | 129200     |
| train/episodes                 | 12920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 516800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.41375196  |
| stats_o/std                    | 0.041005127 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.216564  |
| test/Q_plus_P                  | -39.216564  |
| test/reward_per_eps            | -40         |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 324         |
| stats_o/mean                   | 0.41375694  |
| stats_o/std                    | 0.041018657 |
| test/episodes                  | 3250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.228607  |
| test/Q_plus_P                  | -39.228607  |
| test/reward_per_eps            | -40         |
| test/steps                     | 130000      |
| train/episodes                 | 13000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 520000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.41376862  |
| stats_o/std                    | 0.041031282 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.24095   |
| test/Q_plus_P                  | -39.24095   |
| test/reward_per_eps            | -40         |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.41375637  |
| stats_o/std                    | 0.041027516 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -39.26602   |
| test/Q_plus_P                  | -39.26602   |
| test/reward_per_eps            | -40         |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 327         |
| stats_o/mean                   | 0.41374835  |
| stats_o/std                    | 0.041032355 |
| test/episodes                  | 3280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.241337  |
| test/Q_plus_P                  | -39.241337  |
| test/reward_per_eps            | -40         |
| test/steps                     | 131200      |
| train/episodes                 | 13120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 524800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 328         |
| stats_o/mean                   | 0.41372228  |
| stats_o/std                    | 0.041041855 |
| test/episodes                  | 3290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.31895   |
| test/Q_plus_P                  | -39.31895   |
| test/reward_per_eps            | -40         |
| test/steps                     | 131600      |
| train/episodes                 | 13160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 526400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.413723   |
| stats_o/std                    | 0.04104083 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.28145  |
| test/Q_plus_P                  | -39.28145  |
| test/reward_per_eps            | -40        |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.4137025   |
| stats_o/std                    | 0.041050695 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.291817  |
| test/Q_plus_P                  | -39.291817  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 331         |
| stats_o/mean                   | 0.4137051   |
| stats_o/std                    | 0.041045632 |
| test/episodes                  | 3320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.345      |
| test/Q                         | -39.288433  |
| test/Q_plus_P                  | -39.288433  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132800      |
| train/episodes                 | 13280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 531200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.41371372  |
| stats_o/std                    | 0.041045457 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.226      |
| test/info_shaping_reward_mean  | -0.267      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -39.307396  |
| test/Q_plus_P                  | -39.307396  |
| test/reward_per_eps            | -40         |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 333        |
| stats_o/mean                   | 0.4137186  |
| stats_o/std                    | 0.04105344 |
| test/episodes                  | 3340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.314915 |
| test/Q_plus_P                  | -39.314915 |
| test/reward_per_eps            | -40        |
| test/steps                     | 133600     |
| train/episodes                 | 13360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 534400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.41371378  |
| stats_o/std                    | 0.041054104 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.309364  |
| test/Q_plus_P                  | -39.309364  |
| test/reward_per_eps            | -40         |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.41372296 |
| stats_o/std                    | 0.04105682 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -39.358963 |
| test/Q_plus_P                  | -39.358963 |
| test/reward_per_eps            | -40        |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.41371414  |
| stats_o/std                    | 0.041056786 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -39.35126   |
| test/Q_plus_P                  | -39.35126   |
| test/reward_per_eps            | -40         |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.367      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 337        |
| stats_o/mean                   | 0.41372415 |
| stats_o/std                    | 0.04105768 |
| test/episodes                  | 3380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -39.358475 |
| test/Q_plus_P                  | -39.358475 |
| test/reward_per_eps            | -40        |
| test/steps                     | 135200     |
| train/episodes                 | 13520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.362     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 540800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.413735   |
| stats_o/std                    | 0.04106418 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -39.363693 |
| test/Q_plus_P                  | -39.363693 |
| test/reward_per_eps            | -40        |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 339        |
| stats_o/mean                   | 0.41374382 |
| stats_o/std                    | 0.04106118 |
| test/episodes                  | 3400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.32847  |
| test/Q_plus_P                  | -39.32847  |
| test/reward_per_eps            | -40        |
| test/steps                     | 136000     |
| train/episodes                 | 13600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 544000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.4137536   |
| stats_o/std                    | 0.041056477 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.414482  |
| test/Q_plus_P                  | -39.414482  |
| test/reward_per_eps            | -40         |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.41375497 |
| stats_o/std                    | 0.04105552 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -39.368893 |
| test/Q_plus_P                  | -39.368893 |
| test/reward_per_eps            | -40        |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 342         |
| stats_o/mean                   | 0.41375676  |
| stats_o/std                    | 0.041047182 |
| test/episodes                  | 3430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.391205  |
| test/Q_plus_P                  | -39.391205  |
| test/reward_per_eps            | -40         |
| test/steps                     | 137200      |
| train/episodes                 | 13720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 548800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.41375613  |
| stats_o/std                    | 0.041049317 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.40253   |
| test/Q_plus_P                  | -39.40253   |
| test/reward_per_eps            | -40         |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.41375792  |
| stats_o/std                    | 0.041041944 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0879     |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -39.3978    |
| test/Q_plus_P                  | -39.3978    |
| test/reward_per_eps            | -40         |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.4137464   |
| stats_o/std                    | 0.041031953 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.3852    |
| test/Q_plus_P                  | -39.3852    |
| test/reward_per_eps            | -40         |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.41373762  |
| stats_o/std                    | 0.041029952 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.208      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -39.375484  |
| test/Q_plus_P                  | -39.375484  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.41373524 |
| stats_o/std                    | 0.04104648 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -39.450985 |
| test/Q_plus_P                  | -39.450985 |
| test/reward_per_eps            | -40        |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 348         |
| stats_o/mean                   | 0.41377118  |
| stats_o/std                    | 0.041050743 |
| test/episodes                  | 3490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.126      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.427338  |
| test/Q_plus_P                  | -39.427338  |
| test/reward_per_eps            | -40         |
| test/steps                     | 139600      |
| train/episodes                 | 13960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 558400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.41377446 |
| stats_o/std                    | 0.04105123 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.138     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -39.42775  |
| test/Q_plus_P                  | -39.42775  |
| test/reward_per_eps            | -40        |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 350        |
| stats_o/mean                   | 0.41376498 |
| stats_o/std                    | 0.04104591 |
| test/episodes                  | 3510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.387684 |
| test/Q_plus_P                  | -39.387684 |
| test/reward_per_eps            | -40        |
| test/steps                     | 140400     |
| train/episodes                 | 14040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 561600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 351         |
| stats_o/mean                   | 0.41376162  |
| stats_o/std                    | 0.041042473 |
| test/episodes                  | 3520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.47043   |
| test/Q_plus_P                  | -39.47043   |
| test/reward_per_eps            | -40         |
| test/steps                     | 140800      |
| train/episodes                 | 14080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 563200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.4137669   |
| stats_o/std                    | 0.041039463 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -39.47761   |
| test/Q_plus_P                  | -39.47761   |
| test/reward_per_eps            | -40         |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.4137629   |
| stats_o/std                    | 0.041041516 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.484898  |
| test/Q_plus_P                  | -39.484898  |
| test/reward_per_eps            | -40         |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 354        |
| stats_o/mean                   | 0.41375974 |
| stats_o/std                    | 0.04104843 |
| test/episodes                  | 3550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.333     |
| test/Q                         | -39.452953 |
| test/Q_plus_P                  | -39.452953 |
| test/reward_per_eps            | -40        |
| test/steps                     | 142000     |
| train/episodes                 | 14200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 568000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.41375276  |
| stats_o/std                    | 0.041056957 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -39.46858   |
| test/Q_plus_P                  | -39.46858   |
| test/reward_per_eps            | -40         |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.264      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.4137422   |
| stats_o/std                    | 0.041069664 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.480133  |
| test/Q_plus_P                  | -39.480133  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.41373324  |
| stats_o/std                    | 0.041065272 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.50218   |
| test/Q_plus_P                  | -39.50218   |
| test/reward_per_eps            | -40         |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 358        |
| stats_o/mean                   | 0.41374043 |
| stats_o/std                    | 0.04106008 |
| test/episodes                  | 3590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.5076   |
| test/Q_plus_P                  | -39.5076   |
| test/reward_per_eps            | -40        |
| test/steps                     | 143600     |
| train/episodes                 | 14360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 574400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.4137288   |
| stats_o/std                    | 0.041058104 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.533554  |
| test/Q_plus_P                  | -39.533554  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.413715   |
| stats_o/std                    | 0.04106213 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -39.47997  |
| test/Q_plus_P                  | -39.47997  |
| test/reward_per_eps            | -40        |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 361        |
| stats_o/mean                   | 0.41371837 |
| stats_o/std                    | 0.04106501 |
| test/episodes                  | 3620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.491642 |
| test/Q_plus_P                  | -39.491642 |
| test/reward_per_eps            | -40        |
| test/steps                     | 144800     |
| train/episodes                 | 14480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 579200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.41371432  |
| stats_o/std                    | 0.041055966 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.463657  |
| test/Q_plus_P                  | -39.463657  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.208      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.4137049   |
| stats_o/std                    | 0.041051902 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.574512  |
| test/Q_plus_P                  | -39.574512  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.41369903  |
| stats_o/std                    | 0.041057717 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -39.53476   |
| test/Q_plus_P                  | -39.53476   |
| test/reward_per_eps            | -40         |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.4137037   |
| stats_o/std                    | 0.041059814 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.52082   |
| test/Q_plus_P                  | -39.52082   |
| test/reward_per_eps            | -40         |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 366         |
| stats_o/mean                   | 0.4137074   |
| stats_o/std                    | 0.041058745 |
| test/episodes                  | 3670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.58542   |
| test/Q_plus_P                  | -39.58542   |
| test/reward_per_eps            | -40         |
| test/steps                     | 146800      |
| train/episodes                 | 14680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 587200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 367        |
| stats_o/mean                   | 0.41370586 |
| stats_o/std                    | 0.04105649 |
| test/episodes                  | 3680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.5535   |
| test/Q_plus_P                  | -39.5535   |
| test/reward_per_eps            | -40        |
| test/steps                     | 147200     |
| train/episodes                 | 14720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 588800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 368        |
| stats_o/mean                   | 0.41371325 |
| stats_o/std                    | 0.04106457 |
| test/episodes                  | 3690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.198     |
| test/info_shaping_reward_mean  | -0.254     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.556805 |
| test/Q_plus_P                  | -39.556805 |
| test/reward_per_eps            | -40        |
| test/steps                     | 147600     |
| train/episodes                 | 14760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 590400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.41372108 |
| stats_o/std                    | 0.04106854 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.263     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.565147 |
| test/Q_plus_P                  | -39.565147 |
| test/reward_per_eps            | -40        |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.4137019   |
| stats_o/std                    | 0.041066665 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.351      |
| test/Q                         | -39.571766  |
| test/Q_plus_P                  | -39.571766  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 371         |
| stats_o/mean                   | 0.41368833  |
| stats_o/std                    | 0.041070525 |
| test/episodes                  | 3720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.571636  |
| test/Q_plus_P                  | -39.571636  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148800      |
| train/episodes                 | 14880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 595200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.41368905  |
| stats_o/std                    | 0.041065358 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.54374   |
| test/Q_plus_P                  | -39.54374   |
| test/reward_per_eps            | -40         |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 373         |
| stats_o/mean                   | 0.41366944  |
| stats_o/std                    | 0.041065913 |
| test/episodes                  | 3740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -39.5845    |
| test/Q_plus_P                  | -39.5845    |
| test/reward_per_eps            | -40         |
| test/steps                     | 149600      |
| train/episodes                 | 14960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 598400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 374        |
| stats_o/mean                   | 0.41366115 |
| stats_o/std                    | 0.04106557 |
| test/episodes                  | 3750       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.207     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.56972  |
| test/Q_plus_P                  | -39.56972  |
| test/reward_per_eps            | -40        |
| test/steps                     | 150000     |
| train/episodes                 | 15000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 600000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.41365477 |
| stats_o/std                    | 0.04106375 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -39.552746 |
| test/Q_plus_P                  | -39.552746 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.41363287 |
| stats_o/std                    | 0.04106104 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -39.539337 |
| test/Q_plus_P                  | -39.539337 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 377         |
| stats_o/mean                   | 0.413615    |
| stats_o/std                    | 0.041058384 |
| test/episodes                  | 3780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.595352  |
| test/Q_plus_P                  | -39.595352  |
| test/reward_per_eps            | -40         |
| test/steps                     | 151200      |
| train/episodes                 | 15120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 604800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 378         |
| stats_o/mean                   | 0.4136053   |
| stats_o/std                    | 0.041065242 |
| test/episodes                  | 3790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.592342  |
| test/Q_plus_P                  | -39.592342  |
| test/reward_per_eps            | -40         |
| test/steps                     | 151600      |
| train/episodes                 | 15160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 606400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 379         |
| stats_o/mean                   | 0.41358623  |
| stats_o/std                    | 0.041066125 |
| test/episodes                  | 3800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.627026  |
| test/Q_plus_P                  | -39.627026  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152000      |
| train/episodes                 | 15200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 608000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 380        |
| stats_o/mean                   | 0.4135997  |
| stats_o/std                    | 0.04106928 |
| test/episodes                  | 3810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.207     |
| test/info_shaping_reward_mean  | -0.262     |
| test/info_shaping_reward_min   | -0.322     |
| test/Q                         | -39.59099  |
| test/Q_plus_P                  | -39.59099  |
| test/reward_per_eps            | -40        |
| test/steps                     | 152400     |
| train/episodes                 | 15240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 609600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.41360295 |
| stats_o/std                    | 0.04106832 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.60124  |
| test/Q_plus_P                  | -39.60124  |
| test/reward_per_eps            | -40        |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.41360262 |
| stats_o/std                    | 0.04106773 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -39.557827 |
| test/Q_plus_P                  | -39.557827 |
| test/reward_per_eps            | -40        |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 383        |
| stats_o/mean                   | 0.4135858  |
| stats_o/std                    | 0.04105929 |
| test/episodes                  | 3840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.322     |
| test/Q                         | -39.62287  |
| test/Q_plus_P                  | -39.62287  |
| test/reward_per_eps            | -40        |
| test/steps                     | 153600     |
| train/episodes                 | 15360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 614400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.4135711   |
| stats_o/std                    | 0.041059207 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.624912  |
| test/Q_plus_P                  | -39.624912  |
| test/reward_per_eps            | -40         |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 385         |
| stats_o/mean                   | 0.4135532   |
| stats_o/std                    | 0.041056205 |
| test/episodes                  | 3860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.367      |
| test/Q                         | -39.62046   |
| test/Q_plus_P                  | -39.62046   |
| test/reward_per_eps            | -40         |
| test/steps                     | 154400      |
| train/episodes                 | 15440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 617600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 386         |
| stats_o/mean                   | 0.41353968  |
| stats_o/std                    | 0.041054852 |
| test/episodes                  | 3870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.61709   |
| test/Q_plus_P                  | -39.61709   |
| test/reward_per_eps            | -40         |
| test/steps                     | 154800      |
| train/episodes                 | 15480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 619200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 387         |
| stats_o/mean                   | 0.41353285  |
| stats_o/std                    | 0.041055374 |
| test/episodes                  | 3880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.62333   |
| test/Q_plus_P                  | -39.62333   |
| test/reward_per_eps            | -40         |
| test/steps                     | 155200      |
| train/episodes                 | 15520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 620800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 388         |
| stats_o/mean                   | 0.41352138  |
| stats_o/std                    | 0.041061614 |
| test/episodes                  | 3890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.64088   |
| test/Q_plus_P                  | -39.64088   |
| test/reward_per_eps            | -40         |
| test/steps                     | 155600      |
| train/episodes                 | 15560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.366      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 622400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.413506    |
| stats_o/std                    | 0.041070536 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.640526  |
| test/Q_plus_P                  | -39.640526  |
| test/reward_per_eps            | -40         |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.4135176  |
| stats_o/std                    | 0.04107432 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.629837 |
| test/Q_plus_P                  | -39.629837 |
| test/reward_per_eps            | -40        |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.4135064  |
| stats_o/std                    | 0.0410715  |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.257     |
| test/Q                         | -39.536007 |
| test/Q_plus_P                  | -39.536007 |
| test/reward_per_eps            | -40        |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 392        |
| stats_o/mean                   | 0.4134934  |
| stats_o/std                    | 0.04108018 |
| test/episodes                  | 3930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -39.638542 |
| test/Q_plus_P                  | -39.638542 |
| test/reward_per_eps            | -40        |
| test/steps                     | 157200     |
| train/episodes                 | 15720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 628800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 393        |
| stats_o/mean                   | 0.41349033 |
| stats_o/std                    | 0.04107901 |
| test/episodes                  | 3940       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -39.63954  |
| test/Q_plus_P                  | -39.63954  |
| test/reward_per_eps            | -40        |
| test/steps                     | 157600     |
| train/episodes                 | 15760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 630400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.4134847   |
| stats_o/std                    | 0.041079424 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.632057  |
| test/Q_plus_P                  | -39.632057  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 395         |
| stats_o/mean                   | 0.41347477  |
| stats_o/std                    | 0.041075155 |
| test/episodes                  | 3960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.58668   |
| test/Q_plus_P                  | -39.58668   |
| test/reward_per_eps            | -40         |
| test/steps                     | 158400      |
| train/episodes                 | 15840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 633600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.41346455  |
| stats_o/std                    | 0.041077014 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.276      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -39.62579   |
| test/Q_plus_P                  | -39.62579   |
| test/reward_per_eps            | -40         |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 397        |
| stats_o/mean                   | 0.4134445  |
| stats_o/std                    | 0.0410857  |
| test/episodes                  | 3980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.619995 |
| test/Q_plus_P                  | -39.619995 |
| test/reward_per_eps            | -40        |
| test/steps                     | 159200     |
| train/episodes                 | 15920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.367     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 636800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.413445    |
| stats_o/std                    | 0.041073773 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.652115  |
| test/Q_plus_P                  | -39.652115  |
| test/reward_per_eps            | -40         |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 399        |
| stats_o/mean                   | 0.4134456  |
| stats_o/std                    | 0.04107752 |
| test/episodes                  | 4000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -39.69033  |
| test/Q_plus_P                  | -39.69033  |
| test/reward_per_eps            | -40        |
| test/steps                     | 160000     |
| train/episodes                 | 16000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 640000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.413421   |
| stats_o/std                    | 0.04107887 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.66412  |
| test/Q_plus_P                  | -39.66412  |
| test/reward_per_eps            | -40        |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 401        |
| stats_o/mean                   | 0.4134136  |
| stats_o/std                    | 0.04107791 |
| test/episodes                  | 4020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.189     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.661797 |
| test/Q_plus_P                  | -39.661797 |
| test/reward_per_eps            | -40        |
| test/steps                     | 160800     |
| train/episodes                 | 16080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.172     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 643200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 402        |
| stats_o/mean                   | 0.41341567 |
| stats_o/std                    | 0.04108138 |
| test/episodes                  | 4030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.194     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -39.651913 |
| test/Q_plus_P                  | -39.651913 |
| test/reward_per_eps            | -40        |
| test/steps                     | 161200     |
| train/episodes                 | 16120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 644800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.41340426  |
| stats_o/std                    | 0.041082863 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.67145   |
| test/Q_plus_P                  | -39.67145   |
| test/reward_per_eps            | -40         |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 404        |
| stats_o/mean                   | 0.4134071  |
| stats_o/std                    | 0.04108334 |
| test/episodes                  | 4050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.625504 |
| test/Q_plus_P                  | -39.625504 |
| test/reward_per_eps            | -40        |
| test/steps                     | 162000     |
| train/episodes                 | 16200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 648000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.41339362  |
| stats_o/std                    | 0.041078534 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.677773  |
| test/Q_plus_P                  | -39.677773  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.41338456  |
| stats_o/std                    | 0.041078787 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.705345  |
| test/Q_plus_P                  | -39.705345  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.41338012  |
| stats_o/std                    | 0.041078962 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.6747    |
| test/Q_plus_P                  | -39.6747    |
| test/reward_per_eps            | -40         |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.41337907  |
| stats_o/std                    | 0.041082807 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -39.732075  |
| test/Q_plus_P                  | -39.732075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.41337916  |
| stats_o/std                    | 0.041079942 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.668724  |
| test/Q_plus_P                  | -39.668724  |
| test/reward_per_eps            | -40         |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 410        |
| stats_o/mean                   | 0.4133732  |
| stats_o/std                    | 0.04107792 |
| test/episodes                  | 4110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -39.677456 |
| test/Q_plus_P                  | -39.677456 |
| test/reward_per_eps            | -40        |
| test/steps                     | 164400     |
| train/episodes                 | 16440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 657600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 411         |
| stats_o/mean                   | 0.41338027  |
| stats_o/std                    | 0.041074235 |
| test/episodes                  | 4120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -39.66519   |
| test/Q_plus_P                  | -39.66519   |
| test/reward_per_eps            | -40         |
| test/steps                     | 164800      |
| train/episodes                 | 16480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 659200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.4133878   |
| stats_o/std                    | 0.041073807 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.67129   |
| test/Q_plus_P                  | -39.67129   |
| test/reward_per_eps            | -40         |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 413         |
| stats_o/mean                   | 0.41339585  |
| stats_o/std                    | 0.041064393 |
| test/episodes                  | 4140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.112      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.711044  |
| test/Q_plus_P                  | -39.711044  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165600      |
| train/episodes                 | 16560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 662400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.4134129   |
| stats_o/std                    | 0.041064665 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.701168  |
| test/Q_plus_P                  | -39.701168  |
| test/reward_per_eps            | -40         |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 415         |
| stats_o/mean                   | 0.41341403  |
| stats_o/std                    | 0.041055664 |
| test/episodes                  | 4160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.761173  |
| test/Q_plus_P                  | -39.761173  |
| test/reward_per_eps            | -40         |
| test/steps                     | 166400      |
| train/episodes                 | 16640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 665600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 416        |
| stats_o/mean                   | 0.41340265 |
| stats_o/std                    | 0.04105013 |
| test/episodes                  | 4170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.254     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.714474 |
| test/Q_plus_P                  | -39.714474 |
| test/reward_per_eps            | -40        |
| test/steps                     | 166800     |
| train/episodes                 | 16680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.129     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 667200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.41338095  |
| stats_o/std                    | 0.041054197 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.731773  |
| test/Q_plus_P                  | -39.731773  |
| test/reward_per_eps            | -40         |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.41337755  |
| stats_o/std                    | 0.041050617 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.207      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.7223    |
| test/Q_plus_P                  | -39.7223    |
| test/reward_per_eps            | -40         |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 419        |
| stats_o/mean                   | 0.4133903  |
| stats_o/std                    | 0.04105475 |
| test/episodes                  | 4200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.727036 |
| test/Q_plus_P                  | -39.727036 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168000     |
| train/episodes                 | 16800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 672000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.4133886   |
| stats_o/std                    | 0.041055754 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.72789   |
| test/Q_plus_P                  | -39.72789   |
| test/reward_per_eps            | -40         |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 421         |
| stats_o/mean                   | 0.4133774   |
| stats_o/std                    | 0.041050125 |
| test/episodes                  | 4220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.729195  |
| test/Q_plus_P                  | -39.729195  |
| test/reward_per_eps            | -40         |
| test/steps                     | 168800      |
| train/episodes                 | 16880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 675200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.41335747  |
| stats_o/std                    | 0.041056342 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.75855   |
| test/Q_plus_P                  | -39.75855   |
| test/reward_per_eps            | -40         |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.41334376  |
| stats_o/std                    | 0.041057758 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.795452  |
| test/Q_plus_P                  | -39.795452  |
| test/reward_per_eps            | -40         |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 424        |
| stats_o/mean                   | 0.41333392 |
| stats_o/std                    | 0.0410619  |
| test/episodes                  | 4250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.690952 |
| test/Q_plus_P                  | -39.690952 |
| test/reward_per_eps            | -40        |
| test/steps                     | 170000     |
| train/episodes                 | 17000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 680000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.41331938  |
| stats_o/std                    | 0.041053597 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.7566    |
| test/Q_plus_P                  | -39.7566    |
| test/reward_per_eps            | -40         |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.41331068 |
| stats_o/std                    | 0.04105148 |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.763283 |
| test/Q_plus_P                  | -39.763283 |
| test/reward_per_eps            | -40        |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.36      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.41331258  |
| stats_o/std                    | 0.041049235 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.769157  |
| test/Q_plus_P                  | -39.769157  |
| test/reward_per_eps            | -40         |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.4133059   |
| stats_o/std                    | 0.041048754 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.768013  |
| test/Q_plus_P                  | -39.768013  |
| test/reward_per_eps            | -40         |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.41330814  |
| stats_o/std                    | 0.041044917 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.780396  |
| test/Q_plus_P                  | -39.780396  |
| test/reward_per_eps            | -40         |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 430         |
| stats_o/mean                   | 0.4133257   |
| stats_o/std                    | 0.041048545 |
| test/episodes                  | 4310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.775074  |
| test/Q_plus_P                  | -39.775074  |
| test/reward_per_eps            | -40         |
| test/steps                     | 172400      |
| train/episodes                 | 17240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 689600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.41331935  |
| stats_o/std                    | 0.041047882 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.77382   |
| test/Q_plus_P                  | -39.77382   |
| test/reward_per_eps            | -40         |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.371      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.41331372 |
| stats_o/std                    | 0.04104399 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.248     |
| test/Q                         | -39.771027 |
| test/Q_plus_P                  | -39.771027 |
| test/reward_per_eps            | -40        |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.41330615  |
| stats_o/std                    | 0.041043706 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -39.841724  |
| test/Q_plus_P                  | -39.841724  |
| test/reward_per_eps            | -40         |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 434         |
| stats_o/mean                   | 0.4132931   |
| stats_o/std                    | 0.041049063 |
| test/episodes                  | 4350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -39.759857  |
| test/Q_plus_P                  | -39.759857  |
| test/reward_per_eps            | -40         |
| test/steps                     | 174000      |
| train/episodes                 | 17400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.388      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 696000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 435         |
| stats_o/mean                   | 0.41329315  |
| stats_o/std                    | 0.041050937 |
| test/episodes                  | 4360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.779366  |
| test/Q_plus_P                  | -39.779366  |
| test/reward_per_eps            | -40         |
| test/steps                     | 174400      |
| train/episodes                 | 17440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 697600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.4132916  |
| stats_o/std                    | 0.04104879 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.712425 |
| test/Q_plus_P                  | -39.712425 |
| test/reward_per_eps            | -40        |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.4132998  |
| stats_o/std                    | 0.04105032 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.196     |
| test/info_shaping_reward_mean  | -0.255     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.879505 |
| test/Q_plus_P                  | -39.879505 |
| test/reward_per_eps            | -40        |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.41329026  |
| stats_o/std                    | 0.041048292 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -39.780716  |
| test/Q_plus_P                  | -39.780716  |
| test/reward_per_eps            | -40         |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.41327623  |
| stats_o/std                    | 0.041048937 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.753803  |
| test/Q_plus_P                  | -39.753803  |
| test/reward_per_eps            | -40         |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 440         |
| stats_o/mean                   | 0.41328537  |
| stats_o/std                    | 0.041043524 |
| test/episodes                  | 4410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.770382  |
| test/Q_plus_P                  | -39.770382  |
| test/reward_per_eps            | -40         |
| test/steps                     | 176400      |
| train/episodes                 | 17640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 705600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.4132793  |
| stats_o/std                    | 0.04103898 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -39.757133 |
| test/Q_plus_P                  | -39.757133 |
| test/reward_per_eps            | -40        |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.304     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 442        |
| stats_o/mean                   | 0.41327783 |
| stats_o/std                    | 0.04103374 |
| test/episodes                  | 4430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.199     |
| test/info_shaping_reward_mean  | -0.254     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.752373 |
| test/Q_plus_P                  | -39.752373 |
| test/reward_per_eps            | -40        |
| test/steps                     | 177200     |
| train/episodes                 | 17720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 708800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.41328147  |
| stats_o/std                    | 0.041036155 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.78436   |
| test/Q_plus_P                  | -39.78436   |
| test/reward_per_eps            | -40         |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.4132633   |
| stats_o/std                    | 0.041040704 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.744926  |
| test/Q_plus_P                  | -39.744926  |
| test/reward_per_eps            | -40         |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.178      |
| train/info_shaping_reward_mean | -0.263      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.41326544  |
| stats_o/std                    | 0.041046456 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.72595   |
| test/Q_plus_P                  | -39.72595   |
| test/reward_per_eps            | -40         |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.413268    |
| stats_o/std                    | 0.041038793 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -39.780685  |
| test/Q_plus_P                  | -39.780685  |
| test/reward_per_eps            | -40         |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 447        |
| stats_o/mean                   | 0.41327146 |
| stats_o/std                    | 0.04104367 |
| test/episodes                  | 4480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.78467  |
| test/Q_plus_P                  | -39.78467  |
| test/reward_per_eps            | -40        |
| test/steps                     | 179200     |
| train/episodes                 | 17920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.185     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 716800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.41326973 |
| stats_o/std                    | 0.04103844 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.792213 |
| test/Q_plus_P                  | -39.792213 |
| test/reward_per_eps            | -40        |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.41326478  |
| stats_o/std                    | 0.041041214 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.878937  |
| test/Q_plus_P                  | -39.878937  |
| test/reward_per_eps            | -40         |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.4132582  |
| stats_o/std                    | 0.04103991 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.803043 |
| test/Q_plus_P                  | -39.803043 |
| test/reward_per_eps            | -40        |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 451         |
| stats_o/mean                   | 0.41325617  |
| stats_o/std                    | 0.041037366 |
| test/episodes                  | 4520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.74984   |
| test/Q_plus_P                  | -39.74984   |
| test/reward_per_eps            | -40         |
| test/steps                     | 180800      |
| train/episodes                 | 18080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 723200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.4132588  |
| stats_o/std                    | 0.04103606 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -39.80261  |
| test/Q_plus_P                  | -39.80261  |
| test/reward_per_eps            | -40        |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 453        |
| stats_o/mean                   | 0.4132758  |
| stats_o/std                    | 0.04103591 |
| test/episodes                  | 4540       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.251     |
| test/Q                         | -39.819046 |
| test/Q_plus_P                  | -39.819046 |
| test/reward_per_eps            | -40        |
| test/steps                     | 181600     |
| train/episodes                 | 18160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 726400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 454        |
| stats_o/mean                   | 0.4132836  |
| stats_o/std                    | 0.04103139 |
| test/episodes                  | 4550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -39.834217 |
| test/Q_plus_P                  | -39.834217 |
| test/reward_per_eps            | -40        |
| test/steps                     | 182000     |
| train/episodes                 | 18200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 728000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.4132659   |
| stats_o/std                    | 0.041035566 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -39.80573   |
| test/Q_plus_P                  | -39.80573   |
| test/reward_per_eps            | -40         |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 456         |
| stats_o/mean                   | 0.41324815  |
| stats_o/std                    | 0.041032854 |
| test/episodes                  | 4570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.294      |
| test/Q                         | -39.8107    |
| test/Q_plus_P                  | -39.8107    |
| test/reward_per_eps            | -40         |
| test/steps                     | 182800      |
| train/episodes                 | 18280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 731200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 457        |
| stats_o/mean                   | 0.41322848 |
| stats_o/std                    | 0.04104501 |
| test/episodes                  | 4580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.816647 |
| test/Q_plus_P                  | -39.816647 |
| test/reward_per_eps            | -40        |
| test/steps                     | 183200     |
| train/episodes                 | 18320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.263     |
| train/info_shaping_reward_min  | -0.37      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 732800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 458         |
| stats_o/mean                   | 0.41321385  |
| stats_o/std                    | 0.041051656 |
| test/episodes                  | 4590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.870907  |
| test/Q_plus_P                  | -39.870907  |
| test/reward_per_eps            | -40         |
| test/steps                     | 183600      |
| train/episodes                 | 18360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 734400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.4131951   |
| stats_o/std                    | 0.041054778 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.829536  |
| test/Q_plus_P                  | -39.829536  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.262      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.41317806  |
| stats_o/std                    | 0.041054975 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -39.804573  |
| test/Q_plus_P                  | -39.804573  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.365      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.4131781   |
| stats_o/std                    | 0.041062143 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -39.836246  |
| test/Q_plus_P                  | -39.836246  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 462        |
| stats_o/mean                   | 0.41318786 |
| stats_o/std                    | 0.04106235 |
| test/episodes                  | 4630       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.141     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.83114  |
| test/Q_plus_P                  | -39.83114  |
| test/reward_per_eps            | -40        |
| test/steps                     | 185200     |
| train/episodes                 | 18520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 740800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.41318646 |
| stats_o/std                    | 0.04106192 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.82268  |
| test/Q_plus_P                  | -39.82268  |
| test/reward_per_eps            | -40        |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.41318667  |
| stats_o/std                    | 0.041069444 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.8304    |
| test/Q_plus_P                  | -39.8304    |
| test/reward_per_eps            | -40         |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.4131867   |
| stats_o/std                    | 0.041065432 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.842564  |
| test/Q_plus_P                  | -39.842564  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 466        |
| stats_o/mean                   | 0.41317666 |
| stats_o/std                    | 0.04106941 |
| test/episodes                  | 4670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.8242   |
| test/Q_plus_P                  | -39.8242   |
| test/reward_per_eps            | -40        |
| test/steps                     | 186800     |
| train/episodes                 | 18680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 747200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.41317928 |
| stats_o/std                    | 0.04107527 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -39.86249  |
| test/Q_plus_P                  | -39.86249  |
| test/reward_per_eps            | -40        |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.375     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.4131851   |
| stats_o/std                    | 0.041076936 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.860474  |
| test/Q_plus_P                  | -39.860474  |
| test/reward_per_eps            | -40         |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.41317928  |
| stats_o/std                    | 0.041080464 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -39.835617  |
| test/Q_plus_P                  | -39.835617  |
| test/reward_per_eps            | -40         |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 470        |
| stats_o/mean                   | 0.41317025 |
| stats_o/std                    | 0.04108112 |
| test/episodes                  | 4710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -39.849693 |
| test/Q_plus_P                  | -39.849693 |
| test/reward_per_eps            | -40        |
| test/steps                     | 188400     |
| train/episodes                 | 18840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 753600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 471         |
| stats_o/mean                   | 0.41317728  |
| stats_o/std                    | 0.041081604 |
| test/episodes                  | 4720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.88267   |
| test/Q_plus_P                  | -39.88267   |
| test/reward_per_eps            | -40         |
| test/steps                     | 188800      |
| train/episodes                 | 18880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 755200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 472         |
| stats_o/mean                   | 0.41316798  |
| stats_o/std                    | 0.041081548 |
| test/episodes                  | 4730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.8568    |
| test/Q_plus_P                  | -39.8568    |
| test/reward_per_eps            | -40         |
| test/steps                     | 189200      |
| train/episodes                 | 18920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 756800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 473         |
| stats_o/mean                   | 0.41316485  |
| stats_o/std                    | 0.041074794 |
| test/episodes                  | 4740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.85497   |
| test/Q_plus_P                  | -39.85497   |
| test/reward_per_eps            | -40         |
| test/steps                     | 189600      |
| train/episodes                 | 18960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 758400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.4131676   |
| stats_o/std                    | 0.041076355 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -39.85732   |
| test/Q_plus_P                  | -39.85732   |
| test/reward_per_eps            | -40         |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.41316855 |
| stats_o/std                    | 0.04108256 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.847244 |
| test/Q_plus_P                  | -39.847244 |
| test/reward_per_eps            | -40        |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 476        |
| stats_o/mean                   | 0.41317186 |
| stats_o/std                    | 0.0410771  |
| test/episodes                  | 4770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.133     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -39.793755 |
| test/Q_plus_P                  | -39.793755 |
| test/reward_per_eps            | -40        |
| test/steps                     | 190800     |
| train/episodes                 | 19080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 763200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.4131683   |
| stats_o/std                    | 0.041081734 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.84739   |
| test/Q_plus_P                  | -39.84739   |
| test/reward_per_eps            | -40         |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.41315997 |
| stats_o/std                    | 0.04108009 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.134     |
| test/info_shaping_reward_mean  | -0.198     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -39.83119  |
| test/Q_plus_P                  | -39.83119  |
| test/reward_per_eps            | -40        |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 479         |
| stats_o/mean                   | 0.4131736   |
| stats_o/std                    | 0.041085687 |
| test/episodes                  | 4800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.846344  |
| test/Q_plus_P                  | -39.846344  |
| test/reward_per_eps            | -40         |
| test/steps                     | 192000      |
| train/episodes                 | 19200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 768000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 480        |
| stats_o/mean                   | 0.41317782 |
| stats_o/std                    | 0.04109175 |
| test/episodes                  | 4810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -39.842426 |
| test/Q_plus_P                  | -39.842426 |
| test/reward_per_eps            | -40        |
| test/steps                     | 192400     |
| train/episodes                 | 19240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.339     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 769600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.41317332 |
| stats_o/std                    | 0.04109509 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.84527  |
| test/Q_plus_P                  | -39.84527  |
| test/reward_per_eps            | -40        |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.344     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 482        |
| stats_o/mean                   | 0.4131687  |
| stats_o/std                    | 0.04109614 |
| test/episodes                  | 4830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.857582 |
| test/Q_plus_P                  | -39.857582 |
| test/reward_per_eps            | -40        |
| test/steps                     | 193200     |
| train/episodes                 | 19320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 772800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.41316426 |
| stats_o/std                    | 0.04109898 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.860245 |
| test/Q_plus_P                  | -39.860245 |
| test/reward_per_eps            | -40        |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 484         |
| stats_o/mean                   | 0.41317472  |
| stats_o/std                    | 0.041096188 |
| test/episodes                  | 4850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.849957  |
| test/Q_plus_P                  | -39.849957  |
| test/reward_per_eps            | -40         |
| test/steps                     | 194000      |
| train/episodes                 | 19400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 776000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 485         |
| stats_o/mean                   | 0.41317034  |
| stats_o/std                    | 0.041096628 |
| test/episodes                  | 4860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.858963  |
| test/Q_plus_P                  | -39.858963  |
| test/reward_per_eps            | -40         |
| test/steps                     | 194400      |
| train/episodes                 | 19440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 777600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.41317502  |
| stats_o/std                    | 0.041089926 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.859776  |
| test/Q_plus_P                  | -39.859776  |
| test/reward_per_eps            | -40         |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.41317257 |
| stats_o/std                    | 0.04108631 |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -39.808964 |
| test/Q_plus_P                  | -39.808964 |
| test/reward_per_eps            | -40        |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 488        |
| stats_o/mean                   | 0.41316283 |
| stats_o/std                    | 0.04108804 |
| test/episodes                  | 4890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.834923 |
| test/Q_plus_P                  | -39.834923 |
| test/reward_per_eps            | -40        |
| test/steps                     | 195600     |
| train/episodes                 | 19560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 782400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.41316462 |
| stats_o/std                    | 0.04109453 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.86431  |
| test/Q_plus_P                  | -39.86431  |
| test/reward_per_eps            | -40        |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.4131558   |
| stats_o/std                    | 0.041096766 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.8749    |
| test/Q_plus_P                  | -39.8749    |
| test/reward_per_eps            | -40         |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.41316018  |
| stats_o/std                    | 0.041089963 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -39.893417  |
| test/Q_plus_P                  | -39.893417  |
| test/reward_per_eps            | -40         |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.41315314 |
| stats_o/std                    | 0.04108553 |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.186     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -39.893623 |
| test/Q_plus_P                  | -39.893623 |
| test/reward_per_eps            | -40        |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.4131442   |
| stats_o/std                    | 0.041081686 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.959427  |
| test/Q_plus_P                  | -39.959427  |
| test/reward_per_eps            | -40         |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 494         |
| stats_o/mean                   | 0.41314253  |
| stats_o/std                    | 0.041075945 |
| test/episodes                  | 4950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.860252  |
| test/Q_plus_P                  | -39.860252  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198000      |
| train/episodes                 | 19800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 792000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 495        |
| stats_o/mean                   | 0.41314378 |
| stats_o/std                    | 0.0410736  |
| test/episodes                  | 4960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -39.882328 |
| test/Q_plus_P                  | -39.882328 |
| test/reward_per_eps            | -40        |
| test/steps                     | 198400     |
| train/episodes                 | 19840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 793600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.41315296  |
| stats_o/std                    | 0.041079316 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.88859   |
| test/Q_plus_P                  | -39.88859   |
| test/reward_per_eps            | -40         |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 497         |
| stats_o/mean                   | 0.4131477   |
| stats_o/std                    | 0.041079514 |
| test/episodes                  | 4980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.885796  |
| test/Q_plus_P                  | -39.885796  |
| test/reward_per_eps            | -40         |
| test/steps                     | 199200      |
| train/episodes                 | 19920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 796800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 498         |
| stats_o/mean                   | 0.41313672  |
| stats_o/std                    | 0.041079905 |
| test/episodes                  | 4990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.902416  |
| test/Q_plus_P                  | -39.902416  |
| test/reward_per_eps            | -40         |
| test/steps                     | 199600      |
| train/episodes                 | 19960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 798400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.41313574  |
| stats_o/std                    | 0.041078173 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.864975  |
| test/Q_plus_P                  | -39.864975  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.4131315   |
| stats_o/std                    | 0.041072864 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.895     |
| test/Q_plus_P                  | -39.895     |
| test/reward_per_eps            | -40         |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.4131349  |
| stats_o/std                    | 0.04107311 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.88642  |
| test/Q_plus_P                  | -39.88642  |
| test/reward_per_eps            | -40        |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.151     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 502         |
| stats_o/mean                   | 0.4131422   |
| stats_o/std                    | 0.041079137 |
| test/episodes                  | 5030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.80298   |
| test/Q_plus_P                  | -39.80298   |
| test/reward_per_eps            | -40         |
| test/steps                     | 201200      |
| train/episodes                 | 20120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 804800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.41315165 |
| stats_o/std                    | 0.04108197 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -39.958546 |
| test/Q_plus_P                  | -39.958546 |
| test/reward_per_eps            | -40        |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.41314414  |
| stats_o/std                    | 0.041084796 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.204      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.9133    |
| test/Q_plus_P                  | -39.9133    |
| test/reward_per_eps            | -40         |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 505        |
| stats_o/mean                   | 0.4131517  |
| stats_o/std                    | 0.04108982 |
| test/episodes                  | 5060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -39.89287  |
| test/Q_plus_P                  | -39.89287  |
| test/reward_per_eps            | -40        |
| test/steps                     | 202400     |
| train/episodes                 | 20240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 809600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.41314545  |
| stats_o/std                    | 0.041088607 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.90603   |
| test/Q_plus_P                  | -39.90603   |
| test/reward_per_eps            | -40         |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.41314125 |
| stats_o/std                    | 0.04108775 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.25      |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.883392 |
| test/Q_plus_P                  | -39.883392 |
| test/reward_per_eps            | -40        |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.41314295  |
| stats_o/std                    | 0.041083448 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.203      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.885345  |
| test/Q_plus_P                  | -39.885345  |
| test/reward_per_eps            | -40         |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 509         |
| stats_o/mean                   | 0.41314653  |
| stats_o/std                    | 0.041079964 |
| test/episodes                  | 5100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.905552  |
| test/Q_plus_P                  | -39.905552  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204000      |
| train/episodes                 | 20400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 816000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.41314435  |
| stats_o/std                    | 0.041083355 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.202      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.910534  |
| test/Q_plus_P                  | -39.910534  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 511        |
| stats_o/mean                   | 0.41314337 |
| stats_o/std                    | 0.04108338 |
| test/episodes                  | 5120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -39.897057 |
| test/Q_plus_P                  | -39.897057 |
| test/reward_per_eps            | -40        |
| test/steps                     | 204800     |
| train/episodes                 | 20480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 819200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 512         |
| stats_o/mean                   | 0.41312957  |
| stats_o/std                    | 0.041080814 |
| test/episodes                  | 5130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.909332  |
| test/Q_plus_P                  | -39.909332  |
| test/reward_per_eps            | -40         |
| test/steps                     | 205200      |
| train/episodes                 | 20520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 820800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 513        |
| stats_o/mean                   | 0.41312292 |
| stats_o/std                    | 0.04107887 |
| test/episodes                  | 5140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.90968  |
| test/Q_plus_P                  | -39.90968  |
| test/reward_per_eps            | -40        |
| test/steps                     | 205600     |
| train/episodes                 | 20560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 822400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.4131159  |
| stats_o/std                    | 0.04108034 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.9106   |
| test/Q_plus_P                  | -39.9106   |
| test/reward_per_eps            | -40        |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 515        |
| stats_o/mean                   | 0.41312256 |
| stats_o/std                    | 0.0410784  |
| test/episodes                  | 5160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.206     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.334     |
| test/Q                         | -39.920116 |
| test/Q_plus_P                  | -39.920116 |
| test/reward_per_eps            | -40        |
| test/steps                     | 206400     |
| train/episodes                 | 20640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 825600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.41312066  |
| stats_o/std                    | 0.041071665 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.90357   |
| test/Q_plus_P                  | -39.90357   |
| test/reward_per_eps            | -40         |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 517        |
| stats_o/mean                   | 0.4131056  |
| stats_o/std                    | 0.0410676  |
| test/episodes                  | 5180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.967102 |
| test/Q_plus_P                  | -39.967102 |
| test/reward_per_eps            | -40        |
| test/steps                     | 207200     |
| train/episodes                 | 20720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 828800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.4130911  |
| stats_o/std                    | 0.04105972 |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -39.892216 |
| test/Q_plus_P                  | -39.892216 |
| test/reward_per_eps            | -40        |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.41308472 |
| stats_o/std                    | 0.04106434 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.138     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -39.922718 |
| test/Q_plus_P                  | -39.922718 |
| test/reward_per_eps            | -40        |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.41308323  |
| stats_o/std                    | 0.041069664 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.93408   |
| test/Q_plus_P                  | -39.93408   |
| test/reward_per_eps            | -40         |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.41309416  |
| stats_o/std                    | 0.041064475 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.926323  |
| test/Q_plus_P                  | -39.926323  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.41309312  |
| stats_o/std                    | 0.041059177 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.929996  |
| test/Q_plus_P                  | -39.929996  |
| test/reward_per_eps            | -40         |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 523         |
| stats_o/mean                   | 0.4130801   |
| stats_o/std                    | 0.041056015 |
| test/episodes                  | 5240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.920628  |
| test/Q_plus_P                  | -39.920628  |
| test/reward_per_eps            | -40         |
| test/steps                     | 209600      |
| train/episodes                 | 20960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 838400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.41306952  |
| stats_o/std                    | 0.041051008 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.97248   |
| test/Q_plus_P                  | -39.97248   |
| test/reward_per_eps            | -40         |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.41307497  |
| stats_o/std                    | 0.041046813 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.92836   |
| test/Q_plus_P                  | -39.92836   |
| test/reward_per_eps            | -40         |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.41306654  |
| stats_o/std                    | 0.041042536 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.8607    |
| test/Q_plus_P                  | -39.8607    |
| test/reward_per_eps            | -40         |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.4130614   |
| stats_o/std                    | 0.041040897 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -39.930042  |
| test/Q_plus_P                  | -39.930042  |
| test/reward_per_eps            | -40         |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 528        |
| stats_o/mean                   | 0.4130498  |
| stats_o/std                    | 0.04103685 |
| test/episodes                  | 5290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -39.922417 |
| test/Q_plus_P                  | -39.922417 |
| test/reward_per_eps            | -40        |
| test/steps                     | 211600     |
| train/episodes                 | 21160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 846400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 529         |
| stats_o/mean                   | 0.41305387  |
| stats_o/std                    | 0.041032996 |
| test/episodes                  | 5300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.234      |
| test/Q                         | -39.941296  |
| test/Q_plus_P                  | -39.941296  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212000      |
| train/episodes                 | 21200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 848000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.41304085  |
| stats_o/std                    | 0.041039865 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.101      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.936844  |
| test/Q_plus_P                  | -39.936844  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 531        |
| stats_o/mean                   | 0.41304505 |
| stats_o/std                    | 0.04104246 |
| test/episodes                  | 5320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.109     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.968437 |
| test/Q_plus_P                  | -39.968437 |
| test/reward_per_eps            | -40        |
| test/steps                     | 212800     |
| train/episodes                 | 21280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 851200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.41304433  |
| stats_o/std                    | 0.041043095 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.95093   |
| test/Q_plus_P                  | -39.95093   |
| test/reward_per_eps            | -40         |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 533        |
| stats_o/mean                   | 0.4130359  |
| stats_o/std                    | 0.04104663 |
| test/episodes                  | 5340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.922974 |
| test/Q_plus_P                  | -39.922974 |
| test/reward_per_eps            | -40        |
| test/steps                     | 213600     |
| train/episodes                 | 21360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 854400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 534        |
| stats_o/mean                   | 0.413028   |
| stats_o/std                    | 0.04104903 |
| test/episodes                  | 5350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -39.930264 |
| test/Q_plus_P                  | -39.930264 |
| test/reward_per_eps            | -40        |
| test/steps                     | 214000     |
| train/episodes                 | 21400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 856000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.41301158  |
| stats_o/std                    | 0.041054424 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.930424  |
| test/Q_plus_P                  | -39.930424  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.259      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.4130073   |
| stats_o/std                    | 0.041053206 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.93186   |
| test/Q_plus_P                  | -39.93186   |
| test/reward_per_eps            | -40         |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 537         |
| stats_o/mean                   | 0.41300187  |
| stats_o/std                    | 0.041053023 |
| test/episodes                  | 5380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.952602  |
| test/Q_plus_P                  | -39.952602  |
| test/reward_per_eps            | -40         |
| test/steps                     | 215200      |
| train/episodes                 | 21520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 860800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 538        |
| stats_o/mean                   | 0.4129868  |
| stats_o/std                    | 0.04105128 |
| test/episodes                  | 5390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -39.908585 |
| test/Q_plus_P                  | -39.908585 |
| test/reward_per_eps            | -40        |
| test/steps                     | 215600     |
| train/episodes                 | 21560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 862400     |
-----------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 539       |
| stats_o/mean                   | 0.4129798 |
| stats_o/std                    | 0.0410538 |
| test/episodes                  | 5400      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.154    |
| test/info_shaping_reward_mean  | -0.219    |
| test/info_shaping_reward_min   | -0.287    |
| test/Q                         | -39.9482  |
| test/Q_plus_P                  | -39.9482  |
| test/reward_per_eps            | -40       |
| test/steps                     | 216000    |
| train/episodes                 | 21600     |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.143    |
| train/info_shaping_reward_mean | -0.239    |
| train/info_shaping_reward_min  | -0.343    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 864000    |
----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.41296753 |
| stats_o/std                    | 0.04105359 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.926273 |
| test/Q_plus_P                  | -39.926273 |
| test/reward_per_eps            | -40        |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 541         |
| stats_o/mean                   | 0.41296065  |
| stats_o/std                    | 0.041052308 |
| test/episodes                  | 5420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.920128  |
| test/Q_plus_P                  | -39.920128  |
| test/reward_per_eps            | -40         |
| test/steps                     | 216800      |
| train/episodes                 | 21680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 867200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 542        |
| stats_o/mean                   | 0.41295707 |
| stats_o/std                    | 0.04105225 |
| test/episodes                  | 5430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -39.94205  |
| test/Q_plus_P                  | -39.94205  |
| test/reward_per_eps            | -40        |
| test/steps                     | 217200     |
| train/episodes                 | 21720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 868800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.4129567   |
| stats_o/std                    | 0.041049223 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -39.915306  |
| test/Q_plus_P                  | -39.915306  |
| test/reward_per_eps            | -40         |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.41294584  |
| stats_o/std                    | 0.041046396 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.931732  |
| test/Q_plus_P                  | -39.931732  |
| test/reward_per_eps            | -40         |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.41294837  |
| stats_o/std                    | 0.041041087 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.912632  |
| test/Q_plus_P                  | -39.912632  |
| test/reward_per_eps            | -40         |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 546        |
| stats_o/mean                   | 0.41293073 |
| stats_o/std                    | 0.04103794 |
| test/episodes                  | 5470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.91533  |
| test/Q_plus_P                  | -39.91533  |
| test/reward_per_eps            | -40        |
| test/steps                     | 218800     |
| train/episodes                 | 21880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 875200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.41293335  |
| stats_o/std                    | 0.041041162 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.91514   |
| test/Q_plus_P                  | -39.91514   |
| test/reward_per_eps            | -40         |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.41293398 |
| stats_o/std                    | 0.04103763 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -39.91856  |
| test/Q_plus_P                  | -39.91856  |
| test/reward_per_eps            | -40        |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.41293034  |
| stats_o/std                    | 0.041036207 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.218      |
| test/info_shaping_reward_mean  | -0.277      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.964493  |
| test/Q_plus_P                  | -39.964493  |
| test/reward_per_eps            | -40         |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.4129212  |
| stats_o/std                    | 0.04103815 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.91591  |
| test/Q_plus_P                  | -39.91591  |
| test/reward_per_eps            | -40        |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.41291943 |
| stats_o/std                    | 0.04103505 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.929646 |
| test/Q_plus_P                  | -39.929646 |
| test/reward_per_eps            | -40        |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.41291603  |
| stats_o/std                    | 0.041041408 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.87465   |
| test/Q_plus_P                  | -39.87465   |
| test/reward_per_eps            | -40         |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.41290608  |
| stats_o/std                    | 0.041044105 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.920494  |
| test/Q_plus_P                  | -39.920494  |
| test/reward_per_eps            | -40         |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.41290855 |
| stats_o/std                    | 0.04105536 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.139     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.904373 |
| test/Q_plus_P                  | -39.904373 |
| test/reward_per_eps            | -40        |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.41291642  |
| stats_o/std                    | 0.041059922 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.91312   |
| test/Q_plus_P                  | -39.91312   |
| test/reward_per_eps            | -40         |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 556         |
| stats_o/mean                   | 0.41291723  |
| stats_o/std                    | 0.041057672 |
| test/episodes                  | 5570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.118      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.922478  |
| test/Q_plus_P                  | -39.922478  |
| test/reward_per_eps            | -40         |
| test/steps                     | 222800      |
| train/episodes                 | 22280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 891200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 557        |
| stats_o/mean                   | 0.41291165 |
| stats_o/std                    | 0.0410497  |
| test/episodes                  | 5580       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.894745 |
| test/Q_plus_P                  | -39.894745 |
| test/reward_per_eps            | -40        |
| test/steps                     | 223200     |
| train/episodes                 | 22320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 892800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 558        |
| stats_o/mean                   | 0.41291344 |
| stats_o/std                    | 0.0410491  |
| test/episodes                  | 5590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.203     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.924908 |
| test/Q_plus_P                  | -39.924908 |
| test/reward_per_eps            | -40        |
| test/steps                     | 223600     |
| train/episodes                 | 22360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 894400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.41291693  |
| stats_o/std                    | 0.041046437 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.85623   |
| test/Q_plus_P                  | -39.85623   |
| test/reward_per_eps            | -40         |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.41292202  |
| stats_o/std                    | 0.041039582 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.894787  |
| test/Q_plus_P                  | -39.894787  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.211      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 561         |
| stats_o/mean                   | 0.41292584  |
| stats_o/std                    | 0.041039936 |
| test/episodes                  | 5620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.912174  |
| test/Q_plus_P                  | -39.912174  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224800      |
| train/episodes                 | 22480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 899200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.41293916  |
| stats_o/std                    | 0.041042753 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.92554   |
| test/Q_plus_P                  | -39.92554   |
| test/reward_per_eps            | -40         |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 563         |
| stats_o/mean                   | 0.41294876  |
| stats_o/std                    | 0.041045766 |
| test/episodes                  | 5640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.949406  |
| test/Q_plus_P                  | -39.949406  |
| test/reward_per_eps            | -40         |
| test/steps                     | 225600      |
| train/episodes                 | 22560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 902400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.41295075  |
| stats_o/std                    | 0.041047763 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -39.92396   |
| test/Q_plus_P                  | -39.92396   |
| test/reward_per_eps            | -40         |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 565        |
| stats_o/mean                   | 0.41295293 |
| stats_o/std                    | 0.04105069 |
| test/episodes                  | 5660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.247     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.91847  |
| test/Q_plus_P                  | -39.91847  |
| test/reward_per_eps            | -40        |
| test/steps                     | 226400     |
| train/episodes                 | 22640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 905600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.4129516   |
| stats_o/std                    | 0.041052807 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.930492  |
| test/Q_plus_P                  | -39.930492  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.41296747  |
| stats_o/std                    | 0.041059118 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.887657  |
| test/Q_plus_P                  | -39.887657  |
| test/reward_per_eps            | -40         |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.41296864  |
| stats_o/std                    | 0.041059736 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.114      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.9165    |
| test/Q_plus_P                  | -39.9165    |
| test/reward_per_eps            | -40         |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.41297567  |
| stats_o/std                    | 0.041054565 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.916378  |
| test/Q_plus_P                  | -39.916378  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.106      |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.4129814   |
| stats_o/std                    | 0.041056592 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.933548  |
| test/Q_plus_P                  | -39.933548  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.41297427  |
| stats_o/std                    | 0.041060522 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.91479   |
| test/Q_plus_P                  | -39.91479   |
| test/reward_per_eps            | -40         |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.412972    |
| stats_o/std                    | 0.041059945 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.924435  |
| test/Q_plus_P                  | -39.924435  |
| test/reward_per_eps            | -40         |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.4129751  |
| stats_o/std                    | 0.04106024 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.265     |
| test/Q                         | -39.887787 |
| test/Q_plus_P                  | -39.887787 |
| test/reward_per_eps            | -40        |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.4129708   |
| stats_o/std                    | 0.041061763 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.935715  |
| test/Q_plus_P                  | -39.935715  |
| test/reward_per_eps            | -40         |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.41297054  |
| stats_o/std                    | 0.041061204 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.265      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.91976   |
| test/Q_plus_P                  | -39.91976   |
| test/reward_per_eps            | -40         |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.4129586   |
| stats_o/std                    | 0.041059397 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.88774   |
| test/Q_plus_P                  | -39.88774   |
| test/reward_per_eps            | -40         |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.41295195 |
| stats_o/std                    | 0.04106806 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.947952 |
| test/Q_plus_P                  | -39.947952 |
| test/reward_per_eps            | -40        |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.254     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 578        |
| stats_o/mean                   | 0.41295922 |
| stats_o/std                    | 0.04106619 |
| test/episodes                  | 5790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -39.85777  |
| test/Q_plus_P                  | -39.85777  |
| test/reward_per_eps            | -40        |
| test/steps                     | 231600     |
| train/episodes                 | 23160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 926400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 579        |
| stats_o/mean                   | 0.4129533  |
| stats_o/std                    | 0.04107129 |
| test/episodes                  | 5800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -39.92984  |
| test/Q_plus_P                  | -39.92984  |
| test/reward_per_eps            | -40        |
| test/steps                     | 232000     |
| train/episodes                 | 23200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 928000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.4129536  |
| stats_o/std                    | 0.04107142 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -39.94486  |
| test/Q_plus_P                  | -39.94486  |
| test/reward_per_eps            | -40        |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.4129467  |
| stats_o/std                    | 0.04107385 |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.184     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.288     |
| test/Q                         | -39.92841  |
| test/Q_plus_P                  | -39.92841  |
| test/reward_per_eps            | -40        |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.41293487  |
| stats_o/std                    | 0.041079998 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.932083  |
| test/Q_plus_P                  | -39.932083  |
| test/reward_per_eps            | -40         |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.4129229   |
| stats_o/std                    | 0.041081514 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.932697  |
| test/Q_plus_P                  | -39.932697  |
| test/reward_per_eps            | -40         |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 584         |
| stats_o/mean                   | 0.4129217   |
| stats_o/std                    | 0.041077282 |
| test/episodes                  | 5850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -40.072113  |
| test/Q_plus_P                  | -40.072113  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234000      |
| train/episodes                 | 23400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 936000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.4129221   |
| stats_o/std                    | 0.041083746 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.959858  |
| test/Q_plus_P                  | -39.959858  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.26       |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 586        |
| stats_o/mean                   | 0.4129298  |
| stats_o/std                    | 0.04108525 |
| test/episodes                  | 5870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.960533 |
| test/Q_plus_P                  | -39.960533 |
| test/reward_per_eps            | -40        |
| test/steps                     | 234800     |
| train/episodes                 | 23480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 939200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 587        |
| stats_o/mean                   | 0.41292801 |
| stats_o/std                    | 0.04108887 |
| test/episodes                  | 5880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.958553 |
| test/Q_plus_P                  | -39.958553 |
| test/reward_per_eps            | -40        |
| test/steps                     | 235200     |
| train/episodes                 | 23520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 940800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.41291806  |
| stats_o/std                    | 0.041087713 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.957767  |
| test/Q_plus_P                  | -39.957767  |
| test/reward_per_eps            | -40         |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 589         |
| stats_o/mean                   | 0.41291407  |
| stats_o/std                    | 0.041087396 |
| test/episodes                  | 5900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.932735  |
| test/Q_plus_P                  | -39.932735  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236000      |
| train/episodes                 | 23600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 944000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 590         |
| stats_o/mean                   | 0.41291404  |
| stats_o/std                    | 0.041088585 |
| test/episodes                  | 5910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.13       |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.96053   |
| test/Q_plus_P                  | -39.96053   |
| test/reward_per_eps            | -40         |
| test/steps                     | 236400      |
| train/episodes                 | 23640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 945600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 591         |
| stats_o/mean                   | 0.41291225  |
| stats_o/std                    | 0.041087277 |
| test/episodes                  | 5920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.929962  |
| test/Q_plus_P                  | -39.929962  |
| test/reward_per_eps            | -40         |
| test/steps                     | 236800      |
| train/episodes                 | 23680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 947200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 592         |
| stats_o/mean                   | 0.41290835  |
| stats_o/std                    | 0.041082088 |
| test/episodes                  | 5930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.960426  |
| test/Q_plus_P                  | -39.960426  |
| test/reward_per_eps            | -40         |
| test/steps                     | 237200      |
| train/episodes                 | 23720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 948800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.4129065   |
| stats_o/std                    | 0.041081447 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.908592  |
| test/Q_plus_P                  | -39.908592  |
| test/reward_per_eps            | -40         |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.4129047  |
| stats_o/std                    | 0.04107746 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.203     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.919106 |
| test/Q_plus_P                  | -39.919106 |
| test/reward_per_eps            | -40        |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.41290936 |
| stats_o/std                    | 0.04107915 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -39.915295 |
| test/Q_plus_P                  | -39.915295 |
| test/reward_per_eps            | -40        |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 596        |
| stats_o/mean                   | 0.41291067 |
| stats_o/std                    | 0.04107996 |
| test/episodes                  | 5970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.969437 |
| test/Q_plus_P                  | -39.969437 |
| test/reward_per_eps            | -40        |
| test/steps                     | 238800     |
| train/episodes                 | 23880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 955200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.4129099   |
| stats_o/std                    | 0.041078236 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -39.95672   |
| test/Q_plus_P                  | -39.95672   |
| test/reward_per_eps            | -40         |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 598         |
| stats_o/mean                   | 0.41291094  |
| stats_o/std                    | 0.041075163 |
| test/episodes                  | 5990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.963223  |
| test/Q_plus_P                  | -39.963223  |
| test/reward_per_eps            | -40         |
| test/steps                     | 239600      |
| train/episodes                 | 23960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 958400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.41290355 |
| stats_o/std                    | 0.04107332 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.283     |
| test/Q                         | -39.907238 |
| test/Q_plus_P                  | -39.907238 |
| test/reward_per_eps            | -40        |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
