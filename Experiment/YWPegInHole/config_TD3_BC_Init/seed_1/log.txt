Logging to /home/yuchen/Research/TD3fD-through-Shaping-using-Generative-Models/Experiment/YWPegInHole/config_TD3_BC_Init/seed_1
epoch: 19 policy initialization loss: 0.004017164930701256
epoch: 39 policy initialization loss: 0.0027671311981976032
epoch: 59 policy initialization loss: 0.0018135766731575131
epoch: 79 policy initialization loss: 0.0014421507949009538
epoch: 99 policy initialization loss: 0.000508488854393363
epoch: 119 policy initialization loss: 0.0008406059350818396
epoch: 139 policy initialization loss: 0.00043043046025559306
epoch: 159 policy initialization loss: 0.0008335825405083597
epoch: 179 policy initialization loss: 0.0005705061485059559
epoch: 199 policy initialization loss: 0.000409009400755167
epoch: 219 policy initialization loss: 0.00042663724161684513
epoch: 239 policy initialization loss: 0.00029271887615323067
epoch: 259 policy initialization loss: 0.00043878162978217006
epoch: 279 policy initialization loss: 0.0005514095537364483
epoch: 299 policy initialization loss: 0.00018208430265076458
epoch: 319 policy initialization loss: 0.00019937426259275526
epoch: 339 policy initialization loss: 0.00018909265054389834
epoch: 359 policy initialization loss: 0.000510052137542516
epoch: 379 policy initialization loss: 0.00019965306273661554
epoch: 399 policy initialization loss: 0.00018822404672391713
epoch: 419 policy initialization loss: 0.00021928413480054587
epoch: 439 policy initialization loss: 0.00030185870127752423
epoch: 459 policy initialization loss: 0.00011328895197948441
epoch: 479 policy initialization loss: 0.00023318114108406007
epoch: 499 policy initialization loss: 0.00017270974058192223
epoch: 519 policy initialization loss: 0.0004050589632242918
epoch: 539 policy initialization loss: 0.0006719819502905011
epoch: 559 policy initialization loss: 6.812805077061057e-05
epoch: 579 policy initialization loss: 5.2576309826690704e-05
epoch: 599 policy initialization loss: 0.00018311664462089539
epoch: 619 policy initialization loss: 0.0006971354596316814
epoch: 639 policy initialization loss: 0.0021525919437408447
epoch: 659 policy initialization loss: 0.00019149400759488344
epoch: 679 policy initialization loss: 3.840961289824918e-05
epoch: 699 policy initialization loss: 7.243613072205335e-05
epoch: 719 policy initialization loss: 0.00022659772366750985
epoch: 739 policy initialization loss: 9.4689050456509e-05
epoch: 759 policy initialization loss: 2.8719807232846506e-05
epoch: 779 policy initialization loss: 0.00030102417804300785
epoch: 799 policy initialization loss: 0.002189895836636424
epoch: 819 policy initialization loss: 9.569833491696045e-05
epoch: 839 policy initialization loss: 4.430422995937988e-05
epoch: 859 policy initialization loss: 0.0007071373402141035
epoch: 879 policy initialization loss: 0.002245258539915085
epoch: 899 policy initialization loss: 3.4833694371627644e-05
epoch: 919 policy initialization loss: 5.4142925364430994e-05
epoch: 939 policy initialization loss: 0.0001476573816034943
epoch: 959 policy initialization loss: 2.8654230845859274e-05
epoch: 979 policy initialization loss: 0.000125390593893826
epoch: 999 policy initialization loss: 0.0003102881892118603
epoch: 1019 policy initialization loss: 6.354546349029988e-05
epoch: 1039 policy initialization loss: 0.00014500583347398788
epoch: 1059 policy initialization loss: 3.1427138310391456e-05
epoch: 1079 policy initialization loss: 1.862804128904827e-05
epoch: 1099 policy initialization loss: 5.860118835698813e-05
epoch: 1119 policy initialization loss: 0.003224671585485339
epoch: 1139 policy initialization loss: 6.840958667453378e-05
epoch: 1159 policy initialization loss: 2.627451431180816e-05
epoch: 1179 policy initialization loss: 1.7171725630760193e-05
epoch: 1199 policy initialization loss: 1.3585437045549043e-05
epoch: 1219 policy initialization loss: 0.00026839537895284593
epoch: 1239 policy initialization loss: 5.2318922826088965e-05
epoch: 1259 policy initialization loss: 3.856755211018026e-05
epoch: 1279 policy initialization loss: 3.622118674684316e-05
epoch: 1299 policy initialization loss: 1.286457154492382e-05
epoch: 1319 policy initialization loss: 8.425615669693798e-05
epoch: 1339 policy initialization loss: 6.0473226767499e-05
epoch: 1359 policy initialization loss: 0.00012856617104262114
epoch: 1379 policy initialization loss: 3.9995458791963756e-05
epoch: 1399 policy initialization loss: 1.309243725700071e-05
epoch: 1419 policy initialization loss: 9.154244253295474e-06
epoch: 1439 policy initialization loss: 2.7316018531564623e-05
epoch: 1459 policy initialization loss: 1.3809385563945398e-05
epoch: 1479 policy initialization loss: 6.679332727799192e-05
epoch: 1499 policy initialization loss: 2.9982804335304536e-05
epoch: 1519 policy initialization loss: 1.0526211553951725e-05
epoch: 1539 policy initialization loss: 5.7765559176914394e-05
epoch: 1559 policy initialization loss: 4.486831312533468e-05
epoch: 1579 policy initialization loss: 3.503172774799168e-05
epoch: 1599 policy initialization loss: 4.422480196808465e-05
epoch: 1619 policy initialization loss: 3.535181167535484e-05
epoch: 1639 policy initialization loss: 0.00012161212362116203
epoch: 1659 policy initialization loss: 1.0320309229427949e-05
epoch: 1679 policy initialization loss: 2.9437647754093632e-05
epoch: 1699 policy initialization loss: 3.3519783755764365e-05
epoch: 1719 policy initialization loss: 0.00016552468878217041
epoch: 1739 policy initialization loss: 0.0019824893679469824
epoch: 1759 policy initialization loss: 5.4058502428233624e-05
epoch: 1779 policy initialization loss: 1.9330902432557195e-05
epoch: 1799 policy initialization loss: 4.371949671622133e-06
epoch: 1819 policy initialization loss: 5.0768991059158e-05
epoch: 1839 policy initialization loss: 1.6870724721229635e-05
epoch: 1859 policy initialization loss: 1.3293726624397095e-05
epoch: 1879 policy initialization loss: 5.617459464701824e-06
epoch: 1899 policy initialization loss: 1.1663612895063125e-05
epoch: 1919 policy initialization loss: 3.876187838613987e-05
epoch: 1939 policy initialization loss: 0.00018113545957021415
epoch: 1959 policy initialization loss: 0.001117928884923458
epoch: 1979 policy initialization loss: 2.4619097530376166e-05
epoch: 1999 policy initialization loss: 1.967772186617367e-05
Saving initial policy.
------------------------------------------------
| epoch                          | 0           |
| stats_o/mean                   | 0.43696976  |
| stats_o/std                    | 0.042987257 |
| test/episodes                  | 10          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0604     |
| test/info_shaping_reward_mean  | -0.148      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -1.2499466  |
| test/Q_plus_P                  | -1.2499466  |
| test/reward_per_eps            | -40         |
| test/steps                     | 400         |
| train/episodes                 | 40          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0917     |
| train/info_shaping_reward_mean | -0.209      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 1600        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 1           |
| stats_o/mean                   | 0.43256664  |
| stats_o/std                    | 0.044797983 |
| test/episodes                  | 20          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0598     |
| test/info_shaping_reward_mean  | -0.157      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -1.5255133  |
| test/Q_plus_P                  | -1.5255133  |
| test/reward_per_eps            | -40         |
| test/steps                     | 800         |
| train/episodes                 | 80          |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.0821     |
| train/info_shaping_reward_mean | -0.196      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 3200        |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 2           |
| stats_o/mean                   | 0.43292987  |
| stats_o/std                    | 0.044329405 |
| test/episodes                  | 30          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0547     |
| test/info_shaping_reward_mean  | -0.166      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -1.8856785  |
| test/Q_plus_P                  | -1.8856785  |
| test/reward_per_eps            | -40         |
| test/steps                     | 1200        |
| train/episodes                 | 120         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.071      |
| train/info_shaping_reward_mean | -0.19       |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 4800        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 3          |
| stats_o/mean                   | 0.43442234 |
| stats_o/std                    | 0.04424436 |
| test/episodes                  | 40         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.066     |
| test/info_shaping_reward_mean  | -0.175     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -2.2328413 |
| test/Q_plus_P                  | -2.2328413 |
| test/reward_per_eps            | -40        |
| test/steps                     | 1600       |
| train/episodes                 | 160        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.0877    |
| train/info_shaping_reward_mean | -0.195     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 6400       |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 4           |
| stats_o/mean                   | 0.43537298  |
| stats_o/std                    | 0.043568294 |
| test/episodes                  | 50          |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.109      |
| test/info_shaping_reward_mean  | -0.183      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -2.623581   |
| test/Q_plus_P                  | -2.623581   |
| test/reward_per_eps            | -40         |
| test/steps                     | 2000        |
| train/episodes                 | 200         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.105      |
| train/info_shaping_reward_mean | -0.2        |
| train/info_shaping_reward_min  | -0.299      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 8000        |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 5          |
| stats_o/mean                   | 0.43586716 |
| stats_o/std                    | 0.04280211 |
| test/episodes                  | 60         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.38      |
| test/Q                         | -3.0367377 |
| test/Q_plus_P                  | -3.0367377 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2400       |
| train/episodes                 | 240        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.108     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.294     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 9600       |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 6          |
| stats_o/mean                   | 0.4352628  |
| stats_o/std                    | 0.04216799 |
| test/episodes                  | 70         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.134     |
| test/info_shaping_reward_mean  | -0.195     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -3.4842193 |
| test/Q_plus_P                  | -3.4842193 |
| test/reward_per_eps            | -40        |
| test/steps                     | 2800       |
| train/episodes                 | 280        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.207     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 11200      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 7          |
| stats_o/mean                   | 0.43528664 |
| stats_o/std                    | 0.04175568 |
| test/episodes                  | 80         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.138     |
| test/info_shaping_reward_mean  | -0.18      |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -3.8900597 |
| test/Q_plus_P                  | -3.8900597 |
| test/reward_per_eps            | -40        |
| test/steps                     | 3200       |
| train/episodes                 | 320        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 12800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 8          |
| stats_o/mean                   | 0.4346672  |
| stats_o/std                    | 0.04142855 |
| test/episodes                  | 90         |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.354     |
| test/Q                         | -4.297853  |
| test/Q_plus_P                  | -4.297853  |
| test/reward_per_eps            | -40        |
| test/steps                     | 3600       |
| train/episodes                 | 360        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 14400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 9           |
| stats_o/mean                   | 0.4340849   |
| stats_o/std                    | 0.041274987 |
| test/episodes                  | 100         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -4.7139063  |
| test/Q_plus_P                  | -4.7139063  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4000        |
| train/episodes                 | 400         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 16000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 10          |
| stats_o/mean                   | 0.43317196  |
| stats_o/std                    | 0.041088656 |
| test/episodes                  | 110         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -5.106338   |
| test/Q_plus_P                  | -5.106338   |
| test/reward_per_eps            | -40         |
| test/steps                     | 4400        |
| train/episodes                 | 440         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 17600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 11          |
| stats_o/mean                   | 0.43297693  |
| stats_o/std                    | 0.040947523 |
| test/episodes                  | 120         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -5.5081944  |
| test/Q_plus_P                  | -5.5081944  |
| test/reward_per_eps            | -40         |
| test/steps                     | 4800        |
| train/episodes                 | 480         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 19200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 12          |
| stats_o/mean                   | 0.43213114  |
| stats_o/std                    | 0.041080043 |
| test/episodes                  | 130         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -5.8674803  |
| test/Q_plus_P                  | -5.8674803  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5200        |
| train/episodes                 | 520         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 20800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 13          |
| stats_o/mean                   | 0.43178558  |
| stats_o/std                    | 0.040937632 |
| test/episodes                  | 140         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -6.2846584  |
| test/Q_plus_P                  | -6.2846584  |
| test/reward_per_eps            | -40         |
| test/steps                     | 5600        |
| train/episodes                 | 560         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 22400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 14          |
| stats_o/mean                   | 0.4315584   |
| stats_o/std                    | 0.040957786 |
| test/episodes                  | 150         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -6.6806808  |
| test/Q_plus_P                  | -6.6806808  |
| test/reward_per_eps            | -40         |
| test/steps                     | 6000        |
| train/episodes                 | 600         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 24000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 15          |
| stats_o/mean                   | 0.43144628  |
| stats_o/std                    | 0.040850546 |
| test/episodes                  | 160         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -7.064237   |
| test/Q_plus_P                  | -7.064237   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6400        |
| train/episodes                 | 640         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 25600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 16          |
| stats_o/mean                   | 0.43156353  |
| stats_o/std                    | 0.040775456 |
| test/episodes                  | 170         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -7.450563   |
| test/Q_plus_P                  | -7.450563   |
| test/reward_per_eps            | -40         |
| test/steps                     | 6800        |
| train/episodes                 | 680         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 27200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 17          |
| stats_o/mean                   | 0.43131742  |
| stats_o/std                    | 0.040779296 |
| test/episodes                  | 180         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -7.862472   |
| test/Q_plus_P                  | -7.862472   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7200        |
| train/episodes                 | 720         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.301      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 28800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 18          |
| stats_o/mean                   | 0.43131697  |
| stats_o/std                    | 0.040727403 |
| test/episodes                  | 190         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -8.231262   |
| test/Q_plus_P                  | -8.231262   |
| test/reward_per_eps            | -40         |
| test/steps                     | 7600        |
| train/episodes                 | 760         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 30400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 19         |
| stats_o/mean                   | 0.4310263  |
| stats_o/std                    | 0.04070499 |
| test/episodes                  | 200        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -8.610016  |
| test/Q_plus_P                  | -8.610016  |
| test/reward_per_eps            | -40        |
| test/steps                     | 8000       |
| train/episodes                 | 800        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 32000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 20          |
| stats_o/mean                   | 0.43088102  |
| stats_o/std                    | 0.040799554 |
| test/episodes                  | 210         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -8.9624195  |
| test/Q_plus_P                  | -8.9624195  |
| test/reward_per_eps            | -40         |
| test/steps                     | 8400        |
| train/episodes                 | 840         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 33600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 21          |
| stats_o/mean                   | 0.4308627   |
| stats_o/std                    | 0.040833976 |
| test/episodes                  | 220         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -9.356908   |
| test/Q_plus_P                  | -9.356908   |
| test/reward_per_eps            | -40         |
| test/steps                     | 8800        |
| train/episodes                 | 880         |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 35200       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 22         |
| stats_o/mean                   | 0.43089354 |
| stats_o/std                    | 0.04076151 |
| test/episodes                  | 230        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -9.719855  |
| test/Q_plus_P                  | -9.719855  |
| test/reward_per_eps            | -40        |
| test/steps                     | 9200       |
| train/episodes                 | 920        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.293     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 36800      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 23         |
| stats_o/mean                   | 0.43060336 |
| stats_o/std                    | 0.04073276 |
| test/episodes                  | 240        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -10.089955 |
| test/Q_plus_P                  | -10.089955 |
| test/reward_per_eps            | -40        |
| test/steps                     | 9600       |
| train/episodes                 | 960        |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 38400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 24          |
| stats_o/mean                   | 0.43039322  |
| stats_o/std                    | 0.040767577 |
| test/episodes                  | 250         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -10.418335  |
| test/Q_plus_P                  | -10.418335  |
| test/reward_per_eps            | -40         |
| test/steps                     | 10000       |
| train/episodes                 | 1000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 40000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 25         |
| stats_o/mean                   | 0.43039373 |
| stats_o/std                    | 0.04074097 |
| test/episodes                  | 260        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -10.791509 |
| test/Q_plus_P                  | -10.791509 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10400      |
| train/episodes                 | 1040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 41600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 26         |
| stats_o/mean                   | 0.4303048  |
| stats_o/std                    | 0.04063751 |
| test/episodes                  | 270        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.201     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -11.124738 |
| test/Q_plus_P                  | -11.124738 |
| test/reward_per_eps            | -40        |
| test/steps                     | 10800      |
| train/episodes                 | 1080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 43200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 27          |
| stats_o/mean                   | 0.43034628  |
| stats_o/std                    | 0.040560745 |
| test/episodes                  | 280         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -11.480657  |
| test/Q_plus_P                  | -11.480657  |
| test/reward_per_eps            | -40         |
| test/steps                     | 11200       |
| train/episodes                 | 1120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 44800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 28         |
| stats_o/mean                   | 0.43035936 |
| stats_o/std                    | 0.0404639  |
| test/episodes                  | 290        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -11.848671 |
| test/Q_plus_P                  | -11.848671 |
| test/reward_per_eps            | -40        |
| test/steps                     | 11600      |
| train/episodes                 | 1160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 46400      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 29         |
| stats_o/mean                   | 0.4303849  |
| stats_o/std                    | 0.04039265 |
| test/episodes                  | 300        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.206     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -12.166817 |
| test/Q_plus_P                  | -12.166817 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12000      |
| train/episodes                 | 1200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 48000      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 30         |
| stats_o/mean                   | 0.43036917 |
| stats_o/std                    | 0.04045121 |
| test/episodes                  | 310        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -12.492163 |
| test/Q_plus_P                  | -12.492163 |
| test/reward_per_eps            | -40        |
| test/steps                     | 12400      |
| train/episodes                 | 1240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 49600      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 31          |
| stats_o/mean                   | 0.43025437  |
| stats_o/std                    | 0.040395427 |
| test/episodes                  | 320         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -12.809587  |
| test/Q_plus_P                  | -12.809587  |
| test/reward_per_eps            | -40         |
| test/steps                     | 12800       |
| train/episodes                 | 1280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 51200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 32          |
| stats_o/mean                   | 0.4302343   |
| stats_o/std                    | 0.040310852 |
| test/episodes                  | 330         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -13.162671  |
| test/Q_plus_P                  | -13.162671  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13200       |
| train/episodes                 | 1320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 52800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 33          |
| stats_o/mean                   | 0.43036458  |
| stats_o/std                    | 0.040334616 |
| test/episodes                  | 340         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -13.477675  |
| test/Q_plus_P                  | -13.477675  |
| test/reward_per_eps            | -40         |
| test/steps                     | 13600       |
| train/episodes                 | 1360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 54400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 34          |
| stats_o/mean                   | 0.4302944   |
| stats_o/std                    | 0.040332604 |
| test/episodes                  | 350         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -13.771132  |
| test/Q_plus_P                  | -13.771132  |
| test/reward_per_eps            | -40         |
| test/steps                     | 14000       |
| train/episodes                 | 1400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 56000       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 35         |
| stats_o/mean                   | 0.43040285 |
| stats_o/std                    | 0.04036019 |
| test/episodes                  | 360        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -14.111472 |
| test/Q_plus_P                  | -14.111472 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14400      |
| train/episodes                 | 1440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 57600      |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 36         |
| stats_o/mean                   | 0.43026945 |
| stats_o/std                    | 0.04037452 |
| test/episodes                  | 370        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.273     |
| test/Q                         | -14.413787 |
| test/Q_plus_P                  | -14.413787 |
| test/reward_per_eps            | -40        |
| test/steps                     | 14800      |
| train/episodes                 | 1480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 59200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 37          |
| stats_o/mean                   | 0.43020487  |
| stats_o/std                    | 0.040313702 |
| test/episodes                  | 380         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -14.746516  |
| test/Q_plus_P                  | -14.746516  |
| test/reward_per_eps            | -40         |
| test/steps                     | 15200       |
| train/episodes                 | 1520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 60800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 38         |
| stats_o/mean                   | 0.4300771  |
| stats_o/std                    | 0.04025101 |
| test/episodes                  | 390        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.201     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -15.056205 |
| test/Q_plus_P                  | -15.056205 |
| test/reward_per_eps            | -40        |
| test/steps                     | 15600      |
| train/episodes                 | 1560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 62400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 39          |
| stats_o/mean                   | 0.42979833  |
| stats_o/std                    | 0.040240373 |
| test/episodes                  | 400         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -15.369926  |
| test/Q_plus_P                  | -15.369926  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16000       |
| train/episodes                 | 1600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.213      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 64000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 40          |
| stats_o/mean                   | 0.42973796  |
| stats_o/std                    | 0.040139865 |
| test/episodes                  | 410         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -15.635092  |
| test/Q_plus_P                  | -15.635092  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16400       |
| train/episodes                 | 1640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.203      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 65600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 41          |
| stats_o/mean                   | 0.42949238  |
| stats_o/std                    | 0.040153686 |
| test/episodes                  | 420         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -15.947526  |
| test/Q_plus_P                  | -15.947526  |
| test/reward_per_eps            | -40         |
| test/steps                     | 16800       |
| train/episodes                 | 1680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 67200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 42          |
| stats_o/mean                   | 0.4292723   |
| stats_o/std                    | 0.040198077 |
| test/episodes                  | 430         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.131      |
| test/info_shaping_reward_mean  | -0.194      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -16.23109   |
| test/Q_plus_P                  | -16.23109   |
| test/reward_per_eps            | -40         |
| test/steps                     | 17200       |
| train/episodes                 | 1720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 68800       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 43         |
| stats_o/mean                   | 0.42913866 |
| stats_o/std                    | 0.04015697 |
| test/episodes                  | 440        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.2       |
| test/info_shaping_reward_min   | -0.278     |
| test/Q                         | -16.518333 |
| test/Q_plus_P                  | -16.518333 |
| test/reward_per_eps            | -40        |
| test/steps                     | 17600      |
| train/episodes                 | 1760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.206     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 70400      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 44          |
| stats_o/mean                   | 0.42883906  |
| stats_o/std                    | 0.040213812 |
| test/episodes                  | 450         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -16.785593  |
| test/Q_plus_P                  | -16.785593  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18000       |
| train/episodes                 | 1800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 72000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 45          |
| stats_o/mean                   | 0.42873773  |
| stats_o/std                    | 0.040204596 |
| test/episodes                  | 460         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -17.078701  |
| test/Q_plus_P                  | -17.078701  |
| test/reward_per_eps            | -40         |
| test/steps                     | 18400       |
| train/episodes                 | 1840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 73600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 46         |
| stats_o/mean                   | 0.428676   |
| stats_o/std                    | 0.04020359 |
| test/episodes                  | 470        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -17.34574  |
| test/Q_plus_P                  | -17.34574  |
| test/reward_per_eps            | -40        |
| test/steps                     | 18800      |
| train/episodes                 | 1880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 75200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 47          |
| stats_o/mean                   | 0.4286127   |
| stats_o/std                    | 0.040176705 |
| test/episodes                  | 480         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -17.629444  |
| test/Q_plus_P                  | -17.629444  |
| test/reward_per_eps            | -40         |
| test/steps                     | 19200       |
| train/episodes                 | 1920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 76800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 48          |
| stats_o/mean                   | 0.42852297  |
| stats_o/std                    | 0.040182825 |
| test/episodes                  | 490         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -17.89614   |
| test/Q_plus_P                  | -17.89614   |
| test/reward_per_eps            | -40         |
| test/steps                     | 19600       |
| train/episodes                 | 1960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 78400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 49         |
| stats_o/mean                   | 0.42839122 |
| stats_o/std                    | 0.04022571 |
| test/episodes                  | 500        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -18.191797 |
| test/Q_plus_P                  | -18.191797 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20000      |
| train/episodes                 | 2000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 80000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 50          |
| stats_o/mean                   | 0.42827156  |
| stats_o/std                    | 0.040270254 |
| test/episodes                  | 510         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -18.445381  |
| test/Q_plus_P                  | -18.445381  |
| test/reward_per_eps            | -40         |
| test/steps                     | 20400       |
| train/episodes                 | 2040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 81600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 51         |
| stats_o/mean                   | 0.42805633 |
| stats_o/std                    | 0.04029907 |
| test/episodes                  | 520        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -18.680824 |
| test/Q_plus_P                  | -18.680824 |
| test/reward_per_eps            | -40        |
| test/steps                     | 20800      |
| train/episodes                 | 2080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 83200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 52          |
| stats_o/mean                   | 0.4280106   |
| stats_o/std                    | 0.040347047 |
| test/episodes                  | 530         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -18.98108   |
| test/Q_plus_P                  | -18.98108   |
| test/reward_per_eps            | -40         |
| test/steps                     | 21200       |
| train/episodes                 | 2120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 84800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 53          |
| stats_o/mean                   | 0.42801484  |
| stats_o/std                    | 0.040287513 |
| test/episodes                  | 540         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -19.23847   |
| test/Q_plus_P                  | -19.23847   |
| test/reward_per_eps            | -40         |
| test/steps                     | 21600       |
| train/episodes                 | 2160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 86400       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 54         |
| stats_o/mean                   | 0.42794368 |
| stats_o/std                    | 0.04024673 |
| test/episodes                  | 550        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -19.46226  |
| test/Q_plus_P                  | -19.46226  |
| test/reward_per_eps            | -40        |
| test/steps                     | 22000      |
| train/episodes                 | 2200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.307     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 88000      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 55          |
| stats_o/mean                   | 0.42788896  |
| stats_o/std                    | 0.040225804 |
| test/episodes                  | 560         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -19.743492  |
| test/Q_plus_P                  | -19.743492  |
| test/reward_per_eps            | -40         |
| test/steps                     | 22400       |
| train/episodes                 | 2240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 89600       |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 56         |
| stats_o/mean                   | 0.42775023 |
| stats_o/std                    | 0.04024319 |
| test/episodes                  | 570        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -19.970892 |
| test/Q_plus_P                  | -19.970892 |
| test/reward_per_eps            | -40        |
| test/steps                     | 22800      |
| train/episodes                 | 2280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 91200      |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 57          |
| stats_o/mean                   | 0.42766216  |
| stats_o/std                    | 0.040213462 |
| test/episodes                  | 580         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -20.233303  |
| test/Q_plus_P                  | -20.233303  |
| test/reward_per_eps            | -40         |
| test/steps                     | 23200       |
| train/episodes                 | 2320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 92800       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 58          |
| stats_o/mean                   | 0.42758027  |
| stats_o/std                    | 0.040229853 |
| test/episodes                  | 590         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -20.47495   |
| test/Q_plus_P                  | -20.47495   |
| test/reward_per_eps            | -40         |
| test/steps                     | 23600       |
| train/episodes                 | 2360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 94400       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 59          |
| stats_o/mean                   | 0.42741528  |
| stats_o/std                    | 0.040222004 |
| test/episodes                  | 600         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -20.705494  |
| test/Q_plus_P                  | -20.705494  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24000       |
| train/episodes                 | 2400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 96000       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 60          |
| stats_o/mean                   | 0.42732     |
| stats_o/std                    | 0.040233944 |
| test/episodes                  | 610         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -20.92422   |
| test/Q_plus_P                  | -20.92422   |
| test/reward_per_eps            | -40         |
| test/steps                     | 24400       |
| train/episodes                 | 2440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 97600       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 61          |
| stats_o/mean                   | 0.42717573  |
| stats_o/std                    | 0.040253673 |
| test/episodes                  | 620         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -21.146544  |
| test/Q_plus_P                  | -21.146544  |
| test/reward_per_eps            | -40         |
| test/steps                     | 24800       |
| train/episodes                 | 2480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 99200       |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 62          |
| stats_o/mean                   | 0.4271494   |
| stats_o/std                    | 0.040237423 |
| test/episodes                  | 630         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -21.407637  |
| test/Q_plus_P                  | -21.407637  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25200       |
| train/episodes                 | 2520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 100800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 63          |
| stats_o/mean                   | 0.4270651   |
| stats_o/std                    | 0.040275928 |
| test/episodes                  | 640         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -21.612482  |
| test/Q_plus_P                  | -21.612482  |
| test/reward_per_eps            | -40         |
| test/steps                     | 25600       |
| train/episodes                 | 2560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 102400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 64          |
| stats_o/mean                   | 0.42698237  |
| stats_o/std                    | 0.040294625 |
| test/episodes                  | 650         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -21.855724  |
| test/Q_plus_P                  | -21.855724  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26000       |
| train/episodes                 | 2600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 104000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 65          |
| stats_o/mean                   | 0.42688346  |
| stats_o/std                    | 0.040321417 |
| test/episodes                  | 660         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -22.053411  |
| test/Q_plus_P                  | -22.053411  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26400       |
| train/episodes                 | 2640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 105600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 66          |
| stats_o/mean                   | 0.42675933  |
| stats_o/std                    | 0.040351056 |
| test/episodes                  | 670         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.242      |
| test/Q                         | -22.302675  |
| test/Q_plus_P                  | -22.302675  |
| test/reward_per_eps            | -40         |
| test/steps                     | 26800       |
| train/episodes                 | 2680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 107200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 67         |
| stats_o/mean                   | 0.426675   |
| stats_o/std                    | 0.04036762 |
| test/episodes                  | 680        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.202     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -22.508825 |
| test/Q_plus_P                  | -22.508825 |
| test/reward_per_eps            | -40        |
| test/steps                     | 27200      |
| train/episodes                 | 2720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 108800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 68          |
| stats_o/mean                   | 0.42658496  |
| stats_o/std                    | 0.040344942 |
| test/episodes                  | 690         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -22.71983   |
| test/Q_plus_P                  | -22.71983   |
| test/reward_per_eps            | -40         |
| test/steps                     | 27600       |
| train/episodes                 | 2760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 110400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 69         |
| stats_o/mean                   | 0.42655155 |
| stats_o/std                    | 0.04030547 |
| test/episodes                  | 700        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -22.93228  |
| test/Q_plus_P                  | -22.93228  |
| test/reward_per_eps            | -40        |
| test/steps                     | 28000      |
| train/episodes                 | 2800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.222     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 112000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 70         |
| stats_o/mean                   | 0.42640278 |
| stats_o/std                    | 0.04029445 |
| test/episodes                  | 710        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -23.141338 |
| test/Q_plus_P                  | -23.141338 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28400      |
| train/episodes                 | 2840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 113600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 71         |
| stats_o/mean                   | 0.42621493 |
| stats_o/std                    | 0.04035786 |
| test/episodes                  | 720        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -23.321367 |
| test/Q_plus_P                  | -23.321367 |
| test/reward_per_eps            | -40        |
| test/steps                     | 28800      |
| train/episodes                 | 2880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 115200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 72          |
| stats_o/mean                   | 0.42607507  |
| stats_o/std                    | 0.040359385 |
| test/episodes                  | 730         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -23.549623  |
| test/Q_plus_P                  | -23.549623  |
| test/reward_per_eps            | -40         |
| test/steps                     | 29200       |
| train/episodes                 | 2920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 116800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 73          |
| stats_o/mean                   | 0.42597154  |
| stats_o/std                    | 0.040309764 |
| test/episodes                  | 740         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -23.75313   |
| test/Q_plus_P                  | -23.75313   |
| test/reward_per_eps            | -40         |
| test/steps                     | 29600       |
| train/episodes                 | 2960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 118400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 74          |
| stats_o/mean                   | 0.42579356  |
| stats_o/std                    | 0.040350173 |
| test/episodes                  | 750         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -23.952305  |
| test/Q_plus_P                  | -23.952305  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30000       |
| train/episodes                 | 3000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 120000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 75          |
| stats_o/mean                   | 0.4257191   |
| stats_o/std                    | 0.040373225 |
| test/episodes                  | 760         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -24.128157  |
| test/Q_plus_P                  | -24.128157  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30400       |
| train/episodes                 | 3040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 121600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 76          |
| stats_o/mean                   | 0.4256728   |
| stats_o/std                    | 0.040395696 |
| test/episodes                  | 770         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.201      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -24.359041  |
| test/Q_plus_P                  | -24.359041  |
| test/reward_per_eps            | -40         |
| test/steps                     | 30800       |
| train/episodes                 | 3080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 123200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 77         |
| stats_o/mean                   | 0.4256403  |
| stats_o/std                    | 0.04039421 |
| test/episodes                  | 780        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -24.53321  |
| test/Q_plus_P                  | -24.53321  |
| test/reward_per_eps            | -40        |
| test/steps                     | 31200      |
| train/episodes                 | 3120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 124800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 78          |
| stats_o/mean                   | 0.42557216  |
| stats_o/std                    | 0.040374767 |
| test/episodes                  | 790         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -24.747187  |
| test/Q_plus_P                  | -24.747187  |
| test/reward_per_eps            | -40         |
| test/steps                     | 31600       |
| train/episodes                 | 3160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 126400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 79         |
| stats_o/mean                   | 0.4254252  |
| stats_o/std                    | 0.0403825  |
| test/episodes                  | 800        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -24.930664 |
| test/Q_plus_P                  | -24.930664 |
| test/reward_per_eps            | -40        |
| test/steps                     | 32000      |
| train/episodes                 | 3200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 128000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 80         |
| stats_o/mean                   | 0.42528906 |
| stats_o/std                    | 0.04036103 |
| test/episodes                  | 810        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -25.11287  |
| test/Q_plus_P                  | -25.11287  |
| test/reward_per_eps            | -40        |
| test/steps                     | 32400      |
| train/episodes                 | 3240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 129600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 81          |
| stats_o/mean                   | 0.42523608  |
| stats_o/std                    | 0.040332556 |
| test/episodes                  | 820         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.148      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -25.311752  |
| test/Q_plus_P                  | -25.311752  |
| test/reward_per_eps            | -40         |
| test/steps                     | 32800       |
| train/episodes                 | 3280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 131200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 82          |
| stats_o/mean                   | 0.42511246  |
| stats_o/std                    | 0.040342983 |
| test/episodes                  | 830         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -25.469145  |
| test/Q_plus_P                  | -25.469145  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33200       |
| train/episodes                 | 3320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 132800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 83          |
| stats_o/mean                   | 0.42505762  |
| stats_o/std                    | 0.040350772 |
| test/episodes                  | 840         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -25.657677  |
| test/Q_plus_P                  | -25.657677  |
| test/reward_per_eps            | -40         |
| test/steps                     | 33600       |
| train/episodes                 | 3360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 134400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 84         |
| stats_o/mean                   | 0.424981   |
| stats_o/std                    | 0.04034098 |
| test/episodes                  | 850        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -25.840073 |
| test/Q_plus_P                  | -25.840073 |
| test/reward_per_eps            | -40        |
| test/steps                     | 34000      |
| train/episodes                 | 3400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 136000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 85          |
| stats_o/mean                   | 0.42490342  |
| stats_o/std                    | 0.040320124 |
| test/episodes                  | 860         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -25.9959    |
| test/Q_plus_P                  | -25.9959    |
| test/reward_per_eps            | -40         |
| test/steps                     | 34400       |
| train/episodes                 | 3440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 137600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 86          |
| stats_o/mean                   | 0.4248136   |
| stats_o/std                    | 0.040323295 |
| test/episodes                  | 870         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -26.175474  |
| test/Q_plus_P                  | -26.175474  |
| test/reward_per_eps            | -40         |
| test/steps                     | 34800       |
| train/episodes                 | 3480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 139200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 87         |
| stats_o/mean                   | 0.42476737 |
| stats_o/std                    | 0.04033855 |
| test/episodes                  | 880        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -26.332527 |
| test/Q_plus_P                  | -26.332527 |
| test/reward_per_eps            | -40        |
| test/steps                     | 35200      |
| train/episodes                 | 3520       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 140800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 88          |
| stats_o/mean                   | 0.42470682  |
| stats_o/std                    | 0.040340967 |
| test/episodes                  | 890         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.103      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -26.52874   |
| test/Q_plus_P                  | -26.52874   |
| test/reward_per_eps            | -40         |
| test/steps                     | 35600       |
| train/episodes                 | 3560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 142400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 89         |
| stats_o/mean                   | 0.42464265 |
| stats_o/std                    | 0.04034753 |
| test/episodes                  | 900        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -26.682968 |
| test/Q_plus_P                  | -26.682968 |
| test/reward_per_eps            | -40        |
| test/steps                     | 36000      |
| train/episodes                 | 3600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 144000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 90          |
| stats_o/mean                   | 0.4245464   |
| stats_o/std                    | 0.040323876 |
| test/episodes                  | 910         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -26.837492  |
| test/Q_plus_P                  | -26.837492  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36400       |
| train/episodes                 | 3640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 145600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 91          |
| stats_o/mean                   | 0.42448196  |
| stats_o/std                    | 0.040293884 |
| test/episodes                  | 920         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -27.012363  |
| test/Q_plus_P                  | -27.012363  |
| test/reward_per_eps            | -40         |
| test/steps                     | 36800       |
| train/episodes                 | 3680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 147200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 92         |
| stats_o/mean                   | 0.42447162 |
| stats_o/std                    | 0.04030697 |
| test/episodes                  | 930        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -27.184961 |
| test/Q_plus_P                  | -27.184961 |
| test/reward_per_eps            | -40        |
| test/steps                     | 37200      |
| train/episodes                 | 3720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 148800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 93          |
| stats_o/mean                   | 0.42442644  |
| stats_o/std                    | 0.040314388 |
| test/episodes                  | 940         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.108      |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -27.323175  |
| test/Q_plus_P                  | -27.323175  |
| test/reward_per_eps            | -40         |
| test/steps                     | 37600       |
| train/episodes                 | 3760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 150400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 94         |
| stats_o/mean                   | 0.4243616  |
| stats_o/std                    | 0.04034623 |
| test/episodes                  | 950        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -27.511353 |
| test/Q_plus_P                  | -27.511353 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38000      |
| train/episodes                 | 3800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 152000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 95          |
| stats_o/mean                   | 0.4242793   |
| stats_o/std                    | 0.040320255 |
| test/episodes                  | 960         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.192      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -27.629799  |
| test/Q_plus_P                  | -27.629799  |
| test/reward_per_eps            | -40         |
| test/steps                     | 38400       |
| train/episodes                 | 3840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 153600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 96         |
| stats_o/mean                   | 0.42426333 |
| stats_o/std                    | 0.04033712 |
| test/episodes                  | 970        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -27.805244 |
| test/Q_plus_P                  | -27.805244 |
| test/reward_per_eps            | -40        |
| test/steps                     | 38800      |
| train/episodes                 | 3880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 155200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 97         |
| stats_o/mean                   | 0.42423216 |
| stats_o/std                    | 0.04034897 |
| test/episodes                  | 980        |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -27.942488 |
| test/Q_plus_P                  | -27.942488 |
| test/reward_per_eps            | -40        |
| test/steps                     | 39200      |
| train/episodes                 | 3920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.103     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 156800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 98          |
| stats_o/mean                   | 0.42420784  |
| stats_o/std                    | 0.040363133 |
| test/episodes                  | 990         |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -28.075377  |
| test/Q_plus_P                  | -28.075377  |
| test/reward_per_eps            | -40         |
| test/steps                     | 39600       |
| train/episodes                 | 3960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 158400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 99         |
| stats_o/mean                   | 0.42414197 |
| stats_o/std                    | 0.04038341 |
| test/episodes                  | 1000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -28.24632  |
| test/Q_plus_P                  | -28.24632  |
| test/reward_per_eps            | -40        |
| test/steps                     | 40000      |
| train/episodes                 | 4000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 160000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 100         |
| stats_o/mean                   | 0.42405078  |
| stats_o/std                    | 0.040399075 |
| test/episodes                  | 1010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -28.411392  |
| test/Q_plus_P                  | -28.411392  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40400       |
| train/episodes                 | 4040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 161600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 101         |
| stats_o/mean                   | 0.42396823  |
| stats_o/std                    | 0.040397283 |
| test/episodes                  | 1020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -28.528065  |
| test/Q_plus_P                  | -28.528065  |
| test/reward_per_eps            | -40         |
| test/steps                     | 40800       |
| train/episodes                 | 4080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 163200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 102         |
| stats_o/mean                   | 0.42393896  |
| stats_o/std                    | 0.040391956 |
| test/episodes                  | 1030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -28.68979   |
| test/Q_plus_P                  | -28.68979   |
| test/reward_per_eps            | -40         |
| test/steps                     | 41200       |
| train/episodes                 | 4120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 164800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 103         |
| stats_o/mean                   | 0.42386448  |
| stats_o/std                    | 0.040394958 |
| test/episodes                  | 1040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -28.819231  |
| test/Q_plus_P                  | -28.819231  |
| test/reward_per_eps            | -40         |
| test/steps                     | 41600       |
| train/episodes                 | 4160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 166400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 104         |
| stats_o/mean                   | 0.42376664  |
| stats_o/std                    | 0.040411238 |
| test/episodes                  | 1050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -28.959055  |
| test/Q_plus_P                  | -28.959055  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42000       |
| train/episodes                 | 4200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 168000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 105         |
| stats_o/mean                   | 0.42368984  |
| stats_o/std                    | 0.040425543 |
| test/episodes                  | 1060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.244      |
| test/Q                         | -29.094639  |
| test/Q_plus_P                  | -29.094639  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42400       |
| train/episodes                 | 4240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 169600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 106         |
| stats_o/mean                   | 0.42367315  |
| stats_o/std                    | 0.040458485 |
| test/episodes                  | 1070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -29.224316  |
| test/Q_plus_P                  | -29.224316  |
| test/reward_per_eps            | -40         |
| test/steps                     | 42800       |
| train/episodes                 | 4280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 171200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 107         |
| stats_o/mean                   | 0.42356217  |
| stats_o/std                    | 0.040485434 |
| test/episodes                  | 1080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -29.359968  |
| test/Q_plus_P                  | -29.359968  |
| test/reward_per_eps            | -40         |
| test/steps                     | 43200       |
| train/episodes                 | 4320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 172800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 108        |
| stats_o/mean                   | 0.42349318 |
| stats_o/std                    | 0.04048612 |
| test/episodes                  | 1090       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.279     |
| test/Q                         | -29.501    |
| test/Q_plus_P                  | -29.501    |
| test/reward_per_eps            | -40        |
| test/steps                     | 43600      |
| train/episodes                 | 4360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 174400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 109        |
| stats_o/mean                   | 0.42347312 |
| stats_o/std                    | 0.04050074 |
| test/episodes                  | 1100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -29.620667 |
| test/Q_plus_P                  | -29.620667 |
| test/reward_per_eps            | -40        |
| test/steps                     | 44000      |
| train/episodes                 | 4400       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 176000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 110        |
| stats_o/mean                   | 0.4234185  |
| stats_o/std                    | 0.04051691 |
| test/episodes                  | 1110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -29.7259   |
| test/Q_plus_P                  | -29.7259   |
| test/reward_per_eps            | -40        |
| test/steps                     | 44400      |
| train/episodes                 | 4440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 177600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 111         |
| stats_o/mean                   | 0.42343876  |
| stats_o/std                    | 0.040512893 |
| test/episodes                  | 1120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.12       |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -29.906197  |
| test/Q_plus_P                  | -29.906197  |
| test/reward_per_eps            | -40         |
| test/steps                     | 44800       |
| train/episodes                 | 4480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 179200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 112         |
| stats_o/mean                   | 0.42345762  |
| stats_o/std                    | 0.040522996 |
| test/episodes                  | 1130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -29.989809  |
| test/Q_plus_P                  | -29.989809  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45200       |
| train/episodes                 | 4520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.125      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 180800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 113         |
| stats_o/mean                   | 0.42338789  |
| stats_o/std                    | 0.040509865 |
| test/episodes                  | 1140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -30.130005  |
| test/Q_plus_P                  | -30.130005  |
| test/reward_per_eps            | -40         |
| test/steps                     | 45600       |
| train/episodes                 | 4560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 182400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 114         |
| stats_o/mean                   | 0.42332533  |
| stats_o/std                    | 0.040496834 |
| test/episodes                  | 1150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.127      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -30.289976  |
| test/Q_plus_P                  | -30.289976  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46000       |
| train/episodes                 | 4600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.122      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 184000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 115         |
| stats_o/mean                   | 0.42330626  |
| stats_o/std                    | 0.040513773 |
| test/episodes                  | 1160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -30.373438  |
| test/Q_plus_P                  | -30.373438  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46400       |
| train/episodes                 | 4640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 185600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 116         |
| stats_o/mean                   | 0.42333493  |
| stats_o/std                    | 0.040500782 |
| test/episodes                  | 1170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -30.490679  |
| test/Q_plus_P                  | -30.490679  |
| test/reward_per_eps            | -40         |
| test/steps                     | 46800       |
| train/episodes                 | 4680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.292      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 187200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 117        |
| stats_o/mean                   | 0.4233326  |
| stats_o/std                    | 0.04050542 |
| test/episodes                  | 1180       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.215     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -30.599302 |
| test/Q_plus_P                  | -30.599302 |
| test/reward_per_eps            | -40        |
| test/steps                     | 47200      |
| train/episodes                 | 4720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 188800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 118         |
| stats_o/mean                   | 0.42334017  |
| stats_o/std                    | 0.040511455 |
| test/episodes                  | 1190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -30.732456  |
| test/Q_plus_P                  | -30.732456  |
| test/reward_per_eps            | -40         |
| test/steps                     | 47600       |
| train/episodes                 | 4760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 190400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 119         |
| stats_o/mean                   | 0.4232595   |
| stats_o/std                    | 0.040551487 |
| test/episodes                  | 1200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -30.85458   |
| test/Q_plus_P                  | -30.85458   |
| test/reward_per_eps            | -40         |
| test/steps                     | 48000       |
| train/episodes                 | 4800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 192000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 120         |
| stats_o/mean                   | 0.42317626  |
| stats_o/std                    | 0.040549483 |
| test/episodes                  | 1210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -30.949945  |
| test/Q_plus_P                  | -30.949945  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48400       |
| train/episodes                 | 4840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 193600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 121         |
| stats_o/mean                   | 0.42308506  |
| stats_o/std                    | 0.040562745 |
| test/episodes                  | 1220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -31.081446  |
| test/Q_plus_P                  | -31.081446  |
| test/reward_per_eps            | -40         |
| test/steps                     | 48800       |
| train/episodes                 | 4880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 195200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 122        |
| stats_o/mean                   | 0.42299214 |
| stats_o/std                    | 0.04057574 |
| test/episodes                  | 1230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -31.186409 |
| test/Q_plus_P                  | -31.186409 |
| test/reward_per_eps            | -40        |
| test/steps                     | 49200      |
| train/episodes                 | 4920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.345     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 196800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 123         |
| stats_o/mean                   | 0.42295754  |
| stats_o/std                    | 0.040587742 |
| test/episodes                  | 1240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.249      |
| test/Q                         | -31.292141  |
| test/Q_plus_P                  | -31.292141  |
| test/reward_per_eps            | -40         |
| test/steps                     | 49600       |
| train/episodes                 | 4960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 198400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 124         |
| stats_o/mean                   | 0.42289808  |
| stats_o/std                    | 0.040605243 |
| test/episodes                  | 1250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -31.400782  |
| test/Q_plus_P                  | -31.400782  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50000       |
| train/episodes                 | 5000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 200000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 125        |
| stats_o/mean                   | 0.4227874  |
| stats_o/std                    | 0.04060286 |
| test/episodes                  | 1260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -31.525654 |
| test/Q_plus_P                  | -31.525654 |
| test/reward_per_eps            | -40        |
| test/steps                     | 50400      |
| train/episodes                 | 5040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 201600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 126         |
| stats_o/mean                   | 0.42269543  |
| stats_o/std                    | 0.040599916 |
| test/episodes                  | 1270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -31.645742  |
| test/Q_plus_P                  | -31.645742  |
| test/reward_per_eps            | -40         |
| test/steps                     | 50800       |
| train/episodes                 | 5080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 203200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 127         |
| stats_o/mean                   | 0.42268074  |
| stats_o/std                    | 0.040610667 |
| test/episodes                  | 1280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -31.735968  |
| test/Q_plus_P                  | -31.735968  |
| test/reward_per_eps            | -40         |
| test/steps                     | 51200       |
| train/episodes                 | 5120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 204800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 128        |
| stats_o/mean                   | 0.4225773  |
| stats_o/std                    | 0.04064862 |
| test/episodes                  | 1290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.205     |
| test/info_shaping_reward_mean  | -0.256     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -31.834526 |
| test/Q_plus_P                  | -31.834526 |
| test/reward_per_eps            | -40        |
| test/steps                     | 51600      |
| train/episodes                 | 5160       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 206400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 129         |
| stats_o/mean                   | 0.4225199   |
| stats_o/std                    | 0.040667236 |
| test/episodes                  | 1300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -31.954426  |
| test/Q_plus_P                  | -31.954426  |
| test/reward_per_eps            | -40         |
| test/steps                     | 52000       |
| train/episodes                 | 5200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 208000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 130        |
| stats_o/mean                   | 0.42241076 |
| stats_o/std                    | 0.04068487 |
| test/episodes                  | 1310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -32.05353  |
| test/Q_plus_P                  | -32.05353  |
| test/reward_per_eps            | -40        |
| test/steps                     | 52400      |
| train/episodes                 | 5240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 209600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 131        |
| stats_o/mean                   | 0.42227578 |
| stats_o/std                    | 0.04068542 |
| test/episodes                  | 1320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -32.117943 |
| test/Q_plus_P                  | -32.117943 |
| test/reward_per_eps            | -40        |
| test/steps                     | 52800      |
| train/episodes                 | 5280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 211200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 132         |
| stats_o/mean                   | 0.4222087   |
| stats_o/std                    | 0.040699113 |
| test/episodes                  | 1330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -32.24828   |
| test/Q_plus_P                  | -32.24828   |
| test/reward_per_eps            | -40         |
| test/steps                     | 53200       |
| train/episodes                 | 5320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 212800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 133        |
| stats_o/mean                   | 0.42212078 |
| stats_o/std                    | 0.04073439 |
| test/episodes                  | 1340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -32.350403 |
| test/Q_plus_P                  | -32.350403 |
| test/reward_per_eps            | -40        |
| test/steps                     | 53600      |
| train/episodes                 | 5360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 214400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 134         |
| stats_o/mean                   | 0.42207322  |
| stats_o/std                    | 0.040740438 |
| test/episodes                  | 1350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -32.459484  |
| test/Q_plus_P                  | -32.459484  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54000       |
| train/episodes                 | 5400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 216000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 135         |
| stats_o/mean                   | 0.42199126  |
| stats_o/std                    | 0.040755086 |
| test/episodes                  | 1360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -32.509357  |
| test/Q_plus_P                  | -32.509357  |
| test/reward_per_eps            | -40         |
| test/steps                     | 54400       |
| train/episodes                 | 5440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 217600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 136         |
| stats_o/mean                   | 0.4219402   |
| stats_o/std                    | 0.040759567 |
| test/episodes                  | 1370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -32.58374   |
| test/Q_plus_P                  | -32.58374   |
| test/reward_per_eps            | -40         |
| test/steps                     | 54800       |
| train/episodes                 | 5480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 219200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 137         |
| stats_o/mean                   | 0.42189956  |
| stats_o/std                    | 0.040764757 |
| test/episodes                  | 1380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -32.67498   |
| test/Q_plus_P                  | -32.67498   |
| test/reward_per_eps            | -40         |
| test/steps                     | 55200       |
| train/episodes                 | 5520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 220800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 138        |
| stats_o/mean                   | 0.42179918 |
| stats_o/std                    | 0.04079613 |
| test/episodes                  | 1390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -32.80614  |
| test/Q_plus_P                  | -32.80614  |
| test/reward_per_eps            | -40        |
| test/steps                     | 55600      |
| train/episodes                 | 5560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 222400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 139        |
| stats_o/mean                   | 0.42175588 |
| stats_o/std                    | 0.04080852 |
| test/episodes                  | 1400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -32.898647 |
| test/Q_plus_P                  | -32.898647 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56000      |
| train/episodes                 | 5600       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 224000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 140        |
| stats_o/mean                   | 0.42174986 |
| stats_o/std                    | 0.0408176  |
| test/episodes                  | 1410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.138     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -33.012543 |
| test/Q_plus_P                  | -33.012543 |
| test/reward_per_eps            | -40        |
| test/steps                     | 56400      |
| train/episodes                 | 5640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 225600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 141         |
| stats_o/mean                   | 0.42167965  |
| stats_o/std                    | 0.040809147 |
| test/episodes                  | 1420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -33.05672   |
| test/Q_plus_P                  | -33.05672   |
| test/reward_per_eps            | -40         |
| test/steps                     | 56800       |
| train/episodes                 | 5680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 227200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 142         |
| stats_o/mean                   | 0.42160416  |
| stats_o/std                    | 0.040821645 |
| test/episodes                  | 1430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -33.124012  |
| test/Q_plus_P                  | -33.124012  |
| test/reward_per_eps            | -40         |
| test/steps                     | 57200       |
| train/episodes                 | 5720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 228800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 143         |
| stats_o/mean                   | 0.42155156  |
| stats_o/std                    | 0.040830743 |
| test/episodes                  | 1440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -33.223446  |
| test/Q_plus_P                  | -33.223446  |
| test/reward_per_eps            | -40         |
| test/steps                     | 57600       |
| train/episodes                 | 5760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 230400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 144         |
| stats_o/mean                   | 0.4214823   |
| stats_o/std                    | 0.040817548 |
| test/episodes                  | 1450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -33.324585  |
| test/Q_plus_P                  | -33.324585  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58000       |
| train/episodes                 | 5800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 232000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 145         |
| stats_o/mean                   | 0.42146063  |
| stats_o/std                    | 0.040828068 |
| test/episodes                  | 1460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -33.433052  |
| test/Q_plus_P                  | -33.433052  |
| test/reward_per_eps            | -40         |
| test/steps                     | 58400       |
| train/episodes                 | 5840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 233600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 146        |
| stats_o/mean                   | 0.42143336 |
| stats_o/std                    | 0.04082716 |
| test/episodes                  | 1470       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.114     |
| test/info_shaping_reward_mean  | -0.208     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -33.495655 |
| test/Q_plus_P                  | -33.495655 |
| test/reward_per_eps            | -40        |
| test/steps                     | 58800      |
| train/episodes                 | 5880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.111     |
| train/info_shaping_reward_mean | -0.216     |
| train/info_shaping_reward_min  | -0.303     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 235200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 147        |
| stats_o/mean                   | 0.4214072  |
| stats_o/std                    | 0.040828   |
| test/episodes                  | 1480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.122     |
| test/info_shaping_reward_mean  | -0.188     |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -33.574276 |
| test/Q_plus_P                  | -33.574276 |
| test/reward_per_eps            | -40        |
| test/steps                     | 59200      |
| train/episodes                 | 5920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.316     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 236800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 148        |
| stats_o/mean                   | 0.421374   |
| stats_o/std                    | 0.04083808 |
| test/episodes                  | 1490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -33.611893 |
| test/Q_plus_P                  | -33.611893 |
| test/reward_per_eps            | -40        |
| test/steps                     | 59600      |
| train/episodes                 | 5960       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.11      |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 238400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 149         |
| stats_o/mean                   | 0.42135715  |
| stats_o/std                    | 0.040828682 |
| test/episodes                  | 1500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -33.719566  |
| test/Q_plus_P                  | -33.719566  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60000       |
| train/episodes                 | 6000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 240000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 150        |
| stats_o/mean                   | 0.4213247  |
| stats_o/std                    | 0.04082622 |
| test/episodes                  | 1510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -33.720356 |
| test/Q_plus_P                  | -33.720356 |
| test/reward_per_eps            | -40        |
| test/steps                     | 60400      |
| train/episodes                 | 6040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 241600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 151         |
| stats_o/mean                   | 0.42131987  |
| stats_o/std                    | 0.040808503 |
| test/episodes                  | 1520        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -33.869526  |
| test/Q_plus_P                  | -33.869526  |
| test/reward_per_eps            | -40         |
| test/steps                     | 60800       |
| train/episodes                 | 6080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.297      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 243200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 152        |
| stats_o/mean                   | 0.4212737  |
| stats_o/std                    | 0.04081897 |
| test/episodes                  | 1530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -33.961716 |
| test/Q_plus_P                  | -33.961716 |
| test/reward_per_eps            | -40        |
| test/steps                     | 61200      |
| train/episodes                 | 6120       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.128     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 244800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 153         |
| stats_o/mean                   | 0.42124343  |
| stats_o/std                    | 0.040815968 |
| test/episodes                  | 1540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -33.991867  |
| test/Q_plus_P                  | -33.991867  |
| test/reward_per_eps            | -40         |
| test/steps                     | 61600       |
| train/episodes                 | 6160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 246400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 154        |
| stats_o/mean                   | 0.42119506 |
| stats_o/std                    | 0.04080915 |
| test/episodes                  | 1550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.207     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.263     |
| test/Q                         | -34.128933 |
| test/Q_plus_P                  | -34.128933 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62000      |
| train/episodes                 | 6200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 248000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 155         |
| stats_o/mean                   | 0.42116347  |
| stats_o/std                    | 0.040817607 |
| test/episodes                  | 1560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -34.161213  |
| test/Q_plus_P                  | -34.161213  |
| test/reward_per_eps            | -40         |
| test/steps                     | 62400       |
| train/episodes                 | 6240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 249600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 156        |
| stats_o/mean                   | 0.42107844 |
| stats_o/std                    | 0.0408128  |
| test/episodes                  | 1570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.269     |
| test/Q                         | -34.244335 |
| test/Q_plus_P                  | -34.244335 |
| test/reward_per_eps            | -40        |
| test/steps                     | 62800      |
| train/episodes                 | 6280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 251200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 157         |
| stats_o/mean                   | 0.421016    |
| stats_o/std                    | 0.040830307 |
| test/episodes                  | 1580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -34.298832  |
| test/Q_plus_P                  | -34.298832  |
| test/reward_per_eps            | -40         |
| test/steps                     | 63200       |
| train/episodes                 | 6320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 252800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 158         |
| stats_o/mean                   | 0.42098203  |
| stats_o/std                    | 0.040846676 |
| test/episodes                  | 1590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -34.373405  |
| test/Q_plus_P                  | -34.373405  |
| test/reward_per_eps            | -40         |
| test/steps                     | 63600       |
| train/episodes                 | 6360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 254400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 159         |
| stats_o/mean                   | 0.42088377  |
| stats_o/std                    | 0.040878315 |
| test/episodes                  | 1600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.223      |
| test/info_shaping_reward_mean  | -0.28       |
| test/info_shaping_reward_min   | -0.358      |
| test/Q                         | -34.4226    |
| test/Q_plus_P                  | -34.4226    |
| test/reward_per_eps            | -40         |
| test/steps                     | 64000       |
| train/episodes                 | 6400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.183      |
| train/info_shaping_reward_mean | -0.267      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 256000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 160        |
| stats_o/mean                   | 0.4208301  |
| stats_o/std                    | 0.04089984 |
| test/episodes                  | 1610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.2       |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -34.494    |
| test/Q_plus_P                  | -34.494    |
| test/reward_per_eps            | -40        |
| test/steps                     | 64400      |
| train/episodes                 | 6440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.166     |
| train/info_shaping_reward_mean | -0.259     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 257600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 161         |
| stats_o/mean                   | 0.420796    |
| stats_o/std                    | 0.040912125 |
| test/episodes                  | 1620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -34.593662  |
| test/Q_plus_P                  | -34.593662  |
| test/reward_per_eps            | -40         |
| test/steps                     | 64800       |
| train/episodes                 | 6480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 259200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 162         |
| stats_o/mean                   | 0.42071506  |
| stats_o/std                    | 0.040939856 |
| test/episodes                  | 1630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.254      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -34.661594  |
| test/Q_plus_P                  | -34.661594  |
| test/reward_per_eps            | -40         |
| test/steps                     | 65200       |
| train/episodes                 | 6520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 260800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 163        |
| stats_o/mean                   | 0.42063895 |
| stats_o/std                    | 0.04094518 |
| test/episodes                  | 1640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -34.705875 |
| test/Q_plus_P                  | -34.705875 |
| test/reward_per_eps            | -40        |
| test/steps                     | 65600      |
| train/episodes                 | 6560       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 262400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 164         |
| stats_o/mean                   | 0.42054603  |
| stats_o/std                    | 0.040964454 |
| test/episodes                  | 1650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -34.78446   |
| test/Q_plus_P                  | -34.78446   |
| test/reward_per_eps            | -40         |
| test/steps                     | 66000       |
| train/episodes                 | 6600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 264000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 165        |
| stats_o/mean                   | 0.42049262 |
| stats_o/std                    | 0.04098274 |
| test/episodes                  | 1660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.254     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -34.858517 |
| test/Q_plus_P                  | -34.858517 |
| test/reward_per_eps            | -40        |
| test/steps                     | 66400      |
| train/episodes                 | 6640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.175     |
| train/info_shaping_reward_mean | -0.264     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 265600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 166         |
| stats_o/mean                   | 0.42046487  |
| stats_o/std                    | 0.041006863 |
| test/episodes                  | 1670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -34.874252  |
| test/Q_plus_P                  | -34.874252  |
| test/reward_per_eps            | -40         |
| test/steps                     | 66800       |
| train/episodes                 | 6680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.269      |
| train/info_shaping_reward_min  | -0.374      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 267200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 167        |
| stats_o/mean                   | 0.42039797 |
| stats_o/std                    | 0.04103001 |
| test/episodes                  | 1680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -34.94376  |
| test/Q_plus_P                  | -34.94376  |
| test/reward_per_eps            | -40        |
| test/steps                     | 67200      |
| train/episodes                 | 6720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.363     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 268800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 168        |
| stats_o/mean                   | 0.42038146 |
| stats_o/std                    | 0.04106077 |
| test/episodes                  | 1690       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.259     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -35.040447 |
| test/Q_plus_P                  | -35.040447 |
| test/reward_per_eps            | -40        |
| test/steps                     | 67600      |
| train/episodes                 | 6760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 270400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 169         |
| stats_o/mean                   | 0.4203465   |
| stats_o/std                    | 0.041061874 |
| test/episodes                  | 1700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -35.07948   |
| test/Q_plus_P                  | -35.07948   |
| test/reward_per_eps            | -40         |
| test/steps                     | 68000       |
| train/episodes                 | 6800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.179      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 272000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 170        |
| stats_o/mean                   | 0.4202852  |
| stats_o/std                    | 0.04108702 |
| test/episodes                  | 1710       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -35.18696  |
| test/Q_plus_P                  | -35.18696  |
| test/reward_per_eps            | -40        |
| test/steps                     | 68400      |
| train/episodes                 | 6840       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.368     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 273600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 171         |
| stats_o/mean                   | 0.42025104  |
| stats_o/std                    | 0.041106407 |
| test/episodes                  | 1720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.268      |
| test/Q                         | -35.171707  |
| test/Q_plus_P                  | -35.171707  |
| test/reward_per_eps            | -40         |
| test/steps                     | 68800       |
| train/episodes                 | 6880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 275200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 172        |
| stats_o/mean                   | 0.42019758 |
| stats_o/std                    | 0.04113727 |
| test/episodes                  | 1730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.148     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -35.335342 |
| test/Q_plus_P                  | -35.335342 |
| test/reward_per_eps            | -40        |
| test/steps                     | 69200      |
| train/episodes                 | 6920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 276800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 173         |
| stats_o/mean                   | 0.42013595  |
| stats_o/std                    | 0.041145604 |
| test/episodes                  | 1740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -35.292347  |
| test/Q_plus_P                  | -35.292347  |
| test/reward_per_eps            | -40         |
| test/steps                     | 69600       |
| train/episodes                 | 6960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 278400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 174         |
| stats_o/mean                   | 0.4200945   |
| stats_o/std                    | 0.041131902 |
| test/episodes                  | 1750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -35.365177  |
| test/Q_plus_P                  | -35.365177  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70000       |
| train/episodes                 | 7000        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 280000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 175         |
| stats_o/mean                   | 0.42004696  |
| stats_o/std                    | 0.041131634 |
| test/episodes                  | 1760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.206      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -35.43051   |
| test/Q_plus_P                  | -35.43051   |
| test/reward_per_eps            | -40         |
| test/steps                     | 70400       |
| train/episodes                 | 7040        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 281600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 176         |
| stats_o/mean                   | 0.41995963  |
| stats_o/std                    | 0.041149095 |
| test/episodes                  | 1770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.33       |
| test/Q                         | -35.478046  |
| test/Q_plus_P                  | -35.478046  |
| test/reward_per_eps            | -40         |
| test/steps                     | 70800       |
| train/episodes                 | 7080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.372      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 283200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 177         |
| stats_o/mean                   | 0.4199052   |
| stats_o/std                    | 0.041160315 |
| test/episodes                  | 1780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -35.547813  |
| test/Q_plus_P                  | -35.547813  |
| test/reward_per_eps            | -40         |
| test/steps                     | 71200       |
| train/episodes                 | 7120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.375      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 284800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 178         |
| stats_o/mean                   | 0.4198412   |
| stats_o/std                    | 0.041158963 |
| test/episodes                  | 1790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -35.612076  |
| test/Q_plus_P                  | -35.612076  |
| test/reward_per_eps            | -40         |
| test/steps                     | 71600       |
| train/episodes                 | 7160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 286400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 179         |
| stats_o/mean                   | 0.4198135   |
| stats_o/std                    | 0.041165356 |
| test/episodes                  | 1800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -35.645752  |
| test/Q_plus_P                  | -35.645752  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72000       |
| train/episodes                 | 7200        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 288000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 180        |
| stats_o/mean                   | 0.4197816  |
| stats_o/std                    | 0.04116117 |
| test/episodes                  | 1810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.329     |
| test/Q                         | -35.715572 |
| test/Q_plus_P                  | -35.715572 |
| test/reward_per_eps            | -40        |
| test/steps                     | 72400      |
| train/episodes                 | 7240       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 289600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 181         |
| stats_o/mean                   | 0.41974255  |
| stats_o/std                    | 0.041142765 |
| test/episodes                  | 1820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.364      |
| test/Q                         | -35.727264  |
| test/Q_plus_P                  | -35.727264  |
| test/reward_per_eps            | -40         |
| test/steps                     | 72800       |
| train/episodes                 | 7280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 291200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 182        |
| stats_o/mean                   | 0.41970444 |
| stats_o/std                    | 0.04113525 |
| test/episodes                  | 1830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -35.81888  |
| test/Q_plus_P                  | -35.81888  |
| test/reward_per_eps            | -40        |
| test/steps                     | 73200      |
| train/episodes                 | 7320       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 292800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 183        |
| stats_o/mean                   | 0.4196683  |
| stats_o/std                    | 0.04114565 |
| test/episodes                  | 1840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.279     |
| test/info_shaping_reward_min   | -0.35      |
| test/Q                         | -35.849674 |
| test/Q_plus_P                  | -35.849674 |
| test/reward_per_eps            | -40        |
| test/steps                     | 73600      |
| train/episodes                 | 7360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 294400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 184         |
| stats_o/mean                   | 0.41963348  |
| stats_o/std                    | 0.041142877 |
| test/episodes                  | 1850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.196      |
| test/info_shaping_reward_mean  | -0.273      |
| test/info_shaping_reward_min   | -0.336      |
| test/Q                         | -35.924713  |
| test/Q_plus_P                  | -35.924713  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74000       |
| train/episodes                 | 7400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 296000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 185         |
| stats_o/mean                   | 0.41957965  |
| stats_o/std                    | 0.041142855 |
| test/episodes                  | 1860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.224      |
| test/info_shaping_reward_mean  | -0.288      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -35.954117  |
| test/Q_plus_P                  | -35.954117  |
| test/reward_per_eps            | -40         |
| test/steps                     | 74400       |
| train/episodes                 | 7440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 297600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 186        |
| stats_o/mean                   | 0.4195045  |
| stats_o/std                    | 0.04115612 |
| test/episodes                  | 1870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.332     |
| test/Q                         | -36.030403 |
| test/Q_plus_P                  | -36.030403 |
| test/reward_per_eps            | -40        |
| test/steps                     | 74800      |
| train/episodes                 | 7480       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.185     |
| train/info_shaping_reward_mean | -0.27      |
| train/info_shaping_reward_min  | -0.385     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 299200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 187         |
| stats_o/mean                   | 0.41944087  |
| stats_o/std                    | 0.041161057 |
| test/episodes                  | 1880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.343      |
| test/Q                         | -36.08784   |
| test/Q_plus_P                  | -36.08784   |
| test/reward_per_eps            | -40         |
| test/steps                     | 75200       |
| train/episodes                 | 7520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.267      |
| train/info_shaping_reward_min  | -0.389      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 300800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 188         |
| stats_o/mean                   | 0.41938928  |
| stats_o/std                    | 0.041183677 |
| test/episodes                  | 1890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.25       |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -36.117897  |
| test/Q_plus_P                  | -36.117897  |
| test/reward_per_eps            | -40         |
| test/steps                     | 75600       |
| train/episodes                 | 7560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.38       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 302400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 189         |
| stats_o/mean                   | 0.41934395  |
| stats_o/std                    | 0.041198898 |
| test/episodes                  | 1900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.272      |
| test/info_shaping_reward_min   | -0.39       |
| test/Q                         | -36.194656  |
| test/Q_plus_P                  | -36.194656  |
| test/reward_per_eps            | -40         |
| test/steps                     | 76000       |
| train/episodes                 | 7600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.176      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 304000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 190        |
| stats_o/mean                   | 0.41928723 |
| stats_o/std                    | 0.04119515 |
| test/episodes                  | 1910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -36.179142 |
| test/Q_plus_P                  | -36.179142 |
| test/reward_per_eps            | -40        |
| test/steps                     | 76400      |
| train/episodes                 | 7640       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 305600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 191         |
| stats_o/mean                   | 0.41923812  |
| stats_o/std                    | 0.041192684 |
| test/episodes                  | 1920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -36.27298   |
| test/Q_plus_P                  | -36.27298   |
| test/reward_per_eps            | -40         |
| test/steps                     | 76800       |
| train/episodes                 | 7680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 307200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 192         |
| stats_o/mean                   | 0.4191729   |
| stats_o/std                    | 0.041207995 |
| test/episodes                  | 1930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -36.32706   |
| test/Q_plus_P                  | -36.32706   |
| test/reward_per_eps            | -40         |
| test/steps                     | 77200       |
| train/episodes                 | 7720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 308800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 193         |
| stats_o/mean                   | 0.41908503  |
| stats_o/std                    | 0.041218195 |
| test/episodes                  | 1940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -36.324577  |
| test/Q_plus_P                  | -36.324577  |
| test/reward_per_eps            | -40         |
| test/steps                     | 77600       |
| train/episodes                 | 7760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 310400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 194        |
| stats_o/mean                   | 0.4190295  |
| stats_o/std                    | 0.04122939 |
| test/episodes                  | 1950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.144     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -36.42616  |
| test/Q_plus_P                  | -36.42616  |
| test/reward_per_eps            | -40        |
| test/steps                     | 78000      |
| train/episodes                 | 7800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 312000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 195         |
| stats_o/mean                   | 0.4189802   |
| stats_o/std                    | 0.041242853 |
| test/episodes                  | 1960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -36.477623  |
| test/Q_plus_P                  | -36.477623  |
| test/reward_per_eps            | -40         |
| test/steps                     | 78400       |
| train/episodes                 | 7840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 313600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 196        |
| stats_o/mean                   | 0.41894153 |
| stats_o/std                    | 0.04123417 |
| test/episodes                  | 1970       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -36.494076 |
| test/Q_plus_P                  | -36.494076 |
| test/reward_per_eps            | -40        |
| test/steps                     | 78800      |
| train/episodes                 | 7880       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 315200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 197         |
| stats_o/mean                   | 0.4189374   |
| stats_o/std                    | 0.041248895 |
| test/episodes                  | 1980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -36.578503  |
| test/Q_plus_P                  | -36.578503  |
| test/reward_per_eps            | -40         |
| test/steps                     | 79200       |
| train/episodes                 | 7920        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 316800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 198         |
| stats_o/mean                   | 0.41888133  |
| stats_o/std                    | 0.041255537 |
| test/episodes                  | 1990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -36.59003   |
| test/Q_plus_P                  | -36.59003   |
| test/reward_per_eps            | -40         |
| test/steps                     | 79600       |
| train/episodes                 | 7960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 318400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 199        |
| stats_o/mean                   | 0.41884556 |
| stats_o/std                    | 0.0412566  |
| test/episodes                  | 2000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -36.644375 |
| test/Q_plus_P                  | -36.644375 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80000      |
| train/episodes                 | 8000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 320000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 200        |
| stats_o/mean                   | 0.418766   |
| stats_o/std                    | 0.04127211 |
| test/episodes                  | 2010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -36.696533 |
| test/Q_plus_P                  | -36.696533 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80400      |
| train/episodes                 | 8040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.354     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 321600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 201        |
| stats_o/mean                   | 0.41871408 |
| stats_o/std                    | 0.04128941 |
| test/episodes                  | 2020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -36.719147 |
| test/Q_plus_P                  | -36.719147 |
| test/reward_per_eps            | -40        |
| test/steps                     | 80800      |
| train/episodes                 | 8080       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 323200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 202         |
| stats_o/mean                   | 0.41870108  |
| stats_o/std                    | 0.041297313 |
| test/episodes                  | 2030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -36.766083  |
| test/Q_plus_P                  | -36.766083  |
| test/reward_per_eps            | -40         |
| test/steps                     | 81200       |
| train/episodes                 | 8120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 324800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 203         |
| stats_o/mean                   | 0.41865358  |
| stats_o/std                    | 0.041308936 |
| test/episodes                  | 2040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.261      |
| test/info_shaping_reward_min   | -0.347      |
| test/Q                         | -36.83483   |
| test/Q_plus_P                  | -36.83483   |
| test/reward_per_eps            | -40         |
| test/steps                     | 81600       |
| train/episodes                 | 8160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 326400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 204        |
| stats_o/mean                   | 0.41861215 |
| stats_o/std                    | 0.04131991 |
| test/episodes                  | 2050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -36.818256 |
| test/Q_plus_P                  | -36.818256 |
| test/reward_per_eps            | -40        |
| test/steps                     | 82000      |
| train/episodes                 | 8200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 328000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 205         |
| stats_o/mean                   | 0.41857576  |
| stats_o/std                    | 0.041324735 |
| test/episodes                  | 2060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -36.87535   |
| test/Q_plus_P                  | -36.87535   |
| test/reward_per_eps            | -40         |
| test/steps                     | 82400       |
| train/episodes                 | 8240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 329600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 206         |
| stats_o/mean                   | 0.41854385  |
| stats_o/std                    | 0.041312892 |
| test/episodes                  | 2070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -36.845325  |
| test/Q_plus_P                  | -36.845325  |
| test/reward_per_eps            | -40         |
| test/steps                     | 82800       |
| train/episodes                 | 8280        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 331200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 207         |
| stats_o/mean                   | 0.4184874   |
| stats_o/std                    | 0.041306514 |
| test/episodes                  | 2080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -36.948368  |
| test/Q_plus_P                  | -36.948368  |
| test/reward_per_eps            | -40         |
| test/steps                     | 83200       |
| train/episodes                 | 8320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 332800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 208         |
| stats_o/mean                   | 0.41846368  |
| stats_o/std                    | 0.041306026 |
| test/episodes                  | 2090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -36.971703  |
| test/Q_plus_P                  | -36.971703  |
| test/reward_per_eps            | -40         |
| test/steps                     | 83600       |
| train/episodes                 | 8360        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 334400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 209         |
| stats_o/mean                   | 0.4184362   |
| stats_o/std                    | 0.041302297 |
| test/episodes                  | 2100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -37.00557   |
| test/Q_plus_P                  | -37.00557   |
| test/reward_per_eps            | -40         |
| test/steps                     | 84000       |
| train/episodes                 | 8400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 336000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 210        |
| stats_o/mean                   | 0.4183792  |
| stats_o/std                    | 0.04130933 |
| test/episodes                  | 2110       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -37.01864  |
| test/Q_plus_P                  | -37.01864  |
| test/reward_per_eps            | -40        |
| test/steps                     | 84400      |
| train/episodes                 | 8440       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.178     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.361     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 337600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 211         |
| stats_o/mean                   | 0.4183435   |
| stats_o/std                    | 0.041315872 |
| test/episodes                  | 2120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -37.07207   |
| test/Q_plus_P                  | -37.07207   |
| test/reward_per_eps            | -40         |
| test/steps                     | 84800       |
| train/episodes                 | 8480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 339200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 212         |
| stats_o/mean                   | 0.41830197  |
| stats_o/std                    | 0.041332252 |
| test/episodes                  | 2130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -37.119507  |
| test/Q_plus_P                  | -37.119507  |
| test/reward_per_eps            | -40         |
| test/steps                     | 85200       |
| train/episodes                 | 8520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 340800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 213         |
| stats_o/mean                   | 0.41827694  |
| stats_o/std                    | 0.041342117 |
| test/episodes                  | 2140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -37.174473  |
| test/Q_plus_P                  | -37.174473  |
| test/reward_per_eps            | -40         |
| test/steps                     | 85600       |
| train/episodes                 | 8560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 342400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 214         |
| stats_o/mean                   | 0.41825923  |
| stats_o/std                    | 0.041327897 |
| test/episodes                  | 2150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -37.187374  |
| test/Q_plus_P                  | -37.187374  |
| test/reward_per_eps            | -40         |
| test/steps                     | 86000       |
| train/episodes                 | 8600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 344000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 215         |
| stats_o/mean                   | 0.41822007  |
| stats_o/std                    | 0.041327957 |
| test/episodes                  | 2160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.331      |
| test/Q                         | -37.239513  |
| test/Q_plus_P                  | -37.239513  |
| test/reward_per_eps            | -40         |
| test/steps                     | 86400       |
| train/episodes                 | 8640        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 345600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 216        |
| stats_o/mean                   | 0.4181945  |
| stats_o/std                    | 0.04134916 |
| test/episodes                  | 2170       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -37.27192  |
| test/Q_plus_P                  | -37.27192  |
| test/reward_per_eps            | -40        |
| test/steps                     | 86800      |
| train/episodes                 | 8680       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.177     |
| train/info_shaping_reward_mean | -0.268     |
| train/info_shaping_reward_min  | -0.374     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 347200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 217         |
| stats_o/mean                   | 0.41816065  |
| stats_o/std                    | 0.041348603 |
| test/episodes                  | 2180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -37.302902  |
| test/Q_plus_P                  | -37.302902  |
| test/reward_per_eps            | -40         |
| test/steps                     | 87200       |
| train/episodes                 | 8720        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.17       |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 348800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 218         |
| stats_o/mean                   | 0.41811755  |
| stats_o/std                    | 0.041354455 |
| test/episodes                  | 2190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -37.316338  |
| test/Q_plus_P                  | -37.316338  |
| test/reward_per_eps            | -40         |
| test/steps                     | 87600       |
| train/episodes                 | 8760        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 350400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 219         |
| stats_o/mean                   | 0.41806936  |
| stats_o/std                    | 0.041337155 |
| test/episodes                  | 2200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -37.321625  |
| test/Q_plus_P                  | -37.321625  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88000       |
| train/episodes                 | 8800        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 352000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 220         |
| stats_o/mean                   | 0.4180398   |
| stats_o/std                    | 0.041348662 |
| test/episodes                  | 2210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -37.420277  |
| test/Q_plus_P                  | -37.420277  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88400       |
| train/episodes                 | 8840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 353600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 221         |
| stats_o/mean                   | 0.41800466  |
| stats_o/std                    | 0.041342597 |
| test/episodes                  | 2220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -37.419468  |
| test/Q_plus_P                  | -37.419468  |
| test/reward_per_eps            | -40         |
| test/steps                     | 88800       |
| train/episodes                 | 8880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 355200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 222        |
| stats_o/mean                   | 0.4179692  |
| stats_o/std                    | 0.04132321 |
| test/episodes                  | 2230       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -37.476418 |
| test/Q_plus_P                  | -37.476418 |
| test/reward_per_eps            | -40        |
| test/steps                     | 89200      |
| train/episodes                 | 8920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 356800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 223         |
| stats_o/mean                   | 0.41792572  |
| stats_o/std                    | 0.041324228 |
| test/episodes                  | 2240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -37.478775  |
| test/Q_plus_P                  | -37.478775  |
| test/reward_per_eps            | -40         |
| test/steps                     | 89600       |
| train/episodes                 | 8960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 358400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 224        |
| stats_o/mean                   | 0.4178824  |
| stats_o/std                    | 0.04132522 |
| test/episodes                  | 2250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.182     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.328     |
| test/Q                         | -37.536373 |
| test/Q_plus_P                  | -37.536373 |
| test/reward_per_eps            | -40        |
| test/steps                     | 90000      |
| train/episodes                 | 9000       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 360000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 225        |
| stats_o/mean                   | 0.41783777 |
| stats_o/std                    | 0.04131464 |
| test/episodes                  | 2260       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -37.552155 |
| test/Q_plus_P                  | -37.552155 |
| test/reward_per_eps            | -40        |
| test/steps                     | 90400      |
| train/episodes                 | 9040       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 361600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 226         |
| stats_o/mean                   | 0.4178026   |
| stats_o/std                    | 0.041333523 |
| test/episodes                  | 2270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -37.58698   |
| test/Q_plus_P                  | -37.58698   |
| test/reward_per_eps            | -40         |
| test/steps                     | 90800       |
| train/episodes                 | 9080        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.375      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 363200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 227         |
| stats_o/mean                   | 0.41773143  |
| stats_o/std                    | 0.041340016 |
| test/episodes                  | 2280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.258      |
| test/info_shaping_reward_min   | -0.379      |
| test/Q                         | -37.588577  |
| test/Q_plus_P                  | -37.588577  |
| test/reward_per_eps            | -40         |
| test/steps                     | 91200       |
| train/episodes                 | 9120        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 364800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 228         |
| stats_o/mean                   | 0.41771892  |
| stats_o/std                    | 0.041348834 |
| test/episodes                  | 2290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -37.644073  |
| test/Q_plus_P                  | -37.644073  |
| test/reward_per_eps            | -40         |
| test/steps                     | 91600       |
| train/episodes                 | 9160        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 366400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 229        |
| stats_o/mean                   | 0.4176717  |
| stats_o/std                    | 0.0413507  |
| test/episodes                  | 2300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.257     |
| test/info_shaping_reward_min   | -0.322     |
| test/Q                         | -37.608356 |
| test/Q_plus_P                  | -37.608356 |
| test/reward_per_eps            | -40        |
| test/steps                     | 92000      |
| train/episodes                 | 9200       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.356     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 368000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 230         |
| stats_o/mean                   | 0.41763568  |
| stats_o/std                    | 0.041350733 |
| test/episodes                  | 2310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -37.691074  |
| test/Q_plus_P                  | -37.691074  |
| test/reward_per_eps            | -40         |
| test/steps                     | 92400       |
| train/episodes                 | 9240        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.174      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 369600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 231        |
| stats_o/mean                   | 0.41761792 |
| stats_o/std                    | 0.04135269 |
| test/episodes                  | 2320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.235     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -37.73336  |
| test/Q_plus_P                  | -37.73336  |
| test/reward_per_eps            | -40        |
| test/steps                     | 92800      |
| train/episodes                 | 9280       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 371200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 232         |
| stats_o/mean                   | 0.4175755   |
| stats_o/std                    | 0.041355472 |
| test/episodes                  | 2330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -37.768078  |
| test/Q_plus_P                  | -37.768078  |
| test/reward_per_eps            | -40         |
| test/steps                     | 93200       |
| train/episodes                 | 9320        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 372800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 233        |
| stats_o/mean                   | 0.4175324  |
| stats_o/std                    | 0.04135738 |
| test/episodes                  | 2340       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.152     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.335     |
| test/Q                         | -37.782494 |
| test/Q_plus_P                  | -37.782494 |
| test/reward_per_eps            | -40        |
| test/steps                     | 93600      |
| train/episodes                 | 9360       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.244     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 374400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 234         |
| stats_o/mean                   | 0.41749594  |
| stats_o/std                    | 0.041365612 |
| test/episodes                  | 2350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -37.796734  |
| test/Q_plus_P                  | -37.796734  |
| test/reward_per_eps            | -40         |
| test/steps                     | 94000       |
| train/episodes                 | 9400        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 376000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 235         |
| stats_o/mean                   | 0.41747403  |
| stats_o/std                    | 0.041367516 |
| test/episodes                  | 2360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -37.850132  |
| test/Q_plus_P                  | -37.850132  |
| test/reward_per_eps            | -40         |
| test/steps                     | 94400       |
| train/episodes                 | 9440        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 377600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 236         |
| stats_o/mean                   | 0.41741928  |
| stats_o/std                    | 0.041374784 |
| test/episodes                  | 2370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.315      |
| test/Q                         | -37.85046   |
| test/Q_plus_P                  | -37.85046   |
| test/reward_per_eps            | -40         |
| test/steps                     | 94800       |
| train/episodes                 | 9480        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 379200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 237         |
| stats_o/mean                   | 0.41738412  |
| stats_o/std                    | 0.041366924 |
| test/episodes                  | 2380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -37.94134   |
| test/Q_plus_P                  | -37.94134   |
| test/reward_per_eps            | -40         |
| test/steps                     | 95200       |
| train/episodes                 | 9520        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 380800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 238         |
| stats_o/mean                   | 0.41736123  |
| stats_o/std                    | 0.041362755 |
| test/episodes                  | 2390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.195      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -37.918964  |
| test/Q_plus_P                  | -37.918964  |
| test/reward_per_eps            | -40         |
| test/steps                     | 95600       |
| train/episodes                 | 9560        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 382400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 239         |
| stats_o/mean                   | 0.4173226   |
| stats_o/std                    | 0.041365642 |
| test/episodes                  | 2400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -37.957504  |
| test/Q_plus_P                  | -37.957504  |
| test/reward_per_eps            | -40         |
| test/steps                     | 96000       |
| train/episodes                 | 9600        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.167      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 384000      |
------------------------------------------------
Saving latest policy.
----------------------------------------------
| epoch                          | 240       |
| stats_o/mean                   | 0.4172733 |
| stats_o/std                    | 0.0413807 |
| test/episodes                  | 2410      |
| test/info_is_success_max       | 0         |
| test/info_is_success_mean      | 0         |
| test/info_is_success_min       | 0         |
| test/info_shaping_reward_max   | -0.136    |
| test/info_shaping_reward_mean  | -0.204    |
| test/info_shaping_reward_min   | -0.255    |
| test/Q                         | -37.96226 |
| test/Q_plus_P                  | -37.96226 |
| test/reward_per_eps            | -40       |
| test/steps                     | 96400     |
| train/episodes                 | 9640      |
| train/info_is_success_max      | 0         |
| train/info_is_success_mean     | 0         |
| train/info_is_success_min      | 0         |
| train/info_shaping_reward_max  | -0.148    |
| train/info_shaping_reward_mean | -0.25     |
| train/info_shaping_reward_min  | -0.374    |
| train/Q                        | nan       |
| train/Q_plus_P                 | nan       |
| train/reward_per_eps           | -40       |
| train/steps                    | 385600    |
----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 241         |
| stats_o/mean                   | 0.41725388  |
| stats_o/std                    | 0.041384242 |
| test/episodes                  | 2420        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -38.01393   |
| test/Q_plus_P                  | -38.01393   |
| test/reward_per_eps            | -40         |
| test/steps                     | 96800       |
| train/episodes                 | 9680        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 387200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 242        |
| stats_o/mean                   | 0.41720518 |
| stats_o/std                    | 0.04138431 |
| test/episodes                  | 2430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.145     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -38.039635 |
| test/Q_plus_P                  | -38.039635 |
| test/reward_per_eps            | -40        |
| test/steps                     | 97200      |
| train/episodes                 | 9720       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 388800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 243        |
| stats_o/mean                   | 0.41716966 |
| stats_o/std                    | 0.04137713 |
| test/episodes                  | 2440       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.352     |
| test/Q                         | -38.092484 |
| test/Q_plus_P                  | -38.092484 |
| test/reward_per_eps            | -40        |
| test/steps                     | 97600      |
| train/episodes                 | 9760       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 390400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 244        |
| stats_o/mean                   | 0.41711667 |
| stats_o/std                    | 0.04137344 |
| test/episodes                  | 2450       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.218     |
| test/info_shaping_reward_mean  | -0.275     |
| test/info_shaping_reward_min   | -0.348     |
| test/Q                         | -38.180653 |
| test/Q_plus_P                  | -38.180653 |
| test/reward_per_eps            | -40        |
| test/steps                     | 98000      |
| train/episodes                 | 9800       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 392000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 245         |
| stats_o/mean                   | 0.41709104  |
| stats_o/std                    | 0.041382436 |
| test/episodes                  | 2460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.257      |
| test/info_shaping_reward_min   | -0.335      |
| test/Q                         | -38.141582  |
| test/Q_plus_P                  | -38.141582  |
| test/reward_per_eps            | -40         |
| test/steps                     | 98400       |
| train/episodes                 | 9840        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 393600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 246         |
| stats_o/mean                   | 0.41703996  |
| stats_o/std                    | 0.041394036 |
| test/episodes                  | 2470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.341      |
| test/Q                         | -38.107037  |
| test/Q_plus_P                  | -38.107037  |
| test/reward_per_eps            | -40         |
| test/steps                     | 98800       |
| train/episodes                 | 9880        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.18       |
| train/info_shaping_reward_mean | -0.266      |
| train/info_shaping_reward_min  | -0.373      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 395200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 247        |
| stats_o/mean                   | 0.41699365 |
| stats_o/std                    | 0.04139299 |
| test/episodes                  | 2480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.217     |
| test/info_shaping_reward_mean  | -0.277     |
| test/info_shaping_reward_min   | -0.339     |
| test/Q                         | -38.22865  |
| test/Q_plus_P                  | -38.22865  |
| test/reward_per_eps            | -40        |
| test/steps                     | 99200      |
| train/episodes                 | 9920       |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.257     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 396800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 248         |
| stats_o/mean                   | 0.41697225  |
| stats_o/std                    | 0.041398194 |
| test/episodes                  | 2490        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.34       |
| test/Q                         | -38.18195   |
| test/Q_plus_P                  | -38.18195   |
| test/reward_per_eps            | -40         |
| test/steps                     | 99600       |
| train/episodes                 | 9960        |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 398400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 249        |
| stats_o/mean                   | 0.41691542 |
| stats_o/std                    | 0.04140764 |
| test/episodes                  | 2500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.207     |
| test/info_shaping_reward_mean  | -0.283     |
| test/info_shaping_reward_min   | -0.376     |
| test/Q                         | -38.20099  |
| test/Q_plus_P                  | -38.20099  |
| test/reward_per_eps            | -40        |
| test/steps                     | 100000     |
| train/episodes                 | 10000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.379     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 400000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 250         |
| stats_o/mean                   | 0.4168789   |
| stats_o/std                    | 0.041418888 |
| test/episodes                  | 2510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.26       |
| test/info_shaping_reward_min   | -0.329      |
| test/Q                         | -38.244083  |
| test/Q_plus_P                  | -38.244083  |
| test/reward_per_eps            | -40         |
| test/steps                     | 100400      |
| train/episodes                 | 10040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 401600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 251        |
| stats_o/mean                   | 0.41687143 |
| stats_o/std                    | 0.04142589 |
| test/episodes                  | 2520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.175     |
| test/info_shaping_reward_mean  | -0.26      |
| test/info_shaping_reward_min   | -0.358     |
| test/Q                         | -38.24849  |
| test/Q_plus_P                  | -38.24849  |
| test/reward_per_eps            | -40        |
| test/steps                     | 100800     |
| train/episodes                 | 10080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 403200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 252         |
| stats_o/mean                   | 0.41682807  |
| stats_o/std                    | 0.041428726 |
| test/episodes                  | 2530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -38.360752  |
| test/Q_plus_P                  | -38.360752  |
| test/reward_per_eps            | -40         |
| test/steps                     | 101200      |
| train/episodes                 | 10120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.362      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 404800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 253         |
| stats_o/mean                   | 0.4168185   |
| stats_o/std                    | 0.041432105 |
| test/episodes                  | 2540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -38.336697  |
| test/Q_plus_P                  | -38.336697  |
| test/reward_per_eps            | -40         |
| test/steps                     | 101600      |
| train/episodes                 | 10160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 406400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 254         |
| stats_o/mean                   | 0.41679314  |
| stats_o/std                    | 0.041430715 |
| test/episodes                  | 2550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -38.3525    |
| test/Q_plus_P                  | -38.3525    |
| test/reward_per_eps            | -40         |
| test/steps                     | 102000      |
| train/episodes                 | 10200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 408000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 255         |
| stats_o/mean                   | 0.41677535  |
| stats_o/std                    | 0.041426364 |
| test/episodes                  | 2560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.188      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -38.37165   |
| test/Q_plus_P                  | -38.37165   |
| test/reward_per_eps            | -40         |
| test/steps                     | 102400      |
| train/episodes                 | 10240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 409600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 256         |
| stats_o/mean                   | 0.4167465   |
| stats_o/std                    | 0.041427623 |
| test/episodes                  | 2570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -38.38885   |
| test/Q_plus_P                  | -38.38885   |
| test/reward_per_eps            | -40         |
| test/steps                     | 102800      |
| train/episodes                 | 10280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 411200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 257         |
| stats_o/mean                   | 0.41671047  |
| stats_o/std                    | 0.041426327 |
| test/episodes                  | 2580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.189      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.319      |
| test/Q                         | -38.372784  |
| test/Q_plus_P                  | -38.372784  |
| test/reward_per_eps            | -40         |
| test/steps                     | 103200      |
| train/episodes                 | 10320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.256      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 412800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 258        |
| stats_o/mean                   | 0.41666904 |
| stats_o/std                    | 0.04143337 |
| test/episodes                  | 2590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -38.433983 |
| test/Q_plus_P                  | -38.433983 |
| test/reward_per_eps            | -40        |
| test/steps                     | 103600     |
| train/episodes                 | 10360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.167     |
| train/info_shaping_reward_mean | -0.25      |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 414400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 259         |
| stats_o/mean                   | 0.41662785  |
| stats_o/std                    | 0.041440085 |
| test/episodes                  | 2600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -38.419315  |
| test/Q_plus_P                  | -38.419315  |
| test/reward_per_eps            | -40         |
| test/steps                     | 104000      |
| train/episodes                 | 10400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 416000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 260         |
| stats_o/mean                   | 0.41661873  |
| stats_o/std                    | 0.041449722 |
| test/episodes                  | 2610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.327      |
| test/Q                         | -38.40397   |
| test/Q_plus_P                  | -38.40397   |
| test/reward_per_eps            | -40         |
| test/steps                     | 104400      |
| train/episodes                 | 10440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.357      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 417600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 261        |
| stats_o/mean                   | 0.41661918 |
| stats_o/std                    | 0.04144255 |
| test/episodes                  | 2620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.173     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -38.520073 |
| test/Q_plus_P                  | -38.520073 |
| test/reward_per_eps            | -40        |
| test/steps                     | 104800     |
| train/episodes                 | 10480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 419200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 262         |
| stats_o/mean                   | 0.41661045  |
| stats_o/std                    | 0.041443225 |
| test/episodes                  | 2630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -38.473404  |
| test/Q_plus_P                  | -38.473404  |
| test/reward_per_eps            | -40         |
| test/steps                     | 105200      |
| train/episodes                 | 10520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 420800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 263         |
| stats_o/mean                   | 0.4165856   |
| stats_o/std                    | 0.041442066 |
| test/episodes                  | 2640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.202      |
| test/info_shaping_reward_mean  | -0.266      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -38.50533   |
| test/Q_plus_P                  | -38.50533   |
| test/reward_per_eps            | -40         |
| test/steps                     | 105600      |
| train/episodes                 | 10560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 422400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 264         |
| stats_o/mean                   | 0.4165534   |
| stats_o/std                    | 0.041439243 |
| test/episodes                  | 2650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -38.522472  |
| test/Q_plus_P                  | -38.522472  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106000      |
| train/episodes                 | 10600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 424000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 265        |
| stats_o/mean                   | 0.4165283  |
| stats_o/std                    | 0.04142924 |
| test/episodes                  | 2660       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -38.543053 |
| test/Q_plus_P                  | -38.543053 |
| test/reward_per_eps            | -40        |
| test/steps                     | 106400     |
| train/episodes                 | 10640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 425600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 266         |
| stats_o/mean                   | 0.41650257  |
| stats_o/std                    | 0.041421697 |
| test/episodes                  | 2670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -38.567017  |
| test/Q_plus_P                  | -38.567017  |
| test/reward_per_eps            | -40         |
| test/steps                     | 106800      |
| train/episodes                 | 10680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 427200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 267        |
| stats_o/mean                   | 0.41648528 |
| stats_o/std                    | 0.04141654 |
| test/episodes                  | 2680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.264     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -38.589622 |
| test/Q_plus_P                  | -38.589622 |
| test/reward_per_eps            | -40        |
| test/steps                     | 107200     |
| train/episodes                 | 10720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 428800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 268         |
| stats_o/mean                   | 0.41648063  |
| stats_o/std                    | 0.041408718 |
| test/episodes                  | 2690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -38.599937  |
| test/Q_plus_P                  | -38.599937  |
| test/reward_per_eps            | -40         |
| test/steps                     | 107600      |
| train/episodes                 | 10760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 430400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 269         |
| stats_o/mean                   | 0.41644     |
| stats_o/std                    | 0.041410312 |
| test/episodes                  | 2700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.326      |
| test/Q                         | -38.616505  |
| test/Q_plus_P                  | -38.616505  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108000      |
| train/episodes                 | 10800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 432000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 270         |
| stats_o/mean                   | 0.41642448  |
| stats_o/std                    | 0.041407946 |
| test/episodes                  | 2710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -38.630054  |
| test/Q_plus_P                  | -38.630054  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108400      |
| train/episodes                 | 10840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 433600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 271         |
| stats_o/mean                   | 0.4164013   |
| stats_o/std                    | 0.041402668 |
| test/episodes                  | 2720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -38.614586  |
| test/Q_plus_P                  | -38.614586  |
| test/reward_per_eps            | -40         |
| test/steps                     | 108800      |
| train/episodes                 | 10880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 435200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 272         |
| stats_o/mean                   | 0.41639706  |
| stats_o/std                    | 0.041392475 |
| test/episodes                  | 2730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -38.659878  |
| test/Q_plus_P                  | -38.659878  |
| test/reward_per_eps            | -40         |
| test/steps                     | 109200      |
| train/episodes                 | 10920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 436800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 273         |
| stats_o/mean                   | 0.41636106  |
| stats_o/std                    | 0.041397724 |
| test/episodes                  | 2740        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -38.71665   |
| test/Q_plus_P                  | -38.71665   |
| test/reward_per_eps            | -40         |
| test/steps                     | 109600      |
| train/episodes                 | 10960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 438400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 274         |
| stats_o/mean                   | 0.4163264   |
| stats_o/std                    | 0.041399505 |
| test/episodes                  | 2750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -38.68025   |
| test/Q_plus_P                  | -38.68025   |
| test/reward_per_eps            | -40         |
| test/steps                     | 110000      |
| train/episodes                 | 11000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.346      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 440000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 275        |
| stats_o/mean                   | 0.41630647 |
| stats_o/std                    | 0.0413952  |
| test/episodes                  | 2760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -38.701664 |
| test/Q_plus_P                  | -38.701664 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110400     |
| train/episodes                 | 11040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 441600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 276        |
| stats_o/mean                   | 0.41625252 |
| stats_o/std                    | 0.04140154 |
| test/episodes                  | 2770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.195     |
| test/info_shaping_reward_mean  | -0.245     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -38.701122 |
| test/Q_plus_P                  | -38.701122 |
| test/reward_per_eps            | -40        |
| test/steps                     | 110800     |
| train/episodes                 | 11080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.355     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 443200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 277        |
| stats_o/mean                   | 0.41621208 |
| stats_o/std                    | 0.0413998  |
| test/episodes                  | 2780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.179     |
| test/info_shaping_reward_mean  | -0.253     |
| test/info_shaping_reward_min   | -0.331     |
| test/Q                         | -38.71096  |
| test/Q_plus_P                  | -38.71096  |
| test/reward_per_eps            | -40        |
| test/steps                     | 111200     |
| train/episodes                 | 11120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.357     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 444800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 278         |
| stats_o/mean                   | 0.41617453  |
| stats_o/std                    | 0.041406807 |
| test/episodes                  | 2790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -38.749306  |
| test/Q_plus_P                  | -38.749306  |
| test/reward_per_eps            | -40         |
| test/steps                     | 111600      |
| train/episodes                 | 11160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 446400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 279         |
| stats_o/mean                   | 0.41615835  |
| stats_o/std                    | 0.041393477 |
| test/episodes                  | 2800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -38.71978   |
| test/Q_plus_P                  | -38.71978   |
| test/reward_per_eps            | -40         |
| test/steps                     | 112000      |
| train/episodes                 | 11200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 448000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 280         |
| stats_o/mean                   | 0.4161289   |
| stats_o/std                    | 0.041396726 |
| test/episodes                  | 2810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.223      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -38.78632   |
| test/Q_plus_P                  | -38.78632   |
| test/reward_per_eps            | -40         |
| test/steps                     | 112400      |
| train/episodes                 | 11240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.258      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 449600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 281         |
| stats_o/mean                   | 0.4160986   |
| stats_o/std                    | 0.041396473 |
| test/episodes                  | 2820        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.201      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -38.80113   |
| test/Q_plus_P                  | -38.80113   |
| test/reward_per_eps            | -40         |
| test/steps                     | 112800      |
| train/episodes                 | 11280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 451200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 282        |
| stats_o/mean                   | 0.41608465 |
| stats_o/std                    | 0.04139738 |
| test/episodes                  | 2830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -38.815323 |
| test/Q_plus_P                  | -38.815323 |
| test/reward_per_eps            | -40        |
| test/steps                     | 113200     |
| train/episodes                 | 11320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 452800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 283        |
| stats_o/mean                   | 0.41606653 |
| stats_o/std                    | 0.04139649 |
| test/episodes                  | 2840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -38.83222  |
| test/Q_plus_P                  | -38.83222  |
| test/reward_per_eps            | -40        |
| test/steps                     | 113600     |
| train/episodes                 | 11360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 454400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 284         |
| stats_o/mean                   | 0.41603723  |
| stats_o/std                    | 0.041393302 |
| test/episodes                  | 2850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.237      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -38.821266  |
| test/Q_plus_P                  | -38.821266  |
| test/reward_per_eps            | -40         |
| test/steps                     | 114000      |
| train/episodes                 | 11400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 456000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 285         |
| stats_o/mean                   | 0.4159973   |
| stats_o/std                    | 0.041407015 |
| test/episodes                  | 2860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.344      |
| test/Q                         | -38.86046   |
| test/Q_plus_P                  | -38.86046   |
| test/reward_per_eps            | -40         |
| test/steps                     | 114400      |
| train/episodes                 | 11440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.271      |
| train/info_shaping_reward_min  | -0.403      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 457600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 286        |
| stats_o/mean                   | 0.41595984 |
| stats_o/std                    | 0.04140353 |
| test/episodes                  | 2870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.191     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.323     |
| test/Q                         | -38.87579  |
| test/Q_plus_P                  | -38.87579  |
| test/reward_per_eps            | -40        |
| test/steps                     | 114800     |
| train/episodes                 | 11480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 459200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 287        |
| stats_o/mean                   | 0.41593206 |
| stats_o/std                    | 0.04141193 |
| test/episodes                  | 2880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -38.92691  |
| test/Q_plus_P                  | -38.92691  |
| test/reward_per_eps            | -40        |
| test/steps                     | 115200     |
| train/episodes                 | 11520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.163     |
| train/info_shaping_reward_mean | -0.255     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 460800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 288         |
| stats_o/mean                   | 0.4158757   |
| stats_o/std                    | 0.041429598 |
| test/episodes                  | 2890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.314      |
| test/Q                         | -38.88559   |
| test/Q_plus_P                  | -38.88559   |
| test/reward_per_eps            | -40         |
| test/steps                     | 115600      |
| train/episodes                 | 11560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.166      |
| train/info_shaping_reward_mean | -0.27       |
| train/info_shaping_reward_min  | -0.384      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 462400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 289         |
| stats_o/mean                   | 0.4158261   |
| stats_o/std                    | 0.041444495 |
| test/episodes                  | 2900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -38.928024  |
| test/Q_plus_P                  | -38.928024  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116000      |
| train/episodes                 | 11600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.172      |
| train/info_shaping_reward_mean | -0.261      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 464000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 290         |
| stats_o/mean                   | 0.41580942  |
| stats_o/std                    | 0.041451976 |
| test/episodes                  | 2910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.269      |
| test/Q                         | -38.894638  |
| test/Q_plus_P                  | -38.894638  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116400      |
| train/episodes                 | 11640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 465600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 291         |
| stats_o/mean                   | 0.4157871   |
| stats_o/std                    | 0.041451987 |
| test/episodes                  | 2920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -38.932365  |
| test/Q_plus_P                  | -38.932365  |
| test/reward_per_eps            | -40         |
| test/steps                     | 116800      |
| train/episodes                 | 11680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 467200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 292         |
| stats_o/mean                   | 0.4157602   |
| stats_o/std                    | 0.041443504 |
| test/episodes                  | 2930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -38.97877   |
| test/Q_plus_P                  | -38.97877   |
| test/reward_per_eps            | -40         |
| test/steps                     | 117200      |
| train/episodes                 | 11720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 468800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 293         |
| stats_o/mean                   | 0.41572163  |
| stats_o/std                    | 0.041442912 |
| test/episodes                  | 2940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -38.95455   |
| test/Q_plus_P                  | -38.95455   |
| test/reward_per_eps            | -40         |
| test/steps                     | 117600      |
| train/episodes                 | 11760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 470400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 294         |
| stats_o/mean                   | 0.41568395  |
| stats_o/std                    | 0.041434463 |
| test/episodes                  | 2950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -38.99631   |
| test/Q_plus_P                  | -38.99631   |
| test/reward_per_eps            | -40         |
| test/steps                     | 118000      |
| train/episodes                 | 11800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 472000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 295        |
| stats_o/mean                   | 0.41566408 |
| stats_o/std                    | 0.04143693 |
| test/episodes                  | 2960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.187     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -38.99501  |
| test/Q_plus_P                  | -38.99501  |
| test/reward_per_eps            | -40        |
| test/steps                     | 118400     |
| train/episodes                 | 11840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 473600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 296         |
| stats_o/mean                   | 0.41564023  |
| stats_o/std                    | 0.041435357 |
| test/episodes                  | 2970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -39.002552  |
| test/Q_plus_P                  | -39.002552  |
| test/reward_per_eps            | -40         |
| test/steps                     | 118800      |
| train/episodes                 | 11880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 475200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 297        |
| stats_o/mean                   | 0.41562304 |
| stats_o/std                    | 0.04144436 |
| test/episodes                  | 2980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.178     |
| test/info_shaping_reward_mean  | -0.268     |
| test/info_shaping_reward_min   | -0.349     |
| test/Q                         | -39.018715 |
| test/Q_plus_P                  | -39.018715 |
| test/reward_per_eps            | -40        |
| test/steps                     | 119200     |
| train/episodes                 | 11920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.258     |
| train/info_shaping_reward_min  | -0.359     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 476800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 298         |
| stats_o/mean                   | 0.41560483  |
| stats_o/std                    | 0.041439023 |
| test/episodes                  | 2990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.035954  |
| test/Q_plus_P                  | -39.035954  |
| test/reward_per_eps            | -40         |
| test/steps                     | 119600      |
| train/episodes                 | 11960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 478400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 299         |
| stats_o/mean                   | 0.41560045  |
| stats_o/std                    | 0.041436154 |
| test/episodes                  | 3000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.199      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.359      |
| test/Q                         | -39.05959   |
| test/Q_plus_P                  | -39.05959   |
| test/reward_per_eps            | -40         |
| test/steps                     | 120000      |
| train/episodes                 | 12000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.351      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 480000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 300        |
| stats_o/mean                   | 0.4155669  |
| stats_o/std                    | 0.04143064 |
| test/episodes                  | 3010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.286     |
| test/info_shaping_reward_min   | -0.4       |
| test/Q                         | -39.049328 |
| test/Q_plus_P                  | -39.049328 |
| test/reward_per_eps            | -40        |
| test/steps                     | 120400     |
| train/episodes                 | 12040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 481600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 301         |
| stats_o/mean                   | 0.41551605  |
| stats_o/std                    | 0.041457158 |
| test/episodes                  | 3020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.032093  |
| test/Q_plus_P                  | -39.032093  |
| test/reward_per_eps            | -40         |
| test/steps                     | 120800      |
| train/episodes                 | 12080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.182      |
| train/info_shaping_reward_mean | -0.278      |
| train/info_shaping_reward_min  | -0.398      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 483200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 302         |
| stats_o/mean                   | 0.41546938  |
| stats_o/std                    | 0.041459728 |
| test/episodes                  | 3030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.346      |
| test/Q                         | -39.10842   |
| test/Q_plus_P                  | -39.10842   |
| test/reward_per_eps            | -40         |
| test/steps                     | 121200      |
| train/episodes                 | 12120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.165      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 484800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 303         |
| stats_o/mean                   | 0.41542888  |
| stats_o/std                    | 0.041465294 |
| test/episodes                  | 3040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -39.084946  |
| test/Q_plus_P                  | -39.084946  |
| test/reward_per_eps            | -40         |
| test/steps                     | 121600      |
| train/episodes                 | 12160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.254      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 486400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 304         |
| stats_o/mean                   | 0.41542575  |
| stats_o/std                    | 0.041468184 |
| test/episodes                  | 3050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.10782   |
| test/Q_plus_P                  | -39.10782   |
| test/reward_per_eps            | -40         |
| test/steps                     | 122000      |
| train/episodes                 | 12200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 488000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 305        |
| stats_o/mean                   | 0.41540882 |
| stats_o/std                    | 0.04145479 |
| test/episodes                  | 3060       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.201     |
| test/info_shaping_reward_mean  | -0.243     |
| test/info_shaping_reward_min   | -0.301     |
| test/Q                         | -39.10475  |
| test/Q_plus_P                  | -39.10475  |
| test/reward_per_eps            | -40        |
| test/steps                     | 122400     |
| train/episodes                 | 12240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 489600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 306         |
| stats_o/mean                   | 0.4153745   |
| stats_o/std                    | 0.041450735 |
| test/episodes                  | 3070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.2        |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.12812   |
| test/Q_plus_P                  | -39.12812   |
| test/reward_per_eps            | -40         |
| test/steps                     | 122800      |
| train/episodes                 | 12280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 491200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 307        |
| stats_o/mean                   | 0.41534892 |
| stats_o/std                    | 0.04145436 |
| test/episodes                  | 3080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.167     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.374     |
| test/Q                         | -39.130722 |
| test/Q_plus_P                  | -39.130722 |
| test/reward_per_eps            | -40        |
| test/steps                     | 123200     |
| train/episodes                 | 12320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.358     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 492800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 308         |
| stats_o/mean                   | 0.41531718  |
| stats_o/std                    | 0.041458078 |
| test/episodes                  | 3090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.14117   |
| test/Q_plus_P                  | -39.14117   |
| test/reward_per_eps            | -40         |
| test/steps                     | 123600      |
| train/episodes                 | 12360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 494400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 309         |
| stats_o/mean                   | 0.4152968   |
| stats_o/std                    | 0.041451737 |
| test/episodes                  | 3100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.342      |
| test/Q                         | -39.15793   |
| test/Q_plus_P                  | -39.15793   |
| test/reward_per_eps            | -40         |
| test/steps                     | 124000      |
| train/episodes                 | 12400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 496000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 310         |
| stats_o/mean                   | 0.4152725   |
| stats_o/std                    | 0.041458603 |
| test/episodes                  | 3110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.316      |
| test/Q                         | -39.153423  |
| test/Q_plus_P                  | -39.153423  |
| test/reward_per_eps            | -40         |
| test/steps                     | 124400      |
| train/episodes                 | 12440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.161      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 497600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 311        |
| stats_o/mean                   | 0.4152405  |
| stats_o/std                    | 0.04145436 |
| test/episodes                  | 3120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.205     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.179264 |
| test/Q_plus_P                  | -39.179264 |
| test/reward_per_eps            | -40        |
| test/steps                     | 124800     |
| train/episodes                 | 12480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.147     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 499200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 312         |
| stats_o/mean                   | 0.41524446  |
| stats_o/std                    | 0.041455317 |
| test/episodes                  | 3130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.19187   |
| test/Q_plus_P                  | -39.19187   |
| test/reward_per_eps            | -40         |
| test/steps                     | 125200      |
| train/episodes                 | 12520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 500800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 313         |
| stats_o/mean                   | 0.41524947  |
| stats_o/std                    | 0.041463774 |
| test/episodes                  | 3140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.184      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.20511   |
| test/Q_plus_P                  | -39.20511   |
| test/reward_per_eps            | -40         |
| test/steps                     | 125600      |
| train/episodes                 | 12560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 502400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 314         |
| stats_o/mean                   | 0.41524878  |
| stats_o/std                    | 0.041463424 |
| test/episodes                  | 3150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.134476  |
| test/Q_plus_P                  | -39.134476  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126000      |
| train/episodes                 | 12600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 504000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 315         |
| stats_o/mean                   | 0.4152247   |
| stats_o/std                    | 0.041451406 |
| test/episodes                  | 3160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.21743   |
| test/Q_plus_P                  | -39.21743   |
| test/reward_per_eps            | -40         |
| test/steps                     | 126400      |
| train/episodes                 | 12640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.316      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 505600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 316         |
| stats_o/mean                   | 0.41520575  |
| stats_o/std                    | 0.041442964 |
| test/episodes                  | 3170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.178      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.286728  |
| test/Q_plus_P                  | -39.286728  |
| test/reward_per_eps            | -40         |
| test/steps                     | 126800      |
| train/episodes                 | 12680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 507200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 317         |
| stats_o/mean                   | 0.41519475  |
| stats_o/std                    | 0.041441213 |
| test/episodes                  | 3180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.219      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.24829   |
| test/Q_plus_P                  | -39.24829   |
| test/reward_per_eps            | -40         |
| test/steps                     | 127200      |
| train/episodes                 | 12720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 508800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 318        |
| stats_o/mean                   | 0.41519454 |
| stats_o/std                    | 0.04143783 |
| test/episodes                  | 3190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.196     |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.297     |
| test/Q                         | -39.180954 |
| test/Q_plus_P                  | -39.180954 |
| test/reward_per_eps            | -40        |
| test/steps                     | 127600     |
| train/episodes                 | 12760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 510400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 319         |
| stats_o/mean                   | 0.41516736  |
| stats_o/std                    | 0.041446213 |
| test/episodes                  | 3200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.26788   |
| test/Q_plus_P                  | -39.26788   |
| test/reward_per_eps            | -40         |
| test/steps                     | 128000      |
| train/episodes                 | 12800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.255      |
| train/info_shaping_reward_min  | -0.383      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 512000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 320         |
| stats_o/mean                   | 0.4151419   |
| stats_o/std                    | 0.041443843 |
| test/episodes                  | 3210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -39.253174  |
| test/Q_plus_P                  | -39.253174  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128400      |
| train/episodes                 | 12840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 513600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 321         |
| stats_o/mean                   | 0.41512522  |
| stats_o/std                    | 0.041449133 |
| test/episodes                  | 3220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.198      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.272736  |
| test/Q_plus_P                  | -39.272736  |
| test/reward_per_eps            | -40         |
| test/steps                     | 128800      |
| train/episodes                 | 12880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 515200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 322         |
| stats_o/mean                   | 0.41509736  |
| stats_o/std                    | 0.041460555 |
| test/episodes                  | 3230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.354      |
| test/Q                         | -39.27123   |
| test/Q_plus_P                  | -39.27123   |
| test/reward_per_eps            | -40         |
| test/steps                     | 129200      |
| train/episodes                 | 12920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.171      |
| train/info_shaping_reward_mean | -0.265      |
| train/info_shaping_reward_min  | -0.354      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 516800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 323         |
| stats_o/mean                   | 0.41508755  |
| stats_o/std                    | 0.041454703 |
| test/episodes                  | 3240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.26754   |
| test/Q_plus_P                  | -39.26754   |
| test/reward_per_eps            | -40         |
| test/steps                     | 129600      |
| train/episodes                 | 12960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 518400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 324        |
| stats_o/mean                   | 0.41506734 |
| stats_o/std                    | 0.04145468 |
| test/episodes                  | 3250       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.238     |
| test/info_shaping_reward_min   | -0.342     |
| test/Q                         | -39.29181  |
| test/Q_plus_P                  | -39.29181  |
| test/reward_per_eps            | -40        |
| test/steps                     | 130000     |
| train/episodes                 | 13000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.161     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.373     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 520000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 325         |
| stats_o/mean                   | 0.4150361   |
| stats_o/std                    | 0.041455988 |
| test/episodes                  | 3260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.289146  |
| test/Q_plus_P                  | -39.289146  |
| test/reward_per_eps            | -40         |
| test/steps                     | 130400      |
| train/episodes                 | 13040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.356      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 521600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 326         |
| stats_o/mean                   | 0.41501352  |
| stats_o/std                    | 0.041450303 |
| test/episodes                  | 3270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.204      |
| test/info_shaping_reward_mean  | -0.256      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -39.316463  |
| test/Q_plus_P                  | -39.316463  |
| test/reward_per_eps            | -40         |
| test/steps                     | 130800      |
| train/episodes                 | 13080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.361      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 523200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 327        |
| stats_o/mean                   | 0.41499028 |
| stats_o/std                    | 0.04145067 |
| test/episodes                  | 3280       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.197     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.301064 |
| test/Q_plus_P                  | -39.301064 |
| test/reward_per_eps            | -40        |
| test/steps                     | 131200     |
| train/episodes                 | 13120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.249     |
| train/info_shaping_reward_min  | -0.365     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 524800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 328        |
| stats_o/mean                   | 0.41496828 |
| stats_o/std                    | 0.04145405 |
| test/episodes                  | 3290       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -39.309113 |
| test/Q_plus_P                  | -39.309113 |
| test/reward_per_eps            | -40        |
| test/steps                     | 131600     |
| train/episodes                 | 13160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 526400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 329        |
| stats_o/mean                   | 0.4149363  |
| stats_o/std                    | 0.04146162 |
| test/episodes                  | 3300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.155     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.337     |
| test/Q                         | -39.365875 |
| test/Q_plus_P                  | -39.365875 |
| test/reward_per_eps            | -40        |
| test/steps                     | 132000     |
| train/episodes                 | 13200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.157     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.372     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 528000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 330         |
| stats_o/mean                   | 0.41489992  |
| stats_o/std                    | 0.041461043 |
| test/episodes                  | 3310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.278      |
| test/info_shaping_reward_min   | -0.393      |
| test/Q                         | -39.333996  |
| test/Q_plus_P                  | -39.333996  |
| test/reward_per_eps            | -40         |
| test/steps                     | 132400      |
| train/episodes                 | 13240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 529600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 331        |
| stats_o/mean                   | 0.4148589  |
| stats_o/std                    | 0.04146315 |
| test/episodes                  | 3320       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.336     |
| test/Q                         | -39.30707  |
| test/Q_plus_P                  | -39.30707  |
| test/reward_per_eps            | -40        |
| test/steps                     | 132800     |
| train/episodes                 | 13280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 531200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 332         |
| stats_o/mean                   | 0.41479507  |
| stats_o/std                    | 0.041481327 |
| test/episodes                  | 3330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.328      |
| test/Q                         | -39.294094  |
| test/Q_plus_P                  | -39.294094  |
| test/reward_per_eps            | -40         |
| test/steps                     | 133200      |
| train/episodes                 | 13320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.271      |
| train/info_shaping_reward_min  | -0.379      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 532800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 333         |
| stats_o/mean                   | 0.4147625   |
| stats_o/std                    | 0.041480508 |
| test/episodes                  | 3340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.222      |
| test/info_shaping_reward_mean  | -0.27       |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.358276  |
| test/Q_plus_P                  | -39.358276  |
| test/reward_per_eps            | -40         |
| test/steps                     | 133600      |
| train/episodes                 | 13360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 534400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 334         |
| stats_o/mean                   | 0.4147388   |
| stats_o/std                    | 0.041473832 |
| test/episodes                  | 3350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.353245  |
| test/Q_plus_P                  | -39.353245  |
| test/reward_per_eps            | -40         |
| test/steps                     | 134000      |
| train/episodes                 | 13400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 536000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 335        |
| stats_o/mean                   | 0.4147514  |
| stats_o/std                    | 0.04147354 |
| test/episodes                  | 3360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.193     |
| test/info_shaping_reward_mean  | -0.275     |
| test/info_shaping_reward_min   | -0.363     |
| test/Q                         | -39.36645  |
| test/Q_plus_P                  | -39.36645  |
| test/reward_per_eps            | -40        |
| test/steps                     | 134400     |
| train/episodes                 | 13440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 537600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 336         |
| stats_o/mean                   | 0.41473237  |
| stats_o/std                    | 0.041465558 |
| test/episodes                  | 3370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.185      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.376797  |
| test/Q_plus_P                  | -39.376797  |
| test/reward_per_eps            | -40         |
| test/steps                     | 134800      |
| train/episodes                 | 13480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 539200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 337         |
| stats_o/mean                   | 0.41472605  |
| stats_o/std                    | 0.041460335 |
| test/episodes                  | 3380        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -39.370518  |
| test/Q_plus_P                  | -39.370518  |
| test/reward_per_eps            | -40         |
| test/steps                     | 135200      |
| train/episodes                 | 13520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 540800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 338        |
| stats_o/mean                   | 0.4147145  |
| stats_o/std                    | 0.04146969 |
| test/episodes                  | 3390       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.37082  |
| test/Q_plus_P                  | -39.37082  |
| test/reward_per_eps            | -40        |
| test/steps                     | 135600     |
| train/episodes                 | 13560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.143     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.349     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 542400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 339        |
| stats_o/mean                   | 0.41472602 |
| stats_o/std                    | 0.04146377 |
| test/episodes                  | 3400       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.236     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.344357 |
| test/Q_plus_P                  | -39.344357 |
| test/reward_per_eps            | -40        |
| test/steps                     | 136000     |
| train/episodes                 | 13600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 544000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 340         |
| stats_o/mean                   | 0.4147072   |
| stats_o/std                    | 0.041460108 |
| test/episodes                  | 3410        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.334      |
| test/Q                         | -39.452072  |
| test/Q_plus_P                  | -39.452072  |
| test/reward_per_eps            | -40         |
| test/steps                     | 136400      |
| train/episodes                 | 13640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.169      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 545600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 341        |
| stats_o/mean                   | 0.4146934  |
| stats_o/std                    | 0.04147178 |
| test/episodes                  | 3420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -39.388195 |
| test/Q_plus_P                  | -39.388195 |
| test/reward_per_eps            | -40        |
| test/steps                     | 136800     |
| train/episodes                 | 13680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.168     |
| train/info_shaping_reward_mean | -0.262     |
| train/info_shaping_reward_min  | -0.377     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 547200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 342        |
| stats_o/mean                   | 0.41468096 |
| stats_o/std                    | 0.04147357 |
| test/episodes                  | 3430       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.252     |
| test/info_shaping_reward_min   | -0.316     |
| test/Q                         | -39.464355 |
| test/Q_plus_P                  | -39.464355 |
| test/reward_per_eps            | -40        |
| test/steps                     | 137200     |
| train/episodes                 | 13720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 548800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 343         |
| stats_o/mean                   | 0.41467968  |
| stats_o/std                    | 0.041466285 |
| test/episodes                  | 3440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.286      |
| test/Q                         | -39.42356   |
| test/Q_plus_P                  | -39.42356   |
| test/reward_per_eps            | -40         |
| test/steps                     | 137600      |
| train/episodes                 | 13760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 550400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 344         |
| stats_o/mean                   | 0.41468456  |
| stats_o/std                    | 0.041465398 |
| test/episodes                  | 3450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.157      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.427452  |
| test/Q_plus_P                  | -39.427452  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138000      |
| train/episodes                 | 13800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 552000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 345         |
| stats_o/mean                   | 0.4146774   |
| stats_o/std                    | 0.041467503 |
| test/episodes                  | 3460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.43276   |
| test/Q_plus_P                  | -39.43276   |
| test/reward_per_eps            | -40         |
| test/steps                     | 138400      |
| train/episodes                 | 13840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 553600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 346         |
| stats_o/mean                   | 0.4146478   |
| stats_o/std                    | 0.041470286 |
| test/episodes                  | 3470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.427204  |
| test/Q_plus_P                  | -39.427204  |
| test/reward_per_eps            | -40         |
| test/steps                     | 138800      |
| train/episodes                 | 13880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 555200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 347        |
| stats_o/mean                   | 0.41463688 |
| stats_o/std                    | 0.04147656 |
| test/episodes                  | 3480       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -39.494747 |
| test/Q_plus_P                  | -39.494747 |
| test/reward_per_eps            | -40        |
| test/steps                     | 139200     |
| train/episodes                 | 13920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 556800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 348        |
| stats_o/mean                   | 0.4146156  |
| stats_o/std                    | 0.0414758  |
| test/episodes                  | 3490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.19      |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.313     |
| test/Q                         | -39.520287 |
| test/Q_plus_P                  | -39.520287 |
| test/reward_per_eps            | -40        |
| test/steps                     | 139600     |
| train/episodes                 | 13960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.34      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 558400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 349        |
| stats_o/mean                   | 0.41459474 |
| stats_o/std                    | 0.04147982 |
| test/episodes                  | 3500       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.3       |
| test/Q                         | -39.444584 |
| test/Q_plus_P                  | -39.444584 |
| test/reward_per_eps            | -40        |
| test/steps                     | 140000     |
| train/episodes                 | 14000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.169     |
| train/info_shaping_reward_mean | -0.252     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 560000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 350         |
| stats_o/mean                   | 0.41458365  |
| stats_o/std                    | 0.041486505 |
| test/episodes                  | 3510        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.291      |
| test/Q                         | -39.457542  |
| test/Q_plus_P                  | -39.457542  |
| test/reward_per_eps            | -40         |
| test/steps                     | 140400      |
| train/episodes                 | 14040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.368      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 561600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 351        |
| stats_o/mean                   | 0.41456687 |
| stats_o/std                    | 0.04148422 |
| test/episodes                  | 3520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.224     |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -39.42942  |
| test/Q_plus_P                  | -39.42942  |
| test/reward_per_eps            | -40        |
| test/steps                     | 140800     |
| train/episodes                 | 14080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 563200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 352         |
| stats_o/mean                   | 0.414542    |
| stats_o/std                    | 0.041482083 |
| test/episodes                  | 3530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.245      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.49505   |
| test/Q_plus_P                  | -39.49505   |
| test/reward_per_eps            | -40         |
| test/steps                     | 141200      |
| train/episodes                 | 14120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.36       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 564800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 353         |
| stats_o/mean                   | 0.414529    |
| stats_o/std                    | 0.041486014 |
| test/episodes                  | 3540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.251      |
| test/info_shaping_reward_min   | -0.325      |
| test/Q                         | -39.50158   |
| test/Q_plus_P                  | -39.50158   |
| test/reward_per_eps            | -40         |
| test/steps                     | 141600      |
| train/episodes                 | 14160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 566400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 354         |
| stats_o/mean                   | 0.41452262  |
| stats_o/std                    | 0.041481704 |
| test/episodes                  | 3550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.163      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.482067  |
| test/Q_plus_P                  | -39.482067  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142000      |
| train/episodes                 | 14200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 568000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 355         |
| stats_o/mean                   | 0.41450158  |
| stats_o/std                    | 0.041487444 |
| test/episodes                  | 3560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.454597  |
| test/Q_plus_P                  | -39.454597  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142400      |
| train/episodes                 | 14240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.177      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.37       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 569600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 356         |
| stats_o/mean                   | 0.41449797  |
| stats_o/std                    | 0.041483924 |
| test/episodes                  | 3570        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.193      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.575153  |
| test/Q_plus_P                  | -39.575153  |
| test/reward_per_eps            | -40         |
| test/steps                     | 142800      |
| train/episodes                 | 14280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 571200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 357         |
| stats_o/mean                   | 0.4144793   |
| stats_o/std                    | 0.041484457 |
| test/episodes                  | 3580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.516235  |
| test/Q_plus_P                  | -39.516235  |
| test/reward_per_eps            | -40         |
| test/steps                     | 143200      |
| train/episodes                 | 14320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.121      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 572800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 358         |
| stats_o/mean                   | 0.41446522  |
| stats_o/std                    | 0.041483793 |
| test/episodes                  | 3590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.339      |
| test/Q                         | -39.51338   |
| test/Q_plus_P                  | -39.51338   |
| test/reward_per_eps            | -40         |
| test/steps                     | 143600      |
| train/episodes                 | 14360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 574400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 359         |
| stats_o/mean                   | 0.41446677  |
| stats_o/std                    | 0.041478418 |
| test/episodes                  | 3600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.539867  |
| test/Q_plus_P                  | -39.539867  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144000      |
| train/episodes                 | 14400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 576000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 360        |
| stats_o/mean                   | 0.41444886 |
| stats_o/std                    | 0.04148045 |
| test/episodes                  | 3610       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.188     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.334     |
| test/Q                         | -39.53583  |
| test/Q_plus_P                  | -39.53583  |
| test/reward_per_eps            | -40        |
| test/steps                     | 144400     |
| train/episodes                 | 14440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 577600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 361         |
| stats_o/mean                   | 0.4144313   |
| stats_o/std                    | 0.041478653 |
| test/episodes                  | 3620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.35       |
| test/Q                         | -39.536144  |
| test/Q_plus_P                  | -39.536144  |
| test/reward_per_eps            | -40         |
| test/steps                     | 144800      |
| train/episodes                 | 14480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.16       |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 579200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 362         |
| stats_o/mean                   | 0.41442478  |
| stats_o/std                    | 0.041482657 |
| test/episodes                  | 3630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.214      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.555058  |
| test/Q_plus_P                  | -39.555058  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145200      |
| train/episodes                 | 14520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.364      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 580800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 363         |
| stats_o/mean                   | 0.41441032  |
| stats_o/std                    | 0.041487478 |
| test/episodes                  | 3640        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.542442  |
| test/Q_plus_P                  | -39.542442  |
| test/reward_per_eps            | -40         |
| test/steps                     | 145600      |
| train/episodes                 | 14560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.159      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.352      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 582400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 364         |
| stats_o/mean                   | 0.41440317  |
| stats_o/std                    | 0.041491956 |
| test/episodes                  | 3650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.164      |
| test/info_shaping_reward_mean  | -0.246      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.52833   |
| test/Q_plus_P                  | -39.52833   |
| test/reward_per_eps            | -40         |
| test/steps                     | 146000      |
| train/episodes                 | 14600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.253      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 584000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 365         |
| stats_o/mean                   | 0.4143949   |
| stats_o/std                    | 0.041489247 |
| test/episodes                  | 3660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.56695   |
| test/Q_plus_P                  | -39.56695   |
| test/reward_per_eps            | -40         |
| test/steps                     | 146400      |
| train/episodes                 | 14640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.164      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 585600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 366        |
| stats_o/mean                   | 0.4143939  |
| stats_o/std                    | 0.04149471 |
| test/episodes                  | 3670       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -39.60324  |
| test/Q_plus_P                  | -39.60324  |
| test/reward_per_eps            | -40        |
| test/steps                     | 146800     |
| train/episodes                 | 14680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.154     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 587200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 367        |
| stats_o/mean                   | 0.41440198 |
| stats_o/std                    | 0.04148938 |
| test/episodes                  | 3680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.61103  |
| test/Q_plus_P                  | -39.61103  |
| test/reward_per_eps            | -40        |
| test/steps                     | 147200     |
| train/episodes                 | 14720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 588800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 368         |
| stats_o/mean                   | 0.41438985  |
| stats_o/std                    | 0.041492213 |
| test/episodes                  | 3690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.179      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.56219   |
| test/Q_plus_P                  | -39.56219   |
| test/reward_per_eps            | -40         |
| test/steps                     | 147600      |
| train/episodes                 | 14760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 590400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 369        |
| stats_o/mean                   | 0.4143876  |
| stats_o/std                    | 0.04148311 |
| test/episodes                  | 3700       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.225     |
| test/info_shaping_reward_min   | -0.314     |
| test/Q                         | -39.569496 |
| test/Q_plus_P                  | -39.569496 |
| test/reward_per_eps            | -40        |
| test/steps                     | 148000     |
| train/episodes                 | 14800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.32      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 592000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 370         |
| stats_o/mean                   | 0.4143771   |
| stats_o/std                    | 0.041481283 |
| test/episodes                  | 3710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.19       |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.588676  |
| test/Q_plus_P                  | -39.588676  |
| test/reward_per_eps            | -40         |
| test/steps                     | 148400      |
| train/episodes                 | 14840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 593600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 371        |
| stats_o/mean                   | 0.41437688 |
| stats_o/std                    | 0.04148262 |
| test/episodes                  | 3720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.564396 |
| test/Q_plus_P                  | -39.564396 |
| test/reward_per_eps            | -40        |
| test/steps                     | 148800     |
| train/episodes                 | 14880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.353     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 595200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 372         |
| stats_o/mean                   | 0.4143566   |
| stats_o/std                    | 0.041479375 |
| test/episodes                  | 3730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.61899   |
| test/Q_plus_P                  | -39.61899   |
| test/reward_per_eps            | -40         |
| test/steps                     | 149200      |
| train/episodes                 | 14920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 596800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 373        |
| stats_o/mean                   | 0.41434112 |
| stats_o/std                    | 0.0414723  |
| test/episodes                  | 3740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.319     |
| test/Q                         | -39.601353 |
| test/Q_plus_P                  | -39.601353 |
| test/reward_per_eps            | -40        |
| test/steps                     | 149600     |
| train/episodes                 | 14960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.15      |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 598400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 374         |
| stats_o/mean                   | 0.41432953  |
| stats_o/std                    | 0.041470107 |
| test/episodes                  | 3750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.61938   |
| test/Q_plus_P                  | -39.61938   |
| test/reward_per_eps            | -40         |
| test/steps                     | 150000      |
| train/episodes                 | 15000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.353      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 600000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 375        |
| stats_o/mean                   | 0.41431868 |
| stats_o/std                    | 0.04146945 |
| test/episodes                  | 3760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.211     |
| test/info_shaping_reward_mean  | -0.251     |
| test/info_shaping_reward_min   | -0.298     |
| test/Q                         | -39.554466 |
| test/Q_plus_P                  | -39.554466 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150400     |
| train/episodes                 | 15040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.364     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 601600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 376        |
| stats_o/mean                   | 0.4142977  |
| stats_o/std                    | 0.04146559 |
| test/episodes                  | 3770       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.267     |
| test/info_shaping_reward_min   | -0.317     |
| test/Q                         | -39.656593 |
| test/Q_plus_P                  | -39.656593 |
| test/reward_per_eps            | -40        |
| test/steps                     | 150800     |
| train/episodes                 | 15080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.141     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 603200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 377        |
| stats_o/mean                   | 0.41427943 |
| stats_o/std                    | 0.041468   |
| test/episodes                  | 3780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.308     |
| test/Q                         | -39.519756 |
| test/Q_plus_P                  | -39.519756 |
| test/reward_per_eps            | -40        |
| test/steps                     | 151200     |
| train/episodes                 | 15120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 604800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 378        |
| stats_o/mean                   | 0.41427544 |
| stats_o/std                    | 0.04146544 |
| test/episodes                  | 3790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.232     |
| test/info_shaping_reward_min   | -0.304     |
| test/Q                         | -39.608833 |
| test/Q_plus_P                  | -39.608833 |
| test/reward_per_eps            | -40        |
| test/steps                     | 151600     |
| train/episodes                 | 15160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 606400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 379        |
| stats_o/mean                   | 0.4142604  |
| stats_o/std                    | 0.04147099 |
| test/episodes                  | 3800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.154     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.258     |
| test/Q                         | -39.645058 |
| test/Q_plus_P                  | -39.645058 |
| test/reward_per_eps            | -40        |
| test/steps                     | 152000     |
| train/episodes                 | 15200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.152     |
| train/info_shaping_reward_mean | -0.248     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 608000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 380         |
| stats_o/mean                   | 0.41424873  |
| stats_o/std                    | 0.041458923 |
| test/episodes                  | 3810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.621838  |
| test/Q_plus_P                  | -39.621838  |
| test/reward_per_eps            | -40         |
| test/steps                     | 152400      |
| train/episodes                 | 15240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 609600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 381        |
| stats_o/mean                   | 0.41423687 |
| stats_o/std                    | 0.04146162 |
| test/episodes                  | 3820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.62909  |
| test/Q_plus_P                  | -39.62909  |
| test/reward_per_eps            | -40        |
| test/steps                     | 152800     |
| train/episodes                 | 15280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.351     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 611200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 382        |
| stats_o/mean                   | 0.4142098  |
| stats_o/std                    | 0.04145731 |
| test/episodes                  | 3830       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.213     |
| test/info_shaping_reward_min   | -0.266     |
| test/Q                         | -39.610138 |
| test/Q_plus_P                  | -39.610138 |
| test/reward_per_eps            | -40        |
| test/steps                     | 153200     |
| train/episodes                 | 15320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 612800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 383         |
| stats_o/mean                   | 0.41419625  |
| stats_o/std                    | 0.041463744 |
| test/episodes                  | 3840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.152      |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.76909   |
| test/Q_plus_P                  | -39.76909   |
| test/reward_per_eps            | -40         |
| test/steps                     | 153600      |
| train/episodes                 | 15360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 614400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 384         |
| stats_o/mean                   | 0.4142004   |
| stats_o/std                    | 0.041467335 |
| test/episodes                  | 3850        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.69948   |
| test/Q_plus_P                  | -39.69948   |
| test/reward_per_eps            | -40         |
| test/steps                     | 154000      |
| train/episodes                 | 15400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 616000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 385        |
| stats_o/mean                   | 0.41418257 |
| stats_o/std                    | 0.04145462 |
| test/episodes                  | 3860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.181     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.264     |
| test/Q                         | -39.555958 |
| test/Q_plus_P                  | -39.555958 |
| test/reward_per_eps            | -40        |
| test/steps                     | 154400     |
| train/episodes                 | 15440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 617600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 386        |
| stats_o/mean                   | 0.4141684  |
| stats_o/std                    | 0.04144856 |
| test/episodes                  | 3870       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.226     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.669174 |
| test/Q_plus_P                  | -39.669174 |
| test/reward_per_eps            | -40        |
| test/steps                     | 154800     |
| train/episodes                 | 15480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 619200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 387        |
| stats_o/mean                   | 0.41417158 |
| stats_o/std                    | 0.04144466 |
| test/episodes                  | 3880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.318     |
| test/Q                         | -39.669388 |
| test/Q_plus_P                  | -39.669388 |
| test/reward_per_eps            | -40        |
| test/steps                     | 155200     |
| train/episodes                 | 15520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 620800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 388        |
| stats_o/mean                   | 0.41416466 |
| stats_o/std                    | 0.04143357 |
| test/episodes                  | 3890       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.164     |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -39.773415 |
| test/Q_plus_P                  | -39.773415 |
| test/reward_per_eps            | -40        |
| test/steps                     | 155600     |
| train/episodes                 | 15560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.319     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 622400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 389         |
| stats_o/mean                   | 0.41415045  |
| stats_o/std                    | 0.041431684 |
| test/episodes                  | 3900        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.276      |
| test/Q                         | -39.646748  |
| test/Q_plus_P                  | -39.646748  |
| test/reward_per_eps            | -40         |
| test/steps                     | 156000      |
| train/episodes                 | 15600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 624000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 390        |
| stats_o/mean                   | 0.4141434  |
| stats_o/std                    | 0.04142731 |
| test/episodes                  | 3910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.192     |
| test/info_shaping_reward_mean  | -0.258     |
| test/info_shaping_reward_min   | -0.33      |
| test/Q                         | -39.698147 |
| test/Q_plus_P                  | -39.698147 |
| test/reward_per_eps            | -40        |
| test/steps                     | 156400     |
| train/episodes                 | 15640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.133     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 625600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 391        |
| stats_o/mean                   | 0.4141275  |
| stats_o/std                    | 0.04142579 |
| test/episodes                  | 3920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.143     |
| test/info_shaping_reward_mean  | -0.193     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -39.64015  |
| test/Q_plus_P                  | -39.64015  |
| test/reward_per_eps            | -40        |
| test/steps                     | 156800     |
| train/episodes                 | 15680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.142     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.333     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 627200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 392         |
| stats_o/mean                   | 0.41413203  |
| stats_o/std                    | 0.041424513 |
| test/episodes                  | 3930        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -39.638256  |
| test/Q_plus_P                  | -39.638256  |
| test/reward_per_eps            | -40         |
| test/steps                     | 157200      |
| train/episodes                 | 15720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 628800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 393         |
| stats_o/mean                   | 0.41413248  |
| stats_o/std                    | 0.041427862 |
| test/episodes                  | 3940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -39.707996  |
| test/Q_plus_P                  | -39.707996  |
| test/reward_per_eps            | -40         |
| test/steps                     | 157600      |
| train/episodes                 | 15760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.317      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 630400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 394         |
| stats_o/mean                   | 0.41412315  |
| stats_o/std                    | 0.041434046 |
| test/episodes                  | 3950        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.681007  |
| test/Q_plus_P                  | -39.681007  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158000      |
| train/episodes                 | 15800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 632000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 395        |
| stats_o/mean                   | 0.41413525 |
| stats_o/std                    | 0.04142966 |
| test/episodes                  | 3960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.209     |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -39.73123  |
| test/Q_plus_P                  | -39.73123  |
| test/reward_per_eps            | -40        |
| test/steps                     | 158400     |
| train/episodes                 | 15840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.146     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 633600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 396         |
| stats_o/mean                   | 0.4141253   |
| stats_o/std                    | 0.041429218 |
| test/episodes                  | 3970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.202      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -39.714977  |
| test/Q_plus_P                  | -39.714977  |
| test/reward_per_eps            | -40         |
| test/steps                     | 158800      |
| train/episodes                 | 15880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.144      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 635200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 397         |
| stats_o/mean                   | 0.41413435  |
| stats_o/std                    | 0.041426178 |
| test/episodes                  | 3980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.70224   |
| test/Q_plus_P                  | -39.70224   |
| test/reward_per_eps            | -40         |
| test/steps                     | 159200      |
| train/episodes                 | 15920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 636800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 398         |
| stats_o/mean                   | 0.41411355  |
| stats_o/std                    | 0.041423116 |
| test/episodes                  | 3990        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.774445  |
| test/Q_plus_P                  | -39.774445  |
| test/reward_per_eps            | -40         |
| test/steps                     | 159600      |
| train/episodes                 | 15960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 638400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 399         |
| stats_o/mean                   | 0.4141128   |
| stats_o/std                    | 0.041423157 |
| test/episodes                  | 4000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.735844  |
| test/Q_plus_P                  | -39.735844  |
| test/reward_per_eps            | -40         |
| test/steps                     | 160000      |
| train/episodes                 | 16000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 640000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 400        |
| stats_o/mean                   | 0.41409764 |
| stats_o/std                    | 0.04142505 |
| test/episodes                  | 4010       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.233     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.727745 |
| test/Q_plus_P                  | -39.727745 |
| test/reward_per_eps            | -40        |
| test/steps                     | 160400     |
| train/episodes                 | 16040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 641600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 401         |
| stats_o/mean                   | 0.41408417  |
| stats_o/std                    | 0.041418787 |
| test/episodes                  | 4020        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.201      |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.312      |
| test/Q                         | -39.722313  |
| test/Q_plus_P                  | -39.722313  |
| test/reward_per_eps            | -40         |
| test/steps                     | 160800      |
| train/episodes                 | 16080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 643200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 402         |
| stats_o/mean                   | 0.41407642  |
| stats_o/std                    | 0.041411553 |
| test/episodes                  | 4030        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.156      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.658066  |
| test/Q_plus_P                  | -39.658066  |
| test/reward_per_eps            | -40         |
| test/steps                     | 161200      |
| train/episodes                 | 16120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 644800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 403         |
| stats_o/mean                   | 0.414058    |
| stats_o/std                    | 0.041407954 |
| test/episodes                  | 4040        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.332      |
| test/Q                         | -39.727207  |
| test/Q_plus_P                  | -39.727207  |
| test/reward_per_eps            | -40         |
| test/steps                     | 161600      |
| train/episodes                 | 16160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 646400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 404        |
| stats_o/mean                   | 0.41405678 |
| stats_o/std                    | 0.04140517 |
| test/episodes                  | 4050       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.344     |
| test/Q                         | -39.62463  |
| test/Q_plus_P                  | -39.62463  |
| test/reward_per_eps            | -40        |
| test/steps                     | 162000     |
| train/episodes                 | 16200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.238     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 648000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 405         |
| stats_o/mean                   | 0.41404065  |
| stats_o/std                    | 0.041400887 |
| test/episodes                  | 4060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.168      |
| test/info_shaping_reward_mean  | -0.241      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.718124  |
| test/Q_plus_P                  | -39.718124  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162400      |
| train/episodes                 | 16240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.349      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 649600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 406         |
| stats_o/mean                   | 0.4140263   |
| stats_o/std                    | 0.041398637 |
| test/episodes                  | 4070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.162      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.318      |
| test/Q                         | -39.713547  |
| test/Q_plus_P                  | -39.713547  |
| test/reward_per_eps            | -40         |
| test/steps                     | 162800      |
| train/episodes                 | 16280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.175      |
| train/info_shaping_reward_mean | -0.249      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 651200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 407         |
| stats_o/mean                   | 0.41402197  |
| stats_o/std                    | 0.041395802 |
| test/episodes                  | 4080        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.799328  |
| test/Q_plus_P                  | -39.799328  |
| test/reward_per_eps            | -40         |
| test/steps                     | 163200      |
| train/episodes                 | 16320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.154      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 652800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 408         |
| stats_o/mean                   | 0.41402742  |
| stats_o/std                    | 0.041394684 |
| test/episodes                  | 4090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.187      |
| test/info_shaping_reward_mean  | -0.253      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -39.761703  |
| test/Q_plus_P                  | -39.761703  |
| test/reward_per_eps            | -40         |
| test/steps                     | 163600      |
| train/episodes                 | 16360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 654400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 409         |
| stats_o/mean                   | 0.41401926  |
| stats_o/std                    | 0.041393943 |
| test/episodes                  | 4100        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.165      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -39.717735  |
| test/Q_plus_P                  | -39.717735  |
| test/reward_per_eps            | -40         |
| test/steps                     | 164000      |
| train/episodes                 | 16400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 656000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 410         |
| stats_o/mean                   | 0.41401243  |
| stats_o/std                    | 0.041388594 |
| test/episodes                  | 4110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.22       |
| test/info_shaping_reward_min   | -0.257      |
| test/Q                         | -39.772972  |
| test/Q_plus_P                  | -39.772972  |
| test/reward_per_eps            | -40         |
| test/steps                     | 164400      |
| train/episodes                 | 16440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 657600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 411        |
| stats_o/mean                   | 0.41399384 |
| stats_o/std                    | 0.04138859 |
| test/episodes                  | 4120       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -39.726677 |
| test/Q_plus_P                  | -39.726677 |
| test/reward_per_eps            | -40        |
| test/steps                     | 164800     |
| train/episodes                 | 16480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.352     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 659200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 412         |
| stats_o/mean                   | 0.4139931   |
| stats_o/std                    | 0.041388925 |
| test/episodes                  | 4130        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.752598  |
| test/Q_plus_P                  | -39.752598  |
| test/reward_per_eps            | -40         |
| test/steps                     | 165200      |
| train/episodes                 | 16520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 660800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 413        |
| stats_o/mean                   | 0.41398212 |
| stats_o/std                    | 0.04139217 |
| test/episodes                  | 4140       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.15      |
| test/info_shaping_reward_mean  | -0.234     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.75326  |
| test/Q_plus_P                  | -39.75326  |
| test/reward_per_eps            | -40        |
| test/steps                     | 165600     |
| train/episodes                 | 16560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.242     |
| train/info_shaping_reward_min  | -0.347     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 662400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 414         |
| stats_o/mean                   | 0.41397512  |
| stats_o/std                    | 0.041393895 |
| test/episodes                  | 4150        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.765472  |
| test/Q_plus_P                  | -39.765472  |
| test/reward_per_eps            | -40         |
| test/steps                     | 166000      |
| train/episodes                 | 16600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 664000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 415        |
| stats_o/mean                   | 0.41395935 |
| stats_o/std                    | 0.04139192 |
| test/episodes                  | 4160       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.166     |
| test/info_shaping_reward_mean  | -0.228     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -39.75596  |
| test/Q_plus_P                  | -39.75596  |
| test/reward_per_eps            | -40        |
| test/steps                     | 166400     |
| train/episodes                 | 16640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.156     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.335     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 665600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 416         |
| stats_o/mean                   | 0.41396716  |
| stats_o/std                    | 0.041389976 |
| test/episodes                  | 4170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.247      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.7703    |
| test/Q_plus_P                  | -39.7703    |
| test/reward_per_eps            | -40         |
| test/steps                     | 166800      |
| train/episodes                 | 16680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 667200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 417         |
| stats_o/mean                   | 0.4139718   |
| stats_o/std                    | 0.041389126 |
| test/episodes                  | 4180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.166      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.759056  |
| test/Q_plus_P                  | -39.759056  |
| test/reward_per_eps            | -40         |
| test/steps                     | 167200      |
| train/episodes                 | 16720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 668800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 418         |
| stats_o/mean                   | 0.41397274  |
| stats_o/std                    | 0.041387103 |
| test/episodes                  | 4190        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.186      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.767166  |
| test/Q_plus_P                  | -39.767166  |
| test/reward_per_eps            | -40         |
| test/steps                     | 167600      |
| train/episodes                 | 16760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 670400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 419         |
| stats_o/mean                   | 0.41396952  |
| stats_o/std                    | 0.041384123 |
| test/episodes                  | 4200        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.70337   |
| test/Q_plus_P                  | -39.70337   |
| test/reward_per_eps            | -40         |
| test/steps                     | 168000      |
| train/episodes                 | 16800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 672000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 420         |
| stats_o/mean                   | 0.4139522   |
| stats_o/std                    | 0.041386016 |
| test/episodes                  | 4210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.182      |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.333      |
| test/Q                         | -39.759533  |
| test/Q_plus_P                  | -39.759533  |
| test/reward_per_eps            | -40         |
| test/steps                     | 168400      |
| train/episodes                 | 16840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.251      |
| train/info_shaping_reward_min  | -0.355      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 673600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 421        |
| stats_o/mean                   | 0.41393375 |
| stats_o/std                    | 0.04137858 |
| test/episodes                  | 4220       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.156     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -39.771088 |
| test/Q_plus_P                  | -39.771088 |
| test/reward_per_eps            | -40        |
| test/steps                     | 168800     |
| train/episodes                 | 16880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.241     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 675200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 422         |
| stats_o/mean                   | 0.4139222   |
| stats_o/std                    | 0.041369848 |
| test/episodes                  | 4230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.23       |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.763584  |
| test/Q_plus_P                  | -39.763584  |
| test/reward_per_eps            | -40         |
| test/steps                     | 169200      |
| train/episodes                 | 16920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 676800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 423         |
| stats_o/mean                   | 0.41390803  |
| stats_o/std                    | 0.041367747 |
| test/episodes                  | 4240        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.793602  |
| test/Q_plus_P                  | -39.793602  |
| test/reward_per_eps            | -40         |
| test/steps                     | 169600      |
| train/episodes                 | 16960       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 678400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 424         |
| stats_o/mean                   | 0.41389716  |
| stats_o/std                    | 0.041365467 |
| test/episodes                  | 4250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.191      |
| test/info_shaping_reward_mean  | -0.236      |
| test/info_shaping_reward_min   | -0.3        |
| test/Q                         | -39.773758  |
| test/Q_plus_P                  | -39.773758  |
| test/reward_per_eps            | -40         |
| test/steps                     | 170000      |
| train/episodes                 | 17000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 680000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 425         |
| stats_o/mean                   | 0.41389486  |
| stats_o/std                    | 0.041365184 |
| test/episodes                  | 4260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.784985  |
| test/Q_plus_P                  | -39.784985  |
| test/reward_per_eps            | -40         |
| test/steps                     | 170400      |
| train/episodes                 | 17040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 681600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 426        |
| stats_o/mean                   | 0.413905   |
| stats_o/std                    | 0.04135987 |
| test/episodes                  | 4270       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.163     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.276     |
| test/Q                         | -39.75289  |
| test/Q_plus_P                  | -39.75289  |
| test/reward_per_eps            | -40        |
| test/steps                     | 170800     |
| train/episodes                 | 17080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 683200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 427         |
| stats_o/mean                   | 0.41391173  |
| stats_o/std                    | 0.041362796 |
| test/episodes                  | 4280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.161      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.75393   |
| test/Q_plus_P                  | -39.75393   |
| test/reward_per_eps            | -40         |
| test/steps                     | 171200      |
| train/episodes                 | 17120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 684800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 428         |
| stats_o/mean                   | 0.41391826  |
| stats_o/std                    | 0.041358847 |
| test/episodes                  | 4290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.159      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.255      |
| test/Q                         | -39.710552  |
| test/Q_plus_P                  | -39.710552  |
| test/reward_per_eps            | -40         |
| test/steps                     | 171600      |
| train/episodes                 | 17160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.302      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 686400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 429         |
| stats_o/mean                   | 0.41391847  |
| stats_o/std                    | 0.041359898 |
| test/episodes                  | 4300        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.116      |
| test/info_shaping_reward_mean  | -0.202      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -39.843014  |
| test/Q_plus_P                  | -39.843014  |
| test/reward_per_eps            | -40         |
| test/steps                     | 172000      |
| train/episodes                 | 17200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 688000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 430        |
| stats_o/mean                   | 0.41391847 |
| stats_o/std                    | 0.04135446 |
| test/episodes                  | 4310       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.294     |
| test/Q                         | -39.776478 |
| test/Q_plus_P                  | -39.776478 |
| test/reward_per_eps            | -40        |
| test/steps                     | 172400     |
| train/episodes                 | 17240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.145     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.321     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 689600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 431         |
| stats_o/mean                   | 0.41391566  |
| stats_o/std                    | 0.041354917 |
| test/episodes                  | 4320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.133      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.78508   |
| test/Q_plus_P                  | -39.78508   |
| test/reward_per_eps            | -40         |
| test/steps                     | 172800      |
| train/episodes                 | 17280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.156      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 691200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 432        |
| stats_o/mean                   | 0.41391328 |
| stats_o/std                    | 0.04135798 |
| test/episodes                  | 4330       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.134     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -39.81283  |
| test/Q_plus_P                  | -39.81283  |
| test/reward_per_eps            | -40        |
| test/steps                     | 173200     |
| train/episodes                 | 17320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.317     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 692800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 433         |
| stats_o/mean                   | 0.41393164  |
| stats_o/std                    | 0.041356176 |
| test/episodes                  | 4340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.81086   |
| test/Q_plus_P                  | -39.81086   |
| test/reward_per_eps            | -40         |
| test/steps                     | 173600      |
| train/episodes                 | 17360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.124      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 694400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 434        |
| stats_o/mean                   | 0.41391107 |
| stats_o/std                    | 0.04135512 |
| test/episodes                  | 4350       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.147     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.835697 |
| test/Q_plus_P                  | -39.835697 |
| test/reward_per_eps            | -40        |
| test/steps                     | 174000     |
| train/episodes                 | 17400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.246     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 696000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 435        |
| stats_o/mean                   | 0.41390732 |
| stats_o/std                    | 0.04135632 |
| test/episodes                  | 4360       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.820698 |
| test/Q_plus_P                  | -39.820698 |
| test/reward_per_eps            | -40        |
| test/steps                     | 174400     |
| train/episodes                 | 17440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 697600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 436        |
| stats_o/mean                   | 0.4139044  |
| stats_o/std                    | 0.04135784 |
| test/episodes                  | 4370       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.8906   |
| test/Q_plus_P                  | -39.8906   |
| test/reward_per_eps            | -40        |
| test/steps                     | 174800     |
| train/episodes                 | 17480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.331     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 699200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 437        |
| stats_o/mean                   | 0.41390228 |
| stats_o/std                    | 0.04135477 |
| test/episodes                  | 4380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.151     |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.26      |
| test/Q                         | -39.802505 |
| test/Q_plus_P                  | -39.802505 |
| test/reward_per_eps            | -40        |
| test/steps                     | 175200     |
| train/episodes                 | 17520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 700800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 438         |
| stats_o/mean                   | 0.4139125   |
| stats_o/std                    | 0.041355122 |
| test/episodes                  | 4390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.182      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.809547  |
| test/Q_plus_P                  | -39.809547  |
| test/reward_per_eps            | -40         |
| test/steps                     | 175600      |
| train/episodes                 | 17560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.115      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.32       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 702400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 439         |
| stats_o/mean                   | 0.41391575  |
| stats_o/std                    | 0.041352537 |
| test/episodes                  | 4400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.211      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.78341   |
| test/Q_plus_P                  | -39.78341   |
| test/reward_per_eps            | -40         |
| test/steps                     | 176000      |
| train/episodes                 | 17600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 704000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 440        |
| stats_o/mean                   | 0.4139323  |
| stats_o/std                    | 0.04135107 |
| test/episodes                  | 4410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.137     |
| test/info_shaping_reward_mean  | -0.201     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.765324 |
| test/Q_plus_P                  | -39.765324 |
| test/reward_per_eps            | -40        |
| test/steps                     | 176400     |
| train/episodes                 | 17640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.137     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.308     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 705600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 441        |
| stats_o/mean                   | 0.413944   |
| stats_o/std                    | 0.04135554 |
| test/episodes                  | 4420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.13      |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.295     |
| test/Q                         | -39.76199  |
| test/Q_plus_P                  | -39.76199  |
| test/reward_per_eps            | -40        |
| test/steps                     | 176800     |
| train/episodes                 | 17680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.213     |
| train/info_shaping_reward_min  | -0.3       |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 707200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 442         |
| stats_o/mean                   | 0.4139441   |
| stats_o/std                    | 0.041356202 |
| test/episodes                  | 4430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.1        |
| test/info_shaping_reward_mean  | -0.189      |
| test/info_shaping_reward_min   | -0.27       |
| test/Q                         | -39.807735  |
| test/Q_plus_P                  | -39.807735  |
| test/reward_per_eps            | -40         |
| test/steps                     | 177200      |
| train/episodes                 | 17720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 708800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 443         |
| stats_o/mean                   | 0.41394672  |
| stats_o/std                    | 0.041351385 |
| test/episodes                  | 4440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.788227  |
| test/Q_plus_P                  | -39.788227  |
| test/reward_per_eps            | -40         |
| test/steps                     | 177600      |
| train/episodes                 | 17760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.323      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 710400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 444         |
| stats_o/mean                   | 0.41393265  |
| stats_o/std                    | 0.041346677 |
| test/episodes                  | 4450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.77311   |
| test/Q_plus_P                  | -39.77311   |
| test/reward_per_eps            | -40         |
| test/steps                     | 178000      |
| train/episodes                 | 17800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 712000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 445         |
| stats_o/mean                   | 0.4139347   |
| stats_o/std                    | 0.041345052 |
| test/episodes                  | 4460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.80098   |
| test/Q_plus_P                  | -39.80098   |
| test/reward_per_eps            | -40         |
| test/steps                     | 178400      |
| train/episodes                 | 17840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 713600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 446         |
| stats_o/mean                   | 0.4139394   |
| stats_o/std                    | 0.041341633 |
| test/episodes                  | 4470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.113      |
| test/info_shaping_reward_mean  | -0.207      |
| test/info_shaping_reward_min   | -0.273      |
| test/Q                         | -39.805775  |
| test/Q_plus_P                  | -39.805775  |
| test/reward_per_eps            | -40         |
| test/steps                     | 178800      |
| train/episodes                 | 17880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 715200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 447         |
| stats_o/mean                   | 0.4139401   |
| stats_o/std                    | 0.041333485 |
| test/episodes                  | 4480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.285      |
| test/Q                         | -39.864002  |
| test/Q_plus_P                  | -39.864002  |
| test/reward_per_eps            | -40         |
| test/steps                     | 179200      |
| train/episodes                 | 17920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.148      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 716800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 448        |
| stats_o/mean                   | 0.413951   |
| stats_o/std                    | 0.04133349 |
| test/episodes                  | 4490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.27      |
| test/Q                         | -39.852562 |
| test/Q_plus_P                  | -39.852562 |
| test/reward_per_eps            | -40        |
| test/steps                     | 179600     |
| train/episodes                 | 17960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 718400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 449         |
| stats_o/mean                   | 0.41395903  |
| stats_o/std                    | 0.041327786 |
| test/episodes                  | 4500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.13       |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.81238   |
| test/Q_plus_P                  | -39.81238   |
| test/reward_per_eps            | -40         |
| test/steps                     | 180000      |
| train/episodes                 | 18000       |
| train/info_is_success_max      | 0.1         |
| train/info_is_success_mean     | 0.00375     |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.305      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -39.9       |
| train/steps                    | 720000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 450        |
| stats_o/mean                   | 0.4139626  |
| stats_o/std                    | 0.04133488 |
| test/episodes                  | 4510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0922    |
| test/info_shaping_reward_mean  | -0.196     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.81027  |
| test/Q_plus_P                  | -39.81027  |
| test/reward_per_eps            | -40        |
| test/steps                     | 180400     |
| train/episodes                 | 18040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.116     |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 721600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 451        |
| stats_o/mean                   | 0.41395974 |
| stats_o/std                    | 0.04132966 |
| test/episodes                  | 4520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.121     |
| test/info_shaping_reward_mean  | -0.209     |
| test/info_shaping_reward_min   | -0.268     |
| test/Q                         | -39.84281  |
| test/Q_plus_P                  | -39.84281  |
| test/reward_per_eps            | -40        |
| test/steps                     | 180800     |
| train/episodes                 | 18080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 723200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 452        |
| stats_o/mean                   | 0.41396704 |
| stats_o/std                    | 0.04133636 |
| test/episodes                  | 4530       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.105     |
| test/info_shaping_reward_mean  | -0.182     |
| test/info_shaping_reward_min   | -0.254     |
| test/Q                         | -39.827103 |
| test/Q_plus_P                  | -39.827103 |
| test/reward_per_eps            | -40        |
| test/steps                     | 181200     |
| train/episodes                 | 18120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.102     |
| train/info_shaping_reward_mean | -0.206     |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 724800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 453         |
| stats_o/mean                   | 0.41397575  |
| stats_o/std                    | 0.041340914 |
| test/episodes                  | 4540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.804867  |
| test/Q_plus_P                  | -39.804867  |
| test/reward_per_eps            | -40         |
| test/steps                     | 181600      |
| train/episodes                 | 18160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.309      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 726400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 454         |
| stats_o/mean                   | 0.41399238  |
| stats_o/std                    | 0.041340098 |
| test/episodes                  | 4550        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.108      |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.810883  |
| test/Q_plus_P                  | -39.810883  |
| test/reward_per_eps            | -40         |
| test/steps                     | 182000      |
| train/episodes                 | 18200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.21       |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 728000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 455         |
| stats_o/mean                   | 0.41400647  |
| stats_o/std                    | 0.041340943 |
| test/episodes                  | 4560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0996     |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.308      |
| test/Q                         | -39.820446  |
| test/Q_plus_P                  | -39.820446  |
| test/reward_per_eps            | -40         |
| test/steps                     | 182400      |
| train/episodes                 | 18240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 729600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 456        |
| stats_o/mean                   | 0.4140108  |
| stats_o/std                    | 0.04134386 |
| test/episodes                  | 4570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.281     |
| test/Q                         | -39.81715  |
| test/Q_plus_P                  | -39.81715  |
| test/reward_per_eps            | -40        |
| test/steps                     | 182800     |
| train/episodes                 | 18280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 731200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 457         |
| stats_o/mean                   | 0.41400075  |
| stats_o/std                    | 0.041342217 |
| test/episodes                  | 4580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.139      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.832973  |
| test/Q_plus_P                  | -39.832973  |
| test/reward_per_eps            | -40         |
| test/steps                     | 183200      |
| train/episodes                 | 18320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 732800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 458        |
| stats_o/mean                   | 0.41398457 |
| stats_o/std                    | 0.04135049 |
| test/episodes                  | 4590       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.114     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.315     |
| test/Q                         | -39.817814 |
| test/Q_plus_P                  | -39.817814 |
| test/reward_per_eps            | -40        |
| test/steps                     | 183600     |
| train/episodes                 | 18360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.119     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 734400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 459         |
| stats_o/mean                   | 0.41398108  |
| stats_o/std                    | 0.041341268 |
| test/episodes                  | 4600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.101      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.843197  |
| test/Q_plus_P                  | -39.843197  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184000      |
| train/episodes                 | 18400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.214      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 736000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 460         |
| stats_o/mean                   | 0.4139717   |
| stats_o/std                    | 0.041341536 |
| test/episodes                  | 4610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.828537  |
| test/Q_plus_P                  | -39.828537  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184400      |
| train/episodes                 | 18440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 737600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 461         |
| stats_o/mean                   | 0.41396156  |
| stats_o/std                    | 0.041340206 |
| test/episodes                  | 4620        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.122      |
| test/info_shaping_reward_mean  | -0.196      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -39.898617  |
| test/Q_plus_P                  | -39.898617  |
| test/reward_per_eps            | -40         |
| test/steps                     | 184800      |
| train/episodes                 | 18480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 739200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 462         |
| stats_o/mean                   | 0.41396615  |
| stats_o/std                    | 0.041341137 |
| test/episodes                  | 4630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.11       |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.820717  |
| test/Q_plus_P                  | -39.820717  |
| test/reward_per_eps            | -40         |
| test/steps                     | 185200      |
| train/episodes                 | 18520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 740800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 463        |
| stats_o/mean                   | 0.41397867 |
| stats_o/std                    | 0.04134096 |
| test/episodes                  | 4640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.104     |
| test/info_shaping_reward_mean  | -0.193     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -39.831696 |
| test/Q_plus_P                  | -39.831696 |
| test/reward_per_eps            | -40        |
| test/steps                     | 185600     |
| train/episodes                 | 18560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.114     |
| train/info_shaping_reward_mean | -0.21      |
| train/info_shaping_reward_min  | -0.305     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 742400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 464         |
| stats_o/mean                   | 0.41398537  |
| stats_o/std                    | 0.041339483 |
| test/episodes                  | 4650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.106      |
| test/info_shaping_reward_mean  | -0.184      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -39.814526  |
| test/Q_plus_P                  | -39.814526  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186000      |
| train/episodes                 | 18600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.108      |
| train/info_shaping_reward_mean | -0.202      |
| train/info_shaping_reward_min  | -0.287      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 744000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 465         |
| stats_o/mean                   | 0.4139819   |
| stats_o/std                    | 0.041341294 |
| test/episodes                  | 4660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.209      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.826748  |
| test/Q_plus_P                  | -39.826748  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186400      |
| train/episodes                 | 18640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 745600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 466         |
| stats_o/mean                   | 0.41397908  |
| stats_o/std                    | 0.041345764 |
| test/episodes                  | 4670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.1        |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.263      |
| test/Q                         | -39.868042  |
| test/Q_plus_P                  | -39.868042  |
| test/reward_per_eps            | -40         |
| test/steps                     | 186800      |
| train/episodes                 | 18680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 747200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 467        |
| stats_o/mean                   | 0.4139906  |
| stats_o/std                    | 0.04134823 |
| test/episodes                  | 4680       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -39.80286  |
| test/Q_plus_P                  | -39.80286  |
| test/reward_per_eps            | -40        |
| test/steps                     | 187200     |
| train/episodes                 | 18720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 748800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 468         |
| stats_o/mean                   | 0.41400632  |
| stats_o/std                    | 0.041350927 |
| test/episodes                  | 4690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -39.827606  |
| test/Q_plus_P                  | -39.827606  |
| test/reward_per_eps            | -40         |
| test/steps                     | 187600      |
| train/episodes                 | 18760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.307      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 750400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 469         |
| stats_o/mean                   | 0.41400743  |
| stats_o/std                    | 0.041345768 |
| test/episodes                  | 4700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0978     |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.82328   |
| test/Q_plus_P                  | -39.82328   |
| test/reward_per_eps            | -40         |
| test/steps                     | 188000      |
| train/episodes                 | 18800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 752000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 470         |
| stats_o/mean                   | 0.41404012  |
| stats_o/std                    | 0.041357446 |
| test/episodes                  | 4710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0996     |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.313      |
| test/Q                         | -39.83887   |
| test/Q_plus_P                  | -39.83887   |
| test/reward_per_eps            | -40         |
| test/steps                     | 188400      |
| train/episodes                 | 18840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.103      |
| train/info_shaping_reward_mean | -0.204      |
| train/info_shaping_reward_min  | -0.293      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 753600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 471        |
| stats_o/mean                   | 0.4140484  |
| stats_o/std                    | 0.04136149 |
| test/episodes                  | 4720       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.122     |
| test/info_shaping_reward_mean  | -0.191     |
| test/info_shaping_reward_min   | -0.261     |
| test/Q                         | -39.838505 |
| test/Q_plus_P                  | -39.838505 |
| test/reward_per_eps            | -40        |
| test/steps                     | 188800     |
| train/episodes                 | 18880      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.223     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 755200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 472        |
| stats_o/mean                   | 0.41404963 |
| stats_o/std                    | 0.0413629  |
| test/episodes                  | 4730       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.119     |
| test/info_shaping_reward_mean  | -0.194     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.789257 |
| test/Q_plus_P                  | -39.789257 |
| test/reward_per_eps            | -40        |
| test/steps                     | 189200     |
| train/episodes                 | 18920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 756800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 473        |
| stats_o/mean                   | 0.41404915 |
| stats_o/std                    | 0.04136785 |
| test/episodes                  | 4740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.111     |
| test/info_shaping_reward_mean  | -0.189     |
| test/info_shaping_reward_min   | -0.253     |
| test/Q                         | -39.83308  |
| test/Q_plus_P                  | -39.83308  |
| test/reward_per_eps            | -40        |
| test/steps                     | 189600     |
| train/episodes                 | 18960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.138     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 758400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 474         |
| stats_o/mean                   | 0.41405216  |
| stats_o/std                    | 0.041369874 |
| test/episodes                  | 4750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.108      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.88619   |
| test/Q_plus_P                  | -39.88619   |
| test/reward_per_eps            | -40         |
| test/steps                     | 190000      |
| train/episodes                 | 19000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.318      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 760000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 475        |
| stats_o/mean                   | 0.41405097 |
| stats_o/std                    | 0.04137128 |
| test/episodes                  | 4760       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0826    |
| test/info_shaping_reward_mean  | -0.207     |
| test/info_shaping_reward_min   | -0.287     |
| test/Q                         | -39.792564 |
| test/Q_plus_P                  | -39.792564 |
| test/reward_per_eps            | -40        |
| test/steps                     | 190400     |
| train/episodes                 | 19040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.11      |
| train/info_shaping_reward_mean | -0.211     |
| train/info_shaping_reward_min  | -0.318     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 761600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 476         |
| stats_o/mean                   | 0.41405538  |
| stats_o/std                    | 0.041373726 |
| test/episodes                  | 4770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.13       |
| test/info_shaping_reward_mean  | -0.202      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.851025  |
| test/Q_plus_P                  | -39.851025  |
| test/reward_per_eps            | -40         |
| test/steps                     | 190800      |
| train/episodes                 | 19080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 763200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 477         |
| stats_o/mean                   | 0.41405097  |
| stats_o/std                    | 0.041370533 |
| test/episodes                  | 4780        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0837     |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -39.864853  |
| test/Q_plus_P                  | -39.864853  |
| test/reward_per_eps            | -40         |
| test/steps                     | 191200      |
| train/episodes                 | 19120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.123      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 764800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 478        |
| stats_o/mean                   | 0.41404906 |
| stats_o/std                    | 0.04137331 |
| test/episodes                  | 4790       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.095     |
| test/info_shaping_reward_mean  | -0.214     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.86532  |
| test/Q_plus_P                  | -39.86532  |
| test/reward_per_eps            | -40        |
| test/steps                     | 191600     |
| train/episodes                 | 19160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.105     |
| train/info_shaping_reward_mean | -0.212     |
| train/info_shaping_reward_min  | -0.311     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 766400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 479        |
| stats_o/mean                   | 0.4140432  |
| stats_o/std                    | 0.04137424 |
| test/episodes                  | 4800       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0999    |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.29      |
| test/Q                         | -39.854595 |
| test/Q_plus_P                  | -39.854595 |
| test/reward_per_eps            | -40        |
| test/steps                     | 192000     |
| train/episodes                 | 19200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.314     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 768000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 480         |
| stats_o/mean                   | 0.41405663  |
| stats_o/std                    | 0.041382033 |
| test/episodes                  | 4810        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0954     |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.274      |
| test/Q                         | -39.860645  |
| test/Q_plus_P                  | -39.860645  |
| test/reward_per_eps            | -40         |
| test/steps                     | 192400      |
| train/episodes                 | 19240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.128      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 769600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 481        |
| stats_o/mean                   | 0.41406593 |
| stats_o/std                    | 0.04138048 |
| test/episodes                  | 4820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0672    |
| test/info_shaping_reward_mean  | -0.174     |
| test/info_shaping_reward_min   | -0.247     |
| test/Q                         | -39.865128 |
| test/Q_plus_P                  | -39.865128 |
| test/reward_per_eps            | -40        |
| test/steps                     | 192800     |
| train/episodes                 | 19280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.107     |
| train/info_shaping_reward_mean | -0.21      |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 771200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 482         |
| stats_o/mean                   | 0.41407084  |
| stats_o/std                    | 0.041379638 |
| test/episodes                  | 4830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.142      |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.86734   |
| test/Q_plus_P                  | -39.86734   |
| test/reward_per_eps            | -40         |
| test/steps                     | 193200      |
| train/episodes                 | 19320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.212      |
| train/info_shaping_reward_min  | -0.326      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 772800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 483        |
| stats_o/mean                   | 0.41406825 |
| stats_o/std                    | 0.04138119 |
| test/episodes                  | 4840       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.12      |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -39.88234  |
| test/Q_plus_P                  | -39.88234  |
| test/reward_per_eps            | -40        |
| test/steps                     | 193600     |
| train/episodes                 | 19360      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.299     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 774400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 484        |
| stats_o/mean                   | 0.41406187 |
| stats_o/std                    | 0.04137751 |
| test/episodes                  | 4850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.906933 |
| test/Q_plus_P                  | -39.906933 |
| test/reward_per_eps            | -40        |
| test/steps                     | 194000     |
| train/episodes                 | 19400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.131     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 776000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 485        |
| stats_o/mean                   | 0.41406295 |
| stats_o/std                    | 0.04138464 |
| test/episodes                  | 4860       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0927    |
| test/info_shaping_reward_mean  | -0.196     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -39.91294  |
| test/Q_plus_P                  | -39.91294  |
| test/reward_per_eps            | -40        |
| test/steps                     | 194400     |
| train/episodes                 | 19440      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.239     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 777600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 486         |
| stats_o/mean                   | 0.41406932  |
| stats_o/std                    | 0.041383814 |
| test/episodes                  | 4870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.883987  |
| test/Q_plus_P                  | -39.883987  |
| test/reward_per_eps            | -40         |
| test/steps                     | 194800      |
| train/episodes                 | 19480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.306      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 779200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 487        |
| stats_o/mean                   | 0.41406325 |
| stats_o/std                    | 0.041384   |
| test/episodes                  | 4880       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.124     |
| test/info_shaping_reward_mean  | -0.219     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -39.92413  |
| test/Q_plus_P                  | -39.92413  |
| test/reward_per_eps            | -40        |
| test/steps                     | 195200     |
| train/episodes                 | 19520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.115     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.322     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 780800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 488         |
| stats_o/mean                   | 0.4140625   |
| stats_o/std                    | 0.041380007 |
| test/episodes                  | 4890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0926     |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.857304  |
| test/Q_plus_P                  | -39.857304  |
| test/reward_per_eps            | -40         |
| test/steps                     | 195600      |
| train/episodes                 | 19560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.325      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 782400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 489        |
| stats_o/mean                   | 0.41407916 |
| stats_o/std                    | 0.04138014 |
| test/episodes                  | 4900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.106     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.291     |
| test/Q                         | -39.83023  |
| test/Q_plus_P                  | -39.83023  |
| test/reward_per_eps            | -40        |
| test/steps                     | 196000     |
| train/episodes                 | 19600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.123     |
| train/info_shaping_reward_mean | -0.217     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 784000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 490         |
| stats_o/mean                   | 0.41406628  |
| stats_o/std                    | 0.041379053 |
| test/episodes                  | 4910        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.117      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.871147  |
| test/Q_plus_P                  | -39.871147  |
| test/reward_per_eps            | -40         |
| test/steps                     | 196400      |
| train/episodes                 | 19640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.15       |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 785600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 491         |
| stats_o/mean                   | 0.41407105  |
| stats_o/std                    | 0.041380163 |
| test/episodes                  | 4920        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.113      |
| test/info_shaping_reward_mean  | -0.191      |
| test/info_shaping_reward_min   | -0.264      |
| test/Q                         | -39.876762  |
| test/Q_plus_P                  | -39.876762  |
| test/reward_per_eps            | -40         |
| test/steps                     | 196800      |
| train/episodes                 | 19680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.111      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.322      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 787200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 492        |
| stats_o/mean                   | 0.41406962 |
| stats_o/std                    | 0.041388   |
| test/episodes                  | 4930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0925    |
| test/info_shaping_reward_mean  | -0.199     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.89331  |
| test/Q_plus_P                  | -39.89331  |
| test/reward_per_eps            | -40        |
| test/steps                     | 197200     |
| train/episodes                 | 19720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.117     |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 788800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 493         |
| stats_o/mean                   | 0.41407773  |
| stats_o/std                    | 0.041395497 |
| test/episodes                  | 4940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.189      |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.87639   |
| test/Q_plus_P                  | -39.87639   |
| test/reward_per_eps            | -40         |
| test/steps                     | 197600      |
| train/episodes                 | 19760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 790400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 494        |
| stats_o/mean                   | 0.414077   |
| stats_o/std                    | 0.04139839 |
| test/episodes                  | 4950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.127     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.302     |
| test/Q                         | -39.910213 |
| test/Q_plus_P                  | -39.910213 |
| test/reward_per_eps            | -40        |
| test/steps                     | 198000     |
| train/episodes                 | 19800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.118     |
| train/info_shaping_reward_mean | -0.218     |
| train/info_shaping_reward_min  | -0.315     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 792000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 495         |
| stats_o/mean                   | 0.41408083  |
| stats_o/std                    | 0.041398678 |
| test/episodes                  | 4960        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0826     |
| test/info_shaping_reward_mean  | -0.197      |
| test/info_shaping_reward_min   | -0.29       |
| test/Q                         | -39.904945  |
| test/Q_plus_P                  | -39.904945  |
| test/reward_per_eps            | -40         |
| test/steps                     | 198400      |
| train/episodes                 | 19840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.227      |
| train/info_shaping_reward_min  | -0.34       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 793600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 496         |
| stats_o/mean                   | 0.4140862   |
| stats_o/std                    | 0.041397955 |
| test/episodes                  | 4970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.21       |
| test/info_shaping_reward_min   | -0.282      |
| test/Q                         | -39.88869   |
| test/Q_plus_P                  | -39.88869   |
| test/reward_per_eps            | -40         |
| test/steps                     | 198800      |
| train/episodes                 | 19880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.295      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 795200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 497        |
| stats_o/mean                   | 0.41408002 |
| stats_o/std                    | 0.04139673 |
| test/episodes                  | 4980       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.195     |
| test/info_shaping_reward_min   | -0.252     |
| test/Q                         | -39.88954  |
| test/Q_plus_P                  | -39.88954  |
| test/reward_per_eps            | -40        |
| test/steps                     | 199200     |
| train/episodes                 | 19920      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.126     |
| train/info_shaping_reward_mean | -0.214     |
| train/info_shaping_reward_min  | -0.301     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 796800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 498        |
| stats_o/mean                   | 0.414075   |
| stats_o/std                    | 0.041393   |
| test/episodes                  | 4990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.165     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -39.913124 |
| test/Q_plus_P                  | -39.913124 |
| test/reward_per_eps            | -40        |
| test/steps                     | 199600     |
| train/episodes                 | 19960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 798400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 499         |
| stats_o/mean                   | 0.414066    |
| stats_o/std                    | 0.041398253 |
| test/episodes                  | 5000        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.233      |
| test/info_shaping_reward_min   | -0.278      |
| test/Q                         | -39.894844  |
| test/Q_plus_P                  | -39.894844  |
| test/reward_per_eps            | -40         |
| test/steps                     | 200000      |
| train/episodes                 | 20000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 800000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 500         |
| stats_o/mean                   | 0.4140726   |
| stats_o/std                    | 0.041395783 |
| test/episodes                  | 5010        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.109      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.88499   |
| test/Q_plus_P                  | -39.88499   |
| test/reward_per_eps            | -40         |
| test/steps                     | 200400      |
| train/episodes                 | 20040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.114      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.312      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 801600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 501        |
| stats_o/mean                   | 0.41407433 |
| stats_o/std                    | 0.04139924 |
| test/episodes                  | 5020       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.128     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.311     |
| test/Q                         | -40.037617 |
| test/Q_plus_P                  | -40.037617 |
| test/reward_per_eps            | -40        |
| test/steps                     | 200800     |
| train/episodes                 | 20080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.23      |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 803200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 502        |
| stats_o/mean                   | 0.41406104 |
| stats_o/std                    | 0.04139392 |
| test/episodes                  | 5030       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.0957    |
| test/info_shaping_reward_mean  | -0.21      |
| test/info_shaping_reward_min   | -0.274     |
| test/Q                         | -39.941772 |
| test/Q_plus_P                  | -39.941772 |
| test/reward_per_eps            | -40        |
| test/steps                     | 201200     |
| train/episodes                 | 20120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.22      |
| train/info_shaping_reward_min  | -0.309     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 804800     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 503        |
| stats_o/mean                   | 0.41406012 |
| stats_o/std                    | 0.04139146 |
| test/episodes                  | 5040       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.162     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.325     |
| test/Q                         | -39.988186 |
| test/Q_plus_P                  | -39.988186 |
| test/reward_per_eps            | -40        |
| test/steps                     | 201600     |
| train/episodes                 | 20160      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.226     |
| train/info_shaping_reward_min  | -0.312     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 806400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 504         |
| stats_o/mean                   | 0.41405022  |
| stats_o/std                    | 0.041391272 |
| test/episodes                  | 5050        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.95171   |
| test/Q_plus_P                  | -39.95171   |
| test/reward_per_eps            | -40         |
| test/steps                     | 202000      |
| train/episodes                 | 20200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 808000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 505         |
| stats_o/mean                   | 0.41404915  |
| stats_o/std                    | 0.041388642 |
| test/episodes                  | 5060        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.144      |
| test/info_shaping_reward_mean  | -0.201      |
| test/info_shaping_reward_min   | -0.26       |
| test/Q                         | -39.89368   |
| test/Q_plus_P                  | -39.89368   |
| test/reward_per_eps            | -40         |
| test/steps                     | 202400      |
| train/episodes                 | 20240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.113      |
| train/info_shaping_reward_mean | -0.221      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 809600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 506         |
| stats_o/mean                   | 0.41405857  |
| stats_o/std                    | 0.041390922 |
| test/episodes                  | 5070        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0977     |
| test/info_shaping_reward_mean  | -0.193      |
| test/info_shaping_reward_min   | -0.248      |
| test/Q                         | -39.93793   |
| test/Q_plus_P                  | -39.93793   |
| test/reward_per_eps            | -40         |
| test/steps                     | 202800      |
| train/episodes                 | 20280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.103      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.328      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 811200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 507        |
| stats_o/mean                   | 0.4140421  |
| stats_o/std                    | 0.04139251 |
| test/episodes                  | 5080       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.126     |
| test/info_shaping_reward_mean  | -0.192     |
| test/info_shaping_reward_min   | -0.256     |
| test/Q                         | -39.92855  |
| test/Q_plus_P                  | -39.92855  |
| test/reward_per_eps            | -40        |
| test/steps                     | 203200     |
| train/episodes                 | 20320      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.144     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.31      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 812800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 508         |
| stats_o/mean                   | 0.41402626  |
| stats_o/std                    | 0.041395504 |
| test/episodes                  | 5090        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -39.926376  |
| test/Q_plus_P                  | -39.926376  |
| test/reward_per_eps            | -40         |
| test/steps                     | 203600      |
| train/episodes                 | 20360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.153      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 814400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 509        |
| stats_o/mean                   | 0.41402623 |
| stats_o/std                    | 0.04139731 |
| test/episodes                  | 5100       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.121     |
| test/info_shaping_reward_mean  | -0.211     |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.92806  |
| test/Q_plus_P                  | -39.92806  |
| test/reward_per_eps            | -40        |
| test/steps                     | 204000     |
| train/episodes                 | 20400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.234     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 816000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 510         |
| stats_o/mean                   | 0.41401854  |
| stats_o/std                    | 0.041397683 |
| test/episodes                  | 5110        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.92363   |
| test/Q_plus_P                  | -39.92363   |
| test/reward_per_eps            | -40         |
| test/steps                     | 204400      |
| train/episodes                 | 20440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.229      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 817600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 511         |
| stats_o/mean                   | 0.4140143   |
| stats_o/std                    | 0.041398775 |
| test/episodes                  | 5120        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.143      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.906075  |
| test/Q_plus_P                  | -39.906075  |
| test/reward_per_eps            | -40         |
| test/steps                     | 204800      |
| train/episodes                 | 20480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.335      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 819200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 512        |
| stats_o/mean                   | 0.41401336 |
| stats_o/std                    | 0.04139274 |
| test/episodes                  | 5130       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.948936 |
| test/Q_plus_P                  | -39.948936 |
| test/reward_per_eps            | -40        |
| test/steps                     | 205200     |
| train/episodes                 | 20520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.14      |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.306     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 820800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 513         |
| stats_o/mean                   | 0.41401175  |
| stats_o/std                    | 0.041393947 |
| test/episodes                  | 5140        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.957027  |
| test/Q_plus_P                  | -39.957027  |
| test/reward_per_eps            | -40         |
| test/steps                     | 205600      |
| train/episodes                 | 20560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 822400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 514        |
| stats_o/mean                   | 0.4140055  |
| stats_o/std                    | 0.04139714 |
| test/episodes                  | 5150       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.918285 |
| test/Q_plus_P                  | -39.918285 |
| test/reward_per_eps            | -40        |
| test/steps                     | 206000     |
| train/episodes                 | 20600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.237     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 824000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 515         |
| stats_o/mean                   | 0.41400144  |
| stats_o/std                    | 0.041398097 |
| test/episodes                  | 5160        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.116      |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.284      |
| test/Q                         | -39.95553   |
| test/Q_plus_P                  | -39.95553   |
| test/reward_per_eps            | -40         |
| test/steps                     | 206400      |
| train/episodes                 | 20640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.334      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 825600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 516         |
| stats_o/mean                   | 0.41400325  |
| stats_o/std                    | 0.041400064 |
| test/episodes                  | 5170        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.973614  |
| test/Q_plus_P                  | -39.973614  |
| test/reward_per_eps            | -40         |
| test/steps                     | 206800      |
| train/episodes                 | 20680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.324      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 827200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 517         |
| stats_o/mean                   | 0.41399762  |
| stats_o/std                    | 0.041408557 |
| test/episodes                  | 5180        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.16       |
| test/info_shaping_reward_mean  | -0.214      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.846466  |
| test/Q_plus_P                  | -39.846466  |
| test/reward_per_eps            | -40         |
| test/steps                     | 207200      |
| train/episodes                 | 20720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 828800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 518        |
| stats_o/mean                   | 0.41399884 |
| stats_o/std                    | 0.0414097  |
| test/episodes                  | 5190       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.146     |
| test/info_shaping_reward_mean  | -0.218     |
| test/info_shaping_reward_min   | -0.285     |
| test/Q                         | -39.912037 |
| test/Q_plus_P                  | -39.912037 |
| test/reward_per_eps            | -40        |
| test/steps                     | 207600     |
| train/episodes                 | 20760      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.325     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 830400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 519        |
| stats_o/mean                   | 0.41401705 |
| stats_o/std                    | 0.04141156 |
| test/episodes                  | 5200       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.231     |
| test/info_shaping_reward_min   | -0.289     |
| test/Q                         | -39.932438 |
| test/Q_plus_P                  | -39.932438 |
| test/reward_per_eps            | -40        |
| test/steps                     | 208000     |
| train/episodes                 | 20800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.135     |
| train/info_shaping_reward_mean | -0.221     |
| train/info_shaping_reward_min  | -0.313     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 832000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 520         |
| stats_o/mean                   | 0.41402778  |
| stats_o/std                    | 0.041407835 |
| test/episodes                  | 5210        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.111      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.254      |
| test/Q                         | -39.928253  |
| test/Q_plus_P                  | -39.928253  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208400      |
| train/episodes                 | 20840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.33       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 833600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 521         |
| stats_o/mean                   | 0.41402674  |
| stats_o/std                    | 0.041405834 |
| test/episodes                  | 5220        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.137      |
| test/info_shaping_reward_mean  | -0.2        |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.939735  |
| test/Q_plus_P                  | -39.939735  |
| test/reward_per_eps            | -40         |
| test/steps                     | 208800      |
| train/episodes                 | 20880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.116      |
| train/info_shaping_reward_mean | -0.216      |
| train/info_shaping_reward_min  | -0.31       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 835200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 522         |
| stats_o/mean                   | 0.41403326  |
| stats_o/std                    | 0.041406583 |
| test/episodes                  | 5230        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.0989     |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.306      |
| test/Q                         | -39.898624  |
| test/Q_plus_P                  | -39.898624  |
| test/reward_per_eps            | -40         |
| test/steps                     | 209200      |
| train/episodes                 | 20920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.117      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 836800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 523        |
| stats_o/mean                   | 0.41404417 |
| stats_o/std                    | 0.04140842 |
| test/episodes                  | 5240       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.107     |
| test/info_shaping_reward_mean  | -0.204     |
| test/info_shaping_reward_min   | -0.28      |
| test/Q                         | -39.936886 |
| test/Q_plus_P                  | -39.936886 |
| test/reward_per_eps            | -40        |
| test/steps                     | 209600     |
| train/episodes                 | 20960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.109     |
| train/info_shaping_reward_mean | -0.215     |
| train/info_shaping_reward_min  | -0.324     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 838400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 524         |
| stats_o/mean                   | 0.41404948  |
| stats_o/std                    | 0.041409113 |
| test/episodes                  | 5250        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.136      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.292      |
| test/Q                         | -39.947067  |
| test/Q_plus_P                  | -39.947067  |
| test/reward_per_eps            | -40         |
| test/steps                     | 210000      |
| train/episodes                 | 21000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.118      |
| train/info_shaping_reward_mean | -0.213      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 840000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 525         |
| stats_o/mean                   | 0.41404498  |
| stats_o/std                    | 0.041411933 |
| test/episodes                  | 5260        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.129      |
| test/info_shaping_reward_mean  | -0.203      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.93061   |
| test/Q_plus_P                  | -39.93061   |
| test/reward_per_eps            | -40         |
| test/steps                     | 210400      |
| train/episodes                 | 21040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.126      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 841600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 526         |
| stats_o/mean                   | 0.41403782  |
| stats_o/std                    | 0.041412648 |
| test/episodes                  | 5270        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.154      |
| test/info_shaping_reward_mean  | -0.231      |
| test/info_shaping_reward_min   | -0.305      |
| test/Q                         | -39.911762  |
| test/Q_plus_P                  | -39.911762  |
| test/reward_per_eps            | -40         |
| test/steps                     | 210800      |
| train/episodes                 | 21080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.12       |
| train/info_shaping_reward_mean | -0.22       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 843200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 527         |
| stats_o/mean                   | 0.4140447   |
| stats_o/std                    | 0.041411627 |
| test/episodes                  | 5280        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.119      |
| test/info_shaping_reward_mean  | -0.199      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.86647   |
| test/Q_plus_P                  | -39.86647   |
| test/reward_per_eps            | -40         |
| test/steps                     | 211200      |
| train/episodes                 | 21120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.217      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 844800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 528         |
| stats_o/mean                   | 0.4140434   |
| stats_o/std                    | 0.041406613 |
| test/episodes                  | 5290        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.128      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.293      |
| test/Q                         | -39.951885  |
| test/Q_plus_P                  | -39.951885  |
| test/reward_per_eps            | -40         |
| test/steps                     | 211600      |
| train/episodes                 | 21160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.218      |
| train/info_shaping_reward_min  | -0.314      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 846400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 529        |
| stats_o/mean                   | 0.41404733 |
| stats_o/std                    | 0.04140709 |
| test/episodes                  | 5300       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.132     |
| test/info_shaping_reward_mean  | -0.223     |
| test/info_shaping_reward_min   | -0.305     |
| test/Q                         | -39.916553 |
| test/Q_plus_P                  | -39.916553 |
| test/reward_per_eps            | -40        |
| test/steps                     | 212000     |
| train/episodes                 | 21200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.125     |
| train/info_shaping_reward_mean | -0.229     |
| train/info_shaping_reward_min  | -0.329     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 848000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 530         |
| stats_o/mean                   | 0.4140717   |
| stats_o/std                    | 0.041416045 |
| test/episodes                  | 5310        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.147      |
| test/info_shaping_reward_mean  | -0.218      |
| test/info_shaping_reward_min   | -0.266      |
| test/Q                         | -40.01183   |
| test/Q_plus_P                  | -40.01183   |
| test/reward_per_eps            | -40         |
| test/steps                     | 212400      |
| train/episodes                 | 21240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.139      |
| train/info_shaping_reward_mean | -0.233      |
| train/info_shaping_reward_min  | -0.341      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 849600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 531         |
| stats_o/mean                   | 0.4140816   |
| stats_o/std                    | 0.041410606 |
| test/episodes                  | 5320        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.12       |
| test/info_shaping_reward_mean  | -0.206      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.867577  |
| test/Q_plus_P                  | -39.867577  |
| test/reward_per_eps            | -40         |
| test/steps                     | 212800      |
| train/episodes                 | 21280       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.119      |
| train/info_shaping_reward_mean | -0.211      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 851200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 532         |
| stats_o/mean                   | 0.41406688  |
| stats_o/std                    | 0.041410316 |
| test/episodes                  | 5330        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.172      |
| test/info_shaping_reward_mean  | -0.222      |
| test/info_shaping_reward_min   | -0.288      |
| test/Q                         | -39.93374   |
| test/Q_plus_P                  | -39.93374   |
| test/reward_per_eps            | -40         |
| test/steps                     | 213200      |
| train/episodes                 | 21320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.134      |
| train/info_shaping_reward_mean | -0.222      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 852800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 533         |
| stats_o/mean                   | 0.4140663   |
| stats_o/std                    | 0.041405458 |
| test/episodes                  | 5340        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.138      |
| test/info_shaping_reward_mean  | -0.227      |
| test/info_shaping_reward_min   | -0.275      |
| test/Q                         | -39.90798   |
| test/Q_plus_P                  | -39.90798   |
| test/reward_per_eps            | -40         |
| test/steps                     | 213600      |
| train/episodes                 | 21360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.143      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.315      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 854400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 534         |
| stats_o/mean                   | 0.41406703  |
| stats_o/std                    | 0.041405115 |
| test/episodes                  | 5350        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.123      |
| test/info_shaping_reward_mean  | -0.217      |
| test/info_shaping_reward_min   | -0.317      |
| test/Q                         | -39.917816  |
| test/Q_plus_P                  | -39.917816  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214000      |
| train/episodes                 | 21400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.338      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 856000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 535         |
| stats_o/mean                   | 0.41406226  |
| stats_o/std                    | 0.041404244 |
| test/episodes                  | 5360        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.132      |
| test/info_shaping_reward_mean  | -0.215      |
| test/info_shaping_reward_min   | -0.309      |
| test/Q                         | -39.92257   |
| test/Q_plus_P                  | -39.92257   |
| test/reward_per_eps            | -40         |
| test/steps                     | 214400      |
| train/episodes                 | 21440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.231      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 857600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 536         |
| stats_o/mean                   | 0.41405845  |
| stats_o/std                    | 0.041402563 |
| test/episodes                  | 5370        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.18       |
| test/info_shaping_reward_mean  | -0.242      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.931156  |
| test/Q_plus_P                  | -39.931156  |
| test/reward_per_eps            | -40         |
| test/steps                     | 214800      |
| train/episodes                 | 21480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.141      |
| train/info_shaping_reward_mean | -0.242      |
| train/info_shaping_reward_min  | -0.348      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 859200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 537        |
| stats_o/mean                   | 0.41405156 |
| stats_o/std                    | 0.04140203 |
| test/episodes                  | 5380       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.142     |
| test/info_shaping_reward_mean  | -0.222     |
| test/info_shaping_reward_min   | -0.267     |
| test/Q                         | -39.89467  |
| test/Q_plus_P                  | -39.89467  |
| test/reward_per_eps            | -40        |
| test/steps                     | 215200     |
| train/episodes                 | 21520      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.228     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 860800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 538         |
| stats_o/mean                   | 0.4140437   |
| stats_o/std                    | 0.041393247 |
| test/episodes                  | 5390        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.279      |
| test/Q                         | -39.926483  |
| test/Q_plus_P                  | -39.926483  |
| test/reward_per_eps            | -40         |
| test/steps                     | 215600      |
| train/episodes                 | 21560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.226      |
| train/info_shaping_reward_min  | -0.313      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 862400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 539         |
| stats_o/mean                   | 0.41403592  |
| stats_o/std                    | 0.041396096 |
| test/episodes                  | 5400        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.135      |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.246      |
| test/Q                         | -39.92921   |
| test/Q_plus_P                  | -39.92921   |
| test/reward_per_eps            | -40         |
| test/steps                     | 216000      |
| train/episodes                 | 21600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.246      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 864000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 540        |
| stats_o/mean                   | 0.41404116 |
| stats_o/std                    | 0.04139487 |
| test/episodes                  | 5410       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.204     |
| test/info_shaping_reward_mean  | -0.23      |
| test/info_shaping_reward_min   | -0.275     |
| test/Q                         | -40.001354 |
| test/Q_plus_P                  | -40.001354 |
| test/reward_per_eps            | -40        |
| test/steps                     | 216400     |
| train/episodes                 | 21640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.134     |
| train/info_shaping_reward_mean | -0.224     |
| train/info_shaping_reward_min  | -0.326     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 865600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 541        |
| stats_o/mean                   | 0.4140388  |
| stats_o/std                    | 0.04139239 |
| test/episodes                  | 5420       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.16      |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.94151  |
| test/Q_plus_P                  | -39.94151  |
| test/reward_per_eps            | -40        |
| test/steps                     | 216800     |
| train/episodes                 | 21680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.139     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 867200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 542         |
| stats_o/mean                   | 0.41404048  |
| stats_o/std                    | 0.041386824 |
| test/episodes                  | 5430        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.171      |
| test/info_shaping_reward_mean  | -0.259      |
| test/info_shaping_reward_min   | -0.31       |
| test/Q                         | -39.887497  |
| test/Q_plus_P                  | -39.887497  |
| test/reward_per_eps            | -40         |
| test/steps                     | 217200      |
| train/episodes                 | 21720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.129      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 868800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 543         |
| stats_o/mean                   | 0.41404215  |
| stats_o/std                    | 0.041391656 |
| test/episodes                  | 5440        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.265      |
| test/Q                         | -39.977264  |
| test/Q_plus_P                  | -39.977264  |
| test/reward_per_eps            | -40         |
| test/steps                     | 217600      |
| train/episodes                 | 21760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.232      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 870400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 544         |
| stats_o/mean                   | 0.4140421   |
| stats_o/std                    | 0.041400783 |
| test/episodes                  | 5450        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.272      |
| test/Q                         | -39.940178  |
| test/Q_plus_P                  | -39.940178  |
| test/reward_per_eps            | -40         |
| test/steps                     | 218000      |
| train/episodes                 | 21800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.168      |
| train/info_shaping_reward_mean | -0.257      |
| train/info_shaping_reward_min  | -0.359      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 872000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 545         |
| stats_o/mean                   | 0.41404402  |
| stats_o/std                    | 0.041397426 |
| test/episodes                  | 5460        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.17       |
| test/info_shaping_reward_mean  | -0.234      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.94078   |
| test/Q_plus_P                  | -39.94078   |
| test/reward_per_eps            | -40         |
| test/steps                     | 218400      |
| train/episodes                 | 21840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.137      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.342      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 873600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 546         |
| stats_o/mean                   | 0.41403937  |
| stats_o/std                    | 0.041397076 |
| test/episodes                  | 5470        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.243      |
| test/info_shaping_reward_min   | -0.322      |
| test/Q                         | -39.937916  |
| test/Q_plus_P                  | -39.937916  |
| test/reward_per_eps            | -40         |
| test/steps                     | 218800      |
| train/episodes                 | 21880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 875200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 547         |
| stats_o/mean                   | 0.41403344  |
| stats_o/std                    | 0.041395944 |
| test/episodes                  | 5480        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.21       |
| test/info_shaping_reward_mean  | -0.255      |
| test/info_shaping_reward_min   | -0.321      |
| test/Q                         | -39.933414  |
| test/Q_plus_P                  | -39.933414  |
| test/reward_per_eps            | -40         |
| test/steps                     | 219200      |
| train/episodes                 | 21920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.347      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 876800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 548        |
| stats_o/mean                   | 0.41403982 |
| stats_o/std                    | 0.04139455 |
| test/episodes                  | 5490       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.171     |
| test/info_shaping_reward_mean  | -0.22      |
| test/info_shaping_reward_min   | -0.272     |
| test/Q                         | -39.939938 |
| test/Q_plus_P                  | -39.939938 |
| test/reward_per_eps            | -40        |
| test/steps                     | 219600     |
| train/episodes                 | 21960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.13      |
| train/info_shaping_reward_mean | -0.225     |
| train/info_shaping_reward_min  | -0.327     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 878400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 549         |
| stats_o/mean                   | 0.4140501   |
| stats_o/std                    | 0.041386586 |
| test/episodes                  | 5500        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.167      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.93419   |
| test/Q_plus_P                  | -39.93419   |
| test/reward_per_eps            | -40         |
| test/steps                     | 220000      |
| train/episodes                 | 22000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.13       |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 880000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 550        |
| stats_o/mean                   | 0.4140433  |
| stats_o/std                    | 0.04138783 |
| test/episodes                  | 5510       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.125     |
| test/info_shaping_reward_mean  | -0.195     |
| test/info_shaping_reward_min   | -0.293     |
| test/Q                         | -39.967316 |
| test/Q_plus_P                  | -39.967316 |
| test/reward_per_eps            | -40        |
| test/steps                     | 220400     |
| train/episodes                 | 22040      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.121     |
| train/info_shaping_reward_mean | -0.227     |
| train/info_shaping_reward_min  | -0.338     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 881600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 551        |
| stats_o/mean                   | 0.41405132 |
| stats_o/std                    | 0.04138759 |
| test/episodes                  | 5520       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.2       |
| test/info_shaping_reward_mean  | -0.237     |
| test/info_shaping_reward_min   | -0.286     |
| test/Q                         | -39.94399  |
| test/Q_plus_P                  | -39.94399  |
| test/reward_per_eps            | -40        |
| test/steps                     | 220800     |
| train/episodes                 | 22080      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.12      |
| train/info_shaping_reward_mean | -0.219     |
| train/info_shaping_reward_min  | -0.33      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 883200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 552         |
| stats_o/mean                   | 0.41405773  |
| stats_o/std                    | 0.041386895 |
| test/episodes                  | 5530        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.311      |
| test/Q                         | -40.00182   |
| test/Q_plus_P                  | -40.00182   |
| test/reward_per_eps            | -40         |
| test/steps                     | 221200      |
| train/episodes                 | 22120       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.135      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 884800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 553         |
| stats_o/mean                   | 0.41406417  |
| stats_o/std                    | 0.041385252 |
| test/episodes                  | 5540        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.149      |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.283      |
| test/Q                         | -40.014194  |
| test/Q_plus_P                  | -40.014194  |
| test/reward_per_eps            | -40         |
| test/steps                     | 221600      |
| train/episodes                 | 22160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.163      |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 886400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 554        |
| stats_o/mean                   | 0.41405317 |
| stats_o/std                    | 0.04138026 |
| test/episodes                  | 5550       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.149     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.296     |
| test/Q                         | -39.890533 |
| test/Q_plus_P                  | -39.890533 |
| test/reward_per_eps            | -40        |
| test/steps                     | 222000     |
| train/episodes                 | 22200      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.323     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 888000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 555         |
| stats_o/mean                   | 0.4140512   |
| stats_o/std                    | 0.041380994 |
| test/episodes                  | 5560        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.14       |
| test/info_shaping_reward_mean  | -0.198      |
| test/info_shaping_reward_min   | -0.267      |
| test/Q                         | -40.04255   |
| test/Q_plus_P                  | -40.04255   |
| test/reward_per_eps            | -40         |
| test/steps                     | 222400      |
| train/episodes                 | 22240       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.244      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 889600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 556        |
| stats_o/mean                   | 0.41403303 |
| stats_o/std                    | 0.04137967 |
| test/episodes                  | 5570       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.159     |
| test/info_shaping_reward_mean  | -0.229     |
| test/info_shaping_reward_min   | -0.306     |
| test/Q                         | -39.970036 |
| test/Q_plus_P                  | -39.970036 |
| test/reward_per_eps            | -40        |
| test/steps                     | 222800     |
| train/episodes                 | 22280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.136     |
| train/info_shaping_reward_mean | -0.236     |
| train/info_shaping_reward_min  | -0.342     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 891200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 557         |
| stats_o/mean                   | 0.41402552  |
| stats_o/std                    | 0.041378386 |
| test/episodes                  | 5580        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.175      |
| test/info_shaping_reward_mean  | -0.24       |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.966026  |
| test/Q_plus_P                  | -39.966026  |
| test/reward_per_eps            | -40         |
| test/steps                     | 223200      |
| train/episodes                 | 22320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.238      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 892800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 558         |
| stats_o/mean                   | 0.41402492  |
| stats_o/std                    | 0.041379254 |
| test/episodes                  | 5590        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.302      |
| test/Q                         | -39.959774  |
| test/Q_plus_P                  | -39.959774  |
| test/reward_per_eps            | -40         |
| test/steps                     | 223600      |
| train/episodes                 | 22360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.252      |
| train/info_shaping_reward_min  | -0.358      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 894400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 559         |
| stats_o/mean                   | 0.4140258   |
| stats_o/std                    | 0.041376866 |
| test/episodes                  | 5600        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.221      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.976635  |
| test/Q_plus_P                  | -39.976635  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224000      |
| train/episodes                 | 22400       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.235      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 896000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 560         |
| stats_o/mean                   | 0.4140354   |
| stats_o/std                    | 0.041372184 |
| test/episodes                  | 5610        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.104      |
| test/info_shaping_reward_mean  | -0.195      |
| test/info_shaping_reward_min   | -0.262      |
| test/Q                         | -39.952255  |
| test/Q_plus_P                  | -39.952255  |
| test/reward_per_eps            | -40         |
| test/steps                     | 224400      |
| train/episodes                 | 22440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 897600      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 561        |
| stats_o/mean                   | 0.4140217  |
| stats_o/std                    | 0.04137018 |
| test/episodes                  | 5620       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.127     |
| test/info_shaping_reward_mean  | -0.216     |
| test/info_shaping_reward_min   | -0.271     |
| test/Q                         | -39.970848 |
| test/Q_plus_P                  | -39.970848 |
| test/reward_per_eps            | -40        |
| test/steps                     | 224800     |
| train/episodes                 | 22480      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.127     |
| train/info_shaping_reward_mean | -0.231     |
| train/info_shaping_reward_min  | -0.348     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 899200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 562         |
| stats_o/mean                   | 0.4140161   |
| stats_o/std                    | 0.041371588 |
| test/episodes                  | 5630        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.211      |
| test/info_shaping_reward_mean  | -0.262      |
| test/info_shaping_reward_min   | -0.298      |
| test/Q                         | -39.973866  |
| test/Q_plus_P                  | -39.973866  |
| test/reward_per_eps            | -40         |
| test/steps                     | 225200      |
| train/episodes                 | 22520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.149      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.35       |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 900800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 563        |
| stats_o/mean                   | 0.4140118  |
| stats_o/std                    | 0.04137387 |
| test/episodes                  | 5640       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.282     |
| test/Q                         | -39.974056 |
| test/Q_plus_P                  | -39.974056 |
| test/reward_per_eps            | -40        |
| test/steps                     | 225600     |
| train/episodes                 | 22560      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.124     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.346     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 902400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 564         |
| stats_o/mean                   | 0.41399953  |
| stats_o/std                    | 0.041367706 |
| test/episodes                  | 5650        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.125      |
| test/info_shaping_reward_mean  | -0.205      |
| test/info_shaping_reward_min   | -0.271      |
| test/Q                         | -39.971405  |
| test/Q_plus_P                  | -39.971405  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226000      |
| train/episodes                 | 22600       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.224      |
| train/info_shaping_reward_min  | -0.303      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 904000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 565         |
| stats_o/mean                   | 0.41399428  |
| stats_o/std                    | 0.041369416 |
| test/episodes                  | 5660        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.174      |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.297      |
| test/Q                         | -39.984116  |
| test/Q_plus_P                  | -39.984116  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226400      |
| train/episodes                 | 22640       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 905600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 566         |
| stats_o/mean                   | 0.41399577  |
| stats_o/std                    | 0.041369885 |
| test/episodes                  | 5670        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.146      |
| test/info_shaping_reward_mean  | -0.202      |
| test/info_shaping_reward_min   | -0.25       |
| test/Q                         | -39.980038  |
| test/Q_plus_P                  | -39.980038  |
| test/reward_per_eps            | -40         |
| test/steps                     | 226800      |
| train/episodes                 | 22680       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.162      |
| train/info_shaping_reward_mean | -0.24       |
| train/info_shaping_reward_min  | -0.331      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 907200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 567         |
| stats_o/mean                   | 0.41400075  |
| stats_o/std                    | 0.041364763 |
| test/episodes                  | 5680        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.151      |
| test/info_shaping_reward_mean  | -0.224      |
| test/info_shaping_reward_min   | -0.324      |
| test/Q                         | -39.98806   |
| test/Q_plus_P                  | -39.98806   |
| test/reward_per_eps            | -40         |
| test/steps                     | 227200      |
| train/episodes                 | 22720       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.127      |
| train/info_shaping_reward_mean | -0.215      |
| train/info_shaping_reward_min  | -0.304      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 908800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 568         |
| stats_o/mean                   | 0.41399828  |
| stats_o/std                    | 0.041363746 |
| test/episodes                  | 5690        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.141      |
| test/info_shaping_reward_mean  | -0.204      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.977276  |
| test/Q_plus_P                  | -39.977276  |
| test/reward_per_eps            | -40         |
| test/steps                     | 227600      |
| train/episodes                 | 22760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.131      |
| train/info_shaping_reward_mean | -0.223      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 910400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 569         |
| stats_o/mean                   | 0.41399547  |
| stats_o/std                    | 0.041366495 |
| test/episodes                  | 5700        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.155      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.289      |
| test/Q                         | -40.0084    |
| test/Q_plus_P                  | -40.0084    |
| test/reward_per_eps            | -40         |
| test/steps                     | 228000      |
| train/episodes                 | 22800       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.136      |
| train/info_shaping_reward_mean | -0.228      |
| train/info_shaping_reward_min  | -0.308      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 912000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 570         |
| stats_o/mean                   | 0.41399336  |
| stats_o/std                    | 0.041361853 |
| test/episodes                  | 5710        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.145      |
| test/info_shaping_reward_mean  | -0.213      |
| test/info_shaping_reward_min   | -0.277      |
| test/Q                         | -39.961555  |
| test/Q_plus_P                  | -39.961555  |
| test/reward_per_eps            | -40         |
| test/steps                     | 228400      |
| train/episodes                 | 22840       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.138      |
| train/info_shaping_reward_mean | -0.225      |
| train/info_shaping_reward_min  | -0.319      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 913600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 571         |
| stats_o/mean                   | 0.41398096  |
| stats_o/std                    | 0.041360173 |
| test/episodes                  | 5720        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.192      |
| test/info_shaping_reward_mean  | -0.238      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -40.00093   |
| test/Q_plus_P                  | -40.00093   |
| test/reward_per_eps            | -40         |
| test/steps                     | 228800      |
| train/episodes                 | 22880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.158      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 915200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 572         |
| stats_o/mean                   | 0.41397253  |
| stats_o/std                    | 0.041356776 |
| test/episodes                  | 5730        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.176      |
| test/info_shaping_reward_mean  | -0.229      |
| test/info_shaping_reward_min   | -0.287      |
| test/Q                         | -39.95602   |
| test/Q_plus_P                  | -39.95602   |
| test/reward_per_eps            | -40         |
| test/steps                     | 229200      |
| train/episodes                 | 22920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.157      |
| train/info_shaping_reward_mean | -0.248      |
| train/info_shaping_reward_min  | -0.321      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 916800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 573        |
| stats_o/mean                   | 0.4139763  |
| stats_o/std                    | 0.04135329 |
| test/episodes                  | 5740       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.153     |
| test/info_shaping_reward_mean  | -0.212     |
| test/info_shaping_reward_min   | -0.259     |
| test/Q                         | -40.012157 |
| test/Q_plus_P                  | -40.012157 |
| test/reward_per_eps            | -40        |
| test/steps                     | 229600     |
| train/episodes                 | 22960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.162     |
| train/info_shaping_reward_mean | -0.247     |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 918400     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 574         |
| stats_o/mean                   | 0.41397133  |
| stats_o/std                    | 0.041349996 |
| test/episodes                  | 5750        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.972633  |
| test/Q_plus_P                  | -39.972633  |
| test/reward_per_eps            | -40         |
| test/steps                     | 230000      |
| train/episodes                 | 23000       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.151      |
| train/info_shaping_reward_mean | -0.247      |
| train/info_shaping_reward_min  | -0.339      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 920000      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 575         |
| stats_o/mean                   | 0.41397747  |
| stats_o/std                    | 0.041344404 |
| test/episodes                  | 5760        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.173      |
| test/info_shaping_reward_mean  | -0.228      |
| test/info_shaping_reward_min   | -0.296      |
| test/Q                         | -39.860275  |
| test/Q_plus_P                  | -39.860275  |
| test/reward_per_eps            | -40         |
| test/steps                     | 230400      |
| train/episodes                 | 23040       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.132      |
| train/info_shaping_reward_mean | -0.219      |
| train/info_shaping_reward_min  | -0.311      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 921600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 576         |
| stats_o/mean                   | 0.41397223  |
| stats_o/std                    | 0.041345496 |
| test/episodes                  | 5770        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.177      |
| test/info_shaping_reward_mean  | -0.235      |
| test/info_shaping_reward_min   | -0.301      |
| test/Q                         | -40.00107   |
| test/Q_plus_P                  | -40.00107   |
| test/reward_per_eps            | -40         |
| test/steps                     | 230800      |
| train/episodes                 | 23080       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.25       |
| train/info_shaping_reward_min  | -0.337      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 923200      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 577        |
| stats_o/mean                   | 0.41395986 |
| stats_o/std                    | 0.04134877 |
| test/episodes                  | 5780       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.158     |
| test/info_shaping_reward_mean  | -0.217     |
| test/info_shaping_reward_min   | -0.277     |
| test/Q                         | -39.965015 |
| test/Q_plus_P                  | -39.965015 |
| test/reward_per_eps            | -40        |
| test/steps                     | 231200     |
| train/episodes                 | 23120      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.16      |
| train/info_shaping_reward_mean | -0.26      |
| train/info_shaping_reward_min  | -0.366     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 924800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 578         |
| stats_o/mean                   | 0.41396067  |
| stats_o/std                    | 0.041348707 |
| test/episodes                  | 5790        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.183      |
| test/info_shaping_reward_mean  | -0.225      |
| test/info_shaping_reward_min   | -0.28       |
| test/Q                         | -39.963333  |
| test/Q_plus_P                  | -39.963333  |
| test/reward_per_eps            | -40         |
| test/steps                     | 231600      |
| train/episodes                 | 23160       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.243      |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 926400      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 579         |
| stats_o/mean                   | 0.41394758  |
| stats_o/std                    | 0.041347537 |
| test/episodes                  | 5800        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.181      |
| test/info_shaping_reward_mean  | -0.252      |
| test/info_shaping_reward_min   | -0.299      |
| test/Q                         | -39.93759   |
| test/Q_plus_P                  | -39.93759   |
| test/reward_per_eps            | -40         |
| test/steps                     | 232000      |
| train/episodes                 | 23200       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.145      |
| train/info_shaping_reward_mean | -0.239      |
| train/info_shaping_reward_min  | -0.333      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 928000      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 580        |
| stats_o/mean                   | 0.41393736 |
| stats_o/std                    | 0.04134876 |
| test/episodes                  | 5810       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.246     |
| test/info_shaping_reward_min   | -0.307     |
| test/Q                         | -40.01259  |
| test/Q_plus_P                  | -40.01259  |
| test/reward_per_eps            | -40        |
| test/steps                     | 232400     |
| train/episodes                 | 23240      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.24      |
| train/info_shaping_reward_min  | -0.35      |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 929600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 581        |
| stats_o/mean                   | 0.4139355  |
| stats_o/std                    | 0.041348   |
| test/episodes                  | 5820       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.169     |
| test/info_shaping_reward_mean  | -0.249     |
| test/info_shaping_reward_min   | -0.312     |
| test/Q                         | -39.984653 |
| test/Q_plus_P                  | -39.984653 |
| test/reward_per_eps            | -40        |
| test/steps                     | 232800     |
| train/episodes                 | 23280      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.132     |
| train/info_shaping_reward_mean | -0.235     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 931200     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 582         |
| stats_o/mean                   | 0.41393277  |
| stats_o/std                    | 0.041347936 |
| test/episodes                  | 5830        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.226      |
| test/info_shaping_reward_min   | -0.261      |
| test/Q                         | -39.97626   |
| test/Q_plus_P                  | -39.97626   |
| test/reward_per_eps            | -40         |
| test/steps                     | 233200      |
| train/episodes                 | 23320       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.237      |
| train/info_shaping_reward_min  | -0.327      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 932800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 583         |
| stats_o/mean                   | 0.4139299   |
| stats_o/std                    | 0.041340947 |
| test/episodes                  | 5840        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.197      |
| test/info_shaping_reward_mean  | -0.248      |
| test/info_shaping_reward_min   | -0.32       |
| test/Q                         | -39.950558  |
| test/Q_plus_P                  | -39.950558  |
| test/reward_per_eps            | -40         |
| test/steps                     | 233600      |
| train/episodes                 | 23360       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.155      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.332      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 934400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 584        |
| stats_o/mean                   | 0.41392878 |
| stats_o/std                    | 0.04133856 |
| test/episodes                  | 5850       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.176     |
| test/info_shaping_reward_mean  | -0.221     |
| test/info_shaping_reward_min   | -0.284     |
| test/Q                         | -39.98799  |
| test/Q_plus_P                  | -39.98799  |
| test/reward_per_eps            | -40        |
| test/steps                     | 234000     |
| train/episodes                 | 23400      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.149     |
| train/info_shaping_reward_mean | -0.232     |
| train/info_shaping_reward_min  | -0.332     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 936000     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 585         |
| stats_o/mean                   | 0.41392383  |
| stats_o/std                    | 0.041336156 |
| test/episodes                  | 5860        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.134      |
| test/info_shaping_reward_mean  | -0.212      |
| test/info_shaping_reward_min   | -0.259      |
| test/Q                         | -39.96477   |
| test/Q_plus_P                  | -39.96477   |
| test/reward_per_eps            | -40         |
| test/steps                     | 234400      |
| train/episodes                 | 23440       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.14       |
| train/info_shaping_reward_mean | -0.236      |
| train/info_shaping_reward_min  | -0.345      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 937600      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 586         |
| stats_o/mean                   | 0.41391942  |
| stats_o/std                    | 0.041336875 |
| test/episodes                  | 5870        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.15       |
| test/info_shaping_reward_mean  | -0.216      |
| test/info_shaping_reward_min   | -0.304      |
| test/Q                         | -39.986732  |
| test/Q_plus_P                  | -39.986732  |
| test/reward_per_eps            | -40         |
| test/steps                     | 234800      |
| train/episodes                 | 23480       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.133      |
| train/info_shaping_reward_mean | -0.234      |
| train/info_shaping_reward_min  | -0.336      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 939200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 587         |
| stats_o/mean                   | 0.41391298  |
| stats_o/std                    | 0.041340303 |
| test/episodes                  | 5880        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.21       |
| test/info_shaping_reward_mean  | -0.244      |
| test/info_shaping_reward_min   | -0.281      |
| test/Q                         | -39.98424   |
| test/Q_plus_P                  | -39.98424   |
| test/reward_per_eps            | -40         |
| test/steps                     | 235200      |
| train/episodes                 | 23520       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.142      |
| train/info_shaping_reward_mean | -0.241      |
| train/info_shaping_reward_min  | -0.343      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 940800      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 588         |
| stats_o/mean                   | 0.41391084  |
| stats_o/std                    | 0.041336108 |
| test/episodes                  | 5890        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.153      |
| test/info_shaping_reward_mean  | -0.239      |
| test/info_shaping_reward_min   | -0.303      |
| test/Q                         | -39.984985  |
| test/Q_plus_P                  | -39.984985  |
| test/reward_per_eps            | -40         |
| test/steps                     | 235600      |
| train/episodes                 | 23560       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.146      |
| train/info_shaping_reward_mean | -0.23       |
| train/info_shaping_reward_min  | -0.329      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 942400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 589        |
| stats_o/mean                   | 0.41390803 |
| stats_o/std                    | 0.04133924 |
| test/episodes                  | 5900       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.157     |
| test/info_shaping_reward_mean  | -0.227     |
| test/info_shaping_reward_min   | -0.338     |
| test/Q                         | -40.039024 |
| test/Q_plus_P                  | -40.039024 |
| test/reward_per_eps            | -40        |
| test/steps                     | 236000     |
| train/episodes                 | 23600      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.153     |
| train/info_shaping_reward_mean | -0.253     |
| train/info_shaping_reward_min  | -0.334     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 944000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 590        |
| stats_o/mean                   | 0.41390845 |
| stats_o/std                    | 0.04133895 |
| test/episodes                  | 5910       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.185     |
| test/info_shaping_reward_mean  | -0.244     |
| test/info_shaping_reward_min   | -0.343     |
| test/Q                         | -39.985302 |
| test/Q_plus_P                  | -39.985302 |
| test/reward_per_eps            | -40        |
| test/steps                     | 236400     |
| train/episodes                 | 23640      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.158     |
| train/info_shaping_reward_mean | -0.245     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 945600     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 591        |
| stats_o/mean                   | 0.41390952 |
| stats_o/std                    | 0.04133943 |
| test/episodes                  | 5920       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.168     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.303     |
| test/Q                         | -39.967377 |
| test/Q_plus_P                  | -39.967377 |
| test/reward_per_eps            | -40        |
| test/steps                     | 236800     |
| train/episodes                 | 23680      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 947200     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 592        |
| stats_o/mean                   | 0.41390815 |
| stats_o/std                    | 0.04133606 |
| test/episodes                  | 5930       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.177     |
| test/info_shaping_reward_mean  | -0.24      |
| test/info_shaping_reward_min   | -0.322     |
| test/Q                         | -39.986454 |
| test/Q_plus_P                  | -39.986454 |
| test/reward_per_eps            | -40        |
| test/steps                     | 237200     |
| train/episodes                 | 23720      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.148     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.343     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 948800     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 593         |
| stats_o/mean                   | 0.41390094  |
| stats_o/std                    | 0.041334953 |
| test/episodes                  | 5940        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.169      |
| test/info_shaping_reward_mean  | -0.232      |
| test/info_shaping_reward_min   | -0.295      |
| test/Q                         | -39.977604  |
| test/Q_plus_P                  | -39.977604  |
| test/reward_per_eps            | -40         |
| test/steps                     | 237600      |
| train/episodes                 | 23760       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.152      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.344      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 950400      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 594        |
| stats_o/mean                   | 0.41390666 |
| stats_o/std                    | 0.04133249 |
| test/episodes                  | 5950       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.18      |
| test/info_shaping_reward_mean  | -0.242     |
| test/info_shaping_reward_min   | -0.299     |
| test/Q                         | -39.98331  |
| test/Q_plus_P                  | -39.98331  |
| test/reward_per_eps            | -40        |
| test/steps                     | 238000     |
| train/episodes                 | 23800      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.159     |
| train/info_shaping_reward_mean | -0.243     |
| train/info_shaping_reward_min  | -0.336     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 952000     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 595        |
| stats_o/mean                   | 0.41389894 |
| stats_o/std                    | 0.04133304 |
| test/episodes                  | 5960       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.161     |
| test/info_shaping_reward_mean  | -0.241     |
| test/info_shaping_reward_min   | -0.31      |
| test/Q                         | -40.044327 |
| test/Q_plus_P                  | -40.044327 |
| test/reward_per_eps            | -40        |
| test/steps                     | 238400     |
| train/episodes                 | 23840      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.164     |
| train/info_shaping_reward_mean | -0.251     |
| train/info_shaping_reward_min  | -0.337     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 953600     |
-----------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 596         |
| stats_o/mean                   | 0.4138883   |
| stats_o/std                    | 0.041338503 |
| test/episodes                  | 5970        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.211      |
| test/info_shaping_reward_mean  | -0.263      |
| test/info_shaping_reward_min   | -0.307      |
| test/Q                         | -39.992855  |
| test/Q_plus_P                  | -39.992855  |
| test/reward_per_eps            | -40         |
| test/steps                     | 238800      |
| train/episodes                 | 23880       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.18       |
| train/info_shaping_reward_mean | -0.27       |
| train/info_shaping_reward_min  | -0.386      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 955200      |
------------------------------------------------
Saving latest policy.
------------------------------------------------
| epoch                          | 597         |
| stats_o/mean                   | 0.41389588  |
| stats_o/std                    | 0.041340988 |
| test/episodes                  | 5980        |
| test/info_is_success_max       | 0           |
| test/info_is_success_mean      | 0           |
| test/info_is_success_min       | 0           |
| test/info_shaping_reward_max   | -0.158      |
| test/info_shaping_reward_mean  | -0.249      |
| test/info_shaping_reward_min   | -0.323      |
| test/Q                         | -40.0454    |
| test/Q_plus_P                  | -40.0454    |
| test/reward_per_eps            | -40         |
| test/steps                     | 239200      |
| train/episodes                 | 23920       |
| train/info_is_success_max      | 0           |
| train/info_is_success_mean     | 0           |
| train/info_is_success_min      | 0           |
| train/info_shaping_reward_max  | -0.147      |
| train/info_shaping_reward_mean | -0.245      |
| train/info_shaping_reward_min  | -0.376      |
| train/Q                        | nan         |
| train/Q_plus_P                 | nan         |
| train/reward_per_eps           | -40         |
| train/steps                    | 956800      |
------------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 598        |
| stats_o/mean                   | 0.41388917 |
| stats_o/std                    | 0.04133655 |
| test/episodes                  | 5990       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.17      |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.309     |
| test/Q                         | -39.995342 |
| test/Q_plus_P                  | -39.995342 |
| test/reward_per_eps            | -40        |
| test/steps                     | 239600     |
| train/episodes                 | 23960      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.17      |
| train/info_shaping_reward_mean | -0.256     |
| train/info_shaping_reward_min  | -0.341     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 958400     |
-----------------------------------------------
Saving latest policy.
-----------------------------------------------
| epoch                          | 599        |
| stats_o/mean                   | 0.413875   |
| stats_o/std                    | 0.04133028 |
| test/episodes                  | 6000       |
| test/info_is_success_max       | 0          |
| test/info_is_success_mean      | 0          |
| test/info_is_success_min       | 0          |
| test/info_shaping_reward_max   | -0.174     |
| test/info_shaping_reward_mean  | -0.239     |
| test/info_shaping_reward_min   | -0.292     |
| test/Q                         | -40.032784 |
| test/Q_plus_P                  | -40.032784 |
| test/reward_per_eps            | -40        |
| test/steps                     | 240000     |
| train/episodes                 | 24000      |
| train/info_is_success_max      | 0          |
| train/info_is_success_mean     | 0          |
| train/info_is_success_min      | 0          |
| train/info_shaping_reward_max  | -0.155     |
| train/info_shaping_reward_mean | -0.233     |
| train/info_shaping_reward_min  | -0.328     |
| train/Q                        | nan        |
| train/Q_plus_P                 | nan        |
| train/reward_per_eps           | -40        |
| train/steps                    | 960000     |
-----------------------------------------------
Saving latest policy.
