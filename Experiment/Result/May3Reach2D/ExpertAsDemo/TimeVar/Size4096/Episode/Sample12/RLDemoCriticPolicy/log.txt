Logging to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy/
Launching the training process with 1 cpu core(s).
Setting log level to 2.
Using environment Reach2D with r scale down by 1.000000 shift by 0.500000 and max episode 0.000000
Skip demonstration training.
Will use provided policy file from /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly/rl_demo_critic/policy_latest.pkl.
Creating a Demonstration NN.
Creating an Ensemble NN of sample x hidden x layer: 12 x 256 x 3.
*** her_params ***
k: 4
reward_fun: <function configure_her.<locals>.reward_fun at 0x7fc474e9f598>
strategy: future
*** her_params ***
*** ddpg_params ***
Q_lr: 0.001
T: 42
action_l2: 0.5
aux_loss_weight: 0.0078
batch_size: 256
batch_size_demo: 128
buffer_size: 1000000
ca_ratio: 1
clip_obs: 200.0
clip_pos_returns: False
clip_return: 41.99999999999995
demo_strategy: critic
gamma: 0.9761904761904762
hidden: 256
info: {'env_name': 'Reach2D', 'r_scale': 1.0, 'r_shift': 0.5, 'eps_length': 0}
input_dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
layers: 3
max_u: 2
norm_clip: 5
norm_eps: 0.01
num_demo: 1000
num_sample: 1
pi_lr: 0.001
polyak: 0.95
prm_loss_weight: 0.001
q_filter: 0
relative_goals: False
rollout_batch_size: 4
sample_transitions: <function make_sample_her_transitions.<locals>._sample_her_transitions at 0x7fc4147bdbf8>
scope: ddpg
subtract_goals: <function simple_goal_subtract at 0x7fc474e9b620>
*** ddpg_params ***
Configuring the replay buffer.
Preparing staging area for feeding data to the model.
Creating a DDPG agent with action space 2 x 2.

*** rollout_params ***
T: 42
compute_Q: False
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 0
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 4
use_demo_states: True
use_target_net: False
*** rollout_params ***
*** eval_params ***
T: 42
compute_Q: True
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 1
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 20
use_demo_states: False
use_target_net: False
*** eval_params ***
Training the RL agent.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 0           |
| stats_g/mean       | -0.27847564 |
| stats_g/std        | 2.528091    |
| stats_o/mean       | -0.14203191 |
| stats_o/std        | 1.4441683   |
| test/episode       | 20.0        |
| test/mean_Q        | 4.2962337   |
| test/success_rate  | 0.0         |
| train/episode      | 40.0        |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_0.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 1           |
| stats_g/mean       | -0.17284864 |
| stats_g/std        | 1.90469     |
| stats_o/mean       | -0.089988   |
| stats_o/std        | 1.1245337   |
| test/episode       | 40.0        |
| test/mean_Q        | 8.916871    |
| test/success_rate  | 0.7         |
| train/episode      | 80.0        |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.7.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 2           |
| stats_g/mean       | -0.12956402 |
| stats_g/std        | 1.607331    |
| stats_o/mean       | -0.06878374 |
| stats_o/std        | 0.98391294  |
| test/episode       | 60.0        |
| test/mean_Q        | 10.151563   |
| test/success_rate  | 0.65        |
| train/episode      | 120.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_2.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 3            |
| stats_g/mean       | -0.1345462   |
| stats_g/std        | 1.4793613    |
| stats_o/mean       | -0.072417304 |
| stats_o/std        | 0.9299954    |
| test/episode       | 80.0         |
| test/mean_Q        | 9.150746     |
| test/success_rate  | 0.4          |
| train/episode      | 160.0        |
| train/success_rate | 0.075        |
-------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 4           |
| stats_g/mean       | -0.08997829 |
| stats_g/std        | 1.3718054   |
| stats_o/mean       | -0.04813059 |
| stats_o/std        | 0.8775649   |
| test/episode       | 100.0       |
| test/mean_Q        | 9.452937    |
| test/success_rate  | 0.4         |
| train/episode      | 200.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_4.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 5           |
| stats_g/mean       | -0.07056685 |
| stats_g/std        | 1.3090007   |
| stats_o/mean       | -0.03395372 |
| stats_o/std        | 0.8490076   |
| test/episode       | 120.0       |
| test/mean_Q        | 11.44032    |
| test/success_rate  | 0.8         |
| train/episode      | 240.0       |
| train/success_rate | 0.075       |
------------------------------------
New best success rate: 0.8.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 6            |
| stats_g/mean       | -0.035530664 |
| stats_g/std        | 1.2413757    |
| stats_o/mean       | -0.017327115 |
| stats_o/std        | 0.81461036   |
| test/episode       | 140.0        |
| test/mean_Q        | 12.93566     |
| test/success_rate  | 0.8          |
| train/episode      | 280.0        |
| train/success_rate | 0.025        |
-------------------------------------
New best success rate: 0.8.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_6.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 7            |
| stats_g/mean       | -0.026594326 |
| stats_g/std        | 1.1807878    |
| stats_o/mean       | -0.013799448 |
| stats_o/std        | 0.7837556    |
| test/episode       | 160.0        |
| test/mean_Q        | 12.598106    |
| test/success_rate  | 0.8          |
| train/episode      | 320.0        |
| train/success_rate | 0.025        |
-------------------------------------
New best success rate: 0.8.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 8            |
| stats_g/mean       | -0.019579865 |
| stats_g/std        | 1.1353388    |
| stats_o/mean       | -0.011041351 |
| stats_o/std        | 0.76218206   |
| test/episode       | 180.0        |
| test/mean_Q        | 14.740063    |
| test/success_rate  | 0.9          |
| train/episode      | 360.0        |
| train/success_rate | 0.05         |
-------------------------------------
New best success rate: 0.9.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_8.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 9           |
| stats_g/mean       | -0.01215113 |
| stats_g/std        | 1.0954196   |
| stats_o/mean       | -0.0072663  |
| stats_o/std        | 0.74456465  |
| test/episode       | 200.0       |
| test/mean_Q        | 15.095366   |
| test/success_rate  | 0.8         |
| train/episode      | 400.0       |
| train/success_rate | 0.075       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 10           |
| stats_g/mean       | -0.016029537 |
| stats_g/std        | 1.0645332    |
| stats_o/mean       | -0.00960692  |
| stats_o/std        | 0.7281087    |
| test/episode       | 220.0        |
| test/mean_Q        | 14.4629      |
| test/success_rate  | 0.95         |
| train/episode      | 440.0        |
| train/success_rate | 0.15         |
-------------------------------------
New best success rate: 0.95.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_10.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 11           |
| stats_g/mean       | -0.013020696 |
| stats_g/std        | 1.0344648    |
| stats_o/mean       | -0.008404095 |
| stats_o/std        | 0.7126902    |
| test/episode       | 240.0        |
| test/mean_Q        | 15.323518    |
| test/success_rate  | 0.9          |
| train/episode      | 480.0        |
| train/success_rate | 0.225        |
-------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 12            |
| stats_g/mean       | -0.009714238  |
| stats_g/std        | 1.0119262     |
| stats_o/mean       | -0.0060302243 |
| stats_o/std        | 0.7010671     |
| test/episode       | 260.0         |
| test/mean_Q        | 14.449065     |
| test/success_rate  | 0.85          |
| train/episode      | 520.0         |
| train/success_rate | 0.075         |
--------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_12.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 13            |
| stats_g/mean       | -0.008287372  |
| stats_g/std        | 0.99049145    |
| stats_o/mean       | -0.0057583675 |
| stats_o/std        | 0.69139415    |
| test/episode       | 280.0         |
| test/mean_Q        | 16.077742     |
| test/success_rate  | 0.95          |
| train/episode      | 560.0         |
| train/success_rate | 0.175         |
--------------------------------------
New best success rate: 0.95.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 14            |
| stats_g/mean       | -0.010124262  |
| stats_g/std        | 0.9710518     |
| stats_o/mean       | -0.0061977184 |
| stats_o/std        | 0.68206793    |
| test/episode       | 300.0         |
| test/mean_Q        | 15.349459     |
| test/success_rate  | 0.95          |
| train/episode      | 600.0         |
| train/success_rate | 0.2           |
--------------------------------------
New best success rate: 0.95.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_14.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 15            |
| stats_g/mean       | -0.0060568284 |
| stats_g/std        | 0.9557581     |
| stats_o/mean       | -0.0039294716 |
| stats_o/std        | 0.6748595     |
| test/episode       | 320.0         |
| test/mean_Q        | 15.265267     |
| test/success_rate  | 0.85          |
| train/episode      | 640.0         |
| train/success_rate | 0.125         |
--------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 16            |
| stats_g/mean       | -0.010398715  |
| stats_g/std        | 0.93877494    |
| stats_o/mean       | -0.0055703428 |
| stats_o/std        | 0.6667789     |
| test/episode       | 340.0         |
| test/mean_Q        | 16.090307     |
| test/success_rate  | 1.0           |
| train/episode      | 680.0         |
| train/success_rate | 0.2           |
--------------------------------------
New best success rate: 1.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_16.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 17           |
| stats_g/mean       | -0.013918251 |
| stats_g/std        | 0.92569405   |
| stats_o/mean       | -0.007340649 |
| stats_o/std        | 0.660552     |
| test/episode       | 360.0        |
| test/mean_Q        | 15.017255    |
| test/success_rate  | 0.85         |
| train/episode      | 720.0        |
| train/success_rate | 0.125        |
-------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 18           |
| stats_g/mean       | -0.012783954 |
| stats_g/std        | 0.9138297    |
| stats_o/mean       | -0.006924815 |
| stats_o/std        | 0.6565807    |
| test/episode       | 380.0        |
| test/mean_Q        | 15.812307    |
| test/success_rate  | 0.85         |
| train/episode      | 760.0        |
| train/success_rate | 0.05         |
-------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_18.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 19            |
| stats_g/mean       | -0.0054748096 |
| stats_g/std        | 0.90380824    |
| stats_o/mean       | -0.002980385  |
| stats_o/std        | 0.6523092     |
| test/episode       | 400.0         |
| test/mean_Q        | 15.00947      |
| test/success_rate  | 0.95          |
| train/episode      | 800.0         |
| train/success_rate | 0.2           |
--------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 20            |
| stats_g/mean       | 0.0010276437  |
| stats_g/std        | 0.8932059     |
| stats_o/mean       | 0.00077348296 |
| stats_o/std        | 0.6465638     |
| test/episode       | 420.0         |
| test/mean_Q        | 15.622288     |
| test/success_rate  | 0.85          |
| train/episode      | 840.0         |
| train/success_rate | 0.225         |
--------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_20.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
---------------------------------------
| epoch              | 21             |
| stats_g/mean       | 0.0002877079   |
| stats_g/std        | 0.882612       |
| stats_o/mean       | -6.3739717e-06 |
| stats_o/std        | 0.64197654     |
| test/episode       | 440.0          |
| test/mean_Q        | 15.082726      |
| test/success_rate  | 0.9            |
| train/episode      | 880.0          |
| train/success_rate | 0.175          |
---------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 22            |
| stats_g/mean       | -0.0032062363 |
| stats_g/std        | 0.8728105     |
| stats_o/mean       | -0.0015866896 |
| stats_o/std        | 0.6377162     |
| test/episode       | 460.0         |
| test/mean_Q        | 16.656342     |
| test/success_rate  | 0.8           |
| train/episode      | 920.0         |
| train/success_rate | 0.125         |
--------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_22.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 23            |
| stats_g/mean       | -0.002348639  |
| stats_g/std        | 0.86508477    |
| stats_o/mean       | -0.0011842484 |
| stats_o/std        | 0.63453007    |
| test/episode       | 480.0         |
| test/mean_Q        | 16.062265     |
| test/success_rate  | 0.95          |
| train/episode      | 960.0         |
| train/success_rate | 0.1           |
--------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
--------------------------------------
| epoch              | 24            |
| stats_g/mean       | -0.0022651702 |
| stats_g/std        | 0.8577566     |
| stats_o/mean       | -0.0010805447 |
| stats_o/std        | 0.6316136     |
| test/episode       | 500.0         |
| test/mean_Q        | 15.619155     |
| test/success_rate  | 0.65          |
| train/episode      | 1000.0        |
| train/success_rate | 0.15          |
--------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_24.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
