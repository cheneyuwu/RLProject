Logging to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy/
Launching the training process with 1 cpu core(s).
Setting log level to 2.
Using environment Reach2D with r scale down by 1.000000 shift by 0.500000 and max episode 0.000000
Skip demonstration training.
Will use provided policy file from /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly/rl_demo_critic/policy_latest.pkl.
Creating a Demonstration NN.
Creating an Ensemble NN of sample x hidden x layer: 32 x 256 x 3.
*** her_params ***
k: 4
reward_fun: <function configure_her.<locals>.reward_fun at 0x7f75c0524598>
strategy: future
*** her_params ***
*** ddpg_params ***
Q_lr: 0.001
T: 42
action_l2: 0.5
aux_loss_weight: 0.0078
batch_size: 256
batch_size_demo: 128
buffer_size: 1000000
ca_ratio: 1
clip_obs: 200.0
clip_pos_returns: False
clip_return: 41.99999999999995
demo_strategy: critic
gamma: 0.9761904761904762
hidden: 256
info: {'env_name': 'Reach2D', 'r_scale': 1.0, 'r_shift': 0.5, 'eps_length': 0}
input_dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
layers: 3
max_u: 2
norm_clip: 5
norm_eps: 0.01
num_demo: 1000
num_sample: 1
pi_lr: 0.001
polyak: 0.95
prm_loss_weight: 0.001
q_filter: 0
relative_goals: False
rollout_batch_size: 4
sample_transitions: <function make_sample_her_transitions.<locals>._sample_her_transitions at 0x7f7524427950>
scope: ddpg
subtract_goals: <function simple_goal_subtract at 0x7f75c0520620>
*** ddpg_params ***
Configuring the replay buffer.
Preparing staging area for feeding data to the model.
Creating a DDPG agent with action space 2 x 2.

*** rollout_params ***
T: 42
compute_Q: False
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 0
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 4
use_demo_states: True
use_target_net: False
*** rollout_params ***
*** eval_params ***
T: 42
compute_Q: True
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 1
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 20
use_demo_states: False
use_target_net: False
*** eval_params ***
Training the RL agent.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-------------------------------------
| epoch              | 0            |
| stats_g/mean       | 0.015789717  |
| stats_g/std        | 1.258781     |
| stats_o/mean       | 0.0058608055 |
| stats_o/std        | 0.8224132    |
| test/episode       | 20.0         |
| test/mean_Q        | 5.703841     |
| test/success_rate  | 0.45         |
| train/episode      | 40.0         |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.45.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_0.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 1          |
| stats_g/mean       | 0.14246783 |
| stats_g/std        | 1.393908   |
| stats_o/mean       | 0.07638363 |
| stats_o/std        | 0.8872397  |
| test/episode       | 40.0       |
| test/mean_Q        | 1.2483914  |
| test/success_rate  | 0.25       |
| train/episode      | 80.0       |
| train/success_rate | 0.0        |
-----------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 2          |
| stats_g/mean       | 0.23460719 |
| stats_g/std        | 2.0736642  |
| stats_o/mean       | 0.12704712 |
| stats_o/std        | 1.248891   |
| test/episode       | 60.0       |
| test/mean_Q        | 8.536189   |
| test/success_rate  | 0.4        |
| train/episode      | 120.0      |
| train/success_rate | 0.0        |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_2.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 3          |
| stats_g/mean       | 0.20267414 |
| stats_g/std        | 2.017195   |
| stats_o/mean       | 0.1134697  |
| stats_o/std        | 1.2156398  |
| test/episode       | 80.0       |
| test/mean_Q        | 6.4843345  |
| test/success_rate  | 0.45       |
| train/episode      | 160.0      |
| train/success_rate | 0.05       |
-----------------------------------
New best success rate: 0.45.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 4          |
| stats_g/mean       | 0.20200758 |
| stats_g/std        | 1.8665662  |
| stats_o/mean       | 0.11399311 |
| stats_o/std        | 1.1330639  |
| test/episode       | 100.0      |
| test/mean_Q        | 8.555631   |
| test/success_rate  | 0.5        |
| train/episode      | 200.0      |
| train/success_rate | 0.0        |
-----------------------------------
New best success rate: 0.5.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_4.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 5          |
| stats_g/mean       | 0.18499056 |
| stats_g/std        | 1.7681048  |
| stats_o/mean       | 0.10397094 |
| stats_o/std        | 1.080916   |
| test/episode       | 120.0      |
| test/mean_Q        | 11.260099  |
| test/success_rate  | 0.8        |
| train/episode      | 240.0      |
| train/success_rate | 0.075      |
-----------------------------------
New best success rate: 0.8.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 6          |
| stats_g/mean       | 0.19417076 |
| stats_g/std        | 1.6803342  |
| stats_o/mean       | 0.10684266 |
| stats_o/std        | 1.033889   |
| test/episode       | 140.0      |
| test/mean_Q        | 10.878966  |
| test/success_rate  | 0.6        |
| train/episode      | 280.0      |
| train/success_rate | 0.0        |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_6.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 7          |
| stats_g/mean       | 0.18505263 |
| stats_g/std        | 1.5903628  |
| stats_o/mean       | 0.10183722 |
| stats_o/std        | 0.98586965 |
| test/episode       | 160.0      |
| test/mean_Q        | 9.824815   |
| test/success_rate  | 0.65       |
| train/episode      | 320.0      |
| train/success_rate | 0.125      |
-----------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 8          |
| stats_g/mean       | 0.16953573 |
| stats_g/std        | 1.5234075  |
| stats_o/mean       | 0.09254867 |
| stats_o/std        | 0.9513643  |
| test/episode       | 180.0      |
| test/mean_Q        | 11.784163  |
| test/success_rate  | 0.7        |
| train/episode      | 360.0      |
| train/success_rate | 0.1        |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_8.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 9          |
| stats_g/mean       | 0.1661353  |
| stats_g/std        | 1.463146   |
| stats_o/mean       | 0.09143715 |
| stats_o/std        | 0.92147887 |
| test/episode       | 200.0      |
| test/mean_Q        | 12.67956   |
| test/success_rate  | 0.9        |
| train/episode      | 400.0      |
| train/success_rate | 0.1        |
-----------------------------------
New best success rate: 0.9.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 10          |
| stats_g/mean       | 0.15001446  |
| stats_g/std        | 1.4086723   |
| stats_o/mean       | 0.082640596 |
| stats_o/std        | 0.8926846   |
| test/episode       | 220.0       |
| test/mean_Q        | 12.81097    |
| test/success_rate  | 0.95        |
| train/episode      | 440.0       |
| train/success_rate | 0.15        |
------------------------------------
New best success rate: 0.95.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_10.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 11         |
| stats_g/mean       | 0.14081274 |
| stats_g/std        | 1.3612001  |
| stats_o/mean       | 0.07707289 |
| stats_o/std        | 0.86743844 |
| test/episode       | 240.0      |
| test/mean_Q        | 13.119156  |
| test/success_rate  | 0.95       |
| train/episode      | 480.0      |
| train/success_rate | 0.225      |
-----------------------------------
New best success rate: 0.95.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 12         |
| stats_g/mean       | 0.13196626 |
| stats_g/std        | 1.3201923  |
| stats_o/mean       | 0.07277085 |
| stats_o/std        | 0.84568834 |
| test/episode       | 260.0      |
| test/mean_Q        | 11.97905   |
| test/success_rate  | 0.95       |
| train/episode      | 520.0      |
| train/success_rate | 0.325      |
-----------------------------------
New best success rate: 0.95.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_12.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 13         |
| stats_g/mean       | 0.12357301 |
| stats_g/std        | 1.2831423  |
| stats_o/mean       | 0.06738626 |
| stats_o/std        | 0.82716715 |
| test/episode       | 280.0      |
| test/mean_Q        | 13.622997  |
| test/success_rate  | 1.0        |
| train/episode      | 560.0      |
| train/success_rate | 0.2        |
-----------------------------------
New best success rate: 1.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 14         |
| stats_g/mean       | 0.11416652 |
| stats_g/std        | 1.2505517  |
| stats_o/mean       | 0.06267298 |
| stats_o/std        | 0.81071186 |
| test/episode       | 300.0      |
| test/mean_Q        | 12.282895  |
| test/success_rate  | 0.65       |
| train/episode      | 600.0      |
| train/success_rate | 0.25       |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_14.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 15         |
| stats_g/mean       | 0.11239913 |
| stats_g/std        | 1.2225436  |
| stats_o/mean       | 0.0616787  |
| stats_o/std        | 0.7965133  |
| test/episode       | 320.0      |
| test/mean_Q        | 12.212557  |
| test/success_rate  | 0.7        |
| train/episode      | 640.0      |
| train/success_rate | 0.2        |
-----------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 16          |
| stats_g/mean       | 0.1015206   |
| stats_g/std        | 1.1954547   |
| stats_o/mean       | 0.056323975 |
| stats_o/std        | 0.7833734   |
| test/episode       | 340.0       |
| test/mean_Q        | 13.915941   |
| test/success_rate  | 0.8         |
| train/episode      | 680.0       |
| train/success_rate | 0.35        |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_16.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 17          |
| stats_g/mean       | 0.092934296 |
| stats_g/std        | 1.1721908   |
| stats_o/mean       | 0.051790692 |
| stats_o/std        | 0.7721002   |
| test/episode       | 360.0       |
| test/mean_Q        | 11.716302   |
| test/success_rate  | 0.6         |
| train/episode      | 720.0       |
| train/success_rate | 0.175       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 18          |
| stats_g/mean       | 0.089001186 |
| stats_g/std        | 1.1514231   |
| stats_o/mean       | 0.049439635 |
| stats_o/std        | 0.76359785  |
| test/episode       | 380.0       |
| test/mean_Q        | 11.552168   |
| test/success_rate  | 0.65        |
| train/episode      | 760.0       |
| train/success_rate | 0.125       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_18.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 19          |
| stats_g/mean       | 0.091356486 |
| stats_g/std        | 1.1318308   |
| stats_o/mean       | 0.050574493 |
| stats_o/std        | 0.7545732   |
| test/episode       | 400.0       |
| test/mean_Q        | 12.018925   |
| test/success_rate  | 0.75        |
| train/episode      | 800.0       |
| train/success_rate | 0.05        |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 20          |
| stats_g/mean       | 0.09321565  |
| stats_g/std        | 1.1131754   |
| stats_o/mean       | 0.051778555 |
| stats_o/std        | 0.7452363   |
| test/episode       | 420.0       |
| test/mean_Q        | 11.180436   |
| test/success_rate  | 0.9         |
| train/episode      | 840.0       |
| train/success_rate | 0.2         |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_20.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 21          |
| stats_g/mean       | 0.087253034 |
| stats_g/std        | 1.0956168   |
| stats_o/mean       | 0.04802777  |
| stats_o/std        | 0.7374564   |
| test/episode       | 440.0       |
| test/mean_Q        | 11.4251     |
| test/success_rate  | 0.65        |
| train/episode      | 880.0       |
| train/success_rate | 0.1         |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 22         |
| stats_g/mean       | 0.08007669 |
| stats_g/std        | 1.0804378  |
| stats_o/mean       | 0.04422699 |
| stats_o/std        | 0.7307897  |
| test/episode       | 460.0      |
| test/mean_Q        | 12.319253  |
| test/success_rate  | 0.85       |
| train/episode      | 920.0      |
| train/success_rate | 0.05       |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_22.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 23         |
| stats_g/mean       | 0.07814979 |
| stats_g/std        | 1.0665282  |
| stats_o/mean       | 0.04295946 |
| stats_o/std        | 0.72454625 |
| test/episode       | 480.0      |
| test/mean_Q        | 11.960147  |
| test/success_rate  | 0.65       |
| train/episode      | 960.0      |
| train/success_rate | 0.15       |
-----------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 24         |
| stats_g/mean       | 0.07464473 |
| stats_g/std        | 1.0539505  |
| stats_o/mean       | 0.04111948 |
| stats_o/std        | 0.71908784 |
| test/episode       | 500.0      |
| test/mean_Q        | 11.8376045 |
| test/success_rate  | 0.6        |
| train/episode      | 1000.0     |
| train/success_rate | 0.075      |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_24.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
