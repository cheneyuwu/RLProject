Logging to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy/
Launching the training process with 1 cpu core(s).
Setting log level to 2.
Using environment Reach2D with r scale down by 1.000000 shift by 0.500000 and max episode 0.000000
Skip demonstration training.
Will use provided policy file from /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly/rl_demo_critic/policy_latest.pkl.
Creating a Demonstration NN.
Creating an Ensemble NN of sample x hidden x layer: 12 x 256 x 3.
*** her_params ***
k: 4
reward_fun: <function configure_her.<locals>.reward_fun at 0x7fb248d51598>
strategy: future
*** her_params ***
*** ddpg_params ***
Q_lr: 0.001
T: 42
action_l2: 0.5
aux_loss_weight: 0.0078
batch_size: 256
batch_size_demo: 128
buffer_size: 1000000
ca_ratio: 1
clip_obs: 200.0
clip_pos_returns: False
clip_return: 41.99999999999995
demo_strategy: critic
gamma: 0.9761904761904762
hidden: 256
info: {'env_name': 'Reach2D', 'r_scale': 1.0, 'r_shift': 0.5, 'eps_length': 0}
input_dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
layers: 3
max_u: 2
norm_clip: 5
norm_eps: 0.01
num_demo: 1000
num_sample: 1
pi_lr: 0.001
polyak: 0.95
prm_loss_weight: 0.001
q_filter: 0
relative_goals: False
rollout_batch_size: 4
sample_transitions: <function make_sample_her_transitions.<locals>._sample_her_transitions at 0x7fb2382bdbf8>
scope: ddpg
subtract_goals: <function simple_goal_subtract at 0x7fb248d4d620>
*** ddpg_params ***
Configuring the replay buffer.
Preparing staging area for feeding data to the model.
Creating a DDPG agent with action space 2 x 2.

*** rollout_params ***
T: 42
compute_Q: False
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 0
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 4
use_demo_states: True
use_target_net: False
*** rollout_params ***
*** eval_params ***
T: 42
compute_Q: True
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 1
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 20
use_demo_states: False
use_target_net: False
*** eval_params ***
Training the RL agent.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 0          |
| stats_g/mean       | 0.119349   |
| stats_g/std        | 1.6012794  |
| stats_o/mean       | 0.07637341 |
| stats_o/std        | 0.98885775 |
| test/episode       | 20.0       |
| test/mean_Q        | 0.5333494  |
| test/success_rate  | 0.0        |
| train/episode      | 40.0       |
| train/success_rate | 0.0        |
-----------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_0.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 1          |
| stats_g/mean       | 0.10798788 |
| stats_g/std        | 1.3497952  |
| stats_o/mean       | 0.06771811 |
| stats_o/std        | 0.86087286 |
| test/episode       | 40.0       |
| test/mean_Q        | 2.1993518  |
| test/success_rate  | 0.15       |
| train/episode      | 80.0       |
| train/success_rate | 0.0        |
-----------------------------------
New best success rate: 0.15.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 2          |
| stats_g/mean       | 0.18666062 |
| stats_g/std        | 1.5633637  |
| stats_o/mean       | 0.10913555 |
| stats_o/std        | 0.9840332  |
| test/episode       | 60.0       |
| test/mean_Q        | 3.051707   |
| test/success_rate  | 0.05       |
| train/episode      | 120.0      |
| train/success_rate | 0.0        |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_2.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 3          |
| stats_g/mean       | 0.18329838 |
| stats_g/std        | 1.6737016  |
| stats_o/mean       | 0.10824771 |
| stats_o/std        | 1.036376   |
| test/episode       | 80.0       |
| test/mean_Q        | 2.910941   |
| test/success_rate  | 0.0        |
| train/episode      | 160.0      |
| train/success_rate | 0.0        |
-----------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 4          |
| stats_g/mean       | 0.2414978  |
| stats_g/std        | 1.5997325  |
| stats_o/mean       | 0.13918702 |
| stats_o/std        | 0.99599624 |
| test/episode       | 100.0      |
| test/mean_Q        | 2.4403756  |
| test/success_rate  | 0.15       |
| train/episode      | 200.0      |
| train/success_rate | 0.0        |
-----------------------------------
New best success rate: 0.15.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_4.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 5          |
| stats_g/mean       | 0.2379587  |
| stats_g/std        | 1.561836   |
| stats_o/mean       | 0.13653582 |
| stats_o/std        | 0.97073865 |
| test/episode       | 120.0      |
| test/mean_Q        | 7.1840863  |
| test/success_rate  | 0.2        |
| train/episode      | 240.0      |
| train/success_rate | 0.0        |
-----------------------------------
New best success rate: 0.2.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 6          |
| stats_g/mean       | 0.22920315 |
| stats_g/std        | 1.5072764  |
| stats_o/mean       | 0.12973818 |
| stats_o/std        | 0.9410406  |
| test/episode       | 140.0      |
| test/mean_Q        | 7.9620385  |
| test/success_rate  | 0.15       |
| train/episode      | 280.0      |
| train/success_rate | 0.0        |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_6.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 7           |
| stats_g/mean       | 0.21442434  |
| stats_g/std        | 1.4337523   |
| stats_o/mean       | 0.120764405 |
| stats_o/std        | 0.9027953   |
| test/episode       | 160.0       |
| test/mean_Q        | 8.629503    |
| test/success_rate  | 0.1         |
| train/episode      | 320.0       |
| train/success_rate | 0.0         |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 8          |
| stats_g/mean       | 0.19105926 |
| stats_g/std        | 1.3759043  |
| stats_o/mean       | 0.10645129 |
| stats_o/std        | 0.87365884 |
| test/episode       | 180.0      |
| test/mean_Q        | 10.9318495 |
| test/success_rate  | 0.1        |
| train/episode      | 360.0      |
| train/success_rate | 0.05       |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_8.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 9           |
| stats_g/mean       | 0.18026975  |
| stats_g/std        | 1.3219303   |
| stats_o/mean       | 0.100509286 |
| stats_o/std        | 0.84935063  |
| test/episode       | 200.0       |
| test/mean_Q        | 12.571705   |
| test/success_rate  | 0.3         |
| train/episode      | 400.0       |
| train/success_rate | 0.025       |
------------------------------------
New best success rate: 0.3.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 10         |
| stats_g/mean       | 0.16299269 |
| stats_g/std        | 1.2785912  |
| stats_o/mean       | 0.09080121 |
| stats_o/std        | 0.8270436  |
| test/episode       | 220.0      |
| test/mean_Q        | 12.247919  |
| test/success_rate  | 0.2        |
| train/episode      | 440.0      |
| train/success_rate | 0.1        |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_10.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 11         |
| stats_g/mean       | 0.1532398  |
| stats_g/std        | 1.2374086  |
| stats_o/mean       | 0.08483021 |
| stats_o/std        | 0.8054919  |
| test/episode       | 240.0      |
| test/mean_Q        | 13.039518  |
| test/success_rate  | 0.2        |
| train/episode      | 480.0      |
| train/success_rate | 0.075      |
-----------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 12          |
| stats_g/mean       | 0.14624397  |
| stats_g/std        | 1.2027822   |
| stats_o/mean       | 0.081485614 |
| stats_o/std        | 0.78736275  |
| test/episode       | 260.0       |
| test/mean_Q        | 12.088563   |
| test/success_rate  | 0.2         |
| train/episode      | 520.0       |
| train/success_rate | 0.1         |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_12.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 13         |
| stats_g/mean       | 0.13819627 |
| stats_g/std        | 1.171669   |
| stats_o/mean       | 0.07634093 |
| stats_o/std        | 0.7725613  |
| test/episode       | 280.0      |
| test/mean_Q        | 14.109396  |
| test/success_rate  | 0.2        |
| train/episode      | 560.0      |
| train/success_rate | 0.05       |
-----------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 14         |
| stats_g/mean       | 0.12867868 |
| stats_g/std        | 1.1444454  |
| stats_o/mean       | 0.07181698 |
| stats_o/std        | 0.7593472  |
| test/episode       | 300.0      |
| test/mean_Q        | 13.167571  |
| test/success_rate  | 0.15       |
| train/episode      | 600.0      |
| train/success_rate | 0.05       |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_14.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 15          |
| stats_g/mean       | 0.1253703   |
| stats_g/std        | 1.1209316   |
| stats_o/mean       | 0.069826126 |
| stats_o/std        | 0.7482963   |
| test/episode       | 320.0       |
| test/mean_Q        | 13.251104   |
| test/success_rate  | 0.2         |
| train/episode      | 640.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 16         |
| stats_g/mean       | 0.11566815 |
| stats_g/std        | 1.0973451  |
| stats_o/mean       | 0.06508279 |
| stats_o/std        | 0.73716533 |
| test/episode       | 340.0      |
| test/mean_Q        | 13.539722  |
| test/success_rate  | 0.3        |
| train/episode      | 680.0      |
| train/success_rate | 0.075      |
-----------------------------------
New best success rate: 0.3.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_16.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 17          |
| stats_g/mean       | 0.107188255 |
| stats_g/std        | 1.0767258   |
| stats_o/mean       | 0.060677767 |
| stats_o/std        | 0.7269079   |
| test/episode       | 360.0       |
| test/mean_Q        | 13.592259   |
| test/success_rate  | 0.4         |
| train/episode      | 720.0       |
| train/success_rate | 0.075       |
------------------------------------
New best success rate: 0.4.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 18          |
| stats_g/mean       | 0.10296093  |
| stats_g/std        | 1.0586582   |
| stats_o/mean       | 0.058169786 |
| stats_o/std        | 0.7200655   |
| test/episode       | 380.0       |
| test/mean_Q        | 14.9719305  |
| test/success_rate  | 0.3         |
| train/episode      | 760.0       |
| train/success_rate | 0.05        |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_18.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 19          |
| stats_g/mean       | 0.10540741  |
| stats_g/std        | 1.0413471   |
| stats_o/mean       | 0.059126113 |
| stats_o/std        | 0.7119691   |
| test/episode       | 400.0       |
| test/mean_Q        | 12.637816   |
| test/success_rate  | 0.3         |
| train/episode      | 800.0       |
| train/success_rate | 0.125       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 20          |
| stats_g/mean       | 0.106939964 |
| stats_g/std        | 1.0256557   |
| stats_o/mean       | 0.05997602  |
| stats_o/std        | 0.70378023  |
| test/episode       | 420.0       |
| test/mean_Q        | 13.309783   |
| test/success_rate  | 0.35        |
| train/episode      | 840.0       |
| train/success_rate | 0.1         |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_20.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 21          |
| stats_g/mean       | 0.10181524  |
| stats_g/std        | 1.0106053   |
| stats_o/mean       | 0.056738198 |
| stats_o/std        | 0.69698274  |
| test/episode       | 440.0       |
| test/mean_Q        | 12.915508   |
| test/success_rate  | 0.5         |
| train/episode      | 880.0       |
| train/success_rate | 0.1         |
------------------------------------
New best success rate: 0.5.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 22         |
| stats_g/mean       | 0.09439394 |
| stats_g/std        | 0.9969431  |
| stats_o/mean       | 0.05269996 |
| stats_o/std        | 0.69101644 |
| test/episode       | 460.0      |
| test/mean_Q        | 15.287981  |
| test/success_rate  | 0.3        |
| train/episode      | 920.0      |
| train/success_rate | 0.025      |
-----------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_22.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 23          |
| stats_g/mean       | 0.09227536  |
| stats_g/std        | 0.9851829   |
| stats_o/mean       | 0.051454313 |
| stats_o/std        | 0.68573844  |
| test/episode       | 480.0       |
| test/mean_Q        | 15.653664   |
| test/success_rate  | 0.55        |
| train/episode      | 960.0       |
| train/success_rate | 0.05        |
------------------------------------
New best success rate: 0.55.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 24          |
| stats_g/mean       | 0.088531636 |
| stats_g/std        | 0.9744383   |
| stats_o/mean       | 0.049461126 |
| stats_o/std        | 0.6813374   |
| test/episode       | 500.0       |
| test/mean_Q        | 13.6055     |
| test/success_rate  | 0.5         |
| train/episode      | 1000.0      |
| train/success_rate | 0.05        |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_24.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
