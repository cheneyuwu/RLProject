Logging to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy/
Launching the training process with 1 cpu core(s).
Setting log level to 2.
Using environment Reach2D with r scale down by 1.000000 shift by 0.500000 and max episode 0.000000
Skip demonstration training.
Will use provided policy file from /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly/rl_demo_critic/policy_latest.pkl.
Creating a Demonstration NN.
Creating an Ensemble NN of sample x hidden x layer: 12 x 256 x 3.
*** her_params ***
k: 4
reward_fun: <function configure_her.<locals>.reward_fun at 0x7ff5ecccc598>
strategy: future
*** her_params ***
*** ddpg_params ***
Q_lr: 0.001
T: 42
action_l2: 0.5
aux_loss_weight: 0.0078
batch_size: 256
batch_size_demo: 128
buffer_size: 1000000
ca_ratio: 1
clip_obs: 200.0
clip_pos_returns: False
clip_return: 41.99999999999995
demo_strategy: critic
gamma: 0.9761904761904762
hidden: 256
info: {'env_name': 'Reach2D', 'r_scale': 1.0, 'r_shift': 0.5, 'eps_length': 0}
input_dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
layers: 3
max_u: 2
norm_clip: 5
norm_eps: 0.01
num_demo: 1000
num_sample: 1
pi_lr: 0.001
polyak: 0.95
prm_loss_weight: 0.001
q_filter: 0
relative_goals: False
rollout_batch_size: 4
sample_transitions: <function make_sample_her_transitions.<locals>._sample_her_transitions at 0x7ff5d423ebf8>
scope: ddpg
subtract_goals: <function simple_goal_subtract at 0x7ff5eccc8620>
*** ddpg_params ***
Configuring the replay buffer.
Preparing staging area for feeding data to the model.
Creating a DDPG agent with action space 2 x 2.

*** rollout_params ***
T: 42
compute_Q: False
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 0
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 4
use_demo_states: True
use_target_net: False
*** rollout_params ***
*** eval_params ***
T: 42
compute_Q: True
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 1
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 20
use_demo_states: False
use_target_net: False
*** eval_params ***
Training the RL agent.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 0          |
| stats_g/mean       | 0.11637896 |
| stats_g/std        | 2.3246756  |
| stats_o/mean       | 0.08024645 |
| stats_o/std        | 1.3411777  |
| test/episode       | 20.0       |
| test/mean_Q        | 5.406943   |
| test/success_rate  | 0.0        |
| train/episode      | 40.0       |
| train/success_rate | 0.0        |
-----------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_0.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
-----------------------------------
| epoch              | 1          |
| stats_g/mean       | 0.05807213 |
| stats_g/std        | 1.754054   |
| stats_o/mean       | 0.04181439 |
| stats_o/std        | 1.0572498  |
| test/episode       | 40.0       |
| test/mean_Q        | 8.551451   |
| test/success_rate  | 0.7        |
| train/episode      | 80.0       |
| train/success_rate | 0.025      |
-----------------------------------
New best success rate: 0.7.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 2           |
| stats_g/mean       | 0.029176075 |
| stats_g/std        | 1.501023    |
| stats_o/mean       | 0.022727858 |
| stats_o/std        | 0.94496715  |
| test/episode       | 60.0        |
| test/mean_Q        | 10.75383    |
| test/success_rate  | 0.7         |
| train/episode      | 120.0       |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.7.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_2.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 3           |
| stats_g/mean       | 0.009828474 |
| stats_g/std        | 1.4705144   |
| stats_o/mean       | 0.010853765 |
| stats_o/std        | 0.93888354  |
| test/episode       | 80.0        |
| test/mean_Q        | 9.909114    |
| test/success_rate  | 0.65        |
| train/episode      | 160.0       |
| train/success_rate | 0.075       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 4           |
| stats_g/mean       | 0.026503295 |
| stats_g/std        | 1.3626012   |
| stats_o/mean       | 0.019296166 |
| stats_o/std        | 0.8863504   |
| test/episode       | 100.0       |
| test/mean_Q        | 11.644461   |
| test/success_rate  | 0.7         |
| train/episode      | 200.0       |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.7.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_4.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 5           |
| stats_g/mean       | 0.006844934 |
| stats_g/std        | 1.3334242   |
| stats_o/mean       | 0.008600714 |
| stats_o/std        | 0.8671665   |
| test/episode       | 120.0       |
| test/mean_Q        | 14.322954   |
| test/success_rate  | 0.9         |
| train/episode      | 240.0       |
| train/success_rate | 0.075       |
------------------------------------
New best success rate: 0.9.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 6           |
| stats_g/mean       | 0.028837914 |
| stats_g/std        | 1.2619718   |
| stats_o/mean       | 0.017980535 |
| stats_o/std        | 0.83022046  |
| test/episode       | 140.0       |
| test/mean_Q        | 13.991834   |
| test/success_rate  | 0.85        |
| train/episode      | 280.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_6.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 7           |
| stats_g/mean       | 0.030055432 |
| stats_g/std        | 1.200658    |
| stats_o/mean       | 0.017309558 |
| stats_o/std        | 0.7984772   |
| test/episode       | 160.0       |
| test/mean_Q        | 14.382866   |
| test/success_rate  | 0.85        |
| train/episode      | 320.0       |
| train/success_rate | 0.125       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 8           |
| stats_g/mean       | 0.030565593 |
| stats_g/std        | 1.1545184   |
| stats_o/mean       | 0.016706444 |
| stats_o/std        | 0.77698416  |
| test/episode       | 180.0       |
| test/mean_Q        | 15.620657   |
| test/success_rate  | 0.3         |
| train/episode      | 360.0       |
| train/success_rate | 0.075       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_8.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 9           |
| stats_g/mean       | 0.032849133 |
| stats_g/std        | 1.1140499   |
| stats_o/mean       | 0.017749228 |
| stats_o/std        | 0.75875753  |
| test/episode       | 200.0       |
| test/mean_Q        | 16.731453   |
| test/success_rate  | 0.6         |
| train/episode      | 400.0       |
| train/success_rate | 0.05        |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 10          |
| stats_g/mean       | 0.024238152 |
| stats_g/std        | 1.0905536   |
| stats_o/mean       | 0.012658421 |
| stats_o/std        | 0.7456833   |
| test/episode       | 220.0       |
| test/mean_Q        | 16.356813   |
| test/success_rate  | 0.2         |
| train/episode      | 440.0       |
| train/success_rate | 0.075       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_10.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 11          |
| stats_g/mean       | 0.024304017 |
| stats_g/std        | 1.0581924   |
| stats_o/mean       | 0.012363538 |
| stats_o/std        | 0.7292646   |
| test/episode       | 240.0       |
| test/mean_Q        | 17.16242    |
| test/success_rate  | 0.3         |
| train/episode      | 480.0       |
| train/success_rate | 0.15        |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 12          |
| stats_g/mean       | 0.025472287 |
| stats_g/std        | 1.0341375   |
| stats_o/mean       | 0.013610372 |
| stats_o/std        | 0.71682334  |
| test/episode       | 260.0       |
| test/mean_Q        | 16.248667   |
| test/success_rate  | 0.3         |
| train/episode      | 520.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_12.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 13          |
| stats_g/mean       | 0.025196552 |
| stats_g/std        | 1.012071    |
| stats_o/mean       | 0.012819208 |
| stats_o/std        | 0.7068699   |
| test/episode       | 280.0       |
| test/mean_Q        | 17.577354   |
| test/success_rate  | 0.35        |
| train/episode      | 560.0       |
| train/success_rate | 0.05        |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 14          |
| stats_g/mean       | 0.021965979 |
| stats_g/std        | 0.9917531   |
| stats_o/mean       | 0.011714077 |
| stats_o/std        | 0.69692713  |
| test/episode       | 300.0       |
| test/mean_Q        | 16.683851   |
| test/success_rate  | 0.4         |
| train/episode      | 600.0       |
| train/success_rate | 0.05        |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_14.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 15          |
| stats_g/mean       | 0.024794998 |
| stats_g/std        | 0.9753667   |
| stats_o/mean       | 0.01318722  |
| stats_o/std        | 0.6888405   |
| test/episode       | 320.0       |
| test/mean_Q        | 17.257412   |
| test/success_rate  | 0.6         |
| train/episode      | 640.0       |
| train/success_rate | 0.075       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 16          |
| stats_g/mean       | 0.019224606 |
| stats_g/std        | 0.9582205   |
| stats_o/mean       | 0.010834122 |
| stats_o/std        | 0.6806222   |
| test/episode       | 340.0       |
| test/mean_Q        | 17.28924    |
| test/success_rate  | 0.2         |
| train/episode      | 680.0       |
| train/success_rate | 0.1         |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_16.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 17          |
| stats_g/mean       | 0.015467107 |
| stats_g/std        | 0.9444941   |
| stats_o/mean       | 0.008953052 |
| stats_o/std        | 0.6736919   |
| test/episode       | 360.0       |
| test/mean_Q        | 16.72178    |
| test/success_rate  | 0.5         |
| train/episode      | 720.0       |
| train/success_rate | 0.0         |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 18          |
| stats_g/mean       | 0.016062154 |
| stats_g/std        | 0.9323833   |
| stats_o/mean       | 0.009182926 |
| stats_o/std        | 0.6691735   |
| test/episode       | 380.0       |
| test/mean_Q        | 17.173346   |
| test/success_rate  | 0.65        |
| train/episode      | 760.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_18.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 19          |
| stats_g/mean       | 0.022348788 |
| stats_g/std        | 0.92147744  |
| stats_o/mean       | 0.012445105 |
| stats_o/std        | 0.6642354   |
| test/episode       | 400.0       |
| test/mean_Q        | 16.5922     |
| test/success_rate  | 0.35        |
| train/episode      | 800.0       |
| train/success_rate | 0.05        |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 20          |
| stats_g/mean       | 0.027700681 |
| stats_g/std        | 0.91036665  |
| stats_o/mean       | 0.015600935 |
| stats_o/std        | 0.6581478   |
| test/episode       | 420.0       |
| test/mean_Q        | 17.279287   |
| test/success_rate  | 0.4         |
| train/episode      | 840.0       |
| train/success_rate | 0.05        |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_20.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 21          |
| stats_g/mean       | 0.026185062 |
| stats_g/std        | 0.89997995  |
| stats_o/mean       | 0.014434924 |
| stats_o/std        | 0.65326583  |
| test/episode       | 440.0       |
| test/mean_Q        | 17.507864   |
| test/success_rate  | 0.3         |
| train/episode      | 880.0       |
| train/success_rate | 0.1         |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 22          |
| stats_g/mean       | 0.022168016 |
| stats_g/std        | 0.89003104  |
| stats_o/mean       | 0.012502896 |
| stats_o/std        | 0.64885175  |
| test/episode       | 460.0       |
| test/mean_Q        | 18.292458   |
| test/success_rate  | 0.15        |
| train/episode      | 920.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_22.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 23          |
| stats_g/mean       | 0.023125108 |
| stats_g/std        | 0.8820126   |
| stats_o/mean       | 0.013090008 |
| stats_o/std        | 0.64505595  |
| test/episode       | 480.0       |
| test/mean_Q        | 17.978155   |
| test/success_rate  | 0.3         |
| train/episode      | 960.0       |
| train/success_rate | 0.025       |
------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
DDPG.check_train -> rl variance 0.0
------------------------------------
| epoch              | 24          |
| stats_g/mean       | 0.02320217  |
| stats_g/std        | 0.8738998   |
| stats_o/mean       | 0.013135394 |
| stats_o/std        | 0.6412977   |
| test/episode       | 500.0       |
| test/mean_Q        | 17.468733   |
| test/success_rate  | 0.6         |
| train/episode      | 1000.0      |
| train/success_rate | 0.025       |
------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_24.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLDemoCriticPolicy//rl/policy_latest.pkl.
