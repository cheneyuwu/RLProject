Logging to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo/
Launching the training process with 1 cpu core(s).
Setting log level to 2.
Using environment Reach2D with r scale down by 1.000000 shift by 0.500000 and max episode 0.000000
Skip demonstration training.
Demo policy file does not exit, cannot use demo policy.
*** her_params ***
k: 4
reward_fun: <function configure_her.<locals>.reward_fun at 0x7f1752b056a8>
strategy: future
*** her_params ***
*** ddpg_params ***
Q_lr: 0.001
T: 42
action_l2: 0.5
aux_loss_weight: 0.0078
batch_size: 256
batch_size_demo: 128
buffer_size: 1000000
ca_ratio: 1
clip_obs: 200.0
clip_pos_returns: False
clip_return: 41.99999999999995
demo_strategy: none
gamma: 0.9761904761904762
hidden: 256
info: {'env_name': 'Reach2D', 'r_scale': 1.0, 'r_shift': 0.5, 'eps_length': 0}
input_dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
layers: 3
max_u: 2
norm_clip: 5
norm_eps: 0.01
num_demo: 1000
num_sample: 12
pi_lr: 0.001
polyak: 0.95
prm_loss_weight: 0.001
q_filter: 0
relative_goals: False
rollout_batch_size: 4
sample_transitions: <function make_sample_her_transitions.<locals>._sample_her_transitions at 0x7f1752b05730>
scope: critic_demo
subtract_goals: <function simple_goal_subtract at 0x7f1752b01620>
*** ddpg_params ***
Configuring the replay buffer.
Preparing staging area for feeding data to the model.
Creating a DDPG agent with action space 2 x 2.

*** rollout_params ***
T: 42
compute_Q: False
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 0
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 4
use_demo_states: True
use_target_net: False
*** rollout_params ***
*** eval_params ***
T: 42
compute_Q: True
dims: {'o': 4, 'u': 2, 'g': 2, 'info_is_success': 1}
exploit: 1
noise_eps: 0.2
random_eps: 0.3
rollout_batch_size: 20
use_demo_states: False
use_target_net: False
*** eval_params ***
Training the RL agent.
DDPG.check_train -> rl variance 0.017361411824822426
DDPG.check_train -> rl variance 0.012056989595293999
DDPG.check_train -> rl variance 0.010273361578583717
DDPG.check_train -> rl variance 0.01061222143471241
DDPG.check_train -> rl variance 0.010523681528866291
DDPG.check_train -> rl variance 0.010266546159982681
DDPG.check_train -> rl variance 0.009608166292309761
DDPG.check_train -> rl variance 0.01024449523538351
DDPG.check_train -> rl variance 0.010427073575556278
DDPG.check_train -> rl variance 0.010762102901935577
-----------------------------------
| epoch              | 0          |
| stats_g/mean       | 0.21293743 |
| stats_g/std        | 1.8564768  |
| stats_o/mean       | 0.12400037 |
| stats_o/std        | 1.0790029  |
| test/episode       | 20.0       |
| test/mean_Q        | -1.6238204 |
| test/success_rate  | 0.0        |
| train/episode      | 40.0       |
| train/success_rate | 0.0        |
-----------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_0.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.011635060422122478
DDPG.check_train -> rl variance 0.011557870544493198
DDPG.check_train -> rl variance 0.012333294376730919
DDPG.check_train -> rl variance 0.01241452619433403
DDPG.check_train -> rl variance 0.011185569688677788
DDPG.check_train -> rl variance 0.013849719427525997
DDPG.check_train -> rl variance 0.013714002445340157
DDPG.check_train -> rl variance 0.013689883053302765
DDPG.check_train -> rl variance 0.01307326927781105
DDPG.check_train -> rl variance 0.01397613063454628
------------------------------------
| epoch              | 1           |
| stats_g/mean       | 0.033029072 |
| stats_g/std        | 1.7384415   |
| stats_o/mean       | 0.021252163 |
| stats_o/std        | 1.0154167   |
| test/episode       | 40.0        |
| test/mean_Q        | -1.8424178  |
| test/success_rate  | 0.0         |
| train/episode      | 80.0        |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.01516939140856266
DDPG.check_train -> rl variance 0.01640552468597889
DDPG.check_train -> rl variance 0.01403016783297062
DDPG.check_train -> rl variance 0.015208493918180466
DDPG.check_train -> rl variance 0.015706010162830353
DDPG.check_train -> rl variance 0.01593145728111267
DDPG.check_train -> rl variance 0.015969280153512955
DDPG.check_train -> rl variance 0.015539661981165409
DDPG.check_train -> rl variance 0.014508629217743874
DDPG.check_train -> rl variance 0.015919221565127373
------------------------------------
| epoch              | 2           |
| stats_g/mean       | -0.08634312 |
| stats_g/std        | 1.689785    |
| stats_o/mean       | -0.05704259 |
| stats_o/std        | 0.99410164  |
| test/episode       | 60.0        |
| test/mean_Q        | -1.9380174  |
| test/success_rate  | 0.0         |
| train/episode      | 120.0       |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_2.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.018358539789915085
DDPG.check_train -> rl variance 0.017116906121373177
DDPG.check_train -> rl variance 0.01705688238143921
DDPG.check_train -> rl variance 0.015856537967920303
DDPG.check_train -> rl variance 0.015884429216384888
DDPG.check_train -> rl variance 0.014931324869394302
DDPG.check_train -> rl variance 0.017129715532064438
DDPG.check_train -> rl variance 0.017761968076229095
DDPG.check_train -> rl variance 0.017012979835271835
DDPG.check_train -> rl variance 0.01726282760500908
------------------------------------
| epoch              | 3           |
| stats_g/mean       | -0.07959752 |
| stats_g/std        | 1.6481055   |
| stats_o/mean       | -0.05553656 |
| stats_o/std        | 0.9756002   |
| test/episode       | 80.0        |
| test/mean_Q        | -2.9547303  |
| test/success_rate  | 0.0         |
| train/episode      | 160.0       |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.018699992448091507
DDPG.check_train -> rl variance 0.01696905866265297
DDPG.check_train -> rl variance 0.02105022221803665
DDPG.check_train -> rl variance 0.017957430332899094
DDPG.check_train -> rl variance 0.01752084493637085
DDPG.check_train -> rl variance 0.016798553988337517
DDPG.check_train -> rl variance 0.01907440274953842
DDPG.check_train -> rl variance 0.018829073756933212
DDPG.check_train -> rl variance 0.01890958473086357
DDPG.check_train -> rl variance 0.019654802978038788
------------------------------------
| epoch              | 4           |
| stats_g/mean       | -0.0752199  |
| stats_g/std        | 1.6532884   |
| stats_o/mean       | -0.05493439 |
| stats_o/std        | 0.9847207   |
| test/episode       | 100.0       |
| test/mean_Q        | -2.3848543  |
| test/success_rate  | 0.0         |
| train/episode      | 200.0       |
| train/success_rate | 0.0         |
------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_4.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.016691841185092926
DDPG.check_train -> rl variance 0.01866341568529606
DDPG.check_train -> rl variance 0.019206970930099487
DDPG.check_train -> rl variance 0.018999703228473663
DDPG.check_train -> rl variance 0.018730031326413155
DDPG.check_train -> rl variance 0.018982479348778725
DDPG.check_train -> rl variance 0.018677860498428345
DDPG.check_train -> rl variance 0.020797124132514
DDPG.check_train -> rl variance 0.020182378590106964
DDPG.check_train -> rl variance 0.019885599613189697
-------------------------------------
| epoch              | 5            |
| stats_g/mean       | -0.043077275 |
| stats_g/std        | 1.608971     |
| stats_o/mean       | -0.035737522 |
| stats_o/std        | 0.96420145   |
| test/episode       | 120.0        |
| test/mean_Q        | -2.064723    |
| test/success_rate  | 0.0          |
| train/episode      | 240.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.018938705325126648
DDPG.check_train -> rl variance 0.018693245947360992
DDPG.check_train -> rl variance 0.019492829218506813
DDPG.check_train -> rl variance 0.019263822585344315
DDPG.check_train -> rl variance 0.017202023416757584
DDPG.check_train -> rl variance 0.02013176679611206
DDPG.check_train -> rl variance 0.018737539649009705
DDPG.check_train -> rl variance 0.019187644124031067
DDPG.check_train -> rl variance 0.02184232696890831
DDPG.check_train -> rl variance 0.018395595252513885
-------------------------------------
| epoch              | 6            |
| stats_g/mean       | -0.016160496 |
| stats_g/std        | 1.5563068    |
| stats_o/mean       | -0.019170322 |
| stats_o/std        | 0.9399188    |
| test/episode       | 140.0        |
| test/mean_Q        | -1.2792672   |
| test/success_rate  | 0.0          |
| train/episode      | 280.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_6.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.017413266003131866
DDPG.check_train -> rl variance 0.020182659849524498
DDPG.check_train -> rl variance 0.02132599614560604
DDPG.check_train -> rl variance 0.020447194576263428
DDPG.check_train -> rl variance 0.021373650059103966
DDPG.check_train -> rl variance 0.021776646375656128
DDPG.check_train -> rl variance 0.01996660605072975
DDPG.check_train -> rl variance 0.02027186192572117
DDPG.check_train -> rl variance 0.019656259566545486
DDPG.check_train -> rl variance 0.02138342708349228
-------------------------------------
| epoch              | 7            |
| stats_g/mean       | -0.027666215 |
| stats_g/std        | 1.5024886    |
| stats_o/mean       | -0.022737077 |
| stats_o/std        | 0.91801167   |
| test/episode       | 160.0        |
| test/mean_Q        | -1.0919689   |
| test/success_rate  | 0.0          |
| train/episode      | 320.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.022056763991713524
DDPG.check_train -> rl variance 0.022542160004377365
DDPG.check_train -> rl variance 0.02294629067182541
DDPG.check_train -> rl variance 0.020025178790092468
DDPG.check_train -> rl variance 0.021125037223100662
DDPG.check_train -> rl variance 0.023324348032474518
DDPG.check_train -> rl variance 0.022163324058055878
DDPG.check_train -> rl variance 0.01952865533530712
DDPG.check_train -> rl variance 0.020072976127266884
DDPG.check_train -> rl variance 0.02120935171842575
-------------------------------------
| epoch              | 8            |
| stats_g/mean       | -0.029790971 |
| stats_g/std        | 1.4563951    |
| stats_o/mean       | -0.02182881  |
| stats_o/std        | 0.90134376   |
| test/episode       | 180.0        |
| test/mean_Q        | -0.3051221   |
| test/success_rate  | 0.0          |
| train/episode      | 360.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_8.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.019692178815603256
DDPG.check_train -> rl variance 0.01980471983551979
DDPG.check_train -> rl variance 0.02045167237520218
DDPG.check_train -> rl variance 0.02129664644598961
DDPG.check_train -> rl variance 0.022294174879789352
DDPG.check_train -> rl variance 0.02625284530222416
DDPG.check_train -> rl variance 0.023741144686937332
DDPG.check_train -> rl variance 0.01993752084672451
DDPG.check_train -> rl variance 0.021608080714941025
DDPG.check_train -> rl variance 0.023341000080108643
-------------------------------------
| epoch              | 9            |
| stats_g/mean       | -0.027503863 |
| stats_g/std        | 1.4056001    |
| stats_o/mean       | -0.019774131 |
| stats_o/std        | 0.88112426   |
| test/episode       | 200.0        |
| test/mean_Q        | -0.084368095 |
| test/success_rate  | 0.0          |
| train/episode      | 400.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.024224981665611267
DDPG.check_train -> rl variance 0.02251441217958927
DDPG.check_train -> rl variance 0.019652266055345535
DDPG.check_train -> rl variance 0.0183853879570961
DDPG.check_train -> rl variance 0.022369328886270523
DDPG.check_train -> rl variance 0.023957157507538795
DDPG.check_train -> rl variance 0.022021297365427017
DDPG.check_train -> rl variance 0.021330490708351135
DDPG.check_train -> rl variance 0.02338651567697525
DDPG.check_train -> rl variance 0.02240118756890297
-------------------------------------
| epoch              | 10           |
| stats_g/mean       | -0.028314989 |
| stats_g/std        | 1.3599721    |
| stats_o/mean       | -0.020854395 |
| stats_o/std        | 0.86358035   |
| test/episode       | 220.0        |
| test/mean_Q        | 0.3183653    |
| test/success_rate  | 0.0          |
| train/episode      | 440.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_10.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.020725052803754807
DDPG.check_train -> rl variance 0.023279957473278046
DDPG.check_train -> rl variance 0.020575866103172302
DDPG.check_train -> rl variance 0.022930074483156204
DDPG.check_train -> rl variance 0.02468975819647312
DDPG.check_train -> rl variance 0.022010738030076027
DDPG.check_train -> rl variance 0.02357608452439308
DDPG.check_train -> rl variance 0.022632736712694168
DDPG.check_train -> rl variance 0.021847162395715714
DDPG.check_train -> rl variance 0.02996952086687088
-------------------------------------
| epoch              | 11           |
| stats_g/mean       | -0.029890208 |
| stats_g/std        | 1.3200839    |
| stats_o/mean       | -0.021828499 |
| stats_o/std        | 0.8486172    |
| test/episode       | 240.0        |
| test/mean_Q        | 0.49163952   |
| test/success_rate  | 0.2          |
| train/episode      | 480.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.2.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.02548126131296158
DDPG.check_train -> rl variance 0.02258436568081379
DDPG.check_train -> rl variance 0.020760703831911087
DDPG.check_train -> rl variance 0.023668920621275902
DDPG.check_train -> rl variance 0.019402360543608665
DDPG.check_train -> rl variance 0.028711270540952682
DDPG.check_train -> rl variance 0.023905832320451736
DDPG.check_train -> rl variance 0.024290505796670914
DDPG.check_train -> rl variance 0.022361718118190765
DDPG.check_train -> rl variance 0.022823650389909744
-------------------------------------
| epoch              | 12           |
| stats_g/mean       | -0.042396743 |
| stats_g/std        | 1.2870232    |
| stats_o/mean       | -0.028226374 |
| stats_o/std        | 0.83418715   |
| test/episode       | 260.0        |
| test/mean_Q        | 1.1402999    |
| test/success_rate  | 0.3          |
| train/episode      | 520.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.3.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_12.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.026499615982174873
DDPG.check_train -> rl variance 0.023966535925865173
DDPG.check_train -> rl variance 0.02373853698372841
DDPG.check_train -> rl variance 0.024874286726117134
DDPG.check_train -> rl variance 0.02079848200082779
DDPG.check_train -> rl variance 0.030143385753035545
DDPG.check_train -> rl variance 0.024348001927137375
DDPG.check_train -> rl variance 0.026108214631676674
DDPG.check_train -> rl variance 0.020465563982725143
DDPG.check_train -> rl variance 0.02209765650331974
-------------------------------------
| epoch              | 13           |
| stats_g/mean       | -0.033982124 |
| stats_g/std        | 1.2555494    |
| stats_o/mean       | -0.024468074 |
| stats_o/std        | 0.820546     |
| test/episode       | 280.0        |
| test/mean_Q        | 1.5961833    |
| test/success_rate  | 0.15         |
| train/episode      | 560.0        |
| train/success_rate | 0.0          |
-------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.021665047854185104
DDPG.check_train -> rl variance 0.0247346144169569
DDPG.check_train -> rl variance 0.023529497906565666
DDPG.check_train -> rl variance 0.023789525032043457
DDPG.check_train -> rl variance 0.021052949130535126
DDPG.check_train -> rl variance 0.022686287760734558
DDPG.check_train -> rl variance 0.021067090332508087
DDPG.check_train -> rl variance 0.023473814129829407
DDPG.check_train -> rl variance 0.020410848781466484
DDPG.check_train -> rl variance 0.026838775724172592
-------------------------------------
| epoch              | 14           |
| stats_g/mean       | -0.039809752 |
| stats_g/std        | 1.2259189    |
| stats_o/mean       | -0.027050963 |
| stats_o/std        | 0.80737454   |
| test/episode       | 300.0        |
| test/mean_Q        | 1.7278131    |
| test/success_rate  | 0.5          |
| train/episode      | 600.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.5.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_14.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.025367628782987595
DDPG.check_train -> rl variance 0.023946862667798996
DDPG.check_train -> rl variance 0.02224637195467949
DDPG.check_train -> rl variance 0.02257663942873478
DDPG.check_train -> rl variance 0.02566835656762123
DDPG.check_train -> rl variance 0.020731570199131966
DDPG.check_train -> rl variance 0.0248686783015728
DDPG.check_train -> rl variance 0.021818460896611214
DDPG.check_train -> rl variance 0.022838950157165527
DDPG.check_train -> rl variance 0.021370649337768555
-------------------------------------
| epoch              | 15           |
| stats_g/mean       | -0.033406388 |
| stats_g/std        | 1.1992607    |
| stats_o/mean       | -0.022359245 |
| stats_o/std        | 0.79565763   |
| test/episode       | 320.0        |
| test/mean_Q        | 1.7420453    |
| test/success_rate  | 0.4          |
| train/episode      | 640.0        |
| train/success_rate | 0.0          |
-------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.021509628742933273
DDPG.check_train -> rl variance 0.022246897220611572
DDPG.check_train -> rl variance 0.022777393460273743
DDPG.check_train -> rl variance 0.02258376032114029
DDPG.check_train -> rl variance 0.021735437214374542
DDPG.check_train -> rl variance 0.02216159552335739
DDPG.check_train -> rl variance 0.024432653561234474
DDPG.check_train -> rl variance 0.023975171148777008
DDPG.check_train -> rl variance 0.020255599170923233
DDPG.check_train -> rl variance 0.027187000960111618
-------------------------------------
| epoch              | 16           |
| stats_g/mean       | -0.027502839 |
| stats_g/std        | 1.1762342    |
| stats_o/mean       | -0.019756105 |
| stats_o/std        | 0.7867896    |
| test/episode       | 340.0        |
| test/mean_Q        | 2.1065373    |
| test/success_rate  | 0.1          |
| train/episode      | 680.0        |
| train/success_rate | 0.0          |
-------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_16.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.024044325575232506
DDPG.check_train -> rl variance 0.02423289604485035
DDPG.check_train -> rl variance 0.024974847212433815
DDPG.check_train -> rl variance 0.01946062222123146
DDPG.check_train -> rl variance 0.025033848360180855
DDPG.check_train -> rl variance 0.019260499626398087
DDPG.check_train -> rl variance 0.02036265842616558
DDPG.check_train -> rl variance 0.025988047942519188
DDPG.check_train -> rl variance 0.021310904994606972
DDPG.check_train -> rl variance 0.02326262928545475
-------------------------------------
| epoch              | 17           |
| stats_g/mean       | -0.023643449 |
| stats_g/std        | 1.1535192    |
| stats_o/mean       | -0.017023414 |
| stats_o/std        | 0.77665603   |
| test/episode       | 360.0        |
| test/mean_Q        | 2.2151108    |
| test/success_rate  | 0.55         |
| train/episode      | 720.0        |
| train/success_rate | 0.05         |
-------------------------------------
New best success rate: 0.55.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.02458549290895462
DDPG.check_train -> rl variance 0.02092810347676277
DDPG.check_train -> rl variance 0.022494196891784668
DDPG.check_train -> rl variance 0.022557104006409645
DDPG.check_train -> rl variance 0.02303893491625786
DDPG.check_train -> rl variance 0.02298588678240776
DDPG.check_train -> rl variance 0.02105308324098587
DDPG.check_train -> rl variance 0.023089081048965454
DDPG.check_train -> rl variance 0.02068381756544113
DDPG.check_train -> rl variance 0.02487729862332344
-------------------------------------
| epoch              | 18           |
| stats_g/mean       | -0.017161904 |
| stats_g/std        | 1.132158     |
| stats_o/mean       | -0.013201067 |
| stats_o/std        | 0.7674514    |
| test/episode       | 380.0        |
| test/mean_Q        | 2.6217215    |
| test/success_rate  | 0.5          |
| train/episode      | 760.0        |
| train/success_rate | 0.0          |
-------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_18.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.023009218275547028
DDPG.check_train -> rl variance 0.02449660934507847
DDPG.check_train -> rl variance 0.023780424147844315
DDPG.check_train -> rl variance 0.023797813802957535
DDPG.check_train -> rl variance 0.02353629283607006
DDPG.check_train -> rl variance 0.021805137395858765
DDPG.check_train -> rl variance 0.024361897259950638
DDPG.check_train -> rl variance 0.019756052643060684
DDPG.check_train -> rl variance 0.020705780014395714
DDPG.check_train -> rl variance 0.021776681765913963
-------------------------------------
| epoch              | 19           |
| stats_g/mean       | -0.014569582 |
| stats_g/std        | 1.1120868    |
| stats_o/mean       | -0.011164899 |
| stats_o/std        | 0.75875366   |
| test/episode       | 400.0        |
| test/mean_Q        | 3.178521     |
| test/success_rate  | 0.7          |
| train/episode      | 800.0        |
| train/success_rate | 0.025        |
-------------------------------------
New best success rate: 0.7.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.0250774547457695
DDPG.check_train -> rl variance 0.020510246977210045
DDPG.check_train -> rl variance 0.02385595440864563
DDPG.check_train -> rl variance 0.026136059314012527
DDPG.check_train -> rl variance 0.02528820000588894
DDPG.check_train -> rl variance 0.026998288929462433
DDPG.check_train -> rl variance 0.024035459384322166
DDPG.check_train -> rl variance 0.020413178950548172
DDPG.check_train -> rl variance 0.024716762825846672
DDPG.check_train -> rl variance 0.021126868203282356
-------------------------------------
| epoch              | 20           |
| stats_g/mean       | -0.017437475 |
| stats_g/std        | 1.0940261    |
| stats_o/mean       | -0.012463465 |
| stats_o/std        | 0.750157     |
| test/episode       | 420.0        |
| test/mean_Q        | 2.7713575    |
| test/success_rate  | 0.65         |
| train/episode      | 840.0        |
| train/success_rate | 0.025        |
-------------------------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_20.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.021981237456202507
DDPG.check_train -> rl variance 0.023015260696411133
DDPG.check_train -> rl variance 0.027364380657672882
DDPG.check_train -> rl variance 0.026690607890486717
DDPG.check_train -> rl variance 0.021986622363328934
DDPG.check_train -> rl variance 0.023535430431365967
DDPG.check_train -> rl variance 0.02742817997932434
DDPG.check_train -> rl variance 0.026352442800998688
DDPG.check_train -> rl variance 0.02381269261240959
DDPG.check_train -> rl variance 0.026537582278251648
-------------------------------------
| epoch              | 21           |
| stats_g/mean       | -0.014755828 |
| stats_g/std        | 1.0780929    |
| stats_o/mean       | -0.010473033 |
| stats_o/std        | 0.74210227   |
| test/episode       | 440.0        |
| test/mean_Q        | 3.3884327    |
| test/success_rate  | 0.8          |
| train/episode      | 880.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.8.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.021422099322080612
DDPG.check_train -> rl variance 0.026587991043925285
DDPG.check_train -> rl variance 0.021955091506242752
DDPG.check_train -> rl variance 0.02342822402715683
DDPG.check_train -> rl variance 0.02253374457359314
DDPG.check_train -> rl variance 0.027448220178484917
DDPG.check_train -> rl variance 0.025749117136001587
DDPG.check_train -> rl variance 0.022830350324511528
DDPG.check_train -> rl variance 0.022555211558938026
DDPG.check_train -> rl variance 0.020930413156747818
-------------------------------------
| epoch              | 22           |
| stats_g/mean       | -0.01291485  |
| stats_g/std        | 1.0638134    |
| stats_o/mean       | -0.010042282 |
| stats_o/std        | 0.73585826   |
| test/episode       | 460.0        |
| test/mean_Q        | 3.1986852    |
| test/success_rate  | 0.95         |
| train/episode      | 920.0        |
| train/success_rate | 0.0          |
-------------------------------------
New best success rate: 0.95.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_22.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.026940811425447464
DDPG.check_train -> rl variance 0.024896971881389618
DDPG.check_train -> rl variance 0.02761133760213852
DDPG.check_train -> rl variance 0.0258585624396801
DDPG.check_train -> rl variance 0.02081115171313286
DDPG.check_train -> rl variance 0.03102395310997963
DDPG.check_train -> rl variance 0.020492926239967346
DDPG.check_train -> rl variance 0.025298185646533966
DDPG.check_train -> rl variance 0.031502388417720795
DDPG.check_train -> rl variance 0.023777514696121216
-------------------------------------
| epoch              | 23           |
| stats_g/mean       | -0.011604661 |
| stats_g/std        | 1.0508078    |
| stats_o/mean       | -0.009303465 |
| stats_o/std        | 0.7298912    |
| test/episode       | 480.0        |
| test/mean_Q        | 3.3193269    |
| test/success_rate  | 0.8          |
| train/episode      | 960.0        |
| train/success_rate | 0.025        |
-------------------------------------
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
DDPG.check_train -> rl variance 0.024667512625455856
DDPG.check_train -> rl variance 0.027354415506124496
DDPG.check_train -> rl variance 0.02638411894440651
DDPG.check_train -> rl variance 0.02410729229450226
DDPG.check_train -> rl variance 0.030516566708683968
DDPG.check_train -> rl variance 0.026013048365712166
DDPG.check_train -> rl variance 0.025060860440135002
DDPG.check_train -> rl variance 0.025943180546164513
DDPG.check_train -> rl variance 0.026397589594125748
DDPG.check_train -> rl variance 0.02187822386622429
--------------------------------------
| epoch              | 24            |
| stats_g/mean       | -0.0069546653 |
| stats_g/std        | 1.0370276     |
| stats_o/mean       | -0.0071101915 |
| stats_o/std        | 0.7241783     |
| test/episode       | 500.0         |
| test/mean_Q        | 3.8460298     |
| test/success_rate  | 1.0           |
| train/episode      | 1000.0        |
| train/success_rate | 0.05          |
--------------------------------------
New best success rate: 1.0.
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_best.pkl.
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_24.pkl.
Saving latest policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/RLNoDemo//rl/policy_latest.pkl.
