Logging to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly/
Launching the training process with 1 cpu core(s).
Setting log level to 2.
Using environment Reach2DFirstOrder with r scale down by 1.000000 shift by 0.000000 and max episode 0.000000
Training gaussian process to produce expected Q value.
*** gp_q_estimator_params ***
T: 24
batch_size: 32
buffer_size: 1000000
data_size: 512
hidden: 256
input_dims: {'o': 2, 'u': 2, 'g': 2, 'info_is_success': 1}
layers: 3
lr: 0.001
max_u: 2
net_type: yw.ddpg_main.demo_policy:EnsembleNN
num_sample: 12
scope: demo
test_batch_size: 100
*** gp_q_estimator_params ***
Creating a Gaussian Process.
Adding data to the training set.
Adding data to test set.
GPImitator.check -> average var -0.4184703246043205
--------------------
| epoch | 0        |
| loss  | -631     |
--------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_0.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.4185646133460855
--------------------
| epoch | 50       |
| loss  | -739     |
--------------------
GPImitator.check -> average var -0.4185853971486579
--------------------
| epoch | 100      |
| loss  | -840     |
--------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_100.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.41859399526302954
--------------------
| epoch | 150      |
| loss  | -938     |
--------------------
GPImitator.check -> average var -0.4185994583889085
---------------------
| epoch | 200       |
| loss  | -1.03e+03 |
---------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_200.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.4186030200836095
---------------------
| epoch | 250       |
| loss  | -1.13e+03 |
---------------------
GPImitator.check -> average var -0.4186052913036636
---------------------
| epoch | 300       |
| loss  | -1.22e+03 |
---------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_300.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.4186067190254148
---------------------
| epoch | 350       |
| loss  | -1.31e+03 |
---------------------
GPImitator.check -> average var -0.4186077013841472
---------------------
| epoch | 400       |
| loss  | -1.39e+03 |
---------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_400.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.418608443215964
---------------------
| epoch | 450       |
| loss  | -1.48e+03 |
---------------------
GPImitator.check -> average var -0.41860897484651244
---------------------
| epoch | 500       |
| loss  | -1.55e+03 |
---------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_500.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.41860932568965764
---------------------
| epoch | 550       |
| loss  | -1.63e+03 |
---------------------
GPImitator.check -> average var -0.4186095439947013
--------------------
| epoch | 600      |
| loss  | -1.7e+03 |
--------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_600.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.41860967664981086
---------------------
| epoch | 650       |
| loss  | -1.77e+03 |
---------------------
GPImitator.check -> average var -0.41860976399065536
---------------------
| epoch | 700       |
| loss  | -1.84e+03 |
---------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_700.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.4186098216702884
--------------------
| epoch | 750      |
| loss  | -1.9e+03 |
--------------------
GPImitator.check -> average var -0.418609859756526
---------------------
| epoch | 800       |
| loss  | -1.95e+03 |
---------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_800.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.41860988405318234
---------------------
| epoch | 850       |
| loss  | -1.99e+03 |
---------------------
GPImitator.check -> average var -0.41860989975810703
---------------------
| epoch | 900       |
| loss  | -2.03e+03 |
---------------------
Saving periodic policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_900.pkl ...
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
GPImitator.check -> average var -0.4186099099250916
---------------------
| epoch | 950       |
| loss  | -2.06e+03 |
---------------------
Saving policy to /home/yuchen/Desktop/RLProject/Experiment/Result/Temp/DemoCriticOnly//rl_demo_critic/policy_latest.pkl
Skip RL agent training.
