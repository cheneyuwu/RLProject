Logging to /home/yuchenwu/projects/def-florian7/yuchenwu/RLProject/Experiment/Result/Temp/Exp0/RLNoDemo/
Launching the training process with 1 cpu core(s).
Setting log level to 2.
Using environment SawyerLift with r scale down by 1.000000 shift by 0.000000 and max episode 0.000000
*** nstep_params ***
gamma                         0.9921875
n                             50
*** nstep_params ***
*** ddpg_params ***
Q_lr                          0.001
T                             128
action_l2                     1.0
aux_loss_weight               0.0078
batch_size                    256
batch_size_demo               128
buffer_size                   1000000
ca_ratio                      1
clip_obs                      200.0
clip_pos_returns              False
clip_return                   inf
demo_actor                    none
demo_critic                   none
demo_replay_strategy          {'strategy': 'none', 'args': {'n': 50, 'gamma': 0.9921875}}
gamma                         0.9921875
hidden                        256
info                          {'env_name': 'SawyerLift', 'r_scale': 1.0, 'r_shift': 0.0, 'eps_length': 0, 'env_args': {'has_renderer': False, 'has_offscreen_renderer': False, 'use_object_obs': True, 'use_camera_obs': False, 'reward_shaping': True, 'control_freq': 50}}
input_dims                    {'o': 40, 'u': 8, 'g': 0, 'info_is_success': 1}
layers                        3
max_u                         1.0
norm_clip                     5
norm_eps                      0.01
num_demo                      1000
num_sample                    1
pi_lr                         0.001
polyak                        0.95
prm_loss_weight               0.001
q_filter                      0
replay_strategy               {'strategy': 'none', 'args': {}}
rollout_batch_size            4
scope                         rl_only
*** ddpg_params ***
Configuring the replay buffer.
Creating a DDPG agent with action space 8 x 1.0.

*** rollout_params ***
T                             128
compute_Q                     False
dims                          {'o': 40, 'u': 8, 'g': 0, 'info_is_success': 1}
exploit                       0
noise_eps                     0.2
random_eps                    0.3
rollout_batch_size            4
use_demo_states               True
use_target_net                False
*** rollout_params ***
*** eval_params ***
T                             128
compute_Q                     True
dims                          {'o': 40, 'u': 8, 'g': 0, 'info_is_success': 1}
exploit                       1
noise_eps                     0.2
random_eps                    0.3
rollout_batch_size            5
use_demo_states               False
use_target_net                False
*** eval_params ***
Training the RL agent with n_epochs: 50, n_cycles: 20, n_batches: 40.
-------------------------------------
| epoch              | 0            |
| stats_g/mean       | [nan]        |
| stats_g/std        | [nan]        |
| stats_o/mean       | [0.21668062] |
| stats_o/std        | [0.18362741] |
| test/episode       | [5]          |
| test/mean_Q        | [0.5189086]  |
| test/success_rate  | [0.0]        |
| train/episode      | [80]         |
| train/success_rate | [0.0]        |
-------------------------------------
